Artificial Intelligence
A Modern Approach
Third Edition
PRENTICEHALLSERIES
INARTIFICIALINTELLIGENCE
StuartRussellandPeterNorvig,Editors
FORSYTH & PONCE ComputerVision: AModernApproach
GRAHAM ANSI CommonLisp
JURAFSKY & MARTIN SpeechandLanguageProcessing, 2nded.
NEAPOLITAN LearningBayesianNetworks
RUSSELL & NORVIG ArtificialIntelligence: AModernApproach,3rded.
Artificial Intelligence
A Modern Approach
Third Edition
Stuart J. Russell and Peter Norvig
Contributingwriters:
ErnestDavis
DouglasD. Edwards
David Forsyth
NicholasJ. Hay
JitendraM.Malik
VibhuMittal
Mehran Sahami
Sebastian Thrun
UpperSaddleRiver Boston Columbus SanFrancisco NewYork
Indianapolis London Toronto Sydney Singapore Tokyo Montreal
Dubai Madrid HongKong MexicoCity Munich Paris Amsterdam CapeTown
VicePresidentandEditorialDirector,ECS:MarciaJ.Horton
Editor-in-Chief:MichaelHirsch
ExecutiveEditor:TracyDunkelberger
AssistantEditor:MelindaHaggerty
EditorialAssistant: AllisonMichael
VicePresident,Production:VinceO’Brien
SeniorManagingEditor:ScottDisanno
ProductionEditor:JaneBonnell
SeniorOperationsSupervisor:AlanFischer
OperationsSpecialist: LisaMcDowell
MarketingManager:ErinDavis
MarketingAssistant: MackPatterson
CoverDesigners:KirstenSimsandGeoffreyCassar
CoverImages:StanHonda/Getty,LibraryofCongress,NASA,NationalMuseumofRome,
PeterNorvig,IanParker,Shutterstock,TimeLife/Getty
InteriorDesigners:StuartRussellandPeterNorvig
CopyEditor:MaryLouNohr
ArtEditor:GregDulles
MediaEditor:DanielSandin
MediaProjectManager:DanielleLeone
Copyright(cid:2)c 2010,2003,1995byPearsonEducation,Inc.,
UpperSaddleRiver,NewJersey07458.
Allrightsreserved. ManufacturedintheUnitedStatesofAmerica. Thispublicationisprotectedby
Copyrightandpermissionsshouldbeobtainedfromthepublisherpriortoanyprohibitedreproduction,
storage in a retrievalsystem, or transmission in any form or by any means, electronic, mechanical,
photocopying,recording,orlikewise. Toobtainpermission(s)tousematerialsfromthiswork,please
submitawrittenrequesttoPearsonHigherEducation,PermissionsDepartment,1LakeStreet,Upper
SaddleRiver,NJ07458.
The author and publisher of this book have used their best efforts in preparing this book. These
effortsincludethedevelopment,research,andtestingofthetheoriesandprogramstodeterminetheir
effectiveness. The author and publisher make no warranty of any kind, expressed or implied, with
regardtotheseprogramsorthedocumentationcontainedinthisbook. Theauthorandpublishershall
notbeliablein anyeventforincidentalorconsequentialdamagesin connectionwith, orarisingout
of,thefurnishing,performance,oruseoftheseprograms.
LibraryofCongressCataloging-in-PublicationDataonFile
10 9 8 7 6 5 4 3 2 1
ISBN-13: 978-0-13-604259-4
ISBN-10: 0-13-604259-7
ForLoy, Gordon,Lucy, George, andIsaac —S.J.R.
ForKris,Isabella,and Juliet— P.N.
This page intentionally left blank
Preface
ArtificialIntelligence (AI)isabigfield,andthisisabigbook. Wehavetriedtoexplorethe
fullbreadth ofthefield, whichencompasses logic, probability, andcontinuous mathematics;
perception, reasoning, learning, and action; and everything from microelectronic devices to
roboticplanetary explorers. Thebookisalsobigbecause we gointosomedepth.
Thesubtitleofthisbookis“AModernApproach.” Theintendedmeaningofthisrather
empty phrase is that we have tried to synthesize what is now known into a common frame-
work, rather than trying to explain each subfield of AI in its own historical context. We
apologize tothosewhosesubfieldsare,asaresult, lessrecognizable.
New to this edition
This edition captures the changes in AI that have taken place since the last edition in 2003.
There have been important applications of AI technology, such as the widespread deploy-
ment of practical speech recognition, machine translation, autonomous vehicles, and house-
hold robotics. There have been algorithmic landmarks, such as the solution of the game of
checkers. And there has been a great deal of theoretical progress, particularly in areas such
asprobabilistic reasoning, machine learning, andcomputervision. Mostimportant from our
point of view is the continued evolution in how we think about the field, and thus how we
organize thebook. Themajorchanges areasfollows:
• We place more emphasis on partially observable and nondeterministic environments,
especially in the nonprobabilistic settings of search and planning. The concepts of
beliefstate(asetofpossible worlds)andstateestimation (maintaining thebeliefstate)
areintroduced inthesesettings; laterinthebook,weaddprobabilities.
• Inaddition to discussing the types of environments and types of agents, wenow cover
inmoredepththetypesofrepresentations thatanagentcanuse. Wedistinguishamong
atomic representations (in which each state of the world is treated as a black box),
factoredrepresentations(inwhichastateisasetofattribute/valuepairs),andstructured
representations (inwhichtheworldconsists ofobjectsand relations betweenthem).
• Our coverage of planning goes into more depth on contingent planning in partially
observable environments andincludes anewapproachtohierarchical planning.
• Wehaveaddednewmaterialonfirst-orderprobabilisticmodels,includingopen-universe
modelsforcaseswherethereisuncertainty astowhatobjectsexist.
• We have completely rewritten the introductory machine-learning chapter, stressing a
wider variety of more modern learning algorithms and placing them on a firmer theo-
reticalfooting.
• We have expanded coverage of Web search and information extraction, and of tech-
niquesforlearning fromverylargedatasets.
• 20%ofthecitations inthiseditionaretoworkspublished after2003.
• Weestimate that about 20% of the material is brand new. The remaining 80% reflects
olderworkbuthasbeenlargely rewrittentopresentamoreunifiedpictureofthefield.
vii
viii Preface
Overview of the book
The main unifying theme is the idea of an intelligent agent. We define AI as the study of
agents thatreceive percepts from theenvironment andperform actions. Eachsuchagent im-
plements a function that maps percept sequences to actions, and we cover different ways to
represent these functions, such as reactive agents, real-time planners, and decision-theoretic
systems. Weexplaintheroleoflearningasextendingthereachofthedesignerintounknown
environments, and we show how that role constrains agent design, favoring explicit knowl-
edgerepresentation andreasoning. Wetreatroboticsandvisionnotasindependently defined
problems, butasoccurring intheservice ofachieving goals. Westress theimportance ofthe
taskenvironment indetermining theappropriate agentdesign.
OurprimaryaimistoconveytheideasthathaveemergedoverthepastfiftyyearsofAI
researchandthepasttwomillenniaofrelatedwork. Wehavetriedtoavoidexcessiveformal-
ityinthepresentationoftheseideaswhileretainingprecision. Wehaveincludedpseudocode
algorithms tomakethekeyideasconcrete; ourpseudocode is described inAppendixB.
Thisbookisprimarilyintendedforuseinanundergraduate courseorcoursesequence.
The book has 27 chapters, each requiring about a week’s worth of lectures, so working
through the whole book requires a two-semester sequence. A one-semester course can use
selected chapters to suit the interests of the instructor and students. The book can also be
used in a graduate-level course (perhaps with the addition of some of the primary sources
suggested in the bibliographical notes). Sample syllabi are available at the book’s Web site,
aima.cs.berkeley.edu. The only prerequisite is familiarity with basic concepts of
computer science (algorithms, data structures, complexity) at a sophomore level. Freshman
calculusandlinearalgebraareusefulforsomeofthetopics;therequiredmathematicalback-
groundissuppliedinAppendixA.
Exercises are given at the end of each chapter. Exercises requiring significant pro-
gramming are marked with a keyboard icon. These exercises can best be solved by taking
advantage of the code repository at aima.cs.berkeley.edu. Some of them are large
enough to be considered term projects. A numberof exercises require some investigation of
theliterature; thesearemarkedwitha bookicon.
Throughout the book, important points are marked with a pointing icon. We have in-
cluded an extensive index of around 6,000 items to make it easy to find things in the book.
Wherevera newtermisfirstdefined,itisalsomarkedinthemargin.
NEWTERM
About the Web site
aima.cs.berkeley.edu,theWebsiteforthebook,contains
• implementations ofthealgorithmsinthebookinseveralprogramming languages,
• a list of over 1000 schools that have used the book, many with links to online course
materialsandsyllabi,
• anannotated listofover800linkstositesaroundtheWebwithusefulAIcontent,
• achapter-by-chapter listofsupplementary materialandlinks,
• instructions onhowtojoinadiscussion groupforthebook,
Preface ix
• instructions onhowtocontacttheauthorswithquestions orcomments,
• instructions onhowtoreporterrorsinthebook, inthelikelyeventthatsomeexist,and
• slidesandothermaterialsforinstructors.
About the cover
The cover depicts the final position from the decisive game 6 of the 1997 match between
chess champion Garry Kasparov and program DEEP BLUE. Kasparov, playing Black, was
forced to resign, making this the first time a computer had beaten a world champion in a
chess match. Kasparov is shown at the top. To his left is the Asimo humanoid robot and
to his right is Thomas Bayes (1702–1761), whose ideas about probability as a measure of
beliefunderliemuchofmodernAItechnology. BelowthatweseeaMarsExplorationRover,
a robot that landed on Mars in 2004 and has been exploring the planet ever since. To the
right is Alan Turing (1912–1954), whose fundamental work defined the fields of computer
science in general and artificial intelligence in particular. At the bottom is Shakey (1966–
1972), the first robot to combine perception, world-modeling, planning, and learning. With
Shakey is project leader Charles Rosen (1917–2002). At the bottom right is Aristotle (384
B.C.–322 B.C.),whopioneered thestudyoflogic;hisworkwasstateofthe artuntilthe19th
century(copyofabustbyLysippos). Atthebottomleft,lightlyscreenedbehindtheauthors’
names, isa planning algorithm byAristotle from DeMotu Animaliumin the original Greek.
Behind the title is a portion of the CPSC Bayesian network for medical diagnosis (Pradhan
etal., 1994). Behind thechess board ispart ofaBayesian logic model fordetecting nuclear
explosions fromseismicsignals.
Credits: Stan Honda/Getty (Kasparaov), Library of Congress (Bayes), NASA (Mars
rover), National Museum of Rome (Aristotle), Peter Norvig (book), Ian Parker (Berkeley
skyline), Shutterstock (Asimo,Chesspieces), TimeLife/Getty(Shakey,Turing).
Acknowledgments
Thisbookwouldnothavebeenpossible withoutthemanycontributors whosenamesdidnot
make it to the cover. Jitendra Malik and David Forsyth wrote Chapter 24 (computer vision)
and Sebastian Thrun wrote Chapter 25 (robotics). Vibhu Mittal wrote part of Chapter 22
(naturallanguage). NickHay,MehranSahami,andErnestDaviswrotesomeoftheexercises.
Zoran Duric (George Mason), Thomas C. Henderson (Utah), Leon Reznik (RIT), Michael
Gourley (Central Oklahoma) and Ernest Davis (NYU) reviewed the manuscript and made
helpfulsuggestions. WethankErnieDavisinparticularforhistirelessabilitytoreadmultiple
drafts and help improve the book. Nick Hay whipped the bibliography into shape and on
deadline stayed up to 5:30 AM writing code to make the book better. Jon Barron formatted
and improved the diagrams in this edition, while Tim Huang, Mark Paskin, and Cynthia
Bruyns helped with diagrams and algorithms in previous editions. Ravi Mohan and Ciaran
O’Reilly wrote and maintain the Java code examples on the Web site. John Canny wrote
theroboticschapterforthefirsteditionandDouglasEdwardsresearchedthehistoricalnotes.
TracyDunkelberger, AllisonMichael,ScottDisanno, andJaneBonnellatPearsontriedtheir
best to keep us on schedule and made many helpful suggestions. Most helpful of all has
x Preface
beenJulieSussman,P.P.A.,whoreadeverychapterandprovidedextensiveimprovements. In
previous editionswehadproofreaders whowouldtelluswhen weleftoutacommaandsaid
whichwhenwemeantthat; Julietold uswhenweleftoutaminussignandsaidx whenwe
i
meantx . Foreverytypoorconfusing explanation thatremainsinthe book,restassuredthat
j
Julie hasfixedatleast five. Shepersevered evenwhenapowerfailure forced hertoworkby
lanternlightratherthanLCDglow.
Stuart would like to thank his parents for their support and encouragement and his
wife, Loy Sheflott, for her endless patience and boundless wisdom. He hopes that Gordon,
Lucy, George, and Isaac will soon be reading this book after they have forgiven him for
working so long on it. RUGS (Russell’s Unusual Group of Students) have been unusually
helpful, asalways.
Peter would like to thank his parents (Torsten and Gerda) for getting him started,
and his wife (Kris), children (Bella and Juliet), colleagues, and friends for encouraging and
tolerating himthrough thelonghoursofwritingandlongerhoursofrewriting.
We both thank the librarians at Berkeley, Stanford, and NASA and the developers of
CiteSeer,Wikipedia,andGoogle,whohaverevolutionized thewaywedoresearch. Wecan’t
acknowledgeallthepeoplewhohaveusedthebookandmadesuggestions,butwewouldlike
tonote theespecially helpful comments ofGaganAggarwal, EyalAmir,IonAndroutsopou-
los, Krzysztof Apt, Warren Haley Armstrong, Ellery Aziel, Jeff Van Baalen, Darius Bacon,
Brian Baker, Shumeet Baluja, Don Barker, Tony Barrett, James Newton Bass, Don Beal,
HowardBeck,WolfgangBibel,JohnBinder,LarryBookman, DavidR.Boxall, RonenBraf-
man,JohnBresina,GerhardBrewka,SelmerBringsjord,CarlaBrodley,ChrisBrown,Emma
Brunskill,WilhelmBurger,LaurenBurka,CarlosBustamante,JoaoCachopo,MurrayCamp-
bell,NormanCarver,EmmanuelCastro,AnilChakravarthy,DanChisarick,BertheChoueiry,
RobertoCipolla, DavidCohen,JamesColeman,JulieAnnComparini, CorinnaCortes,Gary
Cottrell, Ernest Davis, Tom Dean, Rina Dechter, Tom Dietterich, Peter Drake, Chuck Dyer,
Doug Edwards, Robert Egginton, Asma’a El-Budrawy, Barbara Engelhardt, Kutluhan Erol,
Oren Etzioni, Hana Filip, Douglas Fisher, Jeffrey Forbes, Ken Ford, Eric Fosler-Lussier,
John Fosler, Jeremy Frank, Alex Franz, Bob Futrelle, Marek Galecki, Stefan Gerberding,
Stuart Gill, Sabine Glesner, Seth Golub, Gosta Grahne, Russ Greiner, Eric Grimson, Bar-
bara Grosz, Larry Hall, Steve Hanks, Othar Hansson, Ernst Heinz, Jim Hendler, Christoph
Herrmann,PaulHilfinger,RobertHolte,VasantHonavar,TimHuang,SethHutchinson,Joost
Jacob,MarkJelasity,MagnusJohansson,IstvanJonyer,DanJurafsky,LeslieKaelbling,Keiji
Kanazawa, Surekha Kasibhatla, Simon Kasif, Henry Kautz, Gernot Kerschbaumer, Max
Khesin, Richard Kirby, Dan Klein, Kevin Knight, Roland Koenig, Sven Koenig, Daphne
Koller, Rich Korf, Benjamin Kuipers, James Kurien, John Lafferty, John Laird, Gus Lars-
son, John Lazzaro, Jon LeBlanc, Jason Leatherman, Frank Lee, Jon Lehto, Edward Lim,
Phil Long, Pierre Louveaux, Don Loveland, Sridhar Mahadevan, TonyMancill, Jim Martin,
Andy Mayer, John McCarthy, David McGrane, Jay Mendelsohn, Risto Miikkulanien, Brian
Milch,SteveMinton,VibhuMittal,MehryarMohri,LeoraMorgenstern,StephenMuggleton,
KevinMurphy,RonMusick,SungMyaeng,EricNadeau,LeeNaish,PanduNayak,Bernhard
Nebel,StuartNelson,XuanLongNguyen,NilsNilsson,IllahNourbakhsh, AliNouri,Arthur
Nunes-Harwitt,SteveOmohundro,DavidPage,DavidPalmer,DavidParkes,RonParr,Mark
Preface xi
Paskin,TonyPassera,AmitPatel,MichaelPazzani,FernandoPereira,JosephPerla,WimPi-
jls,IraPohl,MarthaPollack,DavidPoole,BrucePorter,MalcolmPradhan,BillPringle,Lor-
rainePrior,GregProvan,WilliamRapaport,DeepakRavichandran, IoannisRefanidis,Philip
Resnik, Francesca Rossi, SamRoweis, RichardRussell, Jonathan Schaeffer, Richard Scherl,
Hinrich Schuetze, Lars Schuster, Bart Selman, Soheil Shams, Stuart Shapiro, Jude Shav-
lik, Yoram Singer, Satinder Singh, Daniel Sleator, David Smith, Bryan So, Robert Sproull,
Lynn Stein, Larry Stephens, Andreas Stolcke, Paul Stradling, Devika Subramanian, Marek
Suchenek, Rich Sutton, Jonathan Tash, Austin Tate, Bas Terwijn, Olivier Teytaud, Michael
Thielscher, William Thompson, Sebastian Thrun, Eric Tiedemann, Mark Torrance, Randall
Upham,PaulUtgoff,PetervanBeek,HalVarian,PaulinaVarshavskaya, SunilVemuri,Vandi
Verma,UbboVisser,JimWaldo,TobyWalsh,BonnieWebber,DanWeld,MichaelWellman,
Kamin Whitehouse, Michael Dean White, Brian Williams, David Wolfe, Jason Wolfe, Bill
Woods,AldenWright,JayYagnik,MarkYasuda,RichardYen,EliezerYudkowsky,Weixiong
Zhang,MingZhao,ShlomoZilberstein, andouresteemedcolleague AnonymousReviewer.
About the Authors
Stuart Russell was born in 1962 in Portsmouth, England. He received his B.A. with first-
classhonours inphysics fromOxfordUniversity in1982, and hisPh.D.incomputerscience
fromStanfordin1986. HethenjoinedthefacultyoftheUniversityofCaliforniaatBerkeley,
where he is a professor of computer science, director of the Center for Intelligent Systems,
and holder of the Smith–Zadeh Chair in Engineering. In 1990, he received the Presidential
YoungInvestigator AwardoftheNationalScienceFoundation, andin1995hewascowinner
of the Computers and Thought Award. He was a 1996 Miller Professor of the University of
California and was appointed to a Chancellor’s Professorship in 2000. In 1998, he gave the
Forsythe Memorial Lectures at Stanford University. He is a Fellow and former Executive
Council member of the American Association for Artificial Intelligence. He has published
over 100 papers on a wide range of topics in artificial intelligence. His other books include
TheUseofKnowledgeinAnalogyandInductionand(withEricWefald)DotheRightThing:
StudiesinLimitedRationality.
PeterNorvig iscurrently DirectorofResearch atGoogle, Inc., and wasthedirector respon-
sibleforthecoreWebsearchalgorithms from2002to2005. He isaFellowoftheAmerican
Association forArtificial Intelligence and the Association forComputing Machinery. Previ-
ously,hewasheadoftheComputationalSciencesDivisionatNASAAmesResearchCenter,
where he oversaw NASA’s research and development in artificial intelligence and robotics,
and chief scientist at Junglee, where he helped develop one of the first Internet information
extraction services. He received a B.S. in applied mathematics from Brown University and
a Ph.D. in computer science from the University of California at Berkeley. He received the
DistinguishedAlumniandEngineeringInnovationawardsfromBerkeleyandtheExceptional
AchievementMedalfromNASA.HehasbeenaprofessorattheUniversityofSouthernCal-
ifornia and a research faculty member at Berkeley. His other books are Paradigms of AI
Programming: CaseStudiesinCommonLispandVerbmobil: ATranslationSystemforFace-
to-FaceDialogandIntelligent HelpSystemsforUNIX.
xii
Contents
I Artificial Intelligence
1 Introduction 1
1.1 WhatIsAI? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 TheFoundations ofArtificialIntelligence . . . . . . . . . . . . . . . . . . 5
1.3 TheHistoryofArtificialIntelligence . . . . . . . . . . . . . . . . . . . . 16
1.4 TheStateoftheArt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.5 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 29
2 IntelligentAgents 34
2.1 AgentsandEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.2 GoodBehavior: TheConceptofRationality . . . . . . . . . . . . . . . . 36
2.3 TheNatureofEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . 40
2.4 TheStructureofAgents . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.5 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 59
II Problem-solving
3 SolvingProblemsbySearching 64
3.1 Problem-Solving Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.2 ExampleProblems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.3 SearchingforSolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.4 UninformedSearchStrategies . . . . . . . . . . . . . . . . . . . . . . . . 81
3.5 Informed(Heuristic)SearchStrategies . . . . . . . . . . . . . . . . . . . 92
3.6 HeuristicFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.7 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 108
4 BeyondClassicalSearch 120
4.1 LocalSearchAlgorithmsandOptimizationProblems . . . . . . . . . . . 120
4.2 LocalSearchinContinuous Spaces . . . . . . . . . . . . . . . . . . . . . 129
4.3 SearchingwithNondeterministic Actions . . . . . . . . . . . . . . . . . . 133
4.4 SearchingwithPartialObservations . . . . . . . . . . . . . . . . . . . . . 138
4.5 OnlineSearchAgentsandUnknownEnvironments . . . . . . . . . . . . 147
4.6 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 153
5 Adversarial Search 161
5.1 Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.2 OptimalDecisionsinGames . . . . . . . . . . . . . . . . . . . . . . . . 163
5.3 Alpha–BetaPruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
5.4 ImperfectReal-TimeDecisions . . . . . . . . . . . . . . . . . . . . . . . 171
5.5 StochasticGames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
xiii
xiv Contents
5.6 PartiallyObservable Games . . . . . . . . . . . . . . . . . . . . . . . . . 180
5.7 State-of-the-Art GamePrograms . . . . . . . . . . . . . . . . . . . . . . 185
5.8 AlternativeApproaches . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
5.9 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 189
6 ConstraintSatisfaction Problems 202
6.1 DefiningConstraintSatisfaction Problems . . . . . . . . . . . . . . . . . 202
6.2 ConstraintPropagation: Inference inCSPs . . . . . . . . . . . . . . . . . 208
6.3 Backtracking SearchforCSPs . . . . . . . . . . . . . . . . . . . . . . . . 214
6.4 LocalSearchforCSPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
6.5 TheStructureofProblems . . . . . . . . . . . . . . . . . . . . . . . . . . 222
6.6 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 227
III Knowledge, reasoning, andplanning
7 LogicalAgents 234
7.1 Knowledge-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.2 TheWumpusWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
7.3 Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
7.4 Propositional Logic: AVerySimpleLogic . . . . . . . . . . . . . . . . . 243
7.5 Propositional TheoremProving . . . . . . . . . . . . . . . . . . . . . . . 249
7.6 EffectivePropositional ModelChecking . . . . . . . . . . . . . . . . . . 259
7.7 AgentsBasedonPropositional Logic . . . . . . . . . . . . . . . . . . . . 265
7.8 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 274
8 First-OrderLogic 285
8.1 Representation Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.2 SyntaxandSemanticsofFirst-OrderLogic . . . . . . . . . . . . . . . . . 290
8.3 UsingFirst-OrderLogic . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
8.4 KnowledgeEngineering inFirst-OrderLogic . . . . . . . . . . . . . . . . 307
8.5 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 313
9 InferenceinFirst-OrderLogic 322
9.1 Propositional vs.First-OrderInference . . . . . . . . . . . . . . . . . . . 322
9.2 UnificationandLifting . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
9.3 ForwardChaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
9.4 BackwardChaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9.5 Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
9.6 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 357
10 Classical Planning 366
10.1 DefinitionofClassicalPlanning . . . . . . . . . . . . . . . . . . . . . . . 366
10.2 AlgorithmsforPlanningasState-SpaceSearch . . . . . . . . . . . . . . . 373
10.3 PlanningGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
Contents xv
10.4 OtherClassicalPlanningApproaches . . . . . . . . . . . . . . . . . . . . 387
10.5 AnalysisofPlanningApproaches . . . . . . . . . . . . . . . . . . . . . . 392
10.6 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 393
11 PlanningandActingintheRealWorld 401
11.1 Time,Schedules, andResources . . . . . . . . . . . . . . . . . . . . . . . 401
11.2 Hierarchical Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
11.3 PlanningandActinginNondeterministic Domains . . . . . . . . . . . . . 415
11.4 Multiagent Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
11.5 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 430
12 KnowledgeRepresentation 437
12.1 Ontological Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
12.2 CategoriesandObjects . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
12.3 Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
12.4 MentalEventsandMentalObjects . . . . . . . . . . . . . . . . . . . . . 450
12.5 ReasoningSystemsforCategories . . . . . . . . . . . . . . . . . . . . . 453
12.6 ReasoningwithDefaultInformation . . . . . . . . . . . . . . . . . . . . 458
12.7 TheInternetShopping World . . . . . . . . . . . . . . . . . . . . . . . . 462
12.8 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 467
IV Uncertain knowledge and reasoning
13 QuantifyingUncertainty 480
13.1 ActingunderUncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . 480
13.2 BasicProbabilityNotation . . . . . . . . . . . . . . . . . . . . . . . . . . 483
13.3 Inference UsingFullJointDistributions . . . . . . . . . . . . . . . . . . . 490
13.4 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
13.5 Bayes’RuleandItsUse . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
13.6 TheWumpusWorldRevisited . . . . . . . . . . . . . . . . . . . . . . . . 499
13.7 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 503
14 ProbabilisticReasoning 510
14.1 Representing KnowledgeinanUncertainDomain . . . . . . . . . . . . . 510
14.2 TheSemanticsofBayesianNetworks . . . . . . . . . . . . . . . . . . . . 513
14.3 EfficientRepresentation ofConditional Distributions . . . . . . . . . . . . 518
14.4 ExactInference inBayesianNetworks . . . . . . . . . . . . . . . . . . . 522
14.5 ApproximateInference inBayesianNetworks . . . . . . . . . . . . . . . 530
14.6 RelationalandFirst-OrderProbability Models . . . . . . . . . . . . . . . 539
14.7 OtherApproaches toUncertainReasoning . . . . . . . . . . . . . . . . . 546
14.8 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 551
15 ProbabilisticReasoningoverTime 566
15.1 TimeandUncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
xvi Contents
15.2 Inference inTemporalModels . . . . . . . . . . . . . . . . . . . . . . . . 570
15.3 HiddenMarkovModels . . . . . . . . . . . . . . . . . . . . . . . . . . . 578
15.4 KalmanFilters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
15.5 DynamicBayesianNetworks . . . . . . . . . . . . . . . . . . . . . . . . 590
15.6 KeepingTrackofManyObjects . . . . . . . . . . . . . . . . . . . . . . . 599
15.7 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 603
16 MakingSimpleDecisions 610
16.1 CombiningBeliefsandDesiresunderUncertainty . . . . . . . . . . . . . 610
16.2 TheBasisofUtilityTheory . . . . . . . . . . . . . . . . . . . . . . . . . 611
16.3 UtilityFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 615
16.4 Multiattribute UtilityFunctions . . . . . . . . . . . . . . . . . . . . . . . 622
16.5 DecisionNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
16.6 TheValueofInformation . . . . . . . . . . . . . . . . . . . . . . . . . . 628
16.7 Decision-Theoretic ExpertSystems . . . . . . . . . . . . . . . . . . . . . 633
16.8 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 636
17 MakingComplexDecisions 645
17.1 SequentialDecisionProblems . . . . . . . . . . . . . . . . . . . . . . . . 645
17.2 ValueIteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652
17.3 PolicyIteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656
17.4 PartiallyObservable MDPs . . . . . . . . . . . . . . . . . . . . . . . . . 658
17.5 DecisionswithMultipleAgents: GameTheory . . . . . . . . . . . . . . . 666
17.6 Mechanism Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679
17.7 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 684
V Learning
18 LearningfromExamples 693
18.1 FormsofLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693
18.2 Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 695
18.3 LearningDecisionTrees . . . . . . . . . . . . . . . . . . . . . . . . . . . 697
18.4 EvaluatingandChoosing theBestHypothesis . . . . . . . . . . . . . . . 708
18.5 TheTheoryofLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
18.6 RegressionandClassification withLinearModels . . . . . . . . . . . . . 717
18.7 ArtificialNeuralNetworks . . . . . . . . . . . . . . . . . . . . . . . . . 727
18.8 Nonparametric Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 737
18.9 SupportVectorMachines . . . . . . . . . . . . . . . . . . . . . . . . . . 744
18.10 EnsembleLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 748
18.11 PracticalMachineLearning . . . . . . . . . . . . . . . . . . . . . . . . . 753
18.12 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 757
19 KnowledgeinLearning 768
19.1 ALogicalFormulationofLearning . . . . . . . . . . . . . . . . . . . . . 768
Contents xvii
19.2 KnowledgeinLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . 777
19.3 Explanation-Based Learning . . . . . . . . . . . . . . . . . . . . . . . . 780
19.4 LearningUsingRelevanceInformation . . . . . . . . . . . . . . . . . . . 784
19.5 InductiveLogicProgramming . . . . . . . . . . . . . . . . . . . . . . . . 788
19.6 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 797
20 LearningProbabilisticModels 802
20.1 StatisticalLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
20.2 LearningwithCompleteData . . . . . . . . . . . . . . . . . . . . . . . . 806
20.3 LearningwithHiddenVariables: TheEMAlgorithm . . . . . . . . . . . . 816
20.4 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 825
21 ReinforcementLearning 830
21.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 830
21.2 PassiveReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . 832
21.3 ActiveReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 839
21.4 Generalization inReinforcement Learning . . . . . . . . . . . . . . . . . 845
21.5 PolicySearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 848
21.6 Applications ofReinforcement Learning . . . . . . . . . . . . . . . . . . 850
21.7 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 853
VI Communicating, perceiving, and acting
22 NaturalLanguageProcessing 860
22.1 LanguageModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 860
22.2 TextClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 865
22.3 Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . 867
22.4 Information Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 873
22.5 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 882
23 NaturalLanguageforCommunication 888
23.1 PhraseStructureGrammars . . . . . . . . . . . . . . . . . . . . . . . . . 888
23.2 SyntacticAnalysis(Parsing) . . . . . . . . . . . . . . . . . . . . . . . . . 892
23.3 AugmentedGrammarsandSemanticInterpretation . . . . . . . . . . . . 897
23.4 MachineTranslation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 907
23.5 SpeechRecognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912
23.6 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 918
24 Perception 928
24.1 ImageFormation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929
24.2 EarlyImage-Processing Operations . . . . . . . . . . . . . . . . . . . . . 935
24.3 ObjectRecognition byAppearance . . . . . . . . . . . . . . . . . . . . . 942
24.4 Reconstructing the3DWorld . . . . . . . . . . . . . . . . . . . . . . . . 947
24.5 ObjectRecognition fromStructuralInformation . . . . . . . . . . . . . . 957
xviii Contents
24.6 UsingVision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 961
24.7 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 965
25 Robotics 971
25.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 971
25.2 RobotHardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 973
25.3 RoboticPerception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 978
25.4 PlanningtoMove . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 986
25.5 PlanningUncertainMovements . . . . . . . . . . . . . . . . . . . . . . . 993
25.6 Moving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 997
25.7 RoboticSoftwareArchitectures . . . . . . . . . . . . . . . . . . . . . . . 1003
25.8 Application Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006
25.9 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 1010
VII Conclusions
26 PhilosophicalFoundations 1020
26.1 WeakAI:CanMachines ActIntelligently? . . . . . . . . . . . . . . . . . 1020
26.2 StrongAI:CanMachinesReallyThink? . . . . . . . . . . . . . . . . . . 1026
26.3 TheEthicsandRisksofDeveloping ArtificialIntelligence . . . . . . . . . 1034
26.4 Summary,Bibliographical andHistorical Notes,Exercises . . . . . . . . . 1040
27 AI:ThePresentandFuture 1044
27.1 AgentComponents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044
27.2 AgentArchitectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1047
27.3 AreWeGoingintheRightDirection? . . . . . . . . . . . . . . . . . . . 1049
27.4 WhatIfAIDoesSucceed? . . . . . . . . . . . . . . . . . . . . . . . . . 1051
A Mathematicalbackground 1053
A.1 ComplexityAnalysisandO()Notation . . . . . . . . . . . . . . . . . . . 1053
A.2 Vectors,Matrices, andLinearAlgebra . . . . . . . . . . . . . . . . . . . 1055
A.3 Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1057
B NotesonLanguagesandAlgorithms 1060
B.1 DefiningLanguageswithBackus–NaurForm(BNF) . . . . . . . . . . . . 1060
B.2 Describing AlgorithmswithPseudocode . . . . . . . . . . . . . . . . . . 1061
B.3 OnlineHelp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1062
Bibliography 1063
Index 1095
1
INTRODUCTION
In which we try to explain why we consider artificial intelligence to be a subject
mostworthyofstudy,andinwhichwetrytodecidewhatexactlyitis,thisbeinga
goodthingtodecidebeforeembarking.
We call ourselves Homo sapiens—man the wise—because our intelligence is so important
INTELLIGENCE
tous. Forthousands ofyears, wehavetried tounderstand how wethink; that is, how amere
handful of matter can perceive, understand, predict, and manipulate a world far larger and
ARTIFICIAL more complicated than itself. The field of artificial intelligence, or AI, goes further still: it
INTELLIGENCE
attemptsnotjusttounderstand butalsotobuildintelligent entities.
AIisoneofthe newestfieldsinscience and engineering. Work started inearnest soon
after World War II, and the name itself was coined in 1956. Along with molecular biology,
AIisregularlycitedasthe“fieldIwouldmostliketobein”byscientistsinotherdisciplines.
Astudentinphysicsmightreasonably feelthatallthegoodideashavealreadybeentakenby
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings forseveral
full-timeEinsteinsandEdisons.
AIcurrentlyencompassesahugevarietyofsubfields,rangingfromthegeneral(learning
andperception)tothespecific,suchasplayingchess,provingmathematicaltheorems,writing
poetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any
intellectual task;itistrulyauniversal field.
1.1 WHAT IS AI?
We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see
eight definitions of AI, laid out along twodimensions. The definitions on top are concerned
withthoughtprocessesandreasoning,whereastheonesonthebottomaddressbehavior. The
definitions on the left measure success in terms of fidelity to human performance, whereas
the ones on the right measure against an ideal performance measure, called rationality. A
RATIONALITY
systemisrationalifitdoesthe“rightthing,”givenwhatitknows.
Historically, all four approaches to AI have been followed, each by different people
withdifferentmethods. Ahuman-centered approachmustbeinpartanempiricalscience,in-
1
2 Chapter 1. Introduction
ThinkingHumanly ThinkingRationally
“Theexcitingnewefforttomakecomput- “Thestudyofmentalfacultiesthroughthe
ers think ... machines with minds, in the useofcomputational models.”
fullandliteralsense.” (Haugeland, 1985) (Charniak andMcDermott,1985)
“[The automation of] activities that we “Thestudyofthecomputationsthatmake
associate with human thinking, activities itpossibletoperceive, reason,andact.”
such as decision-making, problem solv- (Winston, 1992)
ing,learning ...”(Bellman,1978)
ActingHumanly ActingRationally
“The art of creating machines that per- “Computational Intelligence is the study
form functions that require intelligence ofthedesignofintelligentagents.” (Poole
when performed by people.” (Kurzweil, etal.,1998)
1990)
“Thestudy ofhowtomakecomputers do “AI ...is concerned with intelligent be-
thingsatwhich,atthemoment,peopleare haviorinartifacts.” (Nilsson,1998)
better.” (RichandKnight,1991)
Figure1.1 Somedefinitionsofartificialintelligence,organizedintofourcategories.
volvingobservationsandhypothesesabouthumanbehavior. Arationalist1 approachinvolves
acombination ofmathematicsandengineering. Thevariousgrouphavebothdisparaged and
helpedeachother. Letuslookatthefourapproaches inmoredetail.
1.1.1 Acting humanly: The Turing Test approach
The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory
TURINGTEST
operationaldefinitionofintelligence. Acomputerpassesthetestifahumaninterrogator,after
posingsomewrittenquestions, cannottellwhetherthewrittenresponsescomefromaperson
orfromacomputer. Chapter26discussesthedetailsofthetestandwhetheracomputerwould
really be intelligent if it passed. For now, we note that programming a computer to pass a
rigorously applied test provides plenty towork on. Thecomputerwould need to possess the
followingcapabilities:
NATURALLANGUAGE • naturallanguageprocessing toenableittocommunicatesuccessfully inEnglish;
PROCESSING
KNOWLEDGE • knowledgerepresentation tostorewhatitknowsorhears;
REPRESENTATION
AUTOMATED • automated reasoning to use the stored information to answer questions and to draw
REASONING
newconclusions;
• machinelearningtoadapttonewcircumstancesandtodetectandextrapolatepatterns.
MACHINELEARNING
1 Bydistinguishingbetweenhumanandrationalbehavior, wearenotsuggestingthathumansarenecessarily
“irrational”inthesenseof “emotionallyunstable” or“insane.” Onemerelyneednotethat wearenot perfect:
notallchessplayersaregrandmasters;and,unfortunately,noteveryonegetsanAontheexam.Somesystematic
errorsinhumanreasoningarecatalogedbyKahnemanetal.(1982).
Section1.1. WhatIsAI? 3
Turing’stestdeliberately avoideddirectphysicalinteraction betweentheinterrogatorandthe
computer, because physical simulation ofaperson isunnecessary forintelligence. However,
the so-called total Turing Test includes a video signal so that the interrogator can test the
TOTALTURINGTEST
subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical
objects“through thehatch.” TopassthetotalTuringTest,thecomputerwillneed
• computervisiontoperceiveobjects, and
COMPUTERVISION
• roboticstomanipulate objectsandmoveabout.
ROBOTICS
These six disciplines compose most of AI, and Turing deserves credit for designing a test
that remains relevant 60 years later. Yet AI researchers have devoted little effort to passing
the Turing Test, believing that it is more important to study the underlying principles of in-
telligence thantoduplicate anexemplar. Thequest for“artificial flight”succeeded whenthe
Wrightbrothers andothers stopped imitating birdsandstarted using windtunnels andlearn-
ing about aerodynamics. Aeronautical engineering texts do not define the goal of their field
asmaking“machinesthatflysoexactlylikepigeonsthattheycanfoolevenotherpigeons.”
1.1.2 Thinking humanly: The cognitivemodeling approach
Ifwearegoing to saythat agiven program thinks like ahuman, wemusthave some wayof
determining how humans think. Weneed toget inside the actual workings of human minds.
There are three ways to do this: through introspection—trying to catch our own thoughts as
they go by; through psychological experiments—observing a person in action; and through
brain imaging—observing the brain in action. Once wehave a sufficiently precise theory of
themind,itbecomespossible toexpress thetheory asacomputer program. Iftheprogram’s
input–output behavior matchescorresponding human behavior, thatisevidence thatsomeof
the program’s mechanisms could also be operating in humans. For example, Allen Newell
andHerbertSimon,whodeveloped GPS,the“GeneralProblemSolver”(NewellandSimon,
1961), were not content merely to have their program solve problems correctly. They were
more concerned with comparing the trace of its reasoning steps to traces of human subjects
solving the same problems. The interdisciplinary field of cognitive science brings together
COGNITIVESCIENCE
computermodelsfromAIandexperimentaltechniques frompsychologytoconstructprecise
andtestabletheories ofthehumanmind.
Cognitivescienceisafascinating fieldinitself,worthyofseveraltextbooksandatleast
one encyclopedia (Wilson andKeil, 1999). Wewilloccasionally comment onsimilarities or
differences betweenAItechniques andhumancognition. Realcognitive science, however,is
necessarily based on experimental investigation of actual humans or animals. We will leave
thatforotherbooks, asweassumethereaderhasonlyacomputerforexperimentation.
In the early days of AI there was often confusion between the approaches: an author
would argue that an algorithm performs well on a task and that it is therefore a good model
of human performance, or vice versa. Modern authors separate the two kinds of claims;
this distinction has allowed both AI and cognitive science to develop more rapidly. Thetwo
fields continue to fertilize each other, most notably in computer vision, which incorporates
neurophysiological evidence intocomputational models.
4 Chapter 1. Introduction
1.1.3 Thinking rationally: The “lawsofthought” approach
TheGreekphilosopherAristotlewasoneofthefirsttoattempttocodify“rightthinking,”that
is, irrefutable reasoning processes. His syllogisms provided patterns forargument structures
SYLLOGISM
thatalwaysyieldedcorrectconclusionswhengivencorrectpremises—forexample,“Socrates
is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were
supposed togoverntheoperation ofthemind;theirstudyinitiated thefieldcalledlogic.
LOGIC
Logiciansinthe19thcenturydevelopedaprecisenotationforstatementsaboutallkinds
ofobjects intheworldandtherelations amongthem. (Contrastthiswithordinaryarithmetic
notation, which provides only for statements about numbers.) By 1965, programs existed
that could, inprinciple, solve anysolvable problem described inlogical notation. (Although
ifnosolution exists, theprogram mightloop forever.) Theso-called logicist tradition within
LOGICIST
artificialintelligence hopestobuildonsuchprogramstocreateintelligent systems.
There are two main obstacles to this approach. First, it is not easy to take informal
knowledge and state it in the formal terms required by logical notation, particularly when
the knowledge is less than 100% certain. Second, there is a big difference between solving
a problem “in principle” and solving it in practice. Even problems with just a few hundred
facts can exhaust the computational resources of any computer unless it has some guidance
astowhichreasoningstepstotryfirst. Althoughbothoftheseobstaclesapplytoanyattempt
tobuildcomputational reasoning systems, theyappearedfirstinthelogicisttradition.
1.1.4 Acting rationally: The rational agentapproach
An agent is just something that acts (agent comes from the Latin agere, to do). Of course,
AGENT
all computer programs do something, but computer agents are expected to do more: operate
autonomously, perceive their environment, persist over a prolonged time period, adapt to
change, and create and pursue goals. A rational agent is one that acts so as to achieve the
RATIONALAGENT
bestoutcomeor,whenthereisuncertainty, thebestexpectedoutcome.
Inthe“lawsofthought” approach toAI,theemphasiswasoncorrectinferences. Mak-
ing correct inferences is sometimes part of being a rational agent, because one way to act
rationally istoreason logically totheconclusion thatagivenaction willachieve one’s goals
and then to act on that conclusion. On the other hand, correct inference is not all of ration-
ality; insomesituations, thereisnoprovably correct thingtodo, butsomething muststillbe
done. There are also ways of acting rationally that cannot be said to involve inference. For
example, recoiling from a hot stove is a reflex action that is usually more successful than a
sloweractiontakenaftercarefuldeliberation.
AlltheskillsneededfortheTuringTestalsoallowanagenttoactrationally. Knowledge
representation and reasoning enable agents to reach good decisions. We need to be able to
generate comprehensible sentences in natural language to get by in a complex society. We
need learning not only for erudition, but also because it improves our ability to generate
effectivebehavior.
The rational-agent approach has two advantages over the other approaches. First, it
is more general than the “laws of thought” approach because correct inference is just one
of several possible mechanisms for achieving rationality. Second, it is more amenable to
Section1.2. TheFoundations ofArtificialIntelligence 5
scientificdevelopmentthanareapproaches basedonhumanbehaviororhumanthought. The
standard of rationality is mathematically well defined and completely general, and can be
“unpacked” togenerateagentdesignsthatprovablyachieve it. Humanbehavior, ontheother
hand, is well adapted for one specific environment and is defined by, well, the sum total
of all the things that humans do. This book therefore concentrates on general principles
of rational agents and on components for constructing them. We will see that despite the
apparent simplicity with which the problem can be stated, an enormous variety of issues
comeupwhenwetrytosolveit. Chapter2outlines someoftheseissuesinmoredetail.
Oneimportantpointtokeepinmind: Wewillseebeforetoolongthatachievingperfect
rationality—always doing the right thing—is notfeasible in complicated environments. The
computational demands are just too high. Formost of the book, however, we will adopt the
working hypothesis that perfect rationality isa good starting point foranalysis. It simplifies
the problem and provides the appropriate setting for most of the foundational material in
LIMITED the field. Chapters 5 and 17 deal explicitly with the issue of limited rationality—acting
RATIONALITY
appropriately whenthereisnotenoughtimetodoallthecomputations onemightlike.
1.2 THE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE
Inthissection,weprovideabriefhistoryofthedisciplinesthatcontributedideas,viewpoints,
and techniques to AI. Like any history, this one is forced to concentrate on a small number
of people, events, and ideas and to ignore others that also were important. We organize the
history aroundaseriesofquestions. Wecertainly wouldnot wishtogivetheimpression that
these questions arethe only ones the disciplines address or that the disciplines have allbeen
workingtowardAIastheirultimatefruition.
1.2.1 Philosophy
• Canformalrulesbeusedtodrawvalidconclusions?
• Howdoesthemindarisefromaphysical brain?
• Wheredoesknowledgecomefrom?
• Howdoesknowledgeleadtoaction?
Aristotle (384–322 B.C.), whose bust appears on the front cover of this book, was the first
to formulate a precise set of laws governing the rational part of the mind. He developed an
informalsystemofsyllogismsforproperreasoning, whichinprincipleallowedonetogener-
ate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had
theideathatuseful reasoning couldactually becarried out byamechanical artifact. Thomas
Hobbes (1588–1679) proposed thatreasoning waslike numerical computation, that“weadd
and subtract in our silent thoughts.” The automation of computation itself was already well
under way. Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me-
chanical calculator; recent reconstructions have shown the design to be functional. Thefirst
known calculating machine was constructed around 1623 by the German scientist Wilhelm
Schickard (1592–1635), although thePascaline, builtin1642byBlaisePascal(1623–1662),
6 Chapter 1. Introduction
is more famous. Pascal wrote that “the arithmetical machine produces effects which appear
nearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)
built amechanical device intended to carry out operations on concepts rather than numbers,
but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
could add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub-
tract. Some speculated that machines might not just do calculations but actually be able to
think and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea
of an “artificial animal,” arguing “For what is the heart but a spring; and the nerves, but so
manystrings; andthejoints,butsomanywheels.”
It’sonethingtosaythatthemindoperates,atleastinpart,accordingtologicalrules,and
to build physical systems that emulate some of those rules; it’s another to say that the mind
itself is such a physical system. Rene´ Descartes (1596–1650) gave the first clear discussion
ofthedistinction betweenmindandmatterandoftheproblemsthatarise. Oneproblemwith
a purely physical conception of the mind is that it seems to leave little room for free will:
if the mind is governed entirely by physical laws, then it has no more free will than a rock
“deciding”tofalltowardthecenteroftheearth. Descarteswasastrongadvocateofthepower
of reasoning in understanding theworld, aphilosophy now called rationalism, and one that
RATIONALISM
counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.
DUALISM
He held that there is a part of the human mind (or soul or spirit) that is outside of nature,
exempt from physical laws. Animals, on the other hand, did not possess this dual quality;
they could be treated as machines. An alternative to dualism is materialism, which holds
MATERIALISM
that the brain’s operation according to the laws of physics constitutes the mind. Free will is
simplythewaythattheperception ofavailable choicesappears tothechoosing entity.
Given a physical mind that manipulates knowledge, the next problem is to establish
the source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561–
EMPIRICISM
1626)NovumOrganum,2 ischaracterizedbyadictumofJohnLocke(1632–1704): “Nothing
is in the understanding, which was not first in the senses.” David Hume’s (1711–1776) A
Treatise of Human Nature (Hume, 1739) proposed what is now known as the principle of
induction: thatgeneralrulesareacquiredbyexposuretorepeatedassociations betweentheir
INDUCTION
elements. Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell
(1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the
doctrineoflogicalpositivism. Thisdoctrineholdsthatallknowledgecanbecharacterizedby
LOGICALPOSITIVISM
OBSERVATION logical theories connected, ultimately, to observation sentences that correspond to sensory
SENTENCES
inputs;thuslogicalpositivismcombinesrationalismandempiricism.3 Theconfirmationthe-
CONFIRMATION oryofCarnapandCarlHempel(1905–1997) attempted toanalyze theacquisition ofknowl-
THEORY
edge from experience. Carnap’s book The Logical Structure of the World (1928) defined an
explicit computational procedure for extracting knowledge from elementary experiences. It
wasprobably thefirsttheoryofmindasacomputational process.
2 TheNovum Organum isanupdate of Aristotle’sOrganon, orinstrument ofthought. ThusAristotlecanbe
seenasbothanempiricistandarationalist.
3 Inthispicture,allmeaningfulstatementscanbeverifiedorfalsifiedeitherbyexperimentationorbyanalysis
ofthemeaningofthewords.Becausethisrulesoutmostofmetaphysics,aswastheintention,logicalpositivism
wasunpopularinsomecircles.
Section1.2. TheFoundations ofArtificialIntelligence 7
The final element in the philosophical picture of the mind is the connection between
knowledgeandaction. ThisquestionisvitaltoAIbecauseintelligencerequiresactionaswell
as reasoning. Moreover, only by understanding how actions are justified can we understand
howtobuildanagentwhoseactionsarejustifiable(orrational). Aristotleargued(inDeMotu
Animalium)thatactionsarejustifiedbyalogicalconnectionbetweengoalsandknowledgeof
theaction’soutcome(thelastpartofthisextractalsoappearsonthefrontcoverofthisbook,
intheoriginal Greek):
Buthowdoesithappenthatthinkingissometimesaccompaniedbyactionandsometimes
not, sometimes by motion, and sometimes not? It looks as if almost the same thing
happensasinthecaseofreasoningandmakinginferencesaboutunchangingobjects.But
in that case the end is a speculative proposition ... whereas here the conclusionwhich
resultsfromthe twopremisesisan action. ...I needcovering;a cloakis a covering. I
needacloak. WhatIneed,Ihavetomake;Ineedacloak. Ihavetomakeacloak. And
theconclusion,the“Ihavetomakeacloak,”isanaction.
In the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,
suggesting analgorithm:
Wedeliberatenotaboutends,butaboutmeans. Foradoctordoesnotdeliberatewhether
he shall heal, nor an orator whether he shall persuade, ... They assume the end and
considerhowandbywhatmeansitisattained,andifitseemseasilyandbestproduced
thereby;whileifitisachievedbyonemeansonlytheyconsiderhowitwillbeachieved
bythisandbywhatmeansthiswillbeachieved,tilltheycometothefirstcause,...and
whatislastintheorderofanalysisseemstobefirstintheorderofbecoming. Andifwe
comeonanimpossibility,wegiveupthesearch,e.g.,ifweneedmoneyandthiscannot
begot;butifathingappearspossiblewetrytodoit.
Aristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS
program. Wewouldnowcallitaregression planning system(seeChapter10).
Goal-based analysis is useful, but does not say what to do when several actions will
achievethegoalorwhennoactionwillachieveitcompletely. AntoineArnauld(1612–1694)
correctly described a quantitative formula for deciding what action to take in cases like this
(seeChapter16). JohnStuartMill’s(1806–1873) bookUtilitarianism (Mill,1863)promoted
theideaofrationaldecisioncriteriainallspheresofhumanactivity. Themoreformaltheory
ofdecisions isdiscussed inthefollowingsection.
1.2.2 Mathematics
• Whataretheformalrulestodrawvalidconclusions?
• Whatcanbecomputed?
• Howdowereasonwithuncertain information?
PhilosophersstakedoutsomeofthefundamentalideasofAI,buttheleaptoaformalscience
required a level of mathematical formalization in three fundamental areas: logic, computa-
tion,andprobability.
Theidea of formal logic can be traced back to the philosophers of ancient Greece, but
itsmathematicaldevelopmentreallybeganwiththeworkofGeorgeBoole(1815–1864),who
8 Chapter 1. Introduction
worked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob
Frege(1848–1925) extendedBoole’slogictoincludeobjectsandrelations, creatingthefirst-
order logic that is used today.4 Alfred Tarski (1902–1983) introduced a theory of reference
thatshowshowtorelatetheobjectsinalogictoobjectsintherealworld.
The next step was to determine the limits of what could be done with logic and com-
putation. The first nontrivial algorithm is thought to be Euclid’s algorithm for computing
ALGORITHM
greatest common divisors. Theword algorithm (and the idea of studying them) comes from
al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced
Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
deduction, and, bythelate 19thcentury, efforts wereunder waytoformalize general mathe-
matical reasoning as logical deduction. In 1930, KurtGo¨del (1906–1978) showed that there
exists aneffective procedure toprove anytrue statement in the first-order logic ofFregeand
Russell, but that first-order logic could not capture the principle of mathematical induction
needed to characterize the natural numbers. In 1931, Go¨del showed that limits on deduc-
INCOMPLETENESS tion do exist. His incompleteness theorem showed that in any formal theory as strong as
THEOREM
Peano arithmetic (the elementary theory of natural numbers), there are true statements that
areundecidable inthesensethattheyhavenoproofwithinthetheory.
This fundamental result can also be interpreted as showing that some functions on the
integers cannot be represented by an algorithm—that is, they cannot be computed. This
motivated Alan Turing (1912–1954) to try to characterize exactly which functions are com-
putable—capable of being computed. This notion is actually slightly problematic because
COMPUTABLE
thenotionofacomputation oreffectiveprocedure reallycannotbegivenaformaldefinition.
However, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is
capableofcomputinganycomputablefunction,isgenerallyacceptedasprovidingasufficient
definition. Turing also showed that there were some functions that no Turing machine can
compute. For example, no machine can tell in general whether a given program will return
anansweronagiveninputorrunforever.
Althoughdecidabilityandcomputabilityareimportanttoanunderstandingofcomputa-
tion,thenotionoftractability hashadanevengreaterimpact. Roughlyspeaking, aproblem
TRACTABILITY
iscalledintractableifthetimerequiredtosolveinstancesoftheproblemgrowsexponentially
with the size of the instances. The distinction between polynomial and exponential growth
incomplexity wasfirstemphasized inthemid-1960s (Cobham, 1964; Edmonds, 1965). Itis
important because exponential growthmeansthatevenmoderately largeinstances cannot be
solved in any reasonable time. Therefore, one should strive to divide the overall problem of
generating intelligent behaviorintotractable subproblems ratherthanintractable ones.
Howcanone recognize an intractable problem? Thetheory of NP-completeness, pio-
NP-COMPLETENESS
neeredbyStevenCook(1971)andRichardKarp(1972), provides amethod. CookandKarp
showed theexistence oflarge classes ofcanonical combinatorial search andreasoning prob-
lems that are NP-complete. Anyproblem class to which the class of NP-complete problems
canbereducedislikelytobeintractable. (AlthoughithasnotbeenprovedthatNP-complete
4 Frege’sproposed notation for first-orderlogic—an arcane combination of textual and geometric features—
neverbecamepopular.
Section1.2. TheFoundations ofArtificialIntelligence 9
problems are necessarily intractable, most theoreticians believe it.) These results contrast
with the optimism with which the popular press greeted the first computers—“Electronic
Super-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,
careful use of resources will characterize intelligent systems. Put crudely, the world is an
extremely large problem instance! Work in AI has helped explain why some instances of
NP-completeproblems arehard,yetothersareeasy(Cheesemanetal.,1991).
Besideslogicandcomputation, thethirdgreatcontribution ofmathematics toAIisthe
theory of probability. The Italian Gerolamo Cardano (1501–1576) first framed the idea of
PROBABILITY
probability, describing it in terms of the possible outcomes of gambling events. In 1654,
Blaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to pre-
dict the future of an unfinished gambling game and assign average payoffs to the gamblers.
Probabilityquicklybecameaninvaluable partofallthequantitative sciences, helpingtodeal
withuncertain measurements and incomplete theories. James Bernoulli (1654–1705), Pierre
Laplace (1749–1827), and others advanced the theory and introduced new statistical meth-
ods. Thomas Bayes (1702–1761), who appears on the front cover of this book, proposed
a rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most
modernapproaches touncertain reasoning inAIsystems.
1.2.3 Economics
• Howshouldwemakedecisions soastomaximizepayoff?
• Howshouldwedothiswhenothersmaynotgoalong?
• Howshouldwedothiswhenthepayoffmaybefarinthefuture?
The science of economics got its start in 1776, when Scottish philosopher Adam Smith
(1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations.
WhiletheancientGreeksandothershadmadecontributions toeconomicthought,Smithwas
the first to treat it as a science, using the idea that economies can be thought of as consist-
ing of individual agents maximizing their own economic well-being. Most people think of
economics as being about money, but economists will say that they are really studying how
people makechoices thatleadtopreferred outcomes. WhenMcDonald’s offersahamburger
foradollar,theyareassertingthattheywouldpreferthedollarandhopingthatcustomerswill
prefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was
UTILITY
firstformalized byLe´onWalras(pronounced “Valrasse”) (1834-1910) andwasimproved by
Frank Ramsey(1931) and laterby John von Neumann and OskarMorgenstern in theirbook
TheTheoryofGamesandEconomicBehavior(1944).
Decisiontheory,whichcombinesprobability theorywithutilitytheory,providesafor-
DECISIONTHEORY
malandcompleteframeworkfordecisions(economicorotherwise)madeunderuncertainty—
that is, in cases where probabilistic descriptions appropriately capture the decision maker’s
environment. This issuitable for“large” economies whereeach agent need pay no attention
to the actions of other agents as individuals. For “small” economies, the situation is much
more like a game: the actions of one player can significantly affect the utility of another
(either positively or negatively). Von Neumann and Morgenstern’s development of game
theory (see also Luceand Raiffa, 1957) included the surprising result that, forsomegames,
GAMETHEORY
10 Chapter 1. Introduction
arational agent should adoptpolicies thatare(orleast appeartobe)randomized. Unlikede-
cisiontheory, gametheorydoesnotofferanunambiguous prescription forselecting actions.
For the most part, economists did not address the third question listed above, namely,
how tomake rational decisions whenpayoffs from actions are not immediate but instead re-
sultfromseveralactionstakeninsequence. Thistopicwaspursuedinthefieldofoperations
OPERATIONS research, which emerged in World War II from efforts in Britain to optimize radar installa-
RESEARCH
tions, and later found civilian applications in complex management decisions. The work of
Richard Bellman (1957) formalized a class of sequential decision problems called Markov
decisionprocesses, whichwestudyinChapters17and21.
Work in economics and operations research has contributed much to our notion of ra-
tional agents, yet for many years AI research developed along entirely separate paths. One
reason was the apparent complexity of making rational decisions. The pioneering AI re-
searcherHerbertSimon(1916–2001) wontheNobelPrizeineconomicsin1978forhisearly
work showing that models based on satisficing—making decisions that are “good enough,”
SATISFICING
rather than laboriously calculating an optimal decision—gave a better description of actual
human behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in
decision-theoretic techniques foragentsystems(Wellman,1995).
1.2.4 Neuroscience
• Howdobrainsprocess information?
Neuroscience is the study of the nervous system, particularly the brain. Although the exact
NEUROSCIENCE
wayinwhichthebrainenablesthoughtisoneofthegreatmysteriesofscience,thefactthatit
doesenablethoughthasbeenappreciated forthousands ofyearsbecauseoftheevidencethat
strong blowsto the head can lead tomental incapacitation. Ithas also long been known that
humanbrains aresomehow different; inabout 335 B.C. Aristotle wrote, “Ofalltheanimals,
man has the largest brain in proportion to his size.”5 Still, it was not until the middle of the
18th century thatthebrain waswidely recognized astheseat ofconsciousness. Before then,
candidate locations included theheartandthespleen.
Paul Broca’s (1824–1880) study of aphasia (speech deficit) in brain-damaged patients
in 1861 demonstrated the existence of localized areas of the brain responsible for specific
cognitive functions. In particular, he showed that speech production was localized to the
portion of the left hemisphere now called Broca’s area.6 By that time, it was known that
the brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi
NEURON
(1843–1926) developed a staining technique allowing the observation of individual neurons
in the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852–
1934)inhispioneering studiesofthebrain’sneuronalstructures.7 NicolasRashevsky(1936,
1938)wasthefirsttoapplymathematical modelstothestudyofthenervous sytem.
5 Sincethen,ithasbeendiscoveredthatthetreeshrew(Scandentia)hasahigherratioofbraintobodymass.
6 ManyciteAlexanderHood(1824)asapossiblepriorsource.
7 Golgipersistedinhisbeliefthatthebrain’sfunctionswerecarriedoutprimarilyinacontinuousmediumin
whichneuronswereembedded, whereasCajalpropounded the“neuronal doctrine.” ThetwosharedtheNobel
prizein1906butgavemutuallyantagonisticacceptancespeeches.
Section1.2. TheFoundations ofArtificialIntelligence 11
Axonal arborization
Axon from another cell
Synapse
Dendrite Axon
Nucleus
Synapses
Cell body or Soma
Figure 1.2 The parts of a nerve cell or neuron. Each neuron consists of a cell body,
or soma, that contains a cell nucleus. Branching out from the cell body are a number of
fiberscalleddendritesanda single longfibercalled the axon. Theaxonstretchesoutfora
long distance, much longer than the scale in this diagram indicates. Typically, an axon is
1cmlong(100timesthediameterofthecellbody),butcanreachupto1meter. Aneuron
makesconnectionswith10to100,000otherneuronsatjunctionscalledsynapses.Signalsare
propagatedfrom neuronto neuronby a complicated electrochemicalreaction. The signals
controlbrainactivityintheshorttermandalsoenablelong-termchangesintheconnectivity
ofneurons. Thesemechanismsarethoughttoformthebasisforlearninginthebrain. Most
informationprocessinggoesoninthecerebralcortex,theouterlayerofthebrain. Thebasic
organizationalunit appears to be a column of tissue about0.5 mm in diameter, containing
about20,000neuronsandextendingthefulldepthofthecortexabout4mminhumans).
Wenowhavesomedataonthemappingbetweenareasofthebrain andthepartsofthe
body that they control orfrom which they receive sensory input. Such mappings are able to
change radically over the course of a few weeks, and some animals seem to have multiple
maps. Moreover, we do not fully understand how other areas can take over functions when
oneareaisdamaged. Thereisalmostnotheoryonhowanindividual memoryisstored.
The measurement of intact brain activity began in 1929 with the invention by Hans
Bergerofthe electroencephalograph (EEG).Therecent development offunctional magnetic
resonance imaging (fMRI) (Ogawa et al., 1990; Cabeza and Nyberg, 2001) is giving neu-
roscientists unprecedentedly detailed images of brain activity, enabling measurements that
correspond in interesting ways to ongoing cognitive processes. These are augmented by
advances in single-cell recording of neuron activity. Individual neurons can be stimulated
electrically, chemically, orevenoptically(HanandBoyden,2007),allowingneuronal input–
output relationships to be mapped. Despite these advances, we are still a long way from
understanding howcognitiveprocesses actually work.
The truly amazing conclusion is that a collection of simple cells can lead to thought,
action, and consciousness or, in the pithy words of John Searle (1992), brains cause minds.
12 Chapter 1. Introduction
Supercomputer PersonalComputer HumanBrain
Computational units 104 CPUs,1012 transistors 4CPUs,109 transistors 1011 neurons
Storageunits 1014 bitsRAM 1011 bitsRAM 1011 neurons
1015 bitsdisk 1013 bitsdisk 1014 synapses
Cycletime 10
−9
sec 10
−9
sec 10
−3
sec
Operations/sec 1015 1010 1017
Memoryupdates/sec 1014 1010 1014
Figure1.3 AcrudecomparisonoftherawcomputationalresourcesavailabletotheIBM
BLUE GENEsupercomputer,atypicalpersonalcomputerof2008,andthehumanbrain.The
brain’s numbersare essentially fixed, whereas the supercomputer’snumbers have been in-
creasing by a factorof 10 every 5 years or so, allowing it to achieveroughparity with the
brain.Thepersonalcomputerlagsbehindonallmetricsexceptcycletime.
Theonlyrealalternative theoryismysticism: thatmindsoperateinsomemysticalrealmthat
isbeyondphysical science.
Brainsanddigitalcomputershavesomewhatdifferentproperties. Figure1.3showsthat
computers have a cycle time that is a million times faster than a brain. The brain makes up
for that with far more storage and interconnection than even a high-end personal computer,
although the largest supercomputers have a capacity that is similar to the brain’s. (It should
be noted, however, that the brain does not seem to use all of its neurons simultaneously.)
Futurists make much of these numbers, pointing to an approaching singularity at which
SINGULARITY
computers reach asuperhuman level ofperformance (Vinge, 1993; Kurzweil, 2005), but the
rawcomparisons arenotespecially informative. Evenwitha computerofvirtuallyunlimited
capacity, westillwouldnotknowhowtoachievethebrain’slevelofintelligence.
1.2.5 Psychology
• Howdohumansandanimalsthinkandact?
The origins of scientific psychology are usually traced to the work of the German physi-
cist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).
Helmholtz applied the scientific method to the study of human vision, and his Handbook
of Physiological Optics is even now described as “the single most important treatise on the
physics and physiology of human vision” (Nalwa, 1993, p.15). In 1879, Wundt opened the
first laboratory of experimental psychology, at the University of Leipzig. Wundt insisted
on carefully controlled experiments in which his workers would perform a perceptual oras-
sociative task while introspecting on their thought processes. The careful controls went a
long way toward making psychology a science, but the subjective nature of the data made
it unlikely that an experimenter would ever disconfirm his or her own theories. Biologists
studying animal behavior, onthe otherhand, lacked introspective data and developed an ob-
jectivemethodology,asdescribedbyH.S.Jennings(1906)inhisinfluentialworkBehaviorof
the Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led
BEHAVIORISM
byJohnWatson(1878–1958), rejectedanytheoryinvolvingmentalprocessesonthegrounds
Section1.2. TheFoundations ofArtificialIntelligence 13
thatintrospection couldnotprovidereliableevidence. Behavioristsinsistedonstudyingonly
objective measures of the percepts (or stimulus) given to an animal and its resulting actions
(or response). Behaviorism discovered a lot about rats and pigeons but had less success at
understanding humans.
COGNITIVE Cognitive psychology, which views the brain as an information-processing device,
PSYCHOLOGY
can be traced back at least to the works of William James (1842–1910). Helmholtz also
insisted that perception involved a form of unconscious logical inference. The cognitive
viewpoint waslargely eclipsed by behaviorism in the United States, but atCambridge’s Ap-
plied Psychology Unit, directed by Frederic Bartlett (1886–1969), cognitive modeling was
able to flourish. The Nature of Explanation, by Bartlett’s student and successor Kenneth
Craik (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs and
goals, arguing that they are just as scientific as, say, using pressure and temperature to talk
about gases, despite their being made of molecules that have neither. Craik specified the
threekeystepsofaknowledge-based agent: (1)thestimulusmustbetranslatedintoaninter-
nalrepresentation, (2)therepresentation ismanipulated bycognitiveprocessestoderivenew
internal representations, and (3) these are in turn retranslated back into action. He clearly
explained whythiswasagooddesignforanagent:
Iftheorganismcarriesa“small-scalemodel”ofexternalrealityandofits ownpossible
actionswithinitshead,itisabletotryoutvariousalternatives,concludewhichisthebest
ofthem,reacttofuturesituationsbeforetheyarise,utilizetheknowledgeofpastevents
indealingwith thepresentandfuture,andin everywaytoreactin amuchfuller,safer,
andmorecompetentmannertotheemergencieswhichfaceit. (Craik,1943)
AfterCraik’s death inabicycle accident in1945, hisworkwascontinued byDonaldBroad-
bent,whosebookPerceptionandCommunication(1958)wasoneofthefirstworkstomodel
psychological phenomena as information processing. Meanwhile, in the United States, the
development ofcomputermodeling ledtothecreation ofthefieldofcognitive science. The
fieldcanbe saidtohave started ataworkshop inSeptember 1956 atMIT.(Weshall see that
thisisjusttwomonthsaftertheconference atwhichAIitselfwas“born.”) Attheworkshop,
GeorgeMillerpresented TheMagicNumberSeven,NoamChomskypresented ThreeModels
of Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.
These three influential papers showed how computer models could be used to address the
psychology of memory, language, and logical thinking, respectively. It is now a common
(although far from universal) view among psychologists that “a cognitive theory should be
likeacomputerprogram”(Anderson,1980);thatis,itshoulddescribeadetailedinformation-
processing mechanismwherebysomecognitivefunction mightbeimplemented.
1.2.6 Computer engineering
• Howcanwebuildanefficientcomputer?
For artificial intelligence to succeed, we need two things: intelligence and an artifact. The
computer has been the artifact of choice. The modern digital electronic computer was in-
ventedindependently andalmostsimultaneously byscientistsinthreecountriesembattledin
14 Chapter 1. Introduction
World War II. The first operational computer was the electromechanical Heath Robinson,8
built in 1940 byAlan Turing’s team forasingle purpose: deciphering German messages. In
1943, the same group developed the Colossus, a powerful general-purpose machine based
on vacuum tubes.9 The first operational programmable computer was the Z-3, the inven-
tionofKonrad ZuseinGermanyin1941. Zusealsoinvented floating-point numbers andthe
first high-level programming language, Plankalku¨l. The first electronic computer, the ABC,
was assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942
at Iowa State University. Atanasoff’s research received little support or recognition; it was
the ENIAC, developed as part of a secret military project at the University of Pennsylvania
by a team including John Mauchly and John Eckert, that proved to be the most influential
forerunnerofmoderncomputers.
Sincethattime,eachgenerationofcomputerhardwarehasbroughtanincreaseinspeed
andcapacityandadecreaseinprice. Performancedoubledevery18monthsorsountilaround
2005,whenpowerdissipationproblemsledmanufacturers to startmultiplyingthenumberof
CPUcoresratherthantheclockspeed. Currentexpectations arethatfutureincreasesinpower
willcomefrommassiveparallelism—a curious convergence withtheproperties ofthebrain.
Of course, there were calculating devices before the electronic computer. The earliest
automated machines, dating from the 17th century, were discussed on page 6. The first pro-
grammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834),
that used punched cards to store instructions for the pattern to be woven. In the mid-19th
century, Charles Babbage (1792–1871) designed two machines, neither of which he com-
pleted. TheDifference Enginewasintended tocomputemathematical tables forengineering
andscientificprojects. Itwasfinallybuiltandshowntoworkin1991attheScienceMuseum
inLondon(Swade,2000). Babbage’s Analytical Enginewasfarmoreambitious: itincluded
addressable memory, stored programs, andconditional jumpsand wasthefirstartifact capa-
ble ofuniversal computation. Babbage’s colleague AdaLovelace, daughter ofthepoet Lord
Byron,wasperhapstheworld’sfirstprogrammer. (TheprogramminglanguageAdaisnamed
afterher.) ShewroteprogramsfortheunfinishedAnalytical Engineandevenspeculated that
themachinecouldplaychessorcomposemusic.
AI also owes a debt to the software side of computer science, which has supplied the
operatingsystems,programminglanguages, andtoolsneededtowritemodernprograms(and
papers aboutthem). Butthisisoneareawherethedebthasbeenrepaid: workinAIhaspio-
neeredmanyideasthathavemadetheirwaybacktomainstream computerscience,including
time sharing, interactive interpreters, personal computers with windows and mice, rapid de-
velopment environments, the linked list data type, automatic storage management, and key
concepts ofsymbolic, functional, declarative, andobject-oriented programming.
8 HeathRobinsonwasacartoonistfamousforhisdepictionsofwhimsicalandabsurdlycomplicatedcontrap-
tionsforeverydaytaskssuchasbutteringtoast.
9 Inthepostwarperiod, Turingwantedtousethesecomputers forAIresearch—forexample, oneofthefirst
chessprograms(Turingetal.,1953).HiseffortswereblockedbytheBritishgovernment.
Section1.2. TheFoundations ofArtificialIntelligence 15
1.2.7 Control theory andcybernetics
• Howcanartifactsoperate undertheirowncontrol?
Ktesibios of Alexandria (c. 250 B.C.) built the first self-controlling machine: a water clock
with a regulator that maintained a constant flow rate. This invention changed the definition
of what an artifact could do. Previously, only living things could modify their behavior in
response to changes inthe environment. Otherexamples of self-regulating feedback control
systems include the steam engine governor, created by James Watt (1736–1819), and the
thermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.
Themathematical theoryofstablefeedback systemswasdevelopedinthe19thcentury.
The central figure in the creation of what is now called control theory was Norbert
CONTROLTHEORY
Wiener(1894–1964). Wienerwasabrilliant mathematician whoworkedwithBertrandRus-
sell,amongothers,beforedevelopinganinterestinbiologicalandmechanicalcontrolsystems
andtheirconnectiontocognition. LikeCraik(whoalsousedcontrolsystemsaspsychological
models), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the
behaviorist orthodoxy (Rosenblueth et al., 1943). They viewed purposive behavior as aris-
ingfromaregulatorymechanismtryingtominimize“error”—thedifferencebetweencurrent
state and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,
and John von Neumann, organized a series of influential conferences that explored the new
mathematicalandcomputational modelsofcognition. Wiener’sbookCybernetics(1948)be-
CYBERNETICS
came a bestseller and awoke the public to the possibility of artificially intelligent machines.
Meanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas. Ashby, Alan
Turing, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas
beforeWiener’s bookappeared.” Ashby’s DesignforaBrain(1948, 1952)elaborated onhis
idea that intelligence could be created by the use of homeostatic devices containing appro-
HOMEOSTATIC
priatefeedback loopstoachievestableadaptivebehavior.
Modern control theory, especially the branch known as stochastic optimal control, has
OBJECTIVE asitsgoalthedesignofsystemsthatmaximizeanobjectivefunctionovertime. Thisroughly
FUNCTION
matches our view of AI: designing systems that behave optimally. Why, then, are AI and
control theory twodifferent fields, despite the close connections among theirfounders? The
answer lies in the close coupling between the mathematical techniques that were familiar to
theparticipantsandthecorresponding setsofproblemsthatwereencompassedineachworld
view. Calculusandmatrixalgebra,thetoolsofcontroltheory,lendthemselvestosystemsthat
aredescribablebyfixedsetsofcontinuousvariables,whereasAIwasfoundedinpartasaway
toescapefromthetheseperceivedlimitations. Thetoolsoflogicalinferenceandcomputation
allowed AIresearchers toconsider problems suchaslanguage, vision, andplanning that fell
completely outsidethecontroltheorist’s purview.
1.2.8 Linguistics
• Howdoeslanguage relatetothought?
In 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed ac-
count of the behaviorist approach to language learning, written by the foremost expert in
16 Chapter 1. Introduction
the field. But curiously, a review of the book became as well known as the book itself, and
served to almost kill off interest in behaviorism. The author of the review was the linguist
Noam Chomsky, who had just published a book on his own theory, Syntactic Structures.
Chomsky pointed out that the behaviorist theory did not address the notion of creativity in
language—it didnotexplain howachild could understand and makeupsentences thatheor
shehadneverheardbefore. Chomsky’stheory—based onsyntacticmodelsgoingbacktothe
Indian linguist Panini(c. 350 B.C.)—could explain this, andunlike previous theories, itwas
formalenoughthatitcouldinprinciple beprogrammed.
Modern linguistics and AI, then, were “born” at about the same time, and grew up
COMPUTATIONAL together, intersectinginahybridfieldcalled computationallinguisticsornaturallanguage
LINGUISTICS
processing. Theproblemofunderstandinglanguagesoonturnedouttobeconsiderablymore
complex than it seemed in 1957. Understanding language requires an understanding of the
subjectmatterandcontext,notjustanunderstandingofthestructureofsentences. Thismight
seem obvious, but it was not widely appreciated until the 1960s. Much of the early work in
knowledge representation (the study of how to put knowledge into a form that a computer
can reason with) was tied to language and informed by research in linguistics, which was
connected inturntodecades ofworkonthephilosophical analysis oflanguage.
1.3 THE HISTORY OF ARTIFICIAL INTELLIGENCE
Withthebackground materialbehindus,wearereadytocover thedevelopment ofAIitself.
1.3.1 The gestationofartificialintelligence (1943–1955)
The first work that is now generally recognized as AI was done by Warren McCulloch and
Walter Pitts (1943). They drew on three sources: knowledge of the basic physiology and
function of neurons in the brain; a formal analysis of propositional logic due to Russell and
Whitehead; andTuring’stheoryofcomputation. Theyproposed amodelofartificialneurons
inwhicheachneuronischaracterized asbeing“on”or“off,” withaswitchto“on”occurring
in response to stimulation by a sufficient number of neighboring neurons. The state of a
neuronwasconceivedofas“factuallyequivalenttoapropositionwhichproposeditsadequate
stimulus.” They showed, for example, that any computable function could be computed by
some network of connected neurons, and that all the logical connectives (and, or, not, etc.)
could be implemented by simple net structures. McCulloch and Pitts also suggested that
suitably defined networks could learn. Donald Hebb(1949) demonstrated asimple updating
ruleformodifying theconnection strengths between neurons. Hisrule, nowcalled Hebbian
learning,remainsaninfluential modeltothisday.
HEBBIANLEARNING
Twoundergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the
first neural network computer in 1950. The SNARC, as it was called, used 3000 vacuum
tubesandasurplusautomaticpilotmechanismfromaB-24bombertosimulateanetworkof
40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.
His Ph.D. committee was skeptical about whether this kind of work should be considered
Section1.3. TheHistoryofArtificialIntelligence 17
mathematics, butvonNeumannreportedly said,“Ifitisn’tnow,itwillbesomeday.” Minsky
waslatertoproveinfluentialtheoremsshowingthelimitationsofneuralnetworkresearch.
There were a number of early examples of work that can be characterized as AI, but
AlanTuring’s vision wasperhaps themostinfluential. Hegavelectures onthetopic asearly
as1947attheLondonMathematical Societyandarticulated apersuasive agenda inhis1950
article “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test,
machine learning, genetic algorithms, and reinforcement learning. He proposed the Child
Programmeidea,explaining “Insteadoftryingtoproduceaprogrammetosimulatetheadult
mind,whynotrathertrytoproduce onewhichsimulated thechild’s?”
1.3.2 The birth ofartificialintelligence (1956)
Princeton was home to another influential figure in AI, John McCarthy. After receiving his
PhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-
fordandthentoDartmouthCollege,whichwastobecometheofficialbirthplace ofthefield.
McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring
together U.S. researchers interested in automata theory, neural nets, and the study of intel-
ligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The
proposal states:10
We propose that a 2 month, 10 man study of artificial intelligence be carried
out during the summer of 1956 at Dartmouth College in Hanover, New Hamp-
shire. The study is to proceed on the basis of the conjecture that every aspect of
learning or any other feature of intelligence can in principle be so precisely de-
scribedthatamachinecanbemadetosimulateit. Anattemptwillbemadetofind
howtomakemachinesuselanguage, formabstractions andconcepts, solvekinds
of problems now reserved for humans, and improve themselves. We think that a
significant advance can be made in one or more of these problems if a carefully
selectedgroupofscientists workonittogetherforasummer.
There were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel
fromIBM,andRaySolomonoffandOliverSelfridgefromMIT.
Two researchers from Carnegie Tech,11 Allen Newell and Herbert Simon, rather stole
the show. Although the others had ideas and in some cases programs for particular appli-
cations such as checkers, Newell and Simon already had a reasoning program, the Logic
Theorist (LT), about whichSimonclaimed, “Wehave invented acomputerprogram capable
ofthinking non-numerically, andtherebysolvedthevenerable mind–body problem.”12 Soon
aftertheworkshop,theprogramwasabletoprovemostofthetheoremsinChapter2ofRus-
10 ThiswasthefirstofficialusageofMcCarthy’stermartificialintelligence.Perhaps“computationalrationality”
wouldhavebeenmorepreciseandlessthreatening,but“AI”hasstuck.Atthe50thanniversaryoftheDartmouth
conference, McCarthystatedthatheresistedtheterms“computer”or“computational”indeferencetoNorbert
Weiner,whowaspromotinganalogcyberneticdevicesratherthandigitalcomputers.
11 NowCarnegieMellonUniversity(CMU).
12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and
translateditintomachinecodebyhand. Toavoiderrors,theyworkedinparallel,callingoutbinarynumbersto
eachotherastheywroteeachinstructiontomakesuretheyagreed.
18 Chapter 1. Introduction
sell andWhitehead’s Principia Mathematica. Russell wasreportedly delighted whenSimon
showedhimthattheprogramhadcomeupwithaproofforonetheoremthatwasshorterthan
theoneinPrincipia. Theeditors ofthe Journal ofSymbolic Logicwerelessimpressed; they
rejectedapapercoauthored byNewell,Simon,andLogicTheorist.
The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce
all the major figures to each other. For the next 20 years, the field would be dominated by
thesepeopleandtheirstudents andcolleagues atMIT,CMU,Stanford, andIBM.
Looking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can
see whyitwasnecessary forAIto become aseparate field. Whycouldn’t all the workdone
in AI have taken place under the name of control theory or operations research or decision
theory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch
of mathematics? The first answer is that AI from the start embraced the idea of duplicating
human faculties such as creativity, self-improvement, and language use. None of the other
fields were addressing these issues. The second answer is methodology. AI is the only one
ofthesefieldsthatisclearlyabranchofcomputerscience(althoughoperationsresearchdoes
share an emphasis on computer simulations), and AI is the only field to attempt to build
machinesthatwillfunction autonomously incomplex, changing environments.
1.3.3 Early enthusiasm, great expectations (1952–1969)
Theearly yearsofAIwerefullofsuccesses—in alimitedway. Giventheprimitive comput-
ers and programming tools of the time and the fact that only a few years earlier computers
wereseenasthingsthatcoulddoarithmeticandnomore,itwasastonishingwheneveracom-
puterdidanything remotely clever. Theintellectual establishment, byandlarge, preferred to
believe that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered
by Turing.) AI researchers naturally responded by demonstrating one X after another. John
McCarthyreferredtothisperiodasthe“Look,Ma,nohands!” era.
Newell and Simon’s early success was followed up with the General Problem Solver,
or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human
problem-solving protocols. Within the limited class of puzzles it could handle, it turned out
that the order in which the program considered subgoals and possible actions was similarto
thatinwhichhumansapproached thesameproblems. Thus, GPS wasprobably thefirstpro-
gramtoembodythe“thinkinghumanly”approach. Thesuccess ofGPS andsubsequentpro-
gramsasmodelsofcognitionledNewellandSimon(1976)toformulatethefamousphysical
PHYSICALSYMBOL symbolsystemhypothesis,whichstatesthat“aphysicalsymbolsystemhasthenecessaryand
SYSTEM
sufficient means forgeneral intelligent action.” What they meant is that any system (human
or machine) exhibiting intelligence must operate by manipulating data structures composed
ofsymbols. Wewillseelaterthatthishypothesis hasbeenchallenged frommanydirections.
At IBM, Nathaniel Rochester and his colleagues produced some of the first AI pro-
grams. Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was
able to prove theorems that many students of mathematics would find quite tricky. Starting
in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually
learned toplay atastrong amateurlevel. Along theway, he disproved theidea that comput-
Section1.3. TheHistoryofArtificialIntelligence 19
erscandoonlywhattheyaretoldto: hisprogram quickly learned toplayabettergamethan
itscreator. Theprogram wasdemonstrated ontelevision inFebruary 1956, creating astrong
impression. Like Turing, Samuel had trouble finding computer time. Working at night, he
used machines that were still on the testing floor at IBM’s manufacturing plant. Chapter 5
coversgameplaying, andChapter21explainsthelearning techniques usedbySamuel.
John McCarthy movedfrom Dartmouth toMITand there madethree crucial contribu-
tionsinonehistoricyear: 1958. InMITAILabMemoNo.1,McCarthydefinedthehigh-level
languageLisp,whichwastobecomethedominantAIprogramminglanguageforthenext30
LISP
years. WithLisp,McCarthyhadthetoolheneeded, butaccess toscarceandexpensivecom-
putingresourceswasalsoaseriousproblem. Inresponse, he andothersatMITinventedtime
sharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense,
in which he described the Advice Taker, ahypothetical program that can be seen as the first
complete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s
program was designed to use knowledge to search forsolutions to problems. But unlike the
others, it was to embody general knowledge of the world. For example, he showed how
somesimpleaxiomswouldenabletheprogram togenerate aplantodrivetotheairport. The
program wasalso designed toaccept new axioms inthe normal course ofoperation, thereby
allowing it to achieve competence in new areas without being reprogrammed. The Advice
Taker thus embodied the central principles of knowledge representation and reasoning: that
it is useful to have a formal, explicit representation of the world and its workings and to be
able to manipulate that representation with deductive processes. It is remarkable how much
ofthe1958paperremainsrelevanttoday.
1958alsomarkedtheyearthatMarvinMinskymovedtoMIT.Hisinitialcollaboration
withMcCarthydidnotlast,however. McCarthystressedrepresentation andreasoning infor-
mal logic, whereas Minsky was more interested in getting programs to work and eventually
developed an anti-logic outlook. In 1963, McCarthy started the AIlab at Stanford. Hisplan
to use logic to build the ultimate Advice Taker was advanced by J. A. Robinson’s discov-
ery in 1965 of the resolution method (a complete theorem-proving algorithm for first-order
logic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical
reasoning. Applications of logic included Cordell Green’s question-answering and planning
systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute
(SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the
completeintegration oflogicalreasoning andphysicalactivity.
Minsky supervised a series of students who chose limited problems that appeared to
require intelligence tosolve. Theselimited domains became known asmicroworlds. James
MICROWORLD
Slagle’s SAINT program (1963) wasable tosolveclosed-form calculus integration problems
typical of first-year college courses. Tom Evans’s ANALOGY program (1968) solved geo-
metricanalogyproblemsthatappearinIQtests. DanielBobrow’sSTUDENT program(1967)
solvedalgebrastoryproblems,suchasthefollowing:
If the number of customers Tom gets is twice the square of 20 percent of the number
of advertisementshe runs, and the numberof advertisements he runs is 45, what is the
numberofcustomersTomgets?
20 Chapter 1. Introduction
Blue
Red
Red Green
Blue
Green
Green
Red
Figure1.4 Ascenefromtheblocksworld. SHRDLU(Winograd,1972)hasjustcompleted
thecommand“Findablockwhichistallerthantheoneyouareholdingandputitinthebox.”
The most famous microworld was the blocks world, which consists of a set of solid blocks
placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.
A typical task in this world is to rearrange the blocks in a certain way, using a robot hand
that can pick up one block at a time. The blocks world was home to the vision project of
David Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),
the learning theory of Patrick Winston (1970), the natural-language-understanding program
ofTerryWinograd(1972), andtheplannerofScottFahlman(1974).
Early work building on the neural networks of McCulloch and Pitts also flourished.
The work of Winograd and Cowan (1963) showed how a large number of elements could
collectively represent anindividual concept, withacorresponding increaseinrobustness and
parallelism. Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,
1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)
with his perceptrons. The perceptron convergence theorem (Block et al., 1962) says that
thelearningalgorithmcanadjusttheconnection strengths ofaperceptrontomatchanyinput
data,providedsuchamatchexists. Thesetopicsarecovered inChapter20.
1.3.4 A doseofreality(1966–1973)
From the beginning, AI researchers were not shy about making predictions of their coming
successes. Thefollowingstatement byHerbertSimonin1957 isoftenquoted:
Itisnotmyaimtosurpriseorshockyou—butthesimplestwayIcansummarizeistosay
thattherearenowintheworldmachinesthatthink,thatlearnandthatcreate. Moreover,
Section1.3. TheHistoryofArtificialIntelligence 21
theirabilitytodothesethingsisgoingtoincreaserapidlyuntil—inavisiblefuture—the
rangeofproblemstheycanhandlewillbecoextensivewiththerangetowhichthehuman
mindhasbeenapplied.
Terms such as “visible future” can be interpreted in various ways, but Simon also made
more concrete predictions: that within 10 years a computer would be chess champion, and
a significant mathematical theorem would be proved by machine. These predictions came
true(orapproximately true)within40yearsratherthan10. Simon’soverconfidence wasdue
to the promising performance of early AI systems on simple examples. In almost all cases,
however, these early systems turned out to fail miserably when tried out onwiderselections
ofproblems andonmoredifficultproblems.
The first kind of difficulty arose because most early programs knew nothing of their
subject matter; they succeeded by means of simple syntactic manipulations. A typical story
occurred inearlymachinetranslation efforts, whichweregenerously fundedbytheU.S.Na-
tionalResearchCouncilinanattempttospeedupthetranslation ofRussianscientificpapers
inthewakeoftheSputniklaunchin1957. Itwasthoughtinitiallythatsimplesyntactictrans-
formations based on the grammars of Russian and English, and word replacement from an
electronic dictionary, would suffice to preserve the exact meanings of sentences. The fact is
that accurate translation requires background knowledge in order to resolve ambiguity and
establish the content of the sentence. The famous retranslation of “the spirit is willing but
the fleshisweak” as“the vodka isgood but themeat isrotten” illustrates the difficulties en-
countered. In1966,areportbyanadvisorycommitteefoundthat“therehasbeennomachine
translationofgeneralscientifictext,andnoneisinimmediateprospect.” AllU.S.government
fundingforacademictranslation projectswascanceled. Today,machinetranslation isanim-
perfectbutwidelyusedtoolfortechnical, commercial, government, andInternetdocuments.
ThesecondkindofdifficultywastheintractabilityofmanyoftheproblemsthatAIwas
attempting to solve. Most of the early AI programs solved problems by trying out different
combinations of steps until the solution was found. This strategy worked initially because
microworlds contained very few objects and hence very few possible actions and very short
solution sequences. Before the theory of computational complexity was developed, it was
widely thought that “scaling up” to larger problems was simply a matter of faster hardware
andlargermemories. Theoptimismthataccompaniedthedevelopmentofresolutiontheorem
proving,forexample,wassoondampenedwhenresearchers failedtoprovetheoremsinvolv-
ingmorethanafewdozenfacts. Thefactthataprogramcanfindasolutioninprincipledoes
notmeanthattheprogramcontains anyofthemechanismsneededtofinditinpractice.
The illusion of unlimited computational power was not confined to problem-solving
programs. Earlyexperiments in machineevolution(nowcalledgeneticalgorithms)(Fried-
MACHINEEVOLUTION
GENETIC berg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by
ALGORITHM
making an appropriate series of small mutations to a machine-code program, one can gen-
erate a program with good performance for any particular task. The idea, then, was to try
random mutations with a selection process to preserve mutations that seemed useful. De-
spitethousandsofhoursofCPUtime,almostnoprogresswasdemonstrated. Moderngenetic
algorithms usebetterrepresentations andhaveshownmoresuccess.
22 Chapter 1. Introduction
Failure to come to grips with the “combinatorial explosion” was one of the main criti-
cismsofAIcontainedintheLighthillreport(Lighthill,1973),whichformedthebasisforthe
decision bytheBritishgovernment toendsupport forAIresearch inallbuttwouniversities.
(Oraltraditionpaintsasomewhatdifferentandmorecolorfulpicture,withpoliticalambitions
andpersonal animosities whosedescription isbesidethepoint.)
Athirddifficultyarosebecauseofsomefundamentallimitationsonthebasicstructures
being used togenerate intelligent behavior. Forexample, Minsky andPapert’s book Percep-
trons (1969) proved that, although perceptrons (a simple form of neural network) could be
showntolearnanythingtheywerecapableofrepresenting, theycouldrepresentverylittle. In
particular,atwo-inputperceptron(restrictedtobesimplerthantheformRosenblattoriginally
studied) couldnotbetrained torecognize whenitstwoinputs weredifferent. Althoughtheir
results did not apply to more complex, multilayer networks, research funding for neural-net
research soon dwindled to almost nothing. Ironically, the newback-propagation learning al-
gorithms for multilayer networks that were to cause an enormous resurgence in neural-net
research inthelate1980swereactually discoveredfirstin1969(BrysonandHo,1969).
1.3.5 Knowledge-based systems: The key to power? (1969–1979)
The picture of problem solving that had arisen during the first decade of AI research was of
a general-purpose search mechanism trying to string together elementary reasoning steps to
findcompletesolutions. Suchapproacheshavebeencalledweakmethodsbecause,although
WEAKMETHOD
general, they do not scale up tolarge ordifficult problem instances. Thealternative to weak
methods is to use more powerful, domain-specific knowledge that allows larger reasoning
stepsandcanmoreeasilyhandle typically occurring casesinnarrowareas ofexpertise. One
mightsaythattosolveahardproblem,youhavetoalmostknow theansweralready.
TheDENDRALprogram(Buchananetal.,1969)wasanearlyexampleofthisapproach.
It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),
Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel
laureate geneticist) teameduptosolvetheproblem ofinferring molecularstructure from the
information provided by a mass spectrometer. The input to the program consists of the ele-
mentaryformulaofthemolecule(e.g.,C H NO )andthemassspectrumgivingthemasses
6 13 2
ofthevariousfragmentsofthemoleculegeneratedwhenitisbombardedbyanelectronbeam.
Forexample, themassspectrum mightcontain apeakat m = 15,corresponding tothemass
ofamethyl(CH )fragment.
3
The naive version of the program generated all possible structures consistent with the
formula,andthenpredictedwhatmassspectrumwouldbeobservedforeach,comparingthis
with the actual spectrum. As one might expect, this is intractable for even moderate-sized
molecules. The DENDRAL researchers consulted analytical chemists and found that they
workedbylookingforwell-knownpatternsofpeaksinthespectrumthatsuggested common
substructures inthe molecule. Forexample, the following rule isused torecognize aketone
(C=O)subgroup (whichweighs28):
iftherearetwopeaksatx andx suchthat
1 2
(a)x +x =M +28(M isthemassofthewholemolecule);
1 2
Section1.3. TheHistoryofArtificialIntelligence 23
(b)x −28isahighpeak;
1
(c)x −28isahighpeak;
2
(d)Atleastoneofx andx ishigh.
1 2
thenthereisaketonesubgroup
Recognizing thatthe molecule contains aparticular substructure reduces thenumberofpos-
siblecandidates enormously. DENDRAL waspowerfulbecause
Alltherelevanttheoreticalknowledgetosolvetheseproblemshasbeenmappedoverfrom
its general form in the [spectrum prediction component] (“first principles”) to efficient
specialforms(“cookbookrecipes”). (Feigenbaumetal.,1971)
The significance of DENDRAL was that it was the first successful knowledge-intensive sys-
tem: its expertise derived from large numbers of special-purpose rules. Later systems also
incorporated themainthemeofMcCarthy’sAdviceTakerapproach—the cleanseparation of
theknowledge(intheformofrules)fromthereasoning component.
With this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro-
gramming Project (HPP) to investigate the extent to which the new methodology of expert
systems could be applied to other areas of human expertise. The next major effort was in
EXPERTSYSTEMS
theareaofmedical diagnosis. Feigenbaum, Buchanan, andDr.EdwardShortliffe developed
MYCIN to diagnose blood infections. With about 450 rules, MYCIN was able to perform
as well as some experts, and considerably better than junior doctors. It also contained two
major differences from DENDRAL. First, unlike the DENDRAL rules, no general theoretical
modelexistedfromwhichtheMYCIN rulescouldbededuced. Theyhadtobeacquiredfrom
extensive interviewing of experts, who in turn acquired them from textbooks, other experts,
anddirectexperienceofcases. Second,theruleshadtoreflecttheuncertaintyassociatedwith
CERTAINTYFACTOR
medical knowledge. MYCIN incorporated acalculus of uncertainty called certainty factors
(seeChapter14),whichseemed(atthetime)tofitwellwithhowdoctorsassessedtheimpact
ofevidence onthediagnosis.
The importance of domain knowledge was also apparent in the area of understanding
natural language. AlthoughWinograd’s SHRDLU system forunderstanding natural language
hadengendered agooddealofexcitement,itsdependence onsyntacticanalysiscausedsome
of the same problems as occurred in the early machine translation work. It was able to
overcome ambiguity and understand pronoun references, but this wasmainly because itwas
designed specifically foronearea—the blocks world. Several researchers, including Eugene
Charniak, a fellow graduate student of Winograd’s at MIT, suggested that robust language
understanding would require general knowledge about the world and a general method for
usingthatknowledge.
At Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming,
“Thereisnosuchthingassyntax,”whichupsetalotoflinguistsbutdidservetostartauseful
discussion. Schank and his students built a series of programs (Schank and Abelson, 1977;
Wilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of under-
standing naturallanguage. Theemphasis, however,wasless onlanguage perseandmoreon
theproblemsofrepresenting andreasoning withtheknowledgerequiredforlanguageunder-
standing. The problems included representing stereotypical situations (Cullingford, 1981),
24 Chapter 1. Introduction
describing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding
plansandgoals(Wilensky, 1983).
Thewidespread growthof applications toreal-world problems caused aconcurrent in-
crease in the demands for workable knowledge representation schemes. A large number
of different representation and reasoning languages were developed. Some were based on
logic—forexample,theProloglanguage becamepopularinEurope,andthe PLANNER fam-
ily in the United States. Others, following Minsky’s idea of frames (1975), adopted a more
FRAMES
structured approach, assembling facts about particular object and event types and arranging
thetypesintoalargetaxonomichierarchy analogous toabiological taxonomy.
1.3.6 AIbecomes anindustry (1980–present)
Thefirstsuccessfulcommercialexpertsystem, R1,beganoperationattheDigitalEquipment
Corporation (McDermott, 1982). The program helped configure orders for new computer
systems; by 1986, it was saving the company an estimated $40 million a year. By 1988,
DEC’sAIgroup had40expert systems deployed, withmoreontheway. DuPonthad100in
useand500indevelopment,savinganestimated$10millionayear. NearlyeverymajorU.S.
corporation haditsownAIgroupandwaseitherusingorinvestigating expertsystems.
In1981,theJapaneseannouncedthe“FifthGeneration”project,a10-yearplantobuild
intelligent computers running Prolog. In response, the United States formed the Microelec-
tronics and ComputerTechnology Corporation (MCC)as aresearch consortium designed to
assure national competitiveness. In both cases, AI was part of a broad effort, including chip
design andhuman-interface research. InBritain, theAlvey report reinstated thefunding that
wascutbytheLighthillreport.13 Inallthreecountries, however,theprojectsnevermettheir
ambitiousgoals.
Overall,theAIindustryboomedfromafewmilliondollarsin1980tobillionsofdollars
in 1988, including hundreds of companies building expert systems, vision systems, robots,
and software and hardware specialized for these purposes. Soon after that came a period
calledthe“AIWinter,”inwhichmanycompaniesfellbythewaysideastheyfailedtodeliver
onextravagant promises.
1.3.7 The return ofneural networks (1986–present)
In the mid-1980s at least four different groups reinvented the back-propagation learning
BACK-PROPAGATION
algorithm first found in 1969 by Bryson and Ho. The algorithm was applied to many learn-
ing problems in computer science and psychology, and the widespread dissemination of the
results in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)
causedgreatexcitement.
These so-called connectionist models of intelligent systems were seen by some as di-
CONNECTIONIST
rect competitors both to the symbolic models promoted by Newell and Simon and to the
logicist approach of McCarthy and others (Smolensky, 1988). It might seem obvious that
at some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic
13 Tosaveembarrassment,anewfieldcalledIKBS(IntelligentKnowledge-BasedSystems)wasinventedbecause
ArtificialIntelligencehadbeenofficiallycanceled.
Section1.3. TheHistoryofArtificialIntelligence 25
Species (1997) suggests that this is the defining characteristic of humans—but the most ar-
dentconnectionistsquestionedwhethersymbolmanipulationhadanyrealexplanatoryrolein
detailed modelsofcognition. Thisquestion remainsunanswered, butthecurrent viewisthat
connectionistandsymbolicapproachesarecomplementary, notcompeting. Asoccurredwith
the separation of AI and cognitive science, modern neural network research has bifurcated
into two fields, one concerned with creating effective network architectures and algorithms
and understanding theirmathematical properties, the other concerned withcareful modeling
oftheempiricalproperties ofactualneuronsandensembles ofneurons.
1.3.8 AIadopts thescientific method (1987–present)
Recent years have seen a revolution in both the content and the methodology of work in
artificialintelligence.14 Itisnowmorecommontobuild onexisting theories thantopropose
brand-new ones, to base claims on rigorous theorems or hard experimental evidence rather
thanonintuition, andtoshowrelevance toreal-world applications ratherthantoyexamples.
AIwasfoundedinpartasarebellionagainstthelimitationsofexistingfieldslikecontrol
theoryandstatistics, butnowitisembracingthosefields. AsDavidMcAllester(1998)putit:
In the early period of AI it seemed plausible that new forms of symbolic computation,
e.g.,framesandsemanticnetworks,mademuchofclassicaltheoryobsolete. Thisledto
a formof isolationism in which AI became largelyseparatedfromthe rest of computer
science. This isolationism is currently being abandoned. There is a recognition that
machinelearningshouldnotbeisolatedfrominformationtheory,thatuncertainreasoning
shouldnotbeisolatedfromstochasticmodeling,thatsearchshouldnotbeisolatedfrom
classical optimizationand control, and that automated reasoning should not be isolated
fromformalmethodsandstaticanalysis.
In terms of methodology, AI has finally come firmly under the scientific method. To be ac-
cepted,hypothesesmustbesubjectedtorigorousempirical experiments,andtheresultsmust
be analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate
experiments byusingsharedrepositories oftestdataandcode.
The field of speech recognition illustrates the pattern. In the 1970s, a wide variety of
different architectures and approaches were tried. Many of these were rather ad hoc and
fragile, and were demonstrated on only a few specially selected examples. In recent years,
HIDDENMARKOV approachesbasedonhiddenMarkovmodels(HMMs)havecometodominatethearea. Two
MODELS
aspects ofHMMsarerelevant. First, theyarebased onarigorous mathematical theory. This
hasallowedspeechresearcherstobuildonseveraldecadesofmathematicalresultsdeveloped
in other fields. Second, they are generated by a process of training on a large corpus of
real speech data. This ensures that the performance is robust, and in rigorous blind tests the
HMMshavebeenimprovingtheirscoressteadily. Speechtechnology andtherelatedfieldof
handwritten character recognition are already making the transition towidespread industrial
14 Somehavecharacterizedthischange asavictoryofthe neats—thosewhothinkthatAItheoriesshouldbe
grounded inmathematicalrigor—overthe scruffies—thosewhowouldrathertryoutlotsof ideas, writesome
programs, andthenassesswhatseemstobeworking. Bothapproachesareimportant. Ashifttowardneatness
impliesthatthefieldhasreachedalevelofstabilityandmaturity. Whetherthatstabilitywillbedisruptedbya
newscruffyideaisanotherquestion.
26 Chapter 1. Introduction
and consumer applications. Note that there is no scientific claim that humans use HMMsto
recognize speech; rather, HMMs provide a mathematical framework for understanding the
problem andsupporttheengineering claimthattheyworkwellinpractice.
Machine translation follows the samecourse asspeech recognition. In the1950s there
was initial enthusiasm for an approach based on sequences of words, with models learned
according to the principles of information theory. That approach fell out of favor in the
1960s, butreturnedinthelate1990sandnowdominatesthefield.
Neural networks also fitthis trend. Much of the work on neural nets in the 1980s was
done inanattempt toscope out whatcould bedone and tolearn howneural netsdifferfrom
“traditional” techniques. Usingimprovedmethodology and theoretical frameworks, thefield
arrived at an understanding in which neural nets can now be compared with corresponding
techniquesfromstatistics,patternrecognition,andmachinelearning,andthemostpromising
technique can be applied to each application. As a result of these developments, so-called
dataminingtechnology hasspawnedavigorous newindustry.
DATAMINING
JudeaPearl’s(1988)Probabilistic ReasoninginIntelligent Systemsledtoanewaccep-
tance ofprobability anddecision theory inAI,following aresurgence ofinterest epitomized
by Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network
BAYESIANNETWORK
formalism was invented to allow efficient representation of, and rigorous reasoning with,
uncertain knowledge. This approach largely overcomes many problems of the probabilistic
reasoningsystemsofthe1960sand1970s;itnowdominatesAIresearchonuncertainreason-
ing and expert systems. The approach allows for learning from experience, and it combines
thebestofclassicalAIandneuralnets. WorkbyJudeaPearl(1982a)andbyEricHorvitzand
DavidHeckerman(HorvitzandHeckerman,1986;Horvitzetal.,1986)promotedtheideaof
normative expert systems: ones that act rationally according to the laws of decision theory
anddonottrytoimitatethethoughtstepsofhumanexperts. TheWindowsTM operatingsys-
tem includes several normative diagnostic expert systems for correcting problems. Chapters
13to16coverthisarea.
Similar gentle revolutions have occurred in robotics, computer vision, and knowledge
representation. Abetterunderstanding oftheproblemsand theircomplexityproperties,com-
bined withincreased mathematical sophistication, has led to workable research agendas and
robustmethods. Althoughincreasedformalizationandspecializationledfieldssuchasvision
androboticstobecomesomewhatisolatedfrom“mainstream” AIinthe1990s,thistrendhas
reversedinrecentyearsastoolsfrommachinelearninginparticularhaveprovedeffectivefor
manyproblems. Theprocessofreintegration isalreadyyielding significantbenefits
1.3.9 The emergence ofintelligentagents (1995–present)
Perhaps encouraged bytheprogress insolving thesubproblems ofAI,researchers havealso
started to look at the “whole agent” problem again. The work of Allen Newell, John Laird,
andPaulRosenbloomonSOAR(Newell,1990;Lairdetal.,1987)isthebest-knownexample
of a complete agent architecture. One of the most important environments for intelligent
agents is the Internet. AI systems have become so common in Web-based applications that
the “-bot” suffix has entered everyday language. Moreover, AI technologies underlie many
Section1.3. TheHistoryofArtificialIntelligence 27
Internettools, suchassearchengines, recommendersystems, andWebsiteaggregators.
Oneconsequenceoftryingtobuildcompleteagentsistherealizationthatthepreviously
isolated subfields of AI might need to be reorganized somewhat when their results are to be
tied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,
speechrecognition, etc.) cannotdeliverperfectlyreliableinformationabouttheenvironment.
Hence, reasoning and planning systems must be able to handle uncertainty. Asecond major
consequence of the agent perspective is that AI has been drawn into much closer contact
with other fields, such as control theory and economics, that also deal with agents. Recent
progressinthecontrolofroboticcarshasderivedfromamixtureofapproachesrangingfrom
better sensors, control-theoretic integration of sensing, localization and mapping, as well as
adegreeofhigh-level planning.
Despite these successes, some influential founders of AI, including John McCarthy
(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and
Winston,2009),haveexpresseddiscontentwiththeprogressofAI.TheythinkthatAIshould
put less emphasis on creating ever-improved versions of applications that are good at a spe-
cific task, such as driving a car, playing chess, or recognizing speech. Instead, they believe
AIshouldreturntoitsrootsofstrivingfor,inSimon’swords,“machinesthatthink,thatlearn
andthatcreate.” Theycalltheeffort human-levelAIorHLAI;theirfirstsymposium wasin
HUMAN-LEVELAI
2004(Minskyetal.,2004). Theeffortwillrequireverylargeknowledgebases; Hendleretal.
(1995)discusswheretheseknowledgebasesmightcomefrom.
ARTIFICIALGENERAL A related idea is the subfield of Artificial General Intelligence or AGI (Goertzel and
INTELLIGENCE
Pennachin,2007),whichhelditsfirstconferenceandorganizedtheJournalofArtificialGen-
eral Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in
any environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten-
dees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really
FriendlyAI isalso aconcern (Yudkowsky, 2008; Omohundro, 2008), one we will return to
FRIENDLYAI
inChapter26.
1.3.10 The availabilityofvery largedata sets (2001–present)
Throughoutthe60-yearhistoryofcomputerscience,theemphasishasbeenonthealgorithm
asthemainsubject ofstudy. ButsomerecentworkinAIsuggests thatformanyproblems,it
makes more sense to worry about the data and be less picky about what algorithm to apply.
This is true because of the increasing availability of very large data sources: for example,
trillionsofwordsofEnglishandbillionsofimagesfromtheWeb(KilgarriffandGrefenstette,
2006);orbillionsofbasepairsofgenomicsequences (Collinsetal.,2003).
One influential paper in this line was Yarowsky’s (1995) work on word-sense disam-
biguation: giventheuseoftheword“plant” inasentence, doesthatrefertofloraorfactory?
Previous approaches to the problem had relied on human-labeled examples combined with
machine learning algorithms. Yarowsky showed that the task can be done, with accuracy
above 96%, with no labeled examples at all. Instead, given a very large corpus of unanno-
tatedtextandjustthedictionary definitions ofthetwosenses—“works, industrial plant” and
“flora, plant life”—one can label examples in the corpus, and from there bootstrap to learn
28 Chapter 1. Introduction
new patterns that help label new examples. Banko and Brill (2001) show that techniques
like this perform even better as the amount of available text goes from a million words to a
billion and that the increase in performance from using more data exceeds any difference in
algorithm choice; a mediocre algorithm with 100 million words of unlabeled training data
outperforms thebestknownalgorithm with1millionwords.
Asanotherexample, HaysandEfros(2007)discuss theproblem offillinginholesina
photograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but
now you need to fill in the masked area with something that matches the background. Hays
andEfrosdefinedanalgorithmthatsearchesthroughacollectionofphotostofindsomething
that will match. They found the performance of their algorithm was poor when they used
a collection of only ten thousand photos, but crossed a threshold into excellent performance
whentheygrewthecollection totwomillionphotos.
Worklikethissuggests thatthe“knowledge bottleneck” inAI—theproblem ofhowto
expressalltheknowledgethatasystemneeds—maybesolvedinmanyapplicationsbylearn-
ingmethodsratherthanhand-codedknowledgeengineering, providedthelearningalgorithms
haveenoughdatatogoon(Halevyetal.,2009). Reportershavenoticedthesurgeofnewap-
plications and have written that “AI Winter” may be yielding to a new Spring (Havenstein,
2005). As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply
embeddedintheinfrastructure ofeveryindustry.”
1.4 THE STATE OF THE ART
What can AI do today? A concise answer isdifficult because there are so many activities in
somanysubfields. Herewesampleafewapplications; othersappearthroughout thebook.
Robotic vehicles: A driverless robotic car named STANLEY sped through the rough
terrain of the Mojave dessert at 22 mph, finishing the 132-mile course first to win the 2005
DARPAGrandChallenge. STANLEY isaVolkswagen Touaregoutfittedwithcameras,radar,
andlaserrangefinders tosensetheenvironment andonboard softwaretocommandthesteer-
ing, braking, andacceleration (Thrun, 2006). Thefollowing yearCMU’s BOSS wonthe Ur-
banChallenge,safelydrivingintrafficthroughthestreets ofaclosedAirForcebase,obeying
trafficrulesandavoiding pedestrians andothervehicles.
Speechrecognition: AtravelercallingUnitedAirlinestobookaflightcanhavetheen-
tireconversationguidedbyanautomatedspeechrecognitionanddialogmanagementsystem.
Autonomousplanningandscheduling: AhundredmillionmilesfromEarth,NASA’s
Remote Agent program became the first on-board autonomous planning program to control
the scheduling of operations for a spacecraft (Jonsson et al., 2000). REMOTE AGENT gen-
erated plansfrom high-level goals specified fromtheground andmonitored theexecution of
thoseplans—detecting, diagnosing, andrecoveringfromproblemsastheyoccurred. Succes-
sorprogram MAPGEN (Al-Chang etal.,2004)plans thedailyoperations forNASA’sMars
ExplorationRovers,andMEXAR2(Cestaetal.,2007)didmissionplanning—both logistics
andscienceplanning—for theEuropeanSpaceAgency’sMarsExpressmissionin2008.
Section1.5. Summary 29
Game playing: IBM’s DEEP BLUE became the first computer program to defeat the
world champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in
anexhibition match (Goodman and Keene, 1997). Kasparov said that hefelt a“new kind of
intelligence” across the board from him. Newsweek magazine described the match as “The
brain’s last stand.” The value of IBM’s stock increased by $18 billion. Human champions
studied Kasparov’s loss and were able to draw a few matches in subsequent years, but the
mostrecenthuman-computer matcheshavebeenwonconvincingly bythecomputer.
Spamfighting: Eachday,learningalgorithmsclassifyoverabillionmessagesasspam,
savingtherecipientfromhavingtowastetimedeletingwhat,formanyusers,couldcomprise
80%or90%ofallmessages,ifnotclassifiedawaybyalgorithms. Becausethespammersare
continually updating theirtactics, itisdifficultforastaticprogrammed approach tokeepup,
andlearning algorithmsworkbest(Sahami etal.,1998;GoodmanandHeckerman, 2004).
Logistics planning: During the Persian Gulf crisis of 1991, U.S. forces deployed a
Dynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to doautomated
logistics planning and scheduling for transportation. This involved up to 50,000 vehicles,
cargo, and people at a time, and had to account for starting points, destinations, routes, and
conflict resolution among all parameters. The AI planning techniques generated in hours
a plan that would have taken weeks with older methods. The Defense Advanced Research
Project Agency (DARPA) stated that this single application more than paid back DARPA’s
30-yearinvestment inAI.
Robotics: The iRobot Corporation has sold overtwomillion Roomba robotic vacuum
cleaners for home use. The company also deploys the more rugged PackBot to Iraq and
Afghanistan, where it is used to handle hazardous materials, clear explosives, and identify
thelocationofsnipers.
Machine Translation: A computer program automatically translates from Arabic to
English, allowing an English speaker to see the headline “Ardogan Confirms That Turkey
Would Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses a
statisticalmodelbuiltfromexamplesofArabic-to-English translationsandfromexamplesof
Englishtexttotalingtwotrillionwords(Brants etal.,2007). Noneofthecomputerscientists
ontheteamspeakArabic,buttheydounderstand statisticsandmachinelearningalgorithms.
These are just a few examples of artificial intelligence systems that exist today. Not
magic or science fiction—but rather science, engineering, and mathematics, to which this
bookprovidesanintroduction.
1.5 SUMMARY
This chapter defines AI and establishes the cultural background against which it has devel-
oped. Someoftheimportantpointsareasfollows:
• Differentpeopleapproach AIwithdifferent goalsinmind. Twoimportant questions to
askare: Areyouconcerned withthinking orbehavior? Doyou wanttomodelhumans
orworkfromanidealstandard?
30 Chapter 1. Introduction
• In this book, we adopt the view that intelligence is concerned mainly with rational
action. Ideally, an intelligent agent takes the best possible action in a situation. We
studytheproblem ofbuilding agentsthatareintelligent inthissense.
• Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas
thatthemindisinsomewayslikeamachine,thatitoperatesonknowledgeencodedin
someinternal language, andthatthought canbeusedtochoosewhatactionstotake.
• Mathematiciansprovidedthetoolstomanipulatestatementsoflogicalcertaintyaswell
asuncertain, probabilistic statements. Theyalso setthe groundwork forunderstanding
computation andreasoning aboutalgorithms.
• Economists formalized the problem of making decisions that maximize the expected
outcometothedecisionmaker.
• Neuroscientistsdiscoveredsomefactsabouthowthebrainworksandthewaysinwhich
itissimilartoanddifferentfromcomputers.
• Psychologistsadoptedtheideathathumansandanimalscanbeconsideredinformation-
processing machines. Linguists showedthatlanguage usefitsintothismodel.
• Computerengineers provided the ever-more-powerful machines that make AI applica-
tionspossible.
• Controltheorydealswithdesigning devicesthatactoptimally onthebasisoffeedback
from the environment. Initially, the mathematical tools of control theory were quite
differentfromAI,butthefieldsarecomingclosertogether.
• ThehistoryofAIhashadcyclesofsuccess,misplacedoptimism,andresultingcutbacks
in enthusiasm and funding. There have also been cycles of introducing new creative
approaches andsystematically refiningthebestones.
• AIhasadvancedmorerapidlyinthepastdecadebecauseofgreateruseofthescientific
methodinexperimenting withandcomparing approaches.
• Recentprogressinunderstanding thetheoretical basisfor intelligence hasgonehandin
hand with improvements in the capabilities of real systems. The subfields of AI have
becomemoreintegrated, andAIhasfoundcommonground withotherdisciplines.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
ThemethodologicalstatusofartificialintelligenceisinvestigatedinTheSciencesoftheArtifi-
cial,byHerbSimon(1981),whichdiscussesresearchareasconcernedwithcomplexartifacts.
It explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an
overviewofexperimental methodology withinAI.
TheTuringTest(Turing, 1950) isdiscussed byShieber(1994), whoseverely criticizes
the usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes
(1995), whoarguethatthetestitselfisnothelpfulforAI.Bringsjord (2008) givesadvicefor
a Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on
the Turing Test. Artificial Intelligence: The Very Idea, by John Haugeland (1985), gives a
Exercises 31
readable account of the philosophical and practical problems of AI. Significant early papers
inAIareanthologizedinthecollectionsbyWebberandNilsson(1981)andbyLuger(1995).
The Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in
AI, as does Wikipedia. These articles usually provide a good entry point into the research
literature on each topic. An insightful and comprehensive history of AI is given by Nils
Nillson(2009), oneoftheearlypioneers ofthefield.
The most recent work appears in the proceedings of the majorAI conferences: the bi-
ennial International JointConference onAI(IJCAI),theannual European Conference onAI
(ECAI),andtheNationalConferenceonAI,moreoftenknownasAAAI,afteritssponsoring
organization. The major journals for general AI are Artificial Intelligence, Computational
Intelligence, theIEEETransactions onPatternAnalysis andMachine Intelligence, IEEEIn-
telligentSystems,andtheelectronicJournalofArtificialIntelligenceResearch. Therearealso
many conferences and journals devoted to specific areas, which we cover in the appropriate
chapters. The main professional societies forAI are the American Association for Artificial
Intelligence (AAAI), the ACM Special Interest Group in Artificial Intelligence (SIGART),
and the Society for Artificial Intelligence and Simulation of Behaviour (AISB). AAAI’s AI
Magazine contains manytopical andtutorial articles, anditsWebsite, aaai.org,contains
news,tutorials, andbackground information.
EXERCISES
Theseexercisesareintendedtostimulatediscussion, andsomemightbesetastermprojects.
Alternatively, preliminary attempts can be made now, and these attempts can be reviewed
afterthecompletion ofthebook.
1.1 Define in your own words: (a) intelligence, (b) artificial intelligence, (c) agent, (d)
rationality, (e)logicalreasoning.
1.2 Read Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several
objectionstohisproposedenterpriseandhistestforintelligence. Whichobjectionsstillcarry
weight? Are his refutations valid? Can you think of new objections arising from develop-
ments since he wrotethe paper? Inthe paper, hepredicts that, bythe year2000, acomputer
will have a 30% chance of passing a five-minute Turing Test with an unskilled interrogator.
Whatchancedoyouthinkacomputerwouldhavetoday? Inanother50years?
1.3 Arereflexactions(suchasflinchingfromahotstove)rational? Aretheyintelligent?
1.4 Suppose weextend Evans’s ANALOGY program so that it can score 200 on a standard
IQtest. Wouldwethenhaveaprogram moreintelligent thanahuman? Explain.
1.5 The neural structure of the sea slug Aplysia has been widely studied (first by Nobel
Laureate Eric Kandel) because it has only about 20,000 neurons, most of them large and
easily manipulated. Assuming that the cycle time foran Aplysia neuron is roughly the same
as fora human neuron, how does the computational power, in terms of memory updates per
second, comparewiththehigh-end computerdescribed inFigure1.3?
32 Chapter 1. Introduction
1.6 How could introspection—reporting on one’s inner thoughts—be inaccurate? Could I
bewrongaboutwhatI’mthinking? Discuss.
1.7 Towhatextentarethefollowingcomputersystemsinstances ofartificialintelligence:
• Supermarketbarcodescanners.
• Websearchengines.
• Voice-activated telephone menus.
• Internetroutingalgorithms thatrespond dynamically tothestateofthenetwork.
1.8 Many of the computational models of cognitive activities that have been proposed in-
volve quite complex mathematical operations, such asconvolving animage withaGaussian
orfindingaminimumoftheentropyfunction. Mosthumans(andcertainlyallanimals)never
learn this kind of mathematics at all, almost no one learns it before college, and almost no
one can compute the convolution of a function with a Gaussian in their head. What sense
does it make to say that the “vision system” is doing this kind of mathematics, whereas the
actualpersonhasnoideahowtodoit?
1.9 Whywouldevolution tendtoresultinsystemsthatactrationally? Whatgoalsaresuch
systemsdesigned toachieve?
1.10 IsAIascience, orisitengineering? Orneitherorboth? Explain.
1.11 “Surely computers cannot be intelligent—they can do only what their programmers
tellthem.” Isthelatterstatement true,anddoesitimplytheformer?
1.12 “Surely animals cannot be intelligent—they can do only what their genes tell them.”
Isthelatterstatementtrue,anddoesitimplytheformer?
1.13 “Surelyanimals,humans,andcomputerscannotbeintelligent—theycandoonlywhat
theirconstituent atomsaretoldtodobythelawsofphysics.” Isthelatterstatementtrue,and
doesitimplytheformer?
1.14 Examine the AI literature to discover whether the following tasks can currently be
solvedbycomputers:
a. Playingadecentgameoftabletennis(Ping-Pong).
b. DrivinginthecenterofCairo,Egypt.
c. DrivinginVictorville, California.
d. Buyingaweek’sworthofgroceries atthemarket.
e. Buyingaweek’sworthofgroceries ontheWeb.
f. Playingadecentgameofbridgeatacompetitive level.
g. Discoveringandprovingnewmathematical theorems.
h. Writinganintentionally funnystory.
i. Givingcompetentlegaladviceinaspecialized areaoflaw.
j. Translatingspoken Englishintospoken Swedishinrealtime.
k. Performingacomplexsurgical operation.
Exercises 33
Forthecurrently infeasible tasks, trytofindoutwhatthedifficultiesareandpredict when,if
ever,theywillbeovercome.
1.15 Varioussubfields ofAIhaveheldcontests bydefining astandard taskandinviting re-
searchers to do their best. Examples include the DARPA Grand Challenge for robotic cars,
TheInternationalPlanningCompetition,theRobocuproboticsoccerleague,theTRECinfor-
mation retrieval event, and contests in machine translation, speech recognition. Investigate
fiveofthesecontests,anddescribetheprogressmadeovertheyears. Towhatdegreehavethe
contestsadvancedtoestateoftheartinAI?Dowhatdegreedotheyhurtthefieldbydrawing
energyawayfromnewideas?
2
INTELLIGENT AGENTS
In which we discuss the nature of agents, perfect or otherwise, the diversity of
environments, andtheresulting menagerieofagenttypes.
Chapter 1 identified the concept of rational agents as central to our approach to artificial
intelligence. Inthischapter,wemakethisnotionmoreconcrete. Wewillseethattheconcept
ofrationality canbeappliedtoawidevarietyofagentsoperating inanyimaginable environ-
ment. Ourplanin thisbook istousethis concept todevelop asmallsetof design principles
forbuilding successful agents—systems thatcanreasonably becalled intelligent.
We begin by examining agents, environments, and the coupling between them. The
observationthatsomeagentsbehavebetterthanothersleadsnaturallytotheideaofarational
agent—one that behaves as well as possible. How well an agent can behave depends on
the nature of the environment; some environments are more difficult than others. We give a
crude categorization of environments and show how properties of an environment influence
thedesignofsuitableagentsforthatenvironment. Wedescribe anumberofbasic“skeleton”
agentdesigns, whichwefleshoutintherestofthebook.
2.1 AGENTS AND ENVIRONMENTS
Anagentisanything thatcanbeviewedasperceiving itsenvironmentthrough sensors and
ENVIRONMENT
actinguponthatenvironmentthrough actuators. ThissimpleideaisillustratedinFigure2.1.
SENSOR
Ahumanagenthaseyes,ears,andotherorgansforsensorsandhands,legs,vocaltract,andso
ACTUATOR
on for actuators. A robotic agent might have cameras and infrared range finders for sensors
and various motors for actuators. A software agent receives keystrokes, file contents, and
network packets as sensory inputs and acts on the environment by displaying on the screen,
writingfiles,andsending networkpackets.
Weusethetermpercepttorefertotheagent’sperceptualinputsatanygiveninstant. An
PERCEPT
agent’sperceptsequenceisthecompletehistoryofeverythingtheagenthaseverperceived.
PERCEPTSEQUENCE
Ingeneral, an agent’s choice ofaction atany given instant candepend on theentire percept
sequenceobservedtodate,butnotonanything ithasn’tperceived. Byspecifying theagent’s
choice of action for every possible percept sequence, we have said more or less everything
34
Section2.1. AgentsandEnvironments 35
Agent
Sensors
Actuators
Environment
Percepts
?
Actions
Figure2.1 Agentsinteractwithenvironmentsthroughsensorsandactuators.
there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is
described bytheagentfunctionthatmapsanygivenperceptsequence toanaction.
AGENTFUNCTION
Wecan imagine tabulating the agent function that describes any given agent; formost
agents, this would be a very large table—infinite, in fact, unless we place a bound on the
lengthofperceptsequenceswewanttoconsider. Givenanagenttoexperimentwith,wecan,
in principle, construct this table by trying out all possible percept sequences and recording
whichactionstheagentdoesinresponse.1 Thetableis,ofcourse,anexternalcharacterization
of the agent. Internally, the agent function for an artificial agent will be implemented by an
agent program. It is important to keep these two ideas distinct. The agent function is an
AGENTPROGRAM
abstract mathematical description; the agent program is a concrete implementation, running
withinsomephysicalsystem.
To illustrate these ideas, we use a very simple example—the vacuum-cleaner world
shown in Figure 2.2. This world is so simple that we can describe everything that happens;
it’salsoamade-upworld,sowecaninventmanyvariations. Thisparticularworldhasjusttwo
locations: squares A and B. The vacuum agent perceives which square it is in and whether
there is dirt in the square. It can choose to move left, move right, suck up the dirt, or do
nothing. One very simple agent function is the following: if the current square is dirty, then
suck;otherwise,movetotheothersquare. Apartialtabulationofthisagentfunctionisshown
inFigure2.3andanagentprogram thatimplementsitappears inFigure2.8onpage48.
LookingatFigure2.3,weseethatvarious vacuum-world agentscanbedefinedsimply
byfillingintheright-handcolumninvariousways. Theobviousquestion,then,isthis: What
is the right way to fill out the table? In other words, what makes an agent good or bad,
intelligent orstupid? Weanswerthesequestions inthenextsection.
1 Iftheagentusessomerandomizationtochoose itsactions, thenwewouldhavetotryeachsequence many
timestoidentifytheprobabilityofeachaction. Onemightimaginethatactingrandomlyisrathersilly,butwe
showlaterinthischapterthatitcanbeveryintelligent.
36 Chapter 2. Intelligent Agents
A B
Figure2.2 Avacuum-cleanerworldwithjusttwolocations.
Perceptsequence Action
[A,Clean] Right
[A,Dirty] Suck
[B,Clean] Left
[B,Dirty] Suck
[A,Clean],[A,Clean] Right
[A,Clean],[A,Dirty] Suck
. .
. .
. .
[A,Clean],[A,Clean],[A,Clean] Right
[A,Clean],[A,Clean],[A,Dirty] Suck
. .
. .
. .
Figure 2.3 Partial tabulation of a simple agent function for the vacuum-cleaner world
showninFigure2.2.
Beforeclosingthissection,weshouldemphasizethatthenotionofanagentismeantto
be a tool for analyzing systems, not an absolute characterization that divides the world into
agents and non-agents. One could view a hand-held calculator as an agent that chooses the
action of displaying “4” when given the percept sequence “2 + 2 =,” but such an analysis
wouldhardly aidourunderstanding ofthecalculator. Inasense, allareasofengineering can
be seen as designing artifacts that interact with the world; AI operates at (what the authors
consider to be) the most interesting end of the spectrum, where the artifacts have significant
computational resources andthetaskenvironment requires nontrivial decision making.
2.2 GOOD BEHAVIOR: THE CONCEPT OF RATIONALITY
Arational agent is one that does the right thing—conceptually speaking, every entry in the
RATIONALAGENT
table for the agent function is filled out correctly. Obviously, doing the right thing is better
thandoingthewrongthing,butwhatdoesitmeantodotherightthing?
Section2.2. GoodBehavior: TheConceptofRationality 37
We answer this age-old question in an age-old way: by considering the consequences
of the agent’s behavior. When an agent is plunked down in an environment, it generates a
sequenceofactionsaccordingtotheperceptsitreceives. Thissequenceofactionscausesthe
environment to go through a sequence of states. If the sequence is desirable, then the agent
PERFORMANCE has performed well. This notion of desirability is captured by aperformance measure that
MEASURE
evaluatesanygivensequence ofenvironment states.
Notice that we said environment states, not agent states. If we define success in terms
of agent’s opinion ofits own performance, an agent could achieve perfect rationality simply
bydeluding itselfthatitsperformance wasperfect. Humanagentsinparticular arenotorious
for “sour grapes”—believing they did not really want something (e.g., a Nobel Prize) after
notgettingit.
Obviously,thereisnotonefixedperformancemeasureforalltasksandagents;typically,
adesigner willdevise one appropriate tothe circumstances. Thisis notas easy asit sounds.
Consider, for example, the vacuum-cleaner agent from the preceding section. We might
proposetomeasureperformancebytheamountofdirtcleaned upinasingleeight-hourshift.
With a rational agent, of course, what you ask for is what you get. A rational agent can
maximizethisperformance measurebycleaning upthedirt,thendumpingitallonthefloor,
thencleaningitupagain,andsoon. Amoresuitableperformancemeasurewouldrewardthe
agentforhavingacleanfloor. Forexample,onepointcouldbeawardedforeachcleansquare
at each time step (perhaps with a penalty for electricity consumed and noise generated). As
a general rule, it is better to design performance measures according to what one actually
wantsintheenvironment, ratherthanaccording tohowonethinkstheagentshouldbehave.
Evenwhentheobviouspitfallsareavoided,thereremainsomeknottyissuestountangle.
For example, the notion of “clean floor” in the preceding paragraph is based on average
cleanliness over time. Yet the same average cleanliness can be achieved by two different
agents,oneofwhichdoesamediocrejoballthetimewhiletheothercleansenergetically but
takeslongbreaks. Whichispreferablemightseemtobeafinepointofjanitorialscience, but
in fact it is a deep philosophical question with far-reaching implications. Which is better—
a reckless life of highs and lows, or a safe but humdrum existence? Which is better—an
economy where everyone lives in moderate poverty, or one in which some live in plenty
whileothersareverypoor? Weleavethesequestions asanexerciseforthediligentreader.
2.2.1 Rationality
Whatisrationalatanygiventimedepends onfourthings:
• Theperformance measurethatdefinesthecriterionofsuccess.
• Theagent’spriorknowledge oftheenvironment.
• Theactionsthattheagentcanperform.
• Theagent’sperceptsequence todate.
DEFINITIONOFA Thisleadstoadefinitionofarationalagent:
RATIONALAGENT
Foreachpossible perceptsequence, a rationalagentshouldselect anaction thatis ex-
pectedtomaximizeitsperformancemeasure,giventheevidenceprovidedbythepercept
sequenceandwhateverbuilt-inknowledgetheagenthas.
38 Chapter 2. Intelligent Agents
Considerthesimplevacuum-cleaner agent thatcleans asquare ifitisdirty andmovestothe
othersquare ifnot; thisistheagentfunction tabulated inFigure2.3. Isthisarational agent?
That depends! First, we need to say what the performance measure is, what is known about
theenvironment, andwhatsensorsandactuators theagenthas. Letusassumethefollowing:
• The performance measure awards one point for each clean square at each time step,
overa“lifetime”of1000timesteps.
• The “geography” of the environment is known a priori (Figure 2.2) but the dirt distri-
butionandtheinitiallocationoftheagentarenot. Cleansquaresstaycleanandsucking
cleans the current square. The Left and Right actions move the agent left and right
exceptwhenthiswouldtaketheagentoutsidetheenvironment, inwhichcasetheagent
remainswhereitis.
• Theonlyavailable actionsareLeft,Right,andSuck.
• Theagentcorrectly perceivesitslocation andwhetherthat location contains dirt.
We claim that under these circumstances the agent is indeed rational; its expected perfor-
manceisatleastashighasanyotheragent’s. Exercise2.2asksyoutoprovethis.
One can see easily that the same agent would be irrational under different circum-
stances. Forexample, once allthedirtiscleaned up, theagent willoscillate needlessly back
andforth;iftheperformancemeasureincludesapenaltyofonepointforeachmovementleft
or right, the agent will fare poorly. A better agent for this case would do nothing once it is
sure that allthe squares are clean. Ifclean squares can become dirty again, the agent should
occasionally check and re-clean them if needed. If the geography of the environment is un-
known, the agent will need to explore it rather than stick to squares A and B. Exercise 2.2
asksyoutodesignagentsforthesecases.
2.2.2 Omniscience, learning, andautonomy
We need to be careful to distinguish between rationality and omniscience. An omniscient
OMNISCIENCE
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs
Elyse´es one day and I see an old friend across the street. There is no traffic nearby and I’m
not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000
feet, a cargo door falls off a passing airliner,2 and before I make it to the other side of the
streetIamflattened. WasIirrationaltocrossthestreet? Itisunlikelythatmyobituarywould
read“Idiotattemptstocrossstreet.”
This example shows that rationality is not the same as perfection. Rationality max-
imizes expected performance, while perfection maximizes actual performance. Retreating
from a requirement of perfection is not just a question of being fair to agents. The point is
that if we expect an agent to do what turns out to be the best action after the fact, it will be
impossibletodesignanagenttofulfillthisspecification—unlessweimprovetheperformance
ofcrystal ballsortimemachines.
2 SeeN.Henderson,“NewdoorlatchesurgedforBoeing747jumbojets,”WashingtonPost,August24,1989.
Section2.2. GoodBehavior: TheConceptofRationality 39
Our definition of rationality does not require omniscience, then, because the rational
choice depends only on the percept sequence to date. We must also ensure that we haven’t
inadvertently allowedtheagenttoengageindecidedly underintelligent activities. Forexam-
ple,ifanagentdoesnotlookbothwaysbeforecrossingabusyroad,thenitsperceptsequence
will not tell it that there is a large truck approaching at high speed. Does our definition of
rationality saythatit’snowOK tocross theroad? Farfromit! First,itwould notberational
tocross the road given thisuninformative percept sequence: the riskofaccident from cross-
ingwithoutlookingistoogreat. Second,arationalagentshouldchoosethe“looking” action
before stepping into the street, because looking helps maximize the expected performance.
Doing actions in order to modify future percepts—sometimes called information gather-
INFORMATION ing—is an important part of rationality and is covered in depth in Chapter 16. A second
GATHERING
exampleofinformationgathering isprovidedbythe exploration thatmustbeundertaken by
EXPLORATION
avacuum-cleaning agentinaninitially unknownenvironment.
Ourdefinition requires arational agentnotonlytogatherinformation butalsotolearn
LEARNING
as much as possible from what it perceives. The agent’s initial configuration could reflect
some prior knowledge of the environment, but as the agent gains experience this may be
modified and augmented. There are extreme cases in which the environment is completely
known a priori. In such cases, the agent need not perceive orlearn; it simply acts correctly.
Ofcourse,suchagentsarefragile. Considerthelowlydungbeetle. Afterdiggingitsnestand
layingitseggs,itfetchesaballofdungfromanearbyheaptoplugtheentrance. Iftheballof
dung isremoved from itsgrasp en route, the beetle continues its task and pantomimes plug-
ging the nest withthe nonexistent dung ball, never noticing that it is missing. Evolution has
builtanassumption intothebeetle’sbehavior, andwhenitisviolated, unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go
out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is
well,dragthecaterpillarinside,andlayitseggs. Thecaterpillarservesasafoodsourcewhen
the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches
away while the sphex isdoing the check, it willrevert to the “drag” step ofits plan and will
continuetheplanwithoutmodification,evenafterdozensofcaterpillar-movinginterventions.
Thesphexisunabletolearnthatitsinnateplanisfailing, andthuswillnotchange it.
To the extent that an agent relies on the prior knowledge of its designer rather than
AUTONOMY on its own percepts, we say that the agent lacks autonomy. A rational agent should be
autonomous—it should learn what it can to compensate forpartial or incorrect prior knowl-
edge. Forexample,avacuum-cleaningagentthatlearnstoforeseewhereandwhenadditional
dirt will appear will do better than one that does not. As a practical matter, one seldom re-
quires complete autonomy from the start: when the agent has had little or no experience, it
would have to act randomly unless the designer gave some assistance. So, just as evolution
providesanimalswithenoughbuilt-inreflexestosurvivelongenoughtolearnforthemselves,
it would be reasonable to provide an artificial intelligent agent with some initial knowledge
as well as an ability to learn. After sufficient experience of its environment, the behavior
of a rational agent can become effectively independent of its prior knowledge. Hence, the
incorporation of learning allows one to design a single rational agent that will succeed in a
vastvarietyofenvironments.
40 Chapter 2. Intelligent Agents
2.3 THE NATURE OF ENVIRONMENTS
Now that we have a definition of rationality, we are almost ready to think about building
rational agents. First, however, we must think about task environments, which are essen-
TASKENVIRONMENT
tiallythe“problems” towhichrational agentsarethe“solutions.” Webeginbyshowinghow
to specify a task environment, illustrating the process with a number of examples. We then
showthattask environments comeinavarietyofflavors. Theflavorofthetaskenvironment
directlyaffectstheappropriate designfortheagentprogram.
2.3.1 Specifying thetaskenvironment
In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify
theperformance measure, theenvironment, and theagent’s actuators and sensors. Wegroup
alltheseundertheheading ofthetaskenvironment. Fortheacronymically minded, wecall
thisthePEAS(Performance, Environment, Actuators, Sensors)description. Indesigning an
PEAS
agent,thefirststepmustalwaysbetospecify thetaskenvironment asfullyaspossible.
Thevacuum worldwasasimpleexample;letusconsider amorecomplexproblem: an
automated taxi driver. We should point out, before the reader becomes alarmed, that a fully
automatedtaxiiscurrentlysomewhatbeyondthecapabilitiesofexistingtechnology. (page28
describes anexisting driving robot.) Thefulldriving task isextremely open-ended. Thereis
nolimittothenovelcombinations ofcircumstances thatcanarise—another reasonwechose
it as a focus for discussion. Figure 2.4 summarizes the PEAS description for the taxi’s task
environment. Wediscuss eachelementinmoredetailinthefollowingparagraphs.
AgentType Performance Environment Actuators Sensors
Measure
Taxidriver Safe,fast,legal, Roads,other Steering, Cameras,sonar,
comfortabletrip, traffic, accelerator, speedometer,
maximizeprofits pedestrians, brake,signal, GPS,odometer,
customers horn,display accelerometer,
enginesensors,
keyboard
Figure2.4 PEASdescriptionofthetaskenvironmentforanautomatedtaxi.
First, what isthe performance measure towhich wewould like ourautomated driver
toaspire? Desirablequalities includegetting tothecorrect destination; minimizingfuelcon-
sumptionandwearandtear;minimizingthetriptimeorcost; minimizingviolationsoftraffic
laws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz-
ingprofits. Obviously, someofthesegoalsconflict, sotradeoffs willberequired.
Next, what is the driving environment that the taxi will face? Any taxi driver must
deal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.
The roads contain other traffic, pedestrians, stray animals, road works, police cars, puddles,
Section2.3. TheNatureofEnvironments 41
andpotholes. Thetaximustalsointeractwithpotentialandactualpassengers. Therearealso
some optional choices. The taxi might need to operate in Southern California, where snow
is seldom aproblem, orin Alaska, where it seldom isnot. Itcould always be driving on the
right, orwemightwantittobeflexibleenough todriveontheleftwheninBritain orJapan.
Obviously, themorerestricted theenvironment, theeasier thedesignproblem.
Theactuatorsforanautomatedtaxiincludethoseavailable toahumandriver: control
overtheengine through theaccelerator andcontrol oversteering andbraking. Inaddition, it
will need output to a display screen or voice synthesizer to talk back to the passengers, and
perhapssomewaytocommunicatewithothervehicles, politely orotherwise.
The basic sensors for the taxi will include one or more controllable video cameras so
that it can see the road; it might augment these with infrared or sonar sensors to detect dis-
tancestoothercarsandobstacles. Toavoidspeedingtickets,thetaxishouldhaveaspeedome-
ter,andtocontrolthevehicleproperly, especiallyoncurves,itshouldhaveanaccelerometer.
Todetermine themechanical state ofthevehicle, itwillneed theusual array ofengine, fuel,
and electrical system sensors. Like many human drivers, it might want a global positioning
system (GPS) so that it doesn’t get lost. Finally, it will need a keyboard or microphone for
thepassengertorequest adestination.
In Figure 2.5, we have sketched the basic PEAS elements for a number of additional
agenttypes. FurtherexamplesappearinExercise2.4. Itmaycomeasasurprisetosomeread-
ers that our list of agent types includes some programs that operate in the entirely artificial
environmentdefinedbykeyboardinputandcharacteroutputonascreen. “Surely,”onemight
say,“thisisnotarealenvironment, isit?” Infact,whatmattersisnotthedistinction between
“real”and“artificial”environments, butthecomplexityof therelationship amongthebehav-
ior of the agent, the percept sequence generated by the environment, and the performance
measure. Some“real”environments areactuallyquitesimple. Forexample,arobotdesigned
toinspectpartsastheycomebyonaconveyorbeltcanmakeuse ofanumberofsimplifying
assumptions: that the lighting is always just so, that the only thing onthe conveyor belt will
bepartsofakindthatitknowsabout,andthatonlytwoactions(acceptorreject)arepossible.
Incontrast, somesoftwareagents(orsoftwarerobotsor softbots)existinrich,unlim-
SOFTWAREAGENT
iteddomains. ImagineasoftbotWebsiteoperatordesignedtoscanInternetnewssourcesand
SOFTBOT
show the interesting items to its users, while selling advertising space to generate revenue.
To do well, that operator will need some natural language processing abilities, it will need
to learn what each user and advertiser is interested in, and it will need to change its plans
dynamically—for example, when the connection for one news source goes down or when a
new one comes online. The Internet is an environment whose complexity rivals that of the
physicalworldandwhoseinhabitants includemanyartificialandhumanagents.
2.3.2 Properties oftaskenvironments
The range of task environments that might arise in AI is obviously vast. We can, however,
identify a fairly small number of dimensions along which task environments can be catego-
rized. These dimensions determine, to a large extent, the appropriate agent design and the
applicability of each of the principal families of techniques foragent implementation. First,
42 Chapter 2. Intelligent Agents
AgentType Performance Environment Actuators Sensors
Measure
Medical Healthypatient, Patient,hospital, Displayof Keyboardentry
diagnosissystem reducedcosts staff questions,tests, ofsymptoms,
diagnoses, findings,patient’s
treatments, answers
referrals
Satelliteimage Correctimage Downlinkfrom Displayofscene Colorpixel
analysissystem categorization orbitingsatellite categorization arrays
Part-picking Percentageof Conveyorbelt Jointedarmand Camera,joint
robot partsincorrect withparts;bins hand anglesensors
bins
Refinery Purity,yield, Refinery, Valves,pumps, Temperature,
controller safety operators heaters,displays pressure,
chemicalsensors
Interactive Student’sscore Setofstudents, Displayof Keyboardentry
Englishtutor ontest testingagency exercises,
suggestions,
corrections
Figure2.5 ExamplesofagenttypesandtheirPEASdescriptions.
welistthedimensions, thenweanalyzeseveraltaskenvironments toillustrate theideas. The
definitionshereareinformal;laterchaptersprovidemoreprecisestatementsandexamplesof
eachkindofenvironment.
Fully observable vs. partially observable: If an agent’s sensors give it access to the
FULLYOBSERVABLE
PARTIALLY complete state of the environment at each point in time, then we say that the task environ-
OBSERVABLE
ment is fully observable. A task environment is effectively fully observable if the sensors
detect all aspects that are relevant tothe choice of action; relevance, inturn, depends on the
performance measure. Fullyobservable environments areconvenient because theagentneed
notmaintainanyinternal statetokeeptrackoftheworld. An environment mightbepartially
observable because of noisy and inaccurate sensors or because parts of the state are simply
missing from the sensor data—for example, a vacuum agent with only a local dirt sensor
cannottellwhetherthereisdirtinothersquares,andanautomatedtaxicannotseewhatother
drivers are thinking. If the agent has no sensors at all then the environment is unobserv-
able. One might think that in such cases the agent’s plight is hopeless, but, as wediscuss in
UNOBSERVABLE
Chapter4,theagent’s goalsmaystillbeachievable, sometimeswithcertainty.
Singleagent vs. multiagent: Thedistinction between single-agent andmultiagent en-
SINGLEAGENT
MULTIAGENT
Section2.3. TheNatureofEnvironments 43
vironments may seem simple enough. Forexample, anagent solving acrossword puzzle by
itself is clearly in a single-agent environment, whereas an agent playing chess is in a two-
agentenvironment. Thereare,however, somesubtle issues. First,wehavedescribed howan
entity may be viewed as an agent, but we have not explained which entities must be viewed
as agents. Does an agent A (the taxi driver for example) have to treat an object B (another
vehicle)asanagent,orcanitbetreatedmerelyasanobjectbehavingaccordingtothelawsof
physics, analogous towavesatthebeach orleaves blowing inthewind? Thekeydistinction
iswhetherB’sbehaviorisbestdescribedasmaximizingaperformancemeasurewhosevalue
depends on agent A’s behavior. For example, in chess, the opponent entity B is trying to
maximize its performance measure, which, by the rules of chess, minimizes agent A’s per-
formancemeasure. Thus,chessisa competitivemultiagent environment. Inthetaxi-driving
COMPETITIVE
environment, on the other hand, avoiding collisions maximizes the performance measure of
all agents, so it is a partially cooperative multiagent environment. It is also partially com-
COOPERATIVE
petitive because, for example, only one car can occupy a parking space. The agent-design
problems inmultiagent environments areoftenquite different fromthose insingle-agent en-
vironments;forexample, communicationoftenemergesasarationalbehaviorinmultiagent
environments; insomecompetitive environments, randomizedbehaviorisrational because
itavoids thepitfallsofpredictability.
Deterministic vs. stochastic. If the next state of the environment is completely deter-
DETERMINISTIC
minedbythecurrentstateandtheactionexecutedbytheagent,thenwesaytheenvironment
STOCHASTIC
isdeterministic;otherwise,itisstochastic. Inprinciple,anagentneednotworryaboutuncer-
tainty in a fully observable, deterministic environment. (In our definition, we ignore uncer-
tainty that arises purely from the actions of other agents in a multiagent environment; thus,
a game can be deterministic even though each agent may be unable to predict the actions of
the others.) If the environment is partially observable, however, then it could appear to be
stochastic. Most real situations are so complex that it is impossible to keep track of all the
unobserved aspects;forpracticalpurposes, theymustbetreatedasstochastic. Taxidrivingis
clearly stochastic inthissense, because one canneverpredict thebehavior oftrafficexactly;
moreover, one’s tires blow out and one’s engine seizes up without warning. The vacuum
worldaswedescribed itisdeterministic, butvariations caninclude stochastic elementssuch
asrandomly appearing dirtandanunreliable suction mechanism (Exercise 2.13). Wesayan
environment is uncertain if it is not fully observable or not deterministic. One final note:
UNCERTAIN
our use of the word “stochastic” generally implies that uncertainty about outcomes is quan-
tified in terms of probabilities; a nondeterministic environment is one in which actions are
NONDETERMINISTIC
characterized by their possible outcomes, but no probabilities are attached to them. Nonde-
terministic environment descriptions are usually associated with performance measures that
requiretheagenttosucceed forallpossible outcomesofitsactions.
Episodic vs. sequential: In an episodic task environment, the agent’s experience is
EPISODIC
dividedintoatomicepisodes. Ineachepisodetheagentreceivesaperceptandthenperforms
SEQUENTIAL
asingle action. Crucially, the next episode does not depend on theactions taken inprevious
episodes. Many classification tasks are episodic. For example, an agent that has to spot
defective parts on an assembly line bases each decision on the current part, regardless of
previous decisions; moreover, the current decision doesn’t affect whether the next part is
44 Chapter 2. Intelligent Agents
defective. In sequential environments, on the other hand, the current decision could affect
allfuture decisions.3 Chessand taxidriving aresequential: inboth cases, short-term actions
can have long-term consequences. Episodic environments are much simpler than sequential
environments becausetheagentdoesnotneedtothinkahead.
Staticvs.dynamic: Iftheenvironment canchangewhileanagentisdeliberating, then
STATIC
wesaytheenvironment isdynamic forthatagent;otherwise, itisstatic. Staticenvironments
DYNAMIC
areeasytodealwithbecausetheagentneednotkeeplookingattheworldwhileitisdeciding
on an action, nor need it worry about the passage of time. Dynamic environments, on the
other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet,
that counts as deciding to do nothing. If the environment itself does not change with the
passage of time but the agent’s performance score does, then we say the environment is
semidynamic. Taxidrivingisclearlydynamic: theothercarsandthetaxi itselfkeepmoving
SEMIDYNAMIC
whilethe driving algorithm dithers about whattodonext. Chess, whenplayed withaclock,
issemidynamic. Crosswordpuzzlesarestatic.
Discretevs.continuous: Thediscrete/continuous distinction applies tothestateofthe
DISCRETE
environment, to the way time is handled, and to the percepts and actions of the agent. For
CONTINUOUS
example, the chess environment has a finite number of distinct states (excluding the clock).
Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and
continuous-time problem: the speed and location ofthe taxi and ofthe other vehicles sweep
through arangeofcontinuous values anddososmoothlyovertime. Taxi-driving actions are
also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak-
ing,butistypically treatedasrepresenting continuously varyingintensities andlocations.
Knownvs. unknown: Strictly speaking, this distinction refers not to the environment
KNOWN
itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of
UNKNOWN
the environment. In a known environment, the outcomes (or outcome probabilities if the
environmentisstochastic)forallactionsaregiven. Obviously,iftheenvironmentisunknown,
the agent will have to learn how it works in order to make good decisions. Note that the
distinction between known and unknown environments is not the same as the one between
fully and partially observable environments. It is quite possible for a known environment
to be partially observable—for example, in solitaire card games, I know the rules but am
still unable to see the cards that have not yet been turned over. Conversely, an unknown
environment can be fully observable—in a new video game, the screen may show the entire
gamestatebutIstilldon’tknowwhatthebuttonsdountilItrythem.
As one might expect, the hardest case is partially observable, multiagent, stochastic,
sequential,dynamic,continuous,andunknown. Taxidrivingishardinallthesesenses,except
thatforthemostpartthedriver’senvironmentisknown. Drivingarentedcarinanewcountry
withunfamiliargeography andtrafficlawsisalotmoreexciting.
Figure 2.6 lists the properties of a number of familiar environments. Note that the
answers are not always cut and dried. For example, we describe the part-picking robot as
episodic, because itnormally considers each part in isolation. Butif oneday there isalarge
3 Theword“sequential”isalsousedincomputerscienceastheantonymof“parallel.” Thetwomeaningsare
largelyunrelated.
Section2.3. TheNatureofEnvironments 45
TaskEnvironment Observable Agents Deterministic Episodic Static Discrete
Crosswordpuzzle Fully Single Deterministic Sequential Static Discrete
Chesswithaclock Fully Multi Deterministic Sequential Semi Discrete
Poker Partially Multi Stochastic Sequential Static Discrete
Backgammon Fully Multi Stochastic Sequential Static Discrete
Taxidriving Partially Multi Stochastic Sequential Dynamic Continuous
Medicaldiagnosis Partially Single Stochastic Sequential Dynamic Continuous
Imageanalysis Fully Single Deterministic Episodic Semi Continuous
Part-pickingrobot Partially Single Stochastic Episodic Dynamic Continuous
Refinerycontroller Partially Single Stochastic Sequential Dynamic Continuous
InteractiveEnglishtutor Partially Multi Stochastic Sequential Dynamic Discrete
Figure2.6 Examplesoftaskenvironmentsandtheircharacteristics.
batchofdefectiveparts,therobotshouldlearnfromseveralobservations thatthedistribution
of defects has changed, and should modify its behavior for subsequent parts. We have not
includeda“known/unknown”columnbecause,asexplainedearlier,thisisnotstrictlyaprop-
erty of the environment. Forsome environments, such aschess and poker, it isquite easy to
supplytheagentwithfullknowledgeoftherules,butitisnonetheless interesting toconsider
howanagentmightlearntoplaythesegameswithoutsuchknowledge.
Severaloftheanswersinthetabledepend onhowthetaskenvironment isdefined. We
havelistedthemedical-diagnosis taskassingle-agentbecausethediseaseprocessinapatient
is not profitably modeled as an agent; but a medical-diagnosis system might also have to
dealwithrecalcitrantpatientsandskepticalstaff,sotheenvironmentcouldhaveamultiagent
aspect. Furthermore, medicaldiagnosis isepisodic ifoneconceives ofthetaskasselecting a
diagnosisgivenalistofsymptoms;theproblemissequentialifthetaskcanincludeproposing
a series of tests, evaluating progress over the course of treatment, and so on. Also, many
environments are episodic at higher levels than the agent’s individual actions. For example,
a chess tournament consists of a sequence of games; each game is an episode because (by
and large) the contribution of the moves in one game to the agent’s overall performance is
notaffected bythemovesinitsprevious game. Ontheotherhand, decision makingwithina
singlegameiscertainly sequential.
The code repository associated with this book (aima.cs.berkeley.edu) includes imple-
mentationsofanumberofenvironments, togetherwithageneral-purpose environmentsimu-
latorthatplacesoneormoreagentsinasimulatedenvironment, observestheirbehaviorover
time, and evaluates them according to a given performance measure. Such experiments are
oftencarried outnotforasingle environment butformanyenvironments drawnfroman en-
ENVIRONMENT vironmentclass. Forexample,toevaluateataxidriverinsimulatedtraffic, wewouldwantto
CLASS
runmanysimulations withdifferent traffic, lighting, and weatherconditions. Ifwedesigned
the agent for a single scenario, we might be able to take advantage of specific properties
of the particular case but might not identify a good design for driving in general. For this
46 Chapter 2. Intelligent Agents
ENVIRONMENT reason, the code repository also includes an environment generator for each environment
GENERATOR
classthatselectsparticularenvironments (withcertainlikelihoods) inwhichtoruntheagent.
Forexample,thevacuumenvironmentgeneratorinitializes thedirtpatternandagentlocation
randomly. We are then interested in the agent’s average performance over the environment
class. A rational agent for a given environment class maximizes this average performance.
Exercises 2.8 to 2.13 take you through the process of developing an environment class and
evaluating variousagentstherein.
2.4 THE STRUCTURE OF AGENTS
Sofarwehavetalkedaboutagentsbydescribing behavior—theactionthatisperformedafter
any given sequence ofpercepts. Nowwemust bite the bullet and talk about how theinsides
work. The job of AI is to design an agent program that implements the agent function—
AGENTPROGRAM
the mapping from percepts to actions. We assume this program will run on some sort of
computing devicewithphysicalsensorsandactuators—we callthisthearchitecture:
ARCHITECTURE
agent = architecture+program .
Obviously,theprogramwechoosehastobeonethatisappropriateforthearchitecture. Ifthe
program isgoing torecommend actions like Walk,thearchitecture hadbetterhave legs. The
architecture might be just an ordinary PC, or it might be a robotic car with several onboard
computers, cameras, and other sensors. In general, the architecture makes the percepts from
thesensorsavailabletotheprogram,runstheprogram,andfeedstheprogram’sactionchoices
to the actuators asthey are generated. Most ofthis book is about designing agent programs,
although Chapters24and25dealdirectly withthesensorsandactuators.
2.4.1 Agentprograms
The agent programs that we design in this book all have the same skeleton: they take the
current percept as input from the sensors and return an action to the actuators.4 Notice the
differencebetweentheagentprogram,whichtakesthecurrentperceptasinput,andtheagent
function, which takes the entire percept history. The agent program takes just the current
perceptasinputbecausenothingmoreisavailablefromtheenvironment;iftheagent’sactions
needtodependontheentireperceptsequence, theagentwillhavetorememberthepercepts.
We describe the agent programs in the simple pseudocode language that is defined in
Appendix B. (The online code repository contains implementations in real programming
languages.) Forexample, Figure 2.7shows arathertrivial agent program thatkeeps track of
the percept sequence and then uses it to index into a table of actions to decide what to do.
The table—an example of which is given for the vacuum world in Figure 2.3—represents
explicitly the agent function that the agent program embodies. To build a rational agent in
4 Thereareotherchoices fortheagent program skeleton; forexample, wecouldhavetheagent programsbe
coroutinesthatrunasynchronouslywiththeenvironment. Eachsuchcoroutinehasaninputandoutputportand
consistsofaloopthatreadstheinputportforperceptsandwritesactionstotheoutputport.
Section2.4. TheStructureofAgents 47
functionTABLE-DRIVEN-AGENT(percept)returnsanaction
persistent: percepts,asequence,initiallyempty
table,atableofactions,indexedbyperceptsequences,initiallyfullyspecified
appendpercept totheendofpercepts
action←LOOKUP(percepts,table)
returnaction
Figure 2.7 The TABLE-DRIVEN-AGENT program is invoked for each new percept and
returnsanactioneachtime. Itretainsthecompleteperceptsequenceinmemory.
thisway,weasdesignersmustconstructatablethatcontainstheappropriateactionforevery
possible perceptsequence.
It is instructive to consider why the table-driven approach to agent construction is
doomed to failure. Let P be the set of possible percepts and let T be the lifet(cid:2)ime of the
agent(thetotalnumberofperceptsitwillreceive). Thelookuptablewillcontain T |P|t
t=1
entries. Consider the automated taxi: the visual input from a single camera comes in at the
rate of roughly 27 megabytes per second (30 frames per second, 640×480 pixels with 24
bitsofcolorinformation). Thisgivesalookup table withover10250,000,000,000 entries foran
hour’s driving. Even the lookup table for chess—a tiny, well-behaved fragment of the real
world—would have at least 10150 entries. The daunting size of these tables (the number of
atoms in the observable universe is less than 1080) means that (a) no physical agent in this
universe willhavethespacetostorethetable, (b)thedesignerwouldnothavetimetocreate
the table, (c)no agent could everlearn allthe right table entries from itsexperience, and (d)
even if the environment is simple enough to yield a feasible table size, the designer still has
noguidance abouthowtofillinthetableentries.
Despite all this, TABLE-DRIVEN-AGENT does do what we want: it implements the
desired agent function. The key challenge for AI is to find out how to write programs that,
to the extent possible, produce rational behavior from a smallish program rather than from
a vast table. We have many examples showing that this can be done successfully in other
areas: forexample,thehugetablesofsquarerootsusedbyengineersandschoolchildrenprior
to the 1970s have now been replaced by a five-line program for Newton’s method running
on electronic calculators. The question is, can AI do for general intelligent behavior what
Newtondidforsquareroots? Webelievetheanswerisyes.
In the remainder of this section, we outline four basic kinds of agent programs that
embodytheprinciples underlying almostallintelligent systems:
• Simplereflexagents;
• Model-based reflexagents;
• Goal-basedagents; and
• Utility-based agents.
Each kind of agent program combines particular components in particular ways to generate
actions. Section2.4.6explains ingeneral termshowtoconvert alltheseagents into learning
48 Chapter 2. Intelligent Agents
functionREFLEX-VACUUM-AGENT([location,status])returnsanaction
ifstatus =Dirty thenreturnSuck
elseiflocation =AthenreturnRight
elseiflocation =B thenreturnLeft
Figure2.8 Theagentprogramforasimplereflexagentinthetwo-statevacuumenviron-
ment.ThisprogramimplementstheagentfunctiontabulatedinFigure2.3.
agentsthatcanimprovetheperformanceoftheircomponentssoasto generatebetteractions.
Finally, Section2.4.7describes thevariety ofwaysinwhichthecomponents themselves can
be represented within the agent. This variety provides a major organizing principle for the
fieldandforthebookitself.
2.4.2 Simplereflex agents
SIMPLEREFLEX Thesimplestkindofagentisthesimplereflexagent. Theseagentsselectactionsonthebasis
AGENT
ofthecurrentpercept,ignoringtherestofthepercepthistory. Forexample,thevacuumagent
whose agent function is tabulated in Figure 2.3is asimple reflexagent, because its decision
is based only on the current location and on whether that location contains dirt. An agent
program forthisagentisshowninFigure2.8.
Noticethatthevacuumagentprogramisverysmallindeedcomparedtothecorrespond-
ing table. The most obvious reduction comes from ignoring the percept history, which cuts
down the number of possibilities from 4T to just 4. A further, small reduction comes from
thefactthatwhenthecurrent squareisdirty,theactiondoesnotdepend onthelocation.
Simple reflex behaviors occur even in more complex environments. Imagine yourself
asthedriveroftheautomatedtaxi. Ifthecarinfrontbrakes anditsbrakelightscomeon,then
you should notice this and initiate braking. In other words, some processing is done on the
visualinputtoestablishtheconditionwecall“Thecarinfrontisbraking.” Then,thistriggers
some established connection in the agent program to the action “initiate braking.” We call
CONDITION–ACTION suchaconnection acondition–action rule,5 writtenas
RULE
ifcar-in-front-is-braking theninitiate-braking.
Humansalsohavemanysuchconnections, someofwhicharelearnedresponses (asfordriv-
ing)andsomeofwhichareinnatereflexes(suchasblinking whensomething approaches the
eye). In the course of the book, we show several different ways in which such connections
canbelearned andimplemented.
The program in Figure 2.8 is specific to one particular vacuum environment. A more
general and flexible approach is first to build a general-purpose interpreter for condition–
action rules and then to create rule sets for specific task environments. Figure 2.9 gives the
structure ofthisgeneral programinschematicform,showinghowthecondition–action rules
allow the agent to make the connection from percept to action. (Do not worry if this seems
5 Alsocalledsituation–actionrules,productions,orif–thenrules.
Section2.4. TheStructureofAgents 49
Agent
Environment
Sensors
What the world
is like now
What action I
Condition-action rules
should do now
Actuators
Figure2.9 Schematicdiagramofasimplereflexagent.
functionSIMPLE-REFLEX-AGENT(percept)returnsanaction
persistent: rules,asetofcondition–actionrules
state←INTERPRET-INPUT(percept)
rule←RULE-MATCH(state,rules)
action←rule.ACTION
returnaction
Figure 2.10 A simple reflex agent. It acts accordingto a rule whose conditionmatches
thecurrentstate,asdefinedbythepercept.
trivial;itgetsmoreinteresting shortly.) Weuserectangles todenotethecurrentinternalstate
of the agent’s decision process, and ovals to represent the background information used in
the process. The agent program, which is also very simple, is shown in Figure 2.10. The
INTERPRET-INPUT function generates anabstracted description ofthecurrentstatefromthe
percept, and the RULE-MATCH function returns the firstrule inthe setof rules that matches
the given state description. Note that the description in terms of “rules” and “matching” is
purely conceptual; actual implementations can be as simple as a collection of logic gates
implementing aBooleancircuit.
Simplereflexagentshavetheadmirablepropertyofbeingsimple,buttheyturnouttobe
oflimitedintelligence. TheagentinFigure2.10willworkonlyifthecorrectdecisioncanbe
madeonthebasisofonlythecurrentpercept—thatis,onlyiftheenvironmentisfullyobserv-
able. Evenalittle bitofunobservability cancause serious trouble. Forexample, thebraking
rule given earlier assumes that the condition car-in-front-is-braking can be determined from
the current percept—a single frame of video. This works if the car in front has a centrally
mounted brake light. Unfortunately, older models have different configurations of taillights,
50 Chapter 2. Intelligent Agents
brake lights, and turn-signal lights, and it is not always possible to tell from a single image
whetherthecarisbraking. Asimplereflexagentdrivingbehindsuchacarwouldeitherbrake
continuously andunnecessarily, or,worse,neverbrakeatall.
Wecanseeasimilarproblemarisinginthevacuumworld. Supposethatasimplereflex
vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent
hasjusttwopossiblepercepts: [Dirty]and[Clean]. ItcanSuck inresponseto[Dirty];what
shoulditdoinresponseto[Clean]? MovingLeft fails(forever)ifithappenstostartinsquare
A,andmovingRight fails(forever)ifithappenstostartinsquare B. Infiniteloopsareoften
unavoidable forsimplereflexagentsoperating inpartially observable environments.
Escape from infinite loops is possible if the agent can randomize its actions. For ex-
RANDOMIZATION
ample,ifthevacuumagentperceives[Clean],itmightflipacointochoosebetweenLeft and
Right. Itiseasytoshowthattheagentwillreachtheothersquareinanaverageoftwosteps.
Then, if that square is dirty, the agent will clean it and the task will be complete. Hence, a
randomized simplereflexagentmightoutperform adeterministic simplereflexagent.
WementionedinSection2.3thatrandomizedbehavioroftherightkindcanberational
insomemultiagentenvironments. Insingle-agentenvironments,randomizationisusuallynot
rational. It is a useful trick that helps a simple reflex agent in some situations, but in most
caseswecandomuchbetterwithmoresophisticated deterministic agents.
2.4.3 Model-based reflex agents
The most effective way to handle partial observability is for the agent to keep track of the
part of the world it can’t see now. That is, the agent should maintain some sort of internal
statethatdepends onthepercepthistoryandtherebyreflectsatleastsomeoftheunobserved
INTERNALSTATE
aspects ofthecurrentstate. Forthebraking problem, theinternal stateisnottooextensive—
just the previous frame from the camera, allowing the agent to detect when twored lights at
theedgeofthevehicle goonoroffsimultaneously. Forother driving taskssuchaschanging
lanes,theagentneedstokeeptrackofwheretheothercarsareifitcan’tseethemallatonce.
Andforanydrivingtobepossibleatall,theagentneedstokeeptrackofwhereitskeysare.
Updating this internal state information as time goes by requires two kinds of knowl-
edge to be encoded in the agent program. First, we need some information about how the
worldevolvesindependently oftheagent—forexample,thatanovertakingcargenerallywill
be closer behind than it was a moment ago. Second, we need some information about how
theagent’sownactionsaffecttheworld—forexample,thatwhentheagentturnsthesteering
wheel clockwise, the car turns to the right, or that after driving for five minutes northbound
onthefreeway,oneisusuallyaboutfivemilesnorthofwhereonewasfiveminutesago. This
knowledge about “how the world works”—whether implemented in simple Boolean circuits
orincomplete scientific theories—is called a modelofthe world. Anagent that uses such a
MODEL-BASED modeliscalledamodel-basedagent.
AGENT
Figure2.11givesthestructureofthemodel-basedreflexagentwithinternalstate,show-
ing how the current percept is combined with the old internal state to generate the updated
descriptionofthecurrentstate,basedontheagent’smodelofhowtheworldworks. Theagent
programisshowninFigure2.12. Theinterestingpartisthefunction UPDATE-STATE,which
Section2.4. TheStructureofAgents 51
Agent
Environment
Sensors
State
How the world evolves What the world
is like now
What my actions do
What action I
Condition-action rules
should do now
Actuators
Figure2.11 Amodel-basedreflexagent.
functionMODEL-BASED-REFLEX-AGENT(percept)returnsanaction
persistent: state,theagent’scurrentconceptionoftheworldstate
model,adescriptionofhowthenextstatedependsoncurrentstateandaction
rules,asetofcondition–actionrules
action,themostrecentaction,initiallynone
state←UPDATE-STATE(state,action,percept,model)
rule←RULE-MATCH(state,rules)
action←rule.ACTION
returnaction
Figure2.12 Amodel-basedreflexagent. Itkeepstrackofthecurrentstateoftheworld,
usinganinternalmodel.Itthenchoosesanactioninthesamewayasthereflexagent.
is responsible forcreating the new internal state description. Thedetails of how models and
states are represented vary widely depending on the type of environment and the particular
technology used in the agent design. Detailed examples of models and updating algorithms
appearinChapters4,12,11,15,17,and25.
Regardless of the kind of representation used, it is seldom possible for the agent to
determine the current state of a partially observable environment exactly. Instead, the box
labeled “what the world is like now” (Figure 2.11) represents the agent’s “best guess” (or
sometimes best guesses). Forexample, an automated taxi may not beable tosee around the
large truck that has stopped in front of it and can only guess about what may be causing the
hold-up. Thus,uncertainty aboutthecurrent statemaybeunavoidable, buttheagentstillhas
tomakeadecision.
A perhaps less obvious point about the internal “state” maintained by a model-based
agent is that it does not have to describe “what the world is like now” in a literal sense. For
52 Chapter 2. Intelligent Agents
Agent
Environment
Sensors
State
What the world
How the world evolves is like now
What it will be like
What my actions do if I do action A
What action I
Goals should do now
Actuators
Figure2.13 Amodel-based,goal-basedagent.Itkeepstrackoftheworldstateaswellas
asetofgoalsitistryingtoachieve,andchoosesanactionthatwill(eventually)leadtothe
achievementofitsgoals.
example, the taxi may be driving back home, and it may have a rule telling it to fill up with
gas on the way home unless it has at least half a tank. Although “driving back home” may
seem toan aspect of theworld state, the fact ofthe taxi’s destination isactually an aspect of
the agent’s internal state. Ifyou findthis puzzling, consider that the taxi could be in exactly
thesameplaceatthesametime,butintending toreachadifferent destination.
2.4.4 Goal-basedagents
Knowingsomethingaboutthecurrentstateoftheenvironmentisnotalwaysenoughtodecide
what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight
on. Thecorrectdecisiondependsonwherethetaxiistryingtogetto. Inotherwords,aswell
as a current state description, the agent needs some sort of goal information that describes
GOAL
situations that are desirable—for example, being at the passenger’s destination. The agent
program can combine this with the model (the same information as was used in the model-
basedreflexagent)tochooseactionsthatachievethegoal. Figure2.13showsthegoal-based
agent’sstructure.
Sometimesgoal-basedactionselectionisstraightforward—forexample,whengoalsat-
isfaction results immediately from a single action. Sometimes it will be more tricky—for
example,whentheagenthastoconsiderlongsequences oftwistsandturnsinordertofinda
waytoachievethegoal. Search(Chapters3to5)andplanning(Chapters10and11)arethe
subfieldsofAIdevoted tofindingactionsequences thatachievetheagent’sgoals.
Noticethatdecisionmakingofthiskindisfundamentally differentfromthecondition–
actionrulesdescribed earlier,inthatitinvolvesconsideration ofthefuture—both“Whatwill
happen ifIdosuch-and-such?” and“Willthatmakemehappy?” Inthereflexagentdesigns,
this information is not explicitly represented, because the built-in rules map directly from
Section2.4. TheStructureofAgents 53
percepts toactions. Thereflexagentbrakeswhenitseesbrakelights. Agoal-based agent, in
principle, couldreasonthatifthecarinfronthasitsbrake lightson,itwillslowdown. Given
the way the world usually evolves, the only action that will achieve the goal of not hitting
othercarsistobrake.
Although the goal-based agent appears less efficient, it is more flexible because the
knowledgethatsupportsitsdecisionsisrepresentedexplicitlyandcanbemodified. Ifitstarts
torain,theagentcanupdateitsknowledgeofhoweffectivelyitsbrakeswilloperate;thiswill
automatically cause all of the relevant behaviors to be altered to suit the new conditions.
For the reflex agent, on the other hand, we would have to rewrite many condition–action
rules. Thegoal-based agent’sbehaviorcaneasilybechanged togotoadifferentdestination,
simply by specifying that destination as the goal. The reflex agent’s rules for when to turn
and when to go straight will work only fora single destination; they must all be replaced to
gosomewherenew.
2.4.5 Utility-basedagents
Goals alone are not enough to generate high-quality behavior in most environments. For
example, many action sequences will get the taxi to its destination (thereby achieving the
goal) but somearequicker, safer, morereliable, orcheaper thanothers. Goalsjust provide a
crudebinarydistinctionbetween“happy”and“unhappy”states. Amoregeneralperformance
measureshouldallowacomparisonofdifferentworldstates accordingtoexactlyhowhappy
theywouldmaketheagent. Because“happy” doesnotsoundveryscientific, economists and
computerscientists usetheterm utilityinstead.6
UTILITY
Wehavealreadyseenthataperformancemeasureassignsascoretoanygivensequence
of environment states, so it can easily distinguish between more and less desirable ways of
getting to the taxi’s destination. An agent’s utility function is essentially an internalization
UTILITYFUNCTION
of the performance measure. If the internal utility function and the external performance
measure are in agreement, then an agent that chooses actions to maximize its utility will be
rationalaccording totheexternalperformance measure.
Let us emphasize again that this is not the only way to be rational—we have already
seen a rational agent program for the vacuum world (Figure 2.8) that has no idea what its
utility function is—but, likegoal-based agents, autility-based agent hasmanyadvantages in
termsofflexibilityandlearning. Furthermore,intwokinds ofcases,goalsareinadequatebut
autility-based agentcanstillmakerationaldecisions. First,whenthereareconflictinggoals,
only some of which can be achieved (for example, speed and safety), the utility function
specifies the appropriate tradeoff. Second, when there are several goals that the agent can
aim for, none of which can be achieved with certainty, utility provides a way in which the
likelihood ofsuccesscanbeweighedagainsttheimportance ofthegoals.
Partialobservabilityandstochasticityareubiquitousintherealworld,andso,therefore,
is decision making under uncertainty. Technically speaking, a rational utility-based agent
chooses the action that maximizes the expected utility of the action outcomes—that is, the
EXPECTEDUTILITY
utility the agent expects to derive, on average, given the probabilities and utilities of each
6 Theword“utility”hererefersto“thequalityofbeinguseful,”nottotheelectriccompanyorwaterworks.
54 Chapter 2. Intelligent Agents
Agent
Environment
Sensors
State
What the world
How the world evolves is like now
What it will be like
What my actions do if I do action A
How happy I will be
Utility
in such a state
What action I
should do now
Actuators
Figure2.14 Amodel-based,utility-basedagent. Itusesamodeloftheworld,alongwith
autilityfunctionthatmeasuresitspreferencesamongstatesoftheworld.Thenitchoosesthe
actionthatleadstothebestexpectedutility,whereexpectedutilityiscomputedbyaveraging
overallpossibleoutcomestates,weightedbytheprobabilityoftheoutcome.
outcome. (AppendixAdefinesexpectation moreprecisely.) InChapter16,weshowthatany
rational agent must behave as if it possesses a utility function whose expected value it tries
tomaximize. Anagentthatpossesses anexplicit utility function canmakerational decisions
with a general-purpose algorithm that does not depend on the specific utility function being
maximized. In this way, the “global” definition of rationality—designating as rational those
agent functions that have the highest performance—is turned into a “local” constraint on
rational-agent designsthatcanbeexpressedinasimpleprogram.
Theutility-based agent structure appears in Figure 2.14. Utility-based agent programs
appearinPartIV,wherewedesign decision-making agents thatmusthandle theuncertainty
inherent instochastic orpartially observable environments.
Atthispoint,thereadermaybewondering, “Isitthatsimple? Wejustbuildagentsthat
maximize expected utility, and we’re done?” It’s true that such agents would be intelligent,
but it’s not simple. A utility-based agent has to model and keep track of its environment,
tasks that have involved a great deal of research on perception, representation, reasoning,
and learning. The results of this research fill many of the chapters of this book. Choosing
theutility-maximizing courseofactionisalsoadifficulttask,requiringingeniousalgorithms
that fill several more chapters. Even with these algorithms, perfect rationality is usually
unachievable inpractice becauseofcomputational complexity, aswenotedinChapter1.
2.4.6 Learning agents
We have described agent programs with various methods for selecting actions. We have
not, so far, explained how the agent programs come into being. In his famous early paper,
Turing (1950) considers the idea of actually programming his intelligent machines by hand.
Section2.4. TheStructureofAgents 55
Performance standard
Agent
Environment
Critic Sensors
feedback
changes
Learning Performance
element element
knowledge
learning
goals
Problem
generator
Actuators
Figure2.15 Agenerallearningagent.
Heestimateshowmuchworkthismighttakeandconcludes“Somemoreexpeditiousmethod
seems desirable.” The method he proposes is to build learning machines and then to teach
them. In many areas of AI, this is now the preferred method for creating state-of-the-art
systems. Learning has another advantage, as wenoted earlier: it allows the agent to operate
ininitially unknown environments andtobecomemorecompetent thanitsinitial knowledge
alone might allow. In this section, we briefly introduce the main ideas of learning agents.
Throughout the book, we comment on opportunities and methods for learning in particular
kindsofagents. PartVgoesintomuchmoredepthonthelearningalgorithms themselves.
A learning agent can be divided into four conceptual components, as shown in Fig-
ure 2.15. The most important distinction is between the learning element, which is re-
LEARNINGELEMENT
PERFORMANCE sponsibleformakingimprovements,andtheperformanceelement,whichisresponsiblefor
ELEMENT
selecting external actions. The performance element is what we have previously considered
tobetheentire agent: ittakes inpercepts and decides onactions. Thelearning element uses
feedback from the critic on how the agent is doing and determines how the performance
CRITIC
elementshouldbemodifiedtodobetterinthefuture.
Thedesignofthelearningelementdependsverymuchonthedesignoftheperformance
element. When trying to design an agent that learns a certain capability, the first question is
not“HowamIgoingtogetittolearnthis?” but“Whatkindofperformanceelementwillmy
agentneedtodothisonceithaslearned how?” Givenanagentdesign, learningmechanisms
canbeconstructed toimproveeverypartoftheagent.
Thecritic tellsthe learning element how welltheagent isdoing withrespect toafixed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agent’s success. For example, a chess program could receive a percept
indicating that it has checkmated its opponent, but it needs a performance standard to know
thatthisisagoodthing;theperceptitselfdoesnotsayso. Itisimportantthattheperformance
56 Chapter 2. Intelligent Agents
standard be fixed. Conceptually, one should think of it as being outside the agent altogether
becausetheagentmustnotmodifyittofititsownbehavior.
PROBLEM The last component of the learning agent is the problem generator. It is responsible
GENERATOR
for suggesting actions that will lead to new and informative experiences. The point is that
if the performance element had its way, it would keep doing the actions that are best, given
whatitknows. Butiftheagent iswillingtoexplore alittle anddosomeperhaps suboptimal
actions inthe short run, itmight discover muchbetteractions forthelong run. Theproblem
generator’s job is to suggest these exploratory actions. This is what scientists do when they
carry out experiments. Galileo did not think that dropping rocks from the top of a tower in
Pisa was valuable in itself. He was not trying to break the rocks or to modify the brains of
unfortunate passers-by. His aim was to modify his own brain by identifying a better theory
ofthemotionofobjects.
Tomake theoverall design moreconcrete, letusreturn tothe automated taxi example.
The performance element consists of whatever collection of knowledge and procedures the
taxi has for selecting its driving actions. The taxi goes out on the road and drives, using
thisperformance element. Thecritic observes theworldand passes information along tothe
learningelement. Forexample,afterthetaximakesaquickleftturnacrossthreelanesoftraf-
fic,thecriticobservestheshockinglanguageusedbyotherdrivers. Fromthisexperience, the
learningelementisabletoformulatearulesayingthiswasabadaction,andtheperformance
element is modified by installation of the new rule. The problem generator might identify
certainareasofbehaviorinneedofimprovementandsuggest experiments,suchastryingout
thebrakesondifferentroadsurfaces underdifferentconditions.
Thelearning elementcanmakechanges toanyofthe“knowledge” components shown
intheagentdiagrams(Figures2.9,2.11,2.13,and2.14). Thesimplestcasesinvolvelearning
directly from the percept sequence. Observation of pairs of successive states of the environ-
mentcanallow theagent tolearn “Howtheworldevolves,” and observation oftheresults of
itsactions canallowtheagenttolearn“Whatmyactions do.” Forexample, ifthetaxiexerts
a certain braking pressure when driving on a wet road, then it will soon find out how much
deceleration is actually achieved. Clearly, these two learning tasks are more difficult if the
environment isonlypartially observable.
The forms of learning in the preceding paragraph do not need to access the external
performance standard—in a sense, the standard is the universal one of making predictions
that agree with experiment. The situation is slightly more complex for a utility-based agent
thatwishestolearnutilityinformation. Forexample,suppose thetaxi-driving agentreceives
no tips from passengers who have been thoroughly shaken up during the trip. The external
performance standard mustinform theagentthatthelossoftipsisanegativecontribution to
its overall performance; then the agent might be able to learn that violent maneuvers do not
contribute to its own utility. In a sense, the performance standard distinguishes part of the
incomingperceptasareward(orpenalty)thatprovidesdirectfeedbackonthequalityofthe
agent’s behavior. Hard-wired performance standards such aspainandhungerinanimals can
beunderstood inthisway. Thisissueisdiscussed furtherin Chapter21.
Insummary, agents haveavarietyofcomponents, andthose components canberepre-
sented in many ways within the agent program, so there appears to be great variety among
Section2.4. TheStructureofAgents 57
learning methods. Thereis, however, asingle unifying theme. Learning inintelligent agents
canbesummarized asaprocess ofmodification ofeachcomponent oftheagenttobring the
components into closer agreement withthe available feedback information, thereby improv-
ingtheoverallperformance oftheagent.
2.4.7 Howthe components ofagentprograms work
Wehavedescribedagentprograms(inveryhigh-levelterms) asconsistingofvariouscompo-
nents,whosefunctionitistoanswerquestionssuchas: “Whatistheworldlikenow?” “What
action should I do now?” “What do my actions do?” The next question for a student of AI
is, “How on earth do these components work?” It takes about a thousand pages to begin to
answerthat question properly, but here wewantto draw thereader’s attention to somebasic
distinctions among thevarious waysthatthecomponents can represent theenvironment that
theagentinhabits.
Roughly speaking, we can place the representations along an axis of increasing com-
plexity and expressive power—atomic, factored, and structured. To illustrate these ideas,
it helps to consider a particular agent component, such as the one that deals with “What my
actions do.” This component describes the changes that might occur in the environment as
the result of taking an action, and Figure 2.16 provides schematic depictions of how those
transitions mightberepresented.
B C
B C
(a) Atomic (b) Factored (b) Structured
Figure2.16 Threewaystorepresentstatesandthetransitionsbetweenthem. (a)Atomic
representation:astate(suchasBorC)isablackboxwithnointernalstructure;(b)Factored
representation: a state consists of a vectorof attribute values; valuescan be Boolean, real-
valued, or one of a fixed set of symbols. (c) Structured representation: a state includes
objects,eachofwhichmayhaveattributesofitsownaswellasrelationshipstootherobjects.
ATOMIC In an atomic representation each state of the world is indivisible—it has no internal
REPRESENTATION
structure. Consider the problem of finding a driving route from one end of a country to the
otherviasomesequenceofcities(weaddressthisprobleminFigure3.2onpage68). Forthe
purposes ofsolving this problem, itmaysufficetoreduce the stateofworldtojustthename
of the city we are in—a single atom of knowledge; a “black box” whose only discernible
property is that of being identical to or different from another black box. The algorithms
58 Chapter 2. Intelligent Agents
underlyingsearchandgame-playing(Chapters3–5),HiddenMarkovmodels(Chapter15),
and Markov decision processes (Chapter 17) all work with atomic representations—or, at
least,theytreatrepresentations asiftheywereatomic.
Nowconsider a higher-fidelity description for the same problem, where weneed to be
concerned with more than just atomic location in one city oranother; wemight need to pay
attention tohowmuchgasisinthetank, ourcurrent GPScoordinates, whetherornottheoil
warning light isworking, howmuch spare change wehave fortoll crossings, whatstation is
FACTORED on the radio, and so on. A factored representation splits up each state into a fixed set of
REPRESENTATION
variables or attributes, each of which can have a value. While two different atomic states
VARIABLE
have nothing in common—they are just different black boxes—two different factored states
ATTRIBUTE
cansharesomeattributes(suchasbeingatsomeparticularGPSlocation)andnotothers(such
VALUE
as having lots of gas or having no gas); this makes it much easier to work out how to turn
onestateintoanother. Withfactored representations, wecanalsorepresent uncertainty—for
example, ignorance about the amount of gas in the tank can be represented by leaving that
attribute blank. Manyimportant areasofAIarebased onfactored representations, including
constraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning
(Chapters 10 and 11), Bayesian networks (Chapters 13–16), and the machine learning al-
gorithmsinChapters18,20,and21.
For many purposes, we need to understand the world as having things in it that are
related to each other, not just variables with values. For example, we might notice that a
largetruck aheadofusisreversing intothedrivewayofadairy farmbutacowhasgotloose
and is blocking the truck’s path. A factored representation is unlikely to be pre-equipped
withtheattributeTruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with
STRUCTURED value true or false. Instead, we would need a structured representation, in which ob-
REPRESENTATION
jects such as cows and trucks and their various and varying relationships can be described
explicitly. (See Figure 2.16(c).) Structured representations underlie relational databases
and first-order logic (Chapters 8, 9, and 12), first-order probability models (Chapter 14),
knowledge-based learning (Chapter 19) and much of natural language understanding
(Chapters 22 and 23). In fact, almost everything that humans express in natural language
concerns objectsandtheirrelationships.
As we mentioned earlier, the axis along which atomic, factored, and structured repre-
sentations lieistheaxis ofincreasing expressiveness. Roughly speaking, amoreexpressive
EXPRESSIVENESS
representation cancapture,atleastasconcisely, everythingalessexpressiveonecancapture,
plussomemore. Often,themoreexpressivelanguageismuchmoreconcise;forexample,the
rules of chess can be written in a page or two of a structured-representation language such
as first-order logic but require thousands of pages when written in a factored-representation
language such as propositional logic. On the other hand, reasoning and learning become
more complex as the expressive power of the representation increases. To gain the benefits
ofexpressive representations whileavoiding theirdrawbacks, intelligent systems forthereal
worldmayneedtooperateatallpointsalongtheaxissimultaneously.
Section2.5. Summary 59
2.5 SUMMARY
This chapter has been something of a whirlwind tour of AI, which we have conceived of as
thescienceofagentdesign. Themajorpointstorecallareas follows:
• Anagent issomething that perceives and acts in anenvironment. The agent function
foranagentspecifiestheactiontakenbytheagentinresponsetoanyperceptsequence.
• The performance measure evaluates the behavior of the agent in an environment. A
rational agentactssoastomaximize theexpected valueoftheperformance measure,
giventhepercept sequence ithasseensofar.
• A task environment specification includes the performance measure, the external en-
vironment, the actuators, and the sensors. In designing an agent, the first step must
alwaysbetospecifythetaskenvironment asfullyaspossible.
• Task environments vary along several significant dimensions. They can be fully or
partiallyobservable, single-agent ormultiagent, deterministic orstochastic, episodicor
sequential, staticordynamic,discrete orcontinuous, and knownorunknown.
• The agent program implements the agent function. There exists a variety of basic
agent-program designs reflecting thekindofinformation madeexplicit andusedinthe
decision process. The designs vary in efficiency, compactness, and flexibility. The
appropriate designoftheagentprogram depends onthenatureoftheenvironment.
• Simplereflexagentsresponddirectlytopercepts, whereas model-basedreflexagents
maintain internal state to track aspects of the world that are not evident in the current
percept. Goal-based agents act to achieve their goals, and utility-based agents try to
maximizetheirownexpected “happiness.”
• Allagentscanimprovetheirperformance through learning.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The central role of action in intelligence—the notion of practical reasoning—goes back at
least as far as Aristotle’s Nicomachean Ethics. Practical reasoning was also the subject of
McCarthy’s(1958)influential paper“ProgramswithCommonSense.” Thefieldsofrobotics
andcontrol theory are, bytheirverynature, concerned principally withphysical agents. The
concept of a controller in control theory is identical to that of an agent in AI. Perhaps sur-
CONTROLLER
prisingly, AI has concentrated for most of its history on isolated components of agents—
question-answering systems, theorem-provers, vision systems, and so on—rather than on
wholeagents. Thediscussion ofagents inthetextbyGenesereth andNilsson(1987) wasan
influentialexception. Thewhole-agentviewisnowwidelyacceptedandisacentralthemein
recenttexts(Pooleetal.,1998;Nilsson,1998;PadghamandWinikoff, 2004;Jones,2007).
Chapter1tracedtherootsoftheconceptofrationalityinphilosophyandeconomics. In
AI,theconceptwasofperipheralinterestuntilthemid-1980s,whenitbegantosuffusemany
60 Chapter 2. Intelligent Agents
discussions aboutthepropertechnical foundations ofthefield. ApaperbyJonDoyle(1983)
predicted that rational agent design would come to be seen as the core mission of AI, while
otherpopulartopicswouldspinofftoformnewdisciplines.
Careful attention to the properties of the environment and their consequences for ra-
tional agent design is most apparent in the control theory tradition—for example, classical
control systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic
environments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,
2007) handles partially observable, stochastic environments; and hybrid control (Henzinger
and Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both
discrete andcontinuous elements. Thedistinction between fully andpartially observable en-
vironments is also central in the dynamic programming literature developed in the field of
operations research (Puterman,1994), whichwediscuss inChapter17.
Reflex agents were the primary model for psychological behaviorists such as Skinner
(1953),whoattemptedtoreducethepsychologyoforganismsstrictlytoinput/outputorstim-
ulus/response mappings. The advance from behaviorism to functionalism in psychology,
which wasatleast partly driven bythe application ofthe computer metaphortoagents (Put-
nam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture. Most
work in AI views the idea of pure reflex agents with state as too simple to provide much
leverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption
(see Chapter 25). In recent years, a great deal of work has gone into finding efficient algo-
rithmsforkeepingtrackofcomplexenvironments(Hamscheretal.,1992;Simon,2006). The
RemoteAgentprogram(describedonpage28)thatcontrolled theDeepSpaceOnespacecraft
isaparticularly impressiveexample(Muscettola etal.,1998;Jonssonetal.,2000).
Goal-basedagentsarepresupposedineverythingfromAristotle’sviewofpracticalrea-
soning to McCarthy’s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,
1971; Nilsson, 1984) was the first robotic embodiment of a logical, goal-based agent. A
full logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and a
goal-basedprogrammingmethodologycalledagent-orientedprogrammingwasdevelopedby
Shoham (1993). The agent-based approach is now extremely popular in software engineer-
ing (Ciancarini and Wooldridge, 2001). It has also infiltrated the area of operating systems,
AUTONOMIC whereautonomiccomputingreferstocomputersystemsandnetworksthatmonitorandcon-
COMPUTING
trolthemselves withaperceive–act loopandmachinelearning methods(KephartandChess,
2003). Noting that a collection of agent programs designed to work well together in a true
multiagentenvironmentnecessarilyexhibitsmodularity—theprogramssharenointernalstate
and communicate with each other only through the environment—it is common within the
MULTIAGENT field of multiagent systems to design the agent program of asingle agent as a collection of
SYSTEMS
autonomous sub-agents. In some cases, one can even prove that the resulting system gives
thesameoptimalsolutions asamonolithicdesign.
Thegoal-based viewofagentsalsodominatesthecognitivepsychology traditioninthe
area of problem solving, beginning with the enormously influential Human Problem Solv-
ing(NewellandSimon,1972)andrunningthroughallofNewell’slaterwork(Newell,1990).
Goals, furtheranalyzed as desires (general) and intentions (currently pursued), arecentral to
the theory of agents developed by Bratman (1987). This theory has been influential both in
Exercises 61
naturallanguage understanding andmultiagent systems.
Horvitz et al. (1988) specifically suggest the use of rationality conceived as the maxi-
mization of expected utility as a basis forAI. Thetext by Pearl (1988) was the first in AI to
coverprobabilityandutilitytheoryindepth;itsexpositionofpracticalmethodsforreasoning
and decision making under uncertainty was probably the single biggest factor in the rapid
shifttowardsutility-based agentsinthe1990s(seePartIV).
ThegeneraldesignforlearningagentsportrayedinFigure2.15isclassicinthemachine
learning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as em-
bodiedinprograms,gobackatleastasfarasArthurSamuel’s (1959,1967)learningprogram
forplayingcheckers. Learningagentsarediscussed indepthinPartV.
Interestinagentsandinagentdesignhasrisenrapidlyinrecentyears,partlybecauseof
thegrowth ofthe Internet and theperceived needforautomated and mobilesoftbot (Etzioni
and Weld, 1994). Relevant papers are collected in Readings in Agents (Huhns and Singh,
1998)andFoundationsofRationalAgency(WooldridgeandRao,1999). Textsonmultiagent
systems usually provide agoodintroduction tomanyaspects ofagent design (Weiss, 2000a;
Wooldridge,2002). Severalconferenceseriesdevotedtoagentsbeganinthe1990s,including
the International Workshop on Agent Theories, Architectures, and Languages (ATAL), the
International Conference on Autonomous Agents (AGENTS), and the International Confer-
enceonMulti-AgentSystems(ICMAS).In2002,thesethreemergedtoformtheInternational
Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS).Thejournal
Autonomous Agents and Multi-Agent Systems was founded in 1998. Finally, Dung Beetle
Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the
behaviorofdungbeetles. YouTubefeatures inspiring video recordings oftheiractivities.
EXERCISES
2.1 Suppose that the performance measure is concerned with just the first T time steps of
the environment and ignores everything thereafter. Show that a rational agent’s action may
dependnotjustonthestateoftheenvironment butalsoonthetimestepithasreached.
2.2 Letusexaminetherationality ofvariousvacuum-cleaner agentfunctions.
a. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed
rationalundertheassumptions listedonpage38.
b. Describearationalagentfunctionforthecaseinwhicheachmovementcostsonepoint.
Doesthecorresponding agentprogram requireinternal state?
c. Discuss possible agent designs for the cases in which clean squares can become dirty
andthegeography oftheenvironment isunknown. Doesitmake sensefortheagentto
learnfromitsexperience inthesecases? Ifso,whatshoulditlearn? Ifnot,whynot?
2.3 For each of the following assertions, say whether it is true or false and support your
answerwithexamplesorcounterexamples whereappropriate.
a. Anagentthatsensesonlypartialinformationaboutthestatecannotbeperfectlyrational.
62 Chapter 2. Intelligent Agents
b. Thereexisttaskenvironments inwhichnopurereflexagentcanbehaverationally.
c. Thereexistsataskenvironment inwhicheveryagentisrational.
d. Theinputtoanagentprogram isthesameastheinputtotheagentfunction.
e. Everyagentfunctionisimplementable bysomeprogram/machine combination.
f. Supposeanagentselectsitsactionuniformlyatrandomfromthesetofpossibleactions.
Thereexistsadeterministic taskenvironment inwhichthis agentisrational.
g. Itispossibleforagivenagenttobeperfectlyrationalintwodistincttaskenvironments.
h. Everyagentisrationalinanunobservable environment.
i. Aperfectly rationalpoker-playing agentneverloses.
2.4 For each of the following activities, give a PEAS description of the task environment
andcharacterize itintermsoftheproperties listedinSection2.3.2.
• Playingsoccer.
• Exploringthesubsurface oceansofTitan.
• ShoppingforusedAIbooksontheInternet.
• Playingatennismatch.
• Practicingtennisagainstawall.
• Performingahighjump.
• Knittingasweater.
• Biddingonanitematanauction.
2.5 Define in your own words the following terms: agent, agent function, agent program,
rationality, autonomy,reflexagent,model-based agent,goal-based agent,utility-based agent,
learning agent.
2.6 Thisexerciseexploresthedifferences betweenagentfunctions andagentprograms.
a. Can there be more than one agent program that implements a given agent function?
Giveanexample,orshowwhyoneisnotpossible.
b. Arethereagentfunctions thatcannotbeimplemented byanyagentprogram?
c. Given a fixed machine architecture, does each agent program implement exactly one
agentfunction?
d. Given an architecture with n bits of storage, how many different possible agent pro-
gramsarethere?
e. Supposewekeeptheagentprogram fixedbutspeedupthemachinebyafactoroftwo.
Doesthatchangetheagentfunction?
2.7 Writepseudocode agentprogramsforthegoal-based andutility-based agents.
The following exercises all concern the implementation of environments and agents for the
vacuum-cleaner world.
Exercises 63
2.8 Implement a performance-measuring environment simulator for the vacuum-cleaner
worlddepictedinFigure2.2andspecifiedonpage38. Yourimplementationshouldbemodu-
larsothatthesensors,actuators,andenvironmentcharacteristics(size,shape,dirtplacement,
etc.) canbechangedeasily. (Note: forsomechoicesofprogramminglanguageandoperating
systemtherearealready implementations intheonlinecode repository.)
2.9 Implement a simple reflex agent for the vacuum environment in Exercise 2.8. Run the
environment with this agent for all possible initial dirt configurations and agent locations.
Recordtheperformance scoreforeachconfiguration andtheoverallaveragescore.
2.10 Consideramodifiedversionofthevacuum environment inExercise 2.8,inwhichthe
agentispenalized onepointforeachmovement.
a. Canasimplereflexagentbeperfectlyrational forthisenvironment? Explain.
b. Whataboutareflexagentwithstate? Designsuchanagent.
c. How do your answers to a and b change if the agent’s percepts give it the clean/dirty
statusofeverysquareintheenvironment?
2.11 Consideramodifiedversionofthevacuum environment inExercise 2.8,inwhichthe
geography of the environment—its extent, boundaries, and obstacles—is unknown, asis the
initialdirtconfiguration. (TheagentcangoUp andDown aswellasLeft andRight.)
a. Canasimplereflexagentbeperfectlyrational forthisenvironment? Explain.
b. Canasimplereflexagentwitharandomizedagentfunction outperform asimplereflex
agent? Designsuchanagentandmeasureitsperformance onseveralenvironments.
c. Canyou design an environment in which yourrandomized agent willperform poorly?
Showyourresults.
d. Can a reflex agent with state outperform a simple reflex agent? Design such an agent
andmeasureitsperformance onseveralenvironments. Canyoudesignarational agent
ofthistype?
2.12 Repeat Exercise 2.11 for the case in which the location sensor is replaced with a
“bump” sensor that detects the agent’s attempts to move into an obstacle or to cross the
boundaries of the environment. Suppose the bump sensor stops working; how should the
agentbehave?
2.13 Thevacuumenvironmentsintheprecedingexerciseshaveallbeendeterministic. Dis-
cusspossible agentprogramsforeachofthefollowingstochastic versions:
a. Murphy’slaw: twenty-fivepercentofthetime,theSuck actionfailstocleanthefloorif
itisdirtyanddepositsdirtontothefloorifthefloorisclean. Howisyouragentprogram
affectedifthedirtsensorgivesthewronganswer10%ofthetime?
b. Small children: At each time step, each clean square has a 10% chance of becoming
dirty. Canyoucomeupwitharational agentdesignforthiscase?
3
SOLVING PROBLEMS BY
SEARCHING
In which we see how an agent can find a sequence of actions that achieves its
goalswhennosingleactionwilldo.
ThesimplestagentsdiscussedinChapter2werethereflexagents,whichbasetheiractionson
adirectmappingfromstatestoactions. Suchagentscannotoperate wellinenvironments for
whichthismappingwouldbetoolargetostoreandwouldtaketoolongtolearn. Goal-based
agents, ontheotherhand,considerfutureactions andthedesirability oftheiroutcomes.
PROBLEM-SOLVING This chapter describes one kind of goal-based agent called a problem-solving agent.
AGENT
Problem-solving agents use atomic representations, as described in Section 2.4.7—that is,
statesoftheworldareconsideredaswholes,withnointernalstructurevisibletotheproblem-
solving algorithms. Goal-based agents that use more advanced factored or structured rep-
resentations areusuallycalled planningagentsandarediscussed inChapters7and10.
Ourdiscussionofproblemsolvingbeginswithprecisedefinitionsofproblemsandtheir
solutions and give several examples to illustrate these definitions. We then describe several
general-purpose search algorithms that can be used to solve these problems. We will see
several uninformed search algorithms—algorithms that are given no information about the
problem otherthan itsdefinition. Although some ofthese algorithms can solve anysolvable
problem, noneofthemcandosoefficiently. Informedsearchalgorithms, ontheotherhand,
candoquitewellgivensomeguidance onwheretolookforsolutions.
In this chapter, we limit ourselves to the simplest kind of task environment, for which
thesolutiontoaproblemisalwaysafixedsequenceofactions. Themoregeneralcase—where
theagent’sfutureactionsmayvarydepending onfuturepercepts—is handledinChapter4.
This chapter uses the concepts of asymptotic complexity (that is, O() notation) and
NP-completeness. Readersunfamiliarwiththeseconcepts shouldconsult AppendixA.
3.1 PROBLEM-SOLVING AGENTS
Intelligent agents are supposed to maximize their performance measure. As we mentioned
inChapter2, achieving this issometimes simplified iftheagent canadopt agoal andaim at
satisfying it. Letusfirstlookatwhyandhowanagentmightdothis.
64
Section3.1. Problem-Solving Agents 65
ImagineanagentinthecityofArad,Romania,enjoying atouringholiday. Theagent’s
performance measure contains manyfactors: itwants toimprove itssuntan, improve its Ro-
manian,takeinthesights,enjoythenightlife (suchasitis),avoidhangovers, andsoon. The
decision problem is a complex one involving many tradeoffs and careful reading of guide-
books. Now,suppose theagenthasanonrefundable tickettoflyoutofBucharest thefollow-
ing day. In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.
Courses of action that don’t reach Bucharest on timecan berejected without further consid-
eration and the agent’s decision problem is greatly simplified. Goals help organize behavior
by limiting the objectives that the agent is trying to achieve and hence the actions it needs
to consider. Goal formulation, based on the current situation and the agent’s performance
GOALFORMULATION
measure,isthefirststepinproblem solving.
We will consider a goal to be a set of world states—exactly those states in which the
goal is satisfied. The agent’s task is to find out how to act, now and in the future, so that it
reaches a goal state. Before it can do this, it needs to decide (or we need to decide on its
behalf) what sorts of actions and states it should consider. If it were to consider actions at
thelevelof“movetheleftfootforwardaninch”or“turnthesteering wheelonedegreeleft,”
the agent would probably never find its way out of the parking lot, let alone to Bucharest,
because at that level of detail there is too much uncertainty in the world and there would be
PROBLEM too many steps in a solution. Problem formulation is the process of deciding what actions
FORMULATION
andstatestoconsider, givenagoal. Wediscuss thisprocess inmoredetaillater. Fornow,let
usassume thattheagent willconsider actions atthe levelof driving from onemajortownto
another. Eachstatetherefore corresponds tobeinginaparticulartown.
Our agent has now adopted the goal of driving to Bucharest and is considering where
togofrom Arad. Threeroads lead outofArad, onetowardSibiu, onetoTimisoara, and one
toZerind. Noneoftheseachievesthegoal,sounlesstheagentisfamiliarwiththegeography
ofRomania, itwillnotknow whichroadtofollow.1 Inotherwords, theagent willnotknow
which of its possible actions is best, because it does not yet know enough about the state
that results from taking each action. If the agent has no additional information—i.e., if the
environment is unknowninthesense defined in Section 2.3—then itishas nochoice but to
tryoneoftheactionsatrandom. Thissadsituationisdiscussed inChapter4.
But suppose the agent has a map of Romania. The point of a map is to provide the
agentwithinformationaboutthestatesitmightgetitselfintoandtheactionsitcantake. The
agent can use this information to consider subsequent stages of a hypothetical journey via
eachofthethreetowns,tryingtofindajourneythateventuallygetstoBucharest. Onceithas
found a path on the map from Arad to Bucharest, it can achieve its goal by carrying out the
driving actions that correspond to the legs of the journey. In general, an agent with several
immediate options ofunknown valuecan decide whattodobyfirst examining future actions
thateventually leadtostatesofknownvalue.
To be more specific about what we mean by “examining future actions,” we have to
be more specific about properties of the environment, as defined in Section 2.3. For now,
1 Weareassumingthatmostreadersareinthesamepositionandcaneasilyimaginethemselvestobeasclueless
asouragent.WeapologizetoRomanianreaderswhoareunabletotakeadvantageofthispedagogicaldevice.
66 Chapter 3. SolvingProblemsbySearching
weassume that the environment is observable, so the agent always knows the current state.
Forthe agent driving in Romania, it’s reasonable to suppose that each city on the maphas a
sign indicating its presence to arriving drivers. Wealso assume the environment is discrete,
so at any given state there are only finitely many actions to choose from. This is true for
navigating in Romania because each city isconnected toa small numberof other cities. We
willassumetheenvironment isknown,sotheagentknowswhichstatesarereachedbyeach
action. (Having an accurate map suffices to meet this condition for navigation problems.)
Finally, we assume that the environment is deterministic, so each action has exactly one
outcome. Under ideal conditions, this is true for the agent in Romania—it means that if it
chooses to drive from Arad to Sibiu, it does end up in Sibiu. Of course, conditions are not
alwaysideal, asweshowinChapter4.
Under these assumptions, the solution to any problem is a fixed sequence of actions.
“Ofcourse!” onemightsay,“Whatelsecoulditbe?” Well,ingeneralitcouldbeabranching
strategy that recommends different actions in the future depending on what percepts arrive.
For example, under less than ideal conditions, the agent might plan to drive from Arad to
Sibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it
arrivesbyaccidentinZerindinstead ofSibiu. Fortunately, iftheagentknowstheinitialstate
and the environment is known and deterministic, it knows exactly where it will be after the
firstaction and whatitwillperceive. Sinceonlyonepercept ispossible afterthefirstaction,
thesolution canspecify onlyonepossible secondaction, andsoon.
Theprocess oflooking forasequence ofactions thatreaches thegoaliscalledsearch.
SEARCH
A search algorithm takes a problem as input and returns a solution in the form of an action
SOLUTION
sequence. Once a solution is found, the actions it recommends can be carried out. This
is called the execution phase. Thus, we have a simple “formulate, search, execute” design
EXECUTION
for the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve,
the agent calls a search procedure to solve it. It then uses the solution to guide its actions,
doingwhateverthesolutionrecommendsasthenextthingtodo—typically, thefirstactionof
the sequence—and then removing that step from the sequence. Once the solution has been
executed, theagentwillformulate anewgoal.
Notice that while the agent is executing the solution sequence it ignores its percepts
when choosing an action because it knows in advance what they will be. An agent that
carries out its plans with its eyes closed, so to speak, must be quite certain of what is going
on. Controltheorists callthisan open-loopsystem,because ignoringthepercepts breaksthe
OPEN-LOOP
loopbetweenagentandenvironment.
We first describe the process of problem formulation, and then devote the bulk of the
chapter to various algorithms for the SEARCH function. We do not discuss the workings of
the UPDATE-STATE and FORMULATE-GOAL functions furtherinthischapter.
3.1.1 Well-defined problems and solutions
Aproblemcanbedefinedformallybyfivecomponents:
PROBLEM
• The initial state that the agent starts in. For example, the initial state for our agent in
INITIALSTATE
Romaniamightbedescribed asIn(Arad).
Section3.1. Problem-Solving Agents 67
functionSIMPLE-PROBLEM-SOLVING-AGENT(percept)returnsanaction
persistent: seq,anactionsequence,initiallyempty
state,somedescriptionofthecurrentworldstate
goal,agoal,initiallynull
problem,aproblemformulation
state←UPDATE-STATE(state,percept)
ifseq isemptythen
goal←FORMULATE-GOAL(state)
problem←FORMULATE-PROBLEM(state,goal)
seq←SEARCH(problem)
ifseq =failure thenreturnanullaction
action←FIRST(seq)
seq←REST(seq)
returnaction
Figure 3.1 A simple problem-solving agent. It first formulates a goal and a problem,
searchesforasequenceofactionsthatwouldsolvetheproblem,andthenexecutestheactions
oneatatime. Whenthisiscomplete,itformulatesanothergoalandstartsover.
• A description of the possible actions available to the agent. Given a particular state s,
ACTIONS
ACTIONS(s) returns the set of actions that can be executed in s. We say that each of
these actions is applicable in s. Forexample, from the state In(Arad), the applicable
APPLICABLE
actionsare{Go(Sibiu),Go(Timisoara),Go(Zerind)}.
• A description of what each action does; the formal name for this is the transition
TRANSITIONMODEL
model, specified by a function RESULT(s,a) that returns the state that results from
doingactionainstates. Wealsousethetermsuccessortorefertoanystatereachable
SUCCESSOR
fromagivenstatebyasingleaction.2 Forexample,wehave
RESULT(In(Arad),Go(Zerind)) = In(Zerind).
Together,theinitialstate,actions,andtransitionmodelimplicitlydefinethestatespace
STATESPACE
of the problem—the set of all states reachable from the initial state by any sequence
of actions. The state space forms a directed network or graph in which the nodes
GRAPH
are states and the links between nodes are actions. (The map of Romania shown in
Figure 3.2 can be interpreted as a state-space graph if we view each road as standing
fortwodriving actions, one ineach direction.) A pathin the state space is asequence
PATH
ofstatesconnected byasequence ofactions.
• Thegoal test, whichdetermines whetheragiven stateisagoal state. Sometimesthere
GOALTEST
is an explicit set of possible goal states, and the test simply checks whether the given
stateisoneofthem. Theagent’sgoalinRomaniaisthesingletonset{In(Bucharest)}.
2 Manytreatmentsofproblemsolving,includingpreviouseditionsofthisbook,useasuccessorfunction,which
returnsthe set of allsuccessors, instead of separate ACTIONSand RESULTfunctions. The successor function
makesitdifficulttodescribeanagentthatknowswhatactionsitcantrybutnotwhattheyachieve. Also,note
someauthoruseRESULT(a,s)insteadofRESULT(s,a),andsomeuseDOinsteadofRESULT.
68 Chapter 3. SolvingProblemsbySearching
Oradea
71
Neamt
Zerind 87
151
75
Iasi
Arad
140
92
Sibiu Fagaras
99
118
Vaslui
80
Rimnicu Vilcea
Timisoara
142
111 Lugoj 97 Pitesti 211
70 98
Hirsova
Mehadia 146 101 85 Urziceni
86
75 138
Bucharest
Drobeta 120
90
Craiova Giurgiu Eforie
Figure3.2 AsimplifiedroadmapofpartofRomania.
Sometimesthegoalisspecifiedbyanabstractpropertyratherthananexplicitlyenumer-
atedsetofstates. Forexample,inchess,thegoalistoreach astatecalled“checkmate,”
wheretheopponent’s kingisunderattackandcan’tescape.
• A path cost function that assigns a numeric cost to each path. The problem-solving
PATHCOST
agent chooses acost function that reflects its ownperformance measure. Forthe agent
tryingtogettoBucharest,timeisoftheessence,sothecostofapathmightbeitslength
inkilometers. Inthischapter, weassumethatthecostofapathcanbedescribed asthe
sumofthecostsoftheindividualactionsalongthepath.3 Thestepcostoftakingaction
STEPCOST
(cid:2) (cid:2)
a in state s to reach state s is denoted by c(s,a,s). The step costs for Romania are
showninFigure3.2asroutedistances. Weassumethatstepcostsarenonnegative.4
The preceding elements define a problem and can be gathered into a single data structure
that is given as input to a problem-solving algorithm. A solution to a problem is an action
sequence that leads from the initial state to agoal state. Solution quality is measured by the
pathcostfunction, andanoptimalsolutionhasthelowestpathcostamongallsolutions.
OPTIMALSOLUTION
3.1.2 Formulatingproblems
Intheprecedingsectionweproposed aformulationoftheproblemofgettingtoBucharestin
termsoftheinitial state, actions, transition model, goal test, andpath cost. Thisformulation
seems reasonable, but it is still a model—an abstract mathematical description—and not the
3 Thisassumptionisalgorithmicallyconvenientbutalsotheoreticallyjustifiable—seepage649inChapter17.
4 TheimplicationsofnegativecostsareexploredinExercise3.8.
Section3.2. ExampleProblems 69
realthing. Comparethesimplestatedescriptionwehavechosen,In(Arad),toanactualcross-
countrytrip,wherethestateoftheworldincludessomanythings: thetravelingcompanions,
the current radio program, the scenery out of the window, the proximity of law enforcement
officers, the distance to the next rest stop, the condition of the road, the weather, and so on.
Alltheseconsiderations areleftoutofourstatedescriptionsbecausetheyareirrelevanttothe
problemoffindingaroutetoBucharest. Theprocessofremovingdetailfromarepresentation
iscalledabstraction.
ABSTRACTION
Inadditiontoabstractingthestatedescription, wemustabstracttheactionsthemselves.
A driving action has many effects. Besides changing the location of the vehicle and its oc-
cupants, ittakes uptime, consumes fuel, generates pollution, andchanges the agent (as they
say, travel is broadening). Our formulation takes into account only the change in location.
Also, there are many actions that we omit altogether: turning on the radio, looking out of
thewindow,slowing downforlawenforcement officers, andso on. Andofcourse, wedon’t
specifyactions atthelevelof“turnsteeringwheeltotheleftbyonedegree.”
Canwebemorepreciseaboutdefiningtheappropriatelevelofabstraction? Thinkofthe
abstract states and actions we have chosen as corresponding to large sets of detailed world
states and detailed action sequences. Now consider a solution to the abstract problem: for
example,thepathfromAradtoSibiutoRimnicuVilceatoPitestitoBucharest. Thisabstract
solution corresponds to alarge numberofmore detailed paths. Forexample, wecould drive
with the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest of
thetrip. Theabstraction is valid ifwecanexpand anyabstract solution intoasolution inthe
more detailed world; a sufficient condition is that for every detailed state that is “in Arad,”
there is a detailed path to some state that is “in Sibiu,” and so on.5 The abstraction is useful
if carrying out each of the actions in the solution is easier than the original problem; in this
case they are easy enough that they can be carried out without further search orplanning by
an average driving agent. The choice of a good abstraction thus involves removing as much
detail as possible while retaining validity and ensuring that the abstract actions are easy to
carryout. Wereitnotfortheabilitytoconstruct usefulabstractions, intelligent agentswould
becompletelyswampedbytherealworld.
3.2 EXAMPLE PROBLEMS
The problem-solving approach has been applied to a vast array of task environments. We
list some of the best known here, distinguishing between toy and real-world problems. A
toyproblem isintended toillustrate orexercise various problem-solving methods. Itcanbe
TOYPROBLEM
givenaconcise,exactdescriptionandhenceisusablebydifferentresearcherstocomparethe
REAL-WORLD performance of algorithms. A real-world problem is one whose solutions people actually
PROBLEM
careabout. Suchproblemstendnottohaveasingleagreed-upondescription, butwecangive
thegeneralflavoroftheirformulations.
5 SeeSection11.2foramorecompletesetofdefinitionsandalgorithms.
70 Chapter 3. SolvingProblemsbySearching
R
L R
L
S S
R R
L R L R
L L
S S
S S
R
L R
L
S S
Figure 3.3 The state space for the vacuum world. Links denote actions: L = Left, R =
Right,S=Suck.
3.2.1 Toy problems
The first example we examine is the vacuum world first introduced in Chapter 2. (See
Figure2.2.) Thiscanbeformulatedasaproblem asfollows:
• States: The state is determined by both the agent location and the dirt locations. The
agent is in one of two locations, each of which might or might not contain dirt. Thus,
there are 2×22 = 8 possible world states. A larger environment with n locations has
n·2n states.
• Initialstate: Anystatecanbedesignated astheinitialstate.
• Actions: In this simple environment, each state has just three actions: Left, Right, and
Suck. Largerenvironments mightalsoinclude UpandDown.
• Transition model: Theactions have their expected effects, except that moving Left in
theleftmostsquare,movingRightintherightmostsquare,andSuckinginacleansquare
havenoeffect. ThecompletestatespaceisshowninFigure3.3.
• Goaltest: Thischeckswhetherallthesquares areclean.
• Pathcost: Eachstepcosts1,sothepathcostisthenumberofstepsinthepath.
Compared with the real world, this toy problem has discrete locations, discrete dirt, reliable
cleaning, anditnevergetsanydirtier. Chapter4relaxessomeoftheseassumptions.
The8-puzzle,aninstanceofwhichisshowninFigure3.4,consistsofa3×3boardwith
8-PUZZLE
eight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the
space. Theobject istoreach aspecified goalstate, such astheoneshownontherightofthe
figure. Thestandard formulation isasfollows:
Section3.2. ExampleProblems 71
7 2 4 1 2
5 6 3 4 5
8 3 1 6 7 8
Start State Goal State
Figure3.4 Atypicalinstanceofthe8-puzzle.
• States: Astatedescription specifiesthelocationofeachoftheeighttilesandtheblank
inoneoftheninesquares.
• Initial state: Any state can be designated as the initial state. Note that any given goal
canbereachedfromexactlyhalfofthepossible initialstates(Exercise3.4).
• Actions: Thesimplestformulationdefinestheactionsasmovementsoftheblankspace
Left, Right, Up, or Down. Different subsets of these are possible depending on where
theblankis.
• Transitionmodel: Givenastateandaction,thisreturnstheresultingstate;forexample,
ifweapplyLefttothestartstateinFigure3.4,theresultingstatehasthe5andtheblank
switched.
• Goaltest: Thischecks whetherthe state matches thegoal configuration showninFig-
ure3.4. (Othergoalconfigurations arepossible.)
• Pathcost: Eachstepcosts1,sothepathcostisthenumberofstepsinthepath.
What abstractions have weincluded here? The actions are abstracted to their beginning and
finalstates,ignoringtheintermediatelocationswheretheblockissliding. Wehaveabstracted
away actions such as shaking the board when pieces get stuck and ruled out extracting the
pieceswithaknifeandputtingthembackagain. Weareleftwithadescriptionoftherulesof
thepuzzle, avoiding allthedetailsofphysicalmanipulations.
SLIDING-BLOCK The 8-puzzle belongs to the family of sliding-block puzzles, which are often used as
PUZZLES
test problems for new search algorithms in AI. This family is known to be NP-complete,
so one does not expect to find methods significantly better in the worst case than the search
algorithmsdescribedinthischapterandthenext. The8-puzzlehas9!/2=181,440reachable
statesandiseasilysolved. The15-puzzle(ona4×4board)hasaround1.3trillionstates,and
randominstancescanbesolvedoptimallyinafewmillisecondsbythebestsearchalgorithms.
The24-puzzle (on a 5×5board) has around 1025 states, and random instances take several
hourstosolveoptimally.
The goal of the 8-queens problem is to place eight queens on a chessboard such that
8-QUEENSPROBLEM
no queen attacks any other. (A queen attacks any piece in the same row, column or diago-
nal.) Figure3.5shows anattempted solution thatfails: the queen inthe rightmost column is
attacked bythequeenatthetopleft.
72 Chapter 3. SolvingProblemsbySearching
Figure3.5 Almostasolutiontothe8-queensproblem.(Solutionisleftasanexercise.)
Although efficient special-purpose algorithms exist forthis problem and forthe whole
n-queens family, it remains auseful test problem forsearch algorithms. Thereare twomain
INCREMENTAL kindsofformulation. Anincrementalformulationinvolvesoperatorsthataugmentthestate
FORMULATION
description, starting with an empty state; for the 8-queens problem, this means that each
COMPLETE-STATE action adds a queen to the state. A complete-state formulation starts with all 8 queens on
FORMULATION
theboard andmovesthemaround. Ineithercase,thepathcost isofnointerest because only
thefinalstatecounts. Thefirstincremental formulation one mighttryisthefollowing:
• States: Anyarrangement of0to8queensontheboardisastate.
• Initialstate: Noqueensontheboard.
• Actions: Addaqueentoanyemptysquare.
• Transitionmodel: Returnstheboardwithaqueenaddedtothespecifiedsquare.
• Goaltest: 8queensareontheboard, noneattacked.
Inthisformulation, wehave64·63···57 ≈ 1.8×1014 possible sequences toinvestigate. A
betterformulation wouldprohibitplacing aqueeninanysquarethatisalready attacked:
• States: All possible arrangements of n queens (0 ≤ n ≤ 8), one per column in the
leftmostncolumns,withnoqueenattacking another.
• Actions: Add a queen to any square in the leftmost empty column such that it is not
attackedbyanyotherqueen.
Thisformulationreducesthe8-queensstatespacefrom1.8×1014tojust2,057,andsolutions
areeasytofind. Ontheotherhand,for100queensthereductionisfromroughly 10400 states
toabout1052 states(Exercise3.5)—abigimprovement,butnotenoughtomaketheproblem
tractable. Section4.1describes thecomplete-state formulation, andChapter6givesasimple
algorithm thatsolveseventhemillion-queens problemwith ease.
Section3.2. ExampleProblems 73
Ourfinaltoyproblem wasdevisedbyDonaldKnuth(1964)andillustrates howinfinite
statespacescanarise. Knuthconjectured that,starting withthenumber4,asequence offac-
torial, square root, andflooroperations willreach anydesired positive integer. Forexample,
wecanreach5from4asfollows:
(cid:4)
(cid:5)(cid:7)
(cid:8)
(cid:3) (cid:5) (cid:6) (cid:9) (cid:10) (cid:11)
(4!)! = 5.
Theproblem definitionisverysimple:
• States: Positivenumbers.
• Initialstate: 4.
• Actions: Applyfactorial, squareroot,orflooroperation (factorial forintegersonly).
• Transitionmodel: Asgivenbythemathematicaldefinitions oftheoperations.
• Goaltest: Stateisthedesiredpositiveinteger.
Toourknowledge thereisnobound onhowlarge anumbermightbeconstructed inthepro-
cessofreaching agiventarget—for example, thenumber620,448,401,733,239,439,360,000
is generated in the expression for 5—so the state space for this problem is infinite. Such
state spaces arise frequently in tasks involving the generation of mathematical expressions,
circuits, proofs, programs, andotherrecursively definedobjects.
3.2.2 Real-worldproblems
ROUTE-FINDING We have already seen how the route-finding problem is defined in terms of specified loca-
PROBLEM
tionsandtransitionsalonglinksbetweenthem. Route-findingalgorithmsareusedinavariety
of applications. Some, such as Web sites and in-car systems that provide driving directions,
are relatively straightforward extensions of the Romania example. Others, such as routing
videostreamsincomputernetworks,militaryoperationsplanning,andairlinetravel-planning
systems,involvemuchmorecomplexspecifications. Considertheairlinetravelproblemsthat
mustbesolvedbyatravel-planning Website:
• States: Eachstate obviously includes alocation (e.g., anairport) and the current time.
Furthermore, because the cost of an action (a flight segment) may depend on previous
segments, their fare bases, and their status as domestic or international, the state must
recordextrainformation aboutthese“historical” aspects.
• Initialstate: Thisisspecifiedbytheuser’squery.
• Actions: Take any flight from the current location, in any seat class, leaving after the
currenttime,leavingenough timeforwithin-airport transferifneeded.
• Transition model: The state resulting from taking a flight will have the flight’s desti-
nationasthecurrentlocation andtheflight’sarrivaltimeasthecurrenttime.
• Goaltest: Areweatthefinaldestination specifiedbytheuser?
• Path cost: This depends on monetary cost, waiting time, flight time, customs and im-
migrationprocedures, seatquality, timeofday,typeofairplane, frequent-flyer mileage
awards,andsoon.
74 Chapter 3. SolvingProblemsbySearching
Commercial travel advice systems use a problem formulation of this kind, with many addi-
tional complications to handle the byzantine fare structures that airlines impose. Any sea-
soned traveler knows, however, that not all air travel goes according to plan. A really good
systemshouldincludecontingencyplans—suchasbackupreservationsonalternateflights—
totheextentthatthesearejustifiedbythecostandlikelihood offailureoftheoriginalplan.
Touring problems are closely related to route-finding problems, but with an impor-
TOURINGPROBLEM
tant difference. Consider, for example, the problem “Visit every city in Figure 3.2 at least
once, starting and ending in Bucharest.” As with route finding, the actions correspond
to trips between adjacent cities. The state space, however, is quite different. Each state
must include not just the current location but also the set of cities the agent has visited.
So the initial state would be In(Bucharest),Visited({Bucharest}), a typical intermedi-
ate state would be In(Vaslui),Visited({Bucharest,Urziceni,Vaslui}), and the goal test
wouldcheckwhethertheagentisinBucharestandall20citieshavebeenvisited.
TRAVELING
The traveling salesperson problem (TSP) is a touring problem in which each city
SALESPERSON
PROBLEM
must be visited exactly once. The aim is to find the shortest tour. The problem is known to
beNP-hard,butanenormousamountofefforthasbeenexpendedtoimprovethecapabilities
of TSP algorithms. In addition to planning trips for traveling salespersons, these algorithms
havebeenusedfortaskssuchasplanningmovementsofautomaticcircuit-board drillsandof
stocking machinesonshopfloors.
A VLSIlayout problem requires positioning millions of components and connections
VLSILAYOUT
on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max-
imize manufacturing yield. The layout problem comes after the logical design phase and is
usually split into two parts: cell layout and channel routing. In cell layout, the primitive
components of the circuit are grouped into cells, each of which performs some recognized
function. Each cell has a fixed footprint (size and shape) and requires a certain number of
connectionstoeachoftheothercells. Theaimistoplacethecellsonthechipsothattheydo
notoverlapandsothatthereisroomfortheconnecting wires tobeplacedbetweenthecells.
Channelroutingfindsaspecificrouteforeachwirethroughthegapsbetweenthecells. These
search problems are extremely complex, but definitely worth solving. Later in this chapter,
wepresentsomealgorithmscapable ofsolving them.
Robot navigation is a generalization of the route-finding problem described earlier.
ROBOTNAVIGATION
Rather than following a discrete set of routes, a robot can move in a continuous space with
(in principle) an infinite set of possible actions and states. For a circular robot moving on a
flat surface, the space is essentially two-dimensional. When the robot has arms and legs or
wheelsthatmustalsobecontrolled, thesearchspacebecomesmany-dimensional. Advanced
techniques are required just to make the search space finite. We examine some of these
methods in Chapter 25. In addition to the complexity of the problem, real robots must also
dealwitherrorsintheirsensorreadings andmotorcontrols.
AUTOMATIC
ASSEMBLY
Automaticassemblysequencingofcomplexobjectsbyarobotwasfirstdemonstrated
SEQUENCING
by FREDDY (Michie, 1972). Progress since then has been slow but sure, to the point where
theassemblyofintricateobjectssuchaselectricmotorsiseconomicallyfeasible. Inassembly
problems, the aim is to find an order in which to assemble the parts of some object. If the
wrong order is chosen, there will be no way to add some part later in the sequence without
Section3.3. SearchingforSolutions 75
undoing some of the work already done. Checking a step in the sequence for feasibility is a
difficultgeometricalsearchproblemcloselyrelatedtorobotnavigation. Thus,thegeneration
of legal actions is the expensive part of assembly sequencing. Any practical algorithm must
avoidexploringallbutatinyfractionofthestatespace. Anotherimportantassemblyproblem
isprotein design, inwhichthegoalistofindasequence ofaminoacids thatwillfoldinto a
PROTEINDESIGN
three-dimensional proteinwiththerightproperties tocuresomedisease.
3.3 SEARCHING FOR SOLUTIONS
Having formulated some problems, we now need to solve them. A solution is an action
sequence, so search algorithms work by considering various possible action sequences. The
possible action sequences starting at the initial state form asearch tree with the initial state
SEARCHTREE
at the root; the branches are actions and the nodes correspond to states in the state space of
NODE
theproblem. Figure3.6showsthefirstfewstepsingrowingthesearchtreeforfindingaroute
from Arad to Bucharest. The root node of the tree corresponds to the initial state, In(Arad).
The first step is to test whether this is a goal state. (Clearly it is not, but it is important to
check so that we can solve trick problems like “starting in Arad, get to Arad.”) Then we
need to consider taking various actions. We do this by expanding the current state; that is,
EXPANDING
applying each legal action to the current state, thereby generating a new set of states. In
GENERATING
this case, we add three branches from the parent node In(Arad) leading to three new child
PARENTNODE
nodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three
CHILDNODE
possibilities toconsiderfurther.
Thisistheessenceofsearch—followinguponeoptionnowandputtingtheothersaside
for later, in case the first choice does not lead to a solution. Suppose we choose Sibiu first.
We check to see whether it is a goal state (it is not) and then expand it to get In(Arad),
In(Fagaras), In(Oradea),andIn(RimnicuVilcea). Wecanthenchooseanyofthesefourorgo
back and choose Timisoara orZerind. Each ofthese sixnodes isa leaf node, that is, anode
LEAFNODE
with no children in the tree. The set of all leaf nodes available for expansion at any given
pointiscalledthefrontier. (Manyauthors callittheopenlist, whichisbothgeographically
FRONTIER
lessevocativeandlessaccurate, becauseotherdatastructures arebettersuitedthanalist.) In
OPENLIST
Figure3.6,thefrontierofeachtreeconsists ofthosenodes withboldoutlines.
Theprocessofexpandingnodesonthefrontiercontinuesuntileitherasolutionisfound
ortherearenomorestatestoexpand. Thegeneral TREE-SEARCH algorithm isshowninfor-
mally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily
according tohowtheychoosewhichstatetoexpandnext—theso-called search strategy.
SEARCHSTRATEGY
Theeagle-eyedreaderwillnoticeonepeculiarthingaboutthesearchtreeshowninFig-
ure3.6: itincludesthepathfromAradtoSibiuandbacktoAradagain! WesaythatIn(Arad)
is a repeated state in the search tree, generated in this case by a loopy path. Considering
REPEATEDSTATE
such loopy paths means that the complete search tree for Romania is infinite because there
LOOPYPATH
is no limit to how often one can traverse a loop. On the other hand, the state space—the
mapshowninFigure3.2—hasonly 20states. Aswediscuss inSection 3.4,loops cancause
76 Chapter 3. SolvingProblemsbySearching
certainalgorithmstofail,makingotherwisesolvableproblemsunsolvable. Fortunately,there
isnoneedtoconsiderloopypaths. Wecanrelyonmorethanintuition forthis: because path
costs are additive and step costs are nonnegative, a loopy path to any given state is never
betterthanthesamepathwiththeloopremoved.
Loopypathsareaspecialcaseofthemoregeneralconceptofredundantpaths,which
REDUNDANTPATH
existwheneverthereismorethanonewaytogetfromonestatetoanother. Considerthepaths
Arad–Sibiu (140 km long) and Arad–Zerind–Oradea–Sibiu (297 km long). Obviously, the
secondpathisredundant—it’s justaworsewaytogettothesamestate. Ifyouareconcerned
about reaching the goal, there’s never any reason to keep more than one path to any given
state, because any goal state that is reachable by extending one path is also reachable by
extending theother.
In some cases, it is possible to define the problem itself so as to eliminate redundant
paths. Forexample, if we formulate the 8-queens problem (page 71) so that a queen can be
placedinanycolumn,theneachstatewithnqueenscanbereachedbyn!differentpaths;but
ifwereformulatetheproblemsothateachnewqueenisplacedintheleftmostemptycolumn,
theneachstatecanbereached onlythrough onepath.
(a) The initial state Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea
(b) After expanding Arad Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea
(c) After expanding Sibiu Arad
Sibiu Timisoara Zerind
Arad Fagaras Oradea Rimnicu Vilcea Arad Lugoj Arad Oradea
Figure 3.6 Partial search trees for finding a route from Arad to Bucharest. Nodes that
have been expanded are shaded; nodes that have been generated but not yet expanded are
outlinedinbold;nodesthathavenotyetbeengeneratedareshowninfaintdashedlines.
Section3.3. SearchingforSolutions 77
functionTREE-SEARCH(problem)returnsasolution,orfailure
initializethefrontierusingtheinitialstateofproblem
loopdo
ifthefrontierisemptythenreturnfailure
choosealeafnodeandremoveitfromthefrontier
ifthenodecontainsagoalstatethenreturnthecorrespondingsolution
expandthechosennode,addingtheresultingnodestothefrontier
functionGRAPH-SEARCH(problem)returnsasolution,orfailure
initializethefrontierusingtheinitialstateofproblem
initializetheexploredsettobeempty
loopdo
ifthefrontierisemptythenreturnfailure
choosealeafnodeandremoveitfromthefrontier
ifthenodecontainsagoalstatethenreturnthecorrespondingsolution
addthenodetotheexploredset
expandthechosennode,addingtheresultingnodestothefrontier
onlyifnotinthefrontierorexploredset
Figure 3.7 An informal description of the general tree-search and graph-search algo-
rithms. The parts of GRAPH-SEARCH marked in bold italic are the additions needed to
handlerepeatedstates.
In other cases, redundant paths are unavoidable. This includes all problems where
the actions are reversible, such as route-finding problems and sliding-block puzzles. Route-
findingonarectangular grid(liketheoneusedlaterforFigure3.9)isaparticularly impor-
RECTANGULARGRID
tant example in computer games. In such a grid, each state has four successors, so a search
treeofdepthdthatincludesrepeatedstateshas4dleaves;butthereareonlyabout2d2distinct
stateswithindstepsofanygivenstate. Ford= 20,thismeansaboutatrillionnodesbutonly
about 800 distinct states. Thus, following redundant paths can cause a tractable problem to
becomeintractable. Thisistrueevenforalgorithms thatknowhowtoavoidinfiniteloops.
As the saying goes, algorithms that forget their history are doomed to repeat it. The
way to avoid exploring redundant paths is to remember where one has been. To do this, we
EXPLOREDSET
augment the TREE-SEARCH algorithm with a data structure called the explored set (also
known as the closed list), which remembers every expanded node. Newly generated nodes
CLOSEDLIST
thatmatchpreviously generated nodes—ones intheexplored setorthefrontier—can bedis-
cardedinsteadofbeingaddedtothefrontier. Thenewalgorithm, called GRAPH-SEARCH,is
shown informally in Figure 3.7. Thespecific algorithms inthis chapter draw onthis general
design.
Clearly,thesearchtreeconstructedbytheGRAPH-SEARCH algorithmcontainsatmost
onecopyofeachstate,sowecanthinkofitasgrowingatreedirectlyonthestate-spacegraph,
as shown in Figure 3.8. The algorithm has another nice property: the frontier separates the
SEPARATOR
state-space graphintotheexploredregionandtheunexplored region,sothateverypathfrom
78 Chapter 3. SolvingProblemsbySearching
Figure3.8 AsequenceofsearchtreesgeneratedbyagraphsearchontheRomaniaprob-
lemofFigure3.2. Ateachstage,wehaveextendedeachpathbyonestep. Noticethatatthe
thirdstage,thenorthernmostcity(Oradea)hasbecomeadeadend:bothofitssuccessorsare
alreadyexploredviaotherpaths.
(a) (b) (c)
Figure3.9 TheseparationpropertyofGRAPH-SEARCH,illustratedonarectangular-grid
problem. Thefrontier(whitenodes)alwaysseparatestheexploredregionofthestatespace
(black nodes) from the unexplored region (gray nodes). In (a), just the root has been ex-
panded.In(b),oneleafnodehasbeenexpanded.In(c),theremainingsuccessorsoftheroot
havebeenexpandedinclockwiseorder.
the initial state to an unexplored state has to pass through a state in the frontier. (If this
seemscompletely obvious, tryExercise 3.13now.) Thisproperty isillustrated inFigure 3.9.
As every step moves a state from the frontier into the explored region while moving some
statesfromtheunexploredregionintothefrontier,weseethatthealgorithmissystematically
examiningthestatesinthestatespace, onebyone,untilitfindsasolution.
3.3.1 Infrastructure forsearch algorithms
Search algorithms require a data structure to keep track of the search tree that is being con-
structed. Foreachnode nofthetree,wehaveastructure thatcontains fourcomponents:
• n.STATE: thestateinthestatespacetowhichthenodecorresponds;
• n.PARENT: thenodeinthesearchtreethatgenerated thisnode;
• n.ACTION: theactionthatwasappliedtotheparenttogeneratethenode;
• n.PATH-COST: thecost,traditionally denotedbyg(n),ofthepathfromtheinitialstate
tothenode,asindicated bytheparentpointers.
Section3.3. SearchingforSolutions 79
PARENT
Node
55 44 ACTION = Right
PATH-COST = 6
66 11 88
STATE
77 33 22
Figure3.10 Nodesarethedatastructuresfromwhichthesearchtreeisconstructed.Each
hasaparent,astate,andvariousbookkeepingfields. Arrowspointfromchildtoparent.
Given the components for a parent node, it is easy to see how to compute the necessary
components fora child node. The function CHILD-NODE takes a parent node and an action
andreturnstheresulting childnode:
functionCHILD-NODE(problem,parent,action)returnsanode
returnanodewith
STATE=problem.RESULT(parent.STATE,action),
PARENT=parent,ACTION=action,
PATH-COST=parent.PATH-COST+problem.STEP-COST(parent.STATE,action)
The node data structure is depicted in Figure 3.10. Notice how the PARENT pointers
stringthenodestogetherintoatreestructure. Thesepointersalsoallowthesolutionpathtobe
extracted when agoal node is found; weuse the SOLUTION function to return the sequence
ofactions obtained byfollowingparentpointers backtotheroot.
Uptonow,wehavenotbeenverycarefultodistinguishbetweennodesandstates,butin
writing detailed algorithms it’s important to make that distinction. A node isa bookkeeping
data structure usedtorepresent the search tree. Astate corresponds toaconfiguration ofthe
world. Thus, nodes are on particular paths, as defined by PARENT pointers, whereas states
are not. Furthermore, two different nodes can contain the same world state if that state is
generated viatwodifferent searchpaths.
Now that we have nodes, we need somewhere to put them. The frontier needs to be
stored in such a way that the search algorithm can easily choose the next node to expand
according to its preferred strategy. The appropriate data structure for this is a queue. The
QUEUE
operations onaqueueareasfollows:
• EMPTY?(queue)returnstrueonlyiftherearenomoreelementsinthequeue.
• POP(queue)removesthefirstelementofthequeueandreturnsit.
• INSERT(element,queue)insertsanelementandreturns theresulting queue.
80 Chapter 3. SolvingProblemsbySearching
Queuesarecharacterized bythe orderinwhichtheystoretheinsertednodes. Threecommon
variantsarethefirst-in,first-outor FIFOqueue,whichpopstheoldestelementofthequeue;
FIFOQUEUE
thelast-in, first-outorLIFOqueue(also knownasastack), whichpops thenewestelement
LIFOQUEUE
of the queue; and the priority queue,which pops the element of the queue with the highest
PRIORITYQUEUE
priorityaccording tosomeorderingfunction.
The explored set can be implemented with a hash table to allow efficient checking for
repeated states. With a good implementation, insertion and lookup can be done in roughly
constant time no matter how many states are stored. One must take care to implement the
hash table with the right notion of equality between states. For example, in the traveling
salesperson problem (page 74), the hash table needs to know that the set of visited cities
{Bucharest,Urziceni,Vaslui} isthesameas{Urziceni,Vaslui,Bucharest}. Sometimesthiscan
be achieved most easily by insisting that the data structures for states be in some canonical
form; that is, logically equivalent states should map to the same data structure. In the case
CANONICALFORM
of states described by sets, for example, a bit-vector representation or a sorted list without
repetition wouldbecanonical, whereasanunsorted listwouldnot.
3.3.2 Measuring problem-solving performance
Before we get into the design of specific search algorithms, we need to consider the criteria
that might be used to choose among them. We can evaluate an algorithm’s performance in
fourways:
• Completeness: Isthealgorithm guaranteed tofindasolution whenthereisone?
COMPLETENESS
• Optimality: Doesthestrategyfindtheoptimalsolution, asdefinedonpage68?
OPTIMALITY
• Timecomplexity: Howlongdoesittaketofindasolution?
TIMECOMPLEXITY
• Spacecomplexity: Howmuchmemoryisneededtoperformthesearch?
SPACECOMPLEXITY
Timeandspacecomplexityarealwaysconsidered withrespecttosomemeasureoftheprob-
lem difficulty. In theoretical computer science, the typical measure is the size of the state
space graph, |V|+|E|, where V is the set of vertices (nodes) of the graph and E is the set
ofedges (links). Thisisappropriate whenthe graph isan explicit data structure that isinput
tothesearchprogram. (ThemapofRomaniaisanexampleofthis.) InAI,thegraphisoften
represented implicitly bytheinitialstate, actions, andtransition modelandisfrequently infi-
nite. Forthesereasons,complexityisexpressedintermsofthreequantities: b,thebranching
factor or maximum number of successors of any node; d, the depth of the shallowest goal
BRANCHINGFACTOR
node(i.e.,thenumberofstepsalong thepathfromtheroot); andm,themaximumlength of
DEPTH
anypathinthestatespace. Timeisoftenmeasuredintermsofthenumberofnodesgenerated
during the search, and space in terms of the maximum number of nodes stored in memory.
For the most part, we describe time and space complexity for search on a tree; for a graph,
theanswerdepends onhow“redundant” thepathsinthestatespaceare.
Toassesstheeffectivenessofasearchalgorithm,wecanconsiderjustthesearchcost—
SEARCHCOST
which typically depends on the time complexity but can also include a term for memory
usage—orwecanusethetotalcost,whichcombinesthesearchcostandthepathcostofthe
TOTALCOST
solution found. For the problem of finding a route from Arad to Bucharest, the search cost
isthe amount of timetaken bythe search and thesolution cost isthe total length of thepath
Section3.4. UninformedSearchStrategies 81
in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.
Thereisno“officialexchangerate”betweenthetwo,butitmightbereasonableinthiscaseto
convertkilometersintomillisecondsbyusinganestimateofthecar’saveragespeed(because
time is what the agent cares about). This enables the agent to find an optimal tradeoff point
at which further computation to find a shorter path becomes counterproductive. The more
generalproblem oftradeoffs betweendifferentgoodsistakenupinChapter16.
3.4 UNINFORMED SEARCH STRATEGIES
This section covers several search strategies that come under the heading of uninformed
UNINFORMED search (also called blind search). The term means that the strategies have no additional
SEARCH
information about states beyond that provided in the problem definition. All they can do is
BLINDSEARCH
generate successors and distinguish a goal state from a non-goal state. All search strategies
are distinguished by the order in which nodes are expanded. Strategies that know whether
onenon-goalstateis“morepromising”thananotherarecalledinformedsearchorheuristic
INFORMEDSEARCH
search strategies; theyarecoveredinSection3.5.
HEURISTICSEARCH
3.4.1 Breadth-first search
BREADTH-FIRST Breadth-firstsearchisasimplestrategyinwhichtherootnodeisexpandedfirst,thenallthe
SEARCH
successors of the root node are expanded next, then their successors, and so on. In general,
all the nodes are expanded at a given depth in the search tree before any nodes at the next
levelareexpanded.
Breadth-firstsearchisaninstanceofthegeneralgraph-search algorithm (Figure3.7)in
whichtheshallowestunexpandednodeischosenforexpansion. Thisisachievedverysimply
byusingaFIFOqueueforthefrontier. Thus,newnodes(which arealwaysdeeperthantheir
parents)gotothebackofthequeue,andoldnodes,whichareshallowerthanthenewnodes,
getexpandedfirst. Thereisoneslighttweakonthegeneralgraph-search algorithm,whichis
thatthegoaltestisappliedtoeachnodewhenitisgeneratedratherthanwhenitisselectedfor
expansion. This decision is explained below, where we discuss time complexity. Note also
thatthealgorithm, followingthegeneral templateforgraphsearch, discards anynewpathto
a state already in the frontier or explored set; it is easy to see that any such path must be at
least as deep as the one already found. Thus, breadth-first search always has the shallowest
pathtoeverynodeonthefrontier.
Pseudocode isgiven inFigure 3.11. Figure 3.12 shows the progress ofthe search on a
simplebinarytree.
Howdoesbreadth-first search rateaccording tothefourcriteria from theprevious sec-
tion? Wecaneasilyseethatitiscomplete—iftheshallowestgoalnodeisatsomefinitedepth
d, breadth-first search will eventually find it after generating all shallower nodes (provided
the branching factor b is finite). Note that as soon as a goal node is generated, we know it
is the shallowest goal node because all shallower nodes must have been generated already
and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one;
82 Chapter 3. SolvingProblemsbySearching
functionBREADTH-FIRST-SEARCH(problem)returnsasolution,orfailure
node←anodewithSTATE=problem.INITIAL-STATE,PATH-COST=0
ifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)
frontier←aFIFOqueuewithnode astheonlyelement
explored←anemptyset
loopdo
ifEMPTY?(frontier)thenreturnfailure
node←POP(frontier) /*choosestheshallowestnodeinfrontier */
addnode.STATEtoexplored
foreachaction inproblem.ACTIONS(node.STATE)do
child←CHILD-NODE(problem,node,action)
ifchild.STATEisnotinexplored orfrontier then
ifproblem.GOAL-TEST(child.STATE)thenreturnSOLUTION(child)
frontier←INSERT(child,frontier)
Figure3.11 Breadth-firstsearchonagraph.
technically, breadth-first search isoptimal ifthe path cost is anondecreasing function of the
depthofthenode. Themostcommonsuchscenarioisthatallactions havethesamecost.
So far, the news about breadth-first search has been good. The news about time and
space is not so good. Imagine searching a uniform tree where every state has b successors.
Therootofthesearchtreegenerates bnodesatthefirstlevel,eachofwhichgenerates bmore
nodes, foratotalofb2 atthesecond level. Eachofthesegenerates bmorenodes, yielding b3
nodes atthe third level, and soon. Nowsuppose that the solution isatdepth d. Inthe worst
case,itisthelastnodegenerated atthatlevel. Thenthetotalnumberofnodesgenerated is
b+b2+b3+···+bd = O(bd).
(Ifthealgorithmweretoapplythegoaltesttonodeswhenselectedforexpansion,ratherthan
whengenerated, thewholelayerofnodesatdepth dwouldbeexpanded before thegoalwas
detected andthetimecomplexitywouldbeO(bd+1).)
As for space complexity: for any kind of graph search, which stores every expanded
node in the explored set, the space complexity is always within a factor of b of the time
complexity. For breadth-first graph search in particular, every node generated remains in
memory. There will be O(bd−1) nodes in the explored set and O(bd) nodes in the frontier,
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
Figure 3.12 Breadth-first search on a simple binary tree. At each stage, the node to be
expandednextisindicatedbyamarker.
Section3.4. UninformedSearchStrategies 83
so the space complexity is O(bd), i.e., it is dominated by the size of the frontier. Switching
toatreesearch would notsavemuchspace, andinastate space withmanyredundant paths,
switchingcouldcostagreatdealoftime.
An exponential complexity bound such as O(bd) is scary. Figure 3.13 shows why. It
lists, forvarious values ofthesolution depth d,thetimeandmemoryrequired forabreadth-
first search with branching factor b = 10. The table assumes that 1 million nodes can be
generated persecond and that anode requires 1000 bytes ofstorage. Manysearch problems
fit roughly within these assumptions (give or take a factor of 100) when run on a modern
personal computer.
Depth Nodes Time Memory
2 110 .11 milliseconds 107 kilobytes
4 11,110 11 milliseconds 10.6 megabytes
6 106 1.1 seconds 1 gigabyte
8 108 2 minutes 103 gigabytes
10 1010 3 hours 10 terabytes
12 1012 13 days 1 petabyte
14 1014 3.5 years 99 petabytes
16 1016 350 years 10 exabytes
Figure3.13 Timeandmemoryrequirementsforbreadth-firstsearch.Thenumbersshown
assumebranchingfactorb=10;1millionnodes/second;1000bytes/node.
Two lessons can be learned from Figure 3.13. First, the memory requirements are a
bigger problem for breadth-first search than is the execution time. One might wait 13 days
forthesolution toanimportant problem withsearch depth 12,butnopersonal computerhas
thepetabyte ofmemoryitwouldtake. Fortunately, otherstrategies requirelessmemory.
The second lesson is that time is still a major factor. If your problem has a solution at
depth16,then(givenourassumptions)itwilltakeabout350yearsforbreadth-firstsearch(or
indeedanyuninformedsearch)tofindit. Ingeneral, exponential-complexity searchproblems
cannotbesolvedbyuninformed methodsforanybutthesmallestinstances.
3.4.2 Uniform-costsearch
When all step costs are equal, breadth-first search is optimal because it always expands the
shallowestunexpandednode. Byasimpleextension,wecanfindanalgorithmthatisoptimal
UNIFORM-COST withanystep-cost function. Instead ofexpanding theshallowest node, uniform-costsearch
SEARCH
expands the node n with the lowest path cost g(n). This is done by storing the frontier as a
priorityqueueordered by g. Thealgorithm isshowninFigure3.14.
In addition to the ordering of the queue by path cost, there are two other significant
differences from breadth-first search. Thefirstisthat thegoal test isapplied toanode when
it is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7)
rather than when it is first generated. The reason is that the first goal node that is generated
84 Chapter 3. SolvingProblemsbySearching
functionUNIFORM-COST-SEARCH(problem)returnsasolution,orfailure
node←anodewithSTATE=problem.INITIAL-STATE,PATH-COST=0
frontier←apriorityqueueorderedbyPATH-COST,withnode astheonlyelement
explored←anemptyset
loopdo
ifEMPTY?(frontier)thenreturnfailure
node←POP(frontier) /*choosesthelowest-costnodeinfrontier */
ifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)
addnode.STATEtoexplored
foreachaction inproblem.ACTIONS(node.STATE)do
child←CHILD-NODE(problem,node,action)
ifchild.STATEisnotinexplored orfrontier then
frontier←INSERT(child,frontier)
elseifchild.STATEisinfrontier withhigherPATH-COSTthen
replacethatfrontier nodewithchild
Figure 3.14 Uniform-cost search on a graph. The algorithm is identical to the general
graphsearchalgorithminFigure3.7,exceptfortheuseofapriorityqueueandtheaddition
ofanextracheckincaseashorterpathtoafrontierstateisdiscovered.Thedatastructurefor
frontier needstosupportefficientmembershiptesting,soitshouldcombinethecapabilities
ofapriorityqueueandahashtable.
Sibiu Fagaras
99
80
Rimnicu Vilcea
211
Pitesti
97
101
Bucharest
Figure3.15 PartoftheRomaniastatespace,selectedtoillustrateuniform-costsearch.
may be on a suboptimal path. The second difference is that a test is added in case a better
pathisfoundtoanodecurrently onthefrontier.
BothofthesemodificationscomeintoplayintheexampleshowninFigure3.15,where
theproblemistogetfromSibiutoBucharest. Thesuccessors ofSibiuareRimnicuVilceaand
Fagaras,withcosts80and99,respectively. Theleast-cost node,RimnicuVilcea,isexpanded
next, adding Pitesti with cost 80 + 97=177. The least-cost node is now Fagaras, so it is
expanded, addingBucharestwithcost99+211=310. Nowagoalnodehasbeengenerated,
butuniform-costsearchkeepsgoing,choosingPitestiforexpansionandaddingasecondpath
Section3.4. UninformedSearchStrategies 85
toBucharestwithcost80+97+101=278. Nowthealgorithmcheckstoseeifthisnewpath
is better than the old one; it is, so the old one is discarded. Bucharest, now with g-cost 278,
isselected forexpansion andthesolution isreturned.
It is easy to see that uniform-cost search is optimal in general. First, we observe that
whenever uniform-cost search selects a node n for expansion, the optimal path to that node
has been found. (Were this not the case, there would have to be another frontier node
n(cid:2)
on
the optimal path from the start node to n, by the graph separation property of Figure 3.9;
(cid:2)
by definition, n would have lower g-cost than n and would have been selected first.) Then,
because step costs are nonnegative, paths never get shorter as nodes are added. These two
facts together imply that uniform-cost search expands nodes in order of their optimal path
cost. Hence,thefirstgoalnodeselectedforexpansion mustbetheoptimalsolution.
Uniform-costsearchdoesnotcareaboutthe numberofstepsapathhas,butonlyabout
theirtotalcost. Therefore,itwillgetstuckinaninfiniteloopifthereisapathwithaninfinite
sequence ofzero-cost actions—for example, asequence of NoOp actions.6 Completeness is
guaranteed providedthecostofeverystepexceedssomesmallpositiveconstant (cid:2).
Uniform-costsearchisguidedbypathcostsratherthandepths,soitscomplexityisnot
easily characterized in terms of band d. Instead, let C ∗ bethe cost ofthe optimal solution,7
andassumethateveryactioncostsatleast(cid:2). Thenthealgorithm’s worst-casetimeandspace
complexity is O(b1+(cid:4)C∗/(cid:2)(cid:5) ), which can be much greater than bd. This is because uniform-
cost search can explore large trees of small steps before exploring paths involving large and
perhaps useful steps. When all step costs are equal, b1+(cid:4)C∗/(cid:2)(cid:5) is just bd+1. When all step
costsarethesame,uniform-costsearchissimilartobreadth-firstsearch,exceptthatthelatter
stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at
the goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly more
workbyexpanding nodesatdepthdunnecessarily.
3.4.3 Depth-first search
DEPTH-FIRST Depth-firstsearchalwaysexpandsthedeepestnodeinthecurrentfrontierofthesearchtree.
SEARCH
The progress of the search is illustrated in Figure 3.16. The search proceeds immediately
to the deepest level of the search tree, where the nodes have no successors. As those nodes
are expanded, they are dropped from the frontier, so then the search “backs up” to the next
deepestnodethatstillhasunexplored successors.
The depth-first search algorithm is an instance of the graph-search algorithm in Fig-
ure3.7;whereasbreadth-first-searchusesaFIFOqueue,depth-firstsearchusesaLIFOqueue.
A LIFO queue means that the most recently generated node is chosen for expansion. This
mustbethedeepestunexpandednodebecauseitisonedeeperthanitsparent—which,inturn,
wasthedeepestunexpanded nodewhenitwasselected.
As an alternative to the GRAPH-SEARCH-style implementation, it is common to im-
plement depth-first search witharecursive function that callsitself oneach ofitschildren in
turn. (Arecursivedepth-first algorithm incorporating adepthlimitisshowninFigure3.17.)
6 NoOp,or“nooperation,”isthenameofanassemblylanguageinstructionthatdoesnothing.
7 Here,andthroughoutthebook,the“star”inC∗meansanoptimalvalueforC.
86 Chapter 3. SolvingProblemsbySearching
A A A
B C B C B C
D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O
A A A
B C B C C
D E F G D E F G E F G
H I J K L M N O I J K L M N O J K L M N O
A A
C B C C
E F G E F G F G
J K L M N O K L M N O L M N O
A A A
C C C
F G F G F G
L M N O L M N O M N O
Figure3.16 Depth-firstsearchonabinarytree. Theunexploredregionis showninlight
gray. Explorednodeswithnodescendantsinthefrontierare removedfrommemory. Nodes
atdepth3havenosuccessorsandM istheonlygoalnode.
The properties of depth-first search depend strongly on whether the graph-search or
tree-search version is used. The graph-search version, which avoids repeated states and re-
dundantpaths,iscompleteinfinitestatespacesbecauseitwilleventuallyexpandeverynode.
The tree-search version, on the other hand, is not complete—for example, in Figure 3.6 the
algorithmwillfollowtheArad–Sibiu–Arad–Sibiuloopforever. Depth-firsttreesearchcanbe
modifiedatnoextra memorycostsothatitchecks newstates against those onthepathfrom
theroottothecurrent node;thisavoids infiniteloopsinfinitestatespaces butdoesnotavoid
the proliferation of redundant paths. In infinite state spaces, both versions fail if an infinite
non-goal path is encountered. For example, in Knuth’s 4 problem, depth-first search would
keepapplying thefactorial operatorforever.
Forsimilarreasons, both versions arenonoptimal. Forexample, inFigure3.16, depth-
firstsearch willexplore theentire left subtree evenifnode C isagoal node. Ifnode J were
also a goal node, then depth-first search would return it as a solution instead of C, which
wouldbeabettersolution; hence, depth-firstsearchisnotoptimal.
Section3.4. UninformedSearchStrategies 87
Thetimecomplexityofdepth-firstgraphsearchisboundedbythesizeofthestatespace
(whichmaybeinfinite,ofcourse). Adepth-firsttreesearch, ontheotherhand,maygenerate
all of the O(bm) nodes in the search tree, where m is the maximum depth of any node; this
can be much greater than the size of the state space. Note that m itself can be much larger
thand(thedepthoftheshallowest solution) andisinfiniteifthetreeisunbounded.
So far, depth-first search seems to have no clear advantage over breadth-first search,
so why do we include it? The reason is the space complexity. For a graph search, there is
no advantage, but a depth-first tree search needs to store only a single path from the root
to a leaf node, along with the remaining unexpanded sibling nodes for each node on the
path. Once a node has been expanded, it can be removed from memory as soon as all its
descendants have been fully explored. (See Figure 3.16.) For a state space with branching
factor b and maximum depth m, depth-first search requires storage of only O(bm) nodes.
Usingthesameassumptions asforFigure3.13andassumingthatnodesatthesamedepthas
thegoalnodehavenosuccessors, wefindthatdepth-firstsearchwouldrequire156kilobytes
instead of 10 exabytes at depth d = 16, a factor of 7 trillion times less space. This has
led to the adoption of depth-first tree search as the basic workhorse of many areas of AI,
includingconstraintsatisfaction(Chapter6),propositionalsatisfiability(Chapter7),andlogic
programming (Chapter 9). Forthe remainder ofthis section, wefocus primarily on thetree-
searchversionofdepth-first search.
BACKTRACKING Avariantofdepth-firstsearchcalled backtrackingsearchusesstilllessmemory. (See
SEARCH
Chapter6formoredetails.) Inbacktracking, onlyonesuccessor isgenerated atatimerather
than all successors; each partially expanded node remembers which successor to generate
next. In this way, only O(m) memory is needed rather than O(bm). Backtracking search
facilitates yet another memory-saving (and time-saving) trick: the idea of generating a suc-
cessor by modifying the current state description directly rather than copying it first. This
reduces thememoryrequirements tojustonestatedescription andO(m)actions. Forthisto
work,wemustbeabletoundoeachmodification whenwegobacktogenerate thenextsuc-
cessor. Forproblemswithlargestatedescriptions, suchas roboticassembly,thesetechniques
arecriticaltosuccess.
3.4.4 Depth-limited search
The embarrassing failure of depth-first search in infinite state spaces can be alleviated by
supplying depth-first searchwithapredetermined depth limit(cid:3). Thatis,nodes atdepth(cid:3)are
DEPTH-LIMITED treated as if they have no successors. This approach is called depth-limited search. The
SEARCH
depth limit solves the infinite-path problem. Unfortunately, it also introduces an additional
sourceofincompleteness ifwechoose (cid:3) < d,thatis,theshallowest goalisbeyondthedepth
limit. (This is likely when d is unknown.) Depth-limited search will also be nonoptimal if
wechoose(cid:3) > d. ItstimecomplexityisO(b(cid:3))anditsspacecomplexityisO(b(cid:3)). Depth-first
searchcanbeviewedasaspecialcaseofdepth-limited searchwith(cid:3)=∞.
Sometimes, depth limits can be based on knowledge of the problem. Forexample, on
themapofRomaniathereare20cities. Therefore,weknowthatifthereisasolution,itmust
be of length 19 at the longest, so (cid:3) = 19 is a possible choice. But in fact if we studied the
88 Chapter 3. SolvingProblemsbySearching
functionDEPTH-LIMITED-SEARCH(problem,limit)returnsasolution,orfailure/cutoff
returnRECURSIVE-DLS(MAKE-NODE(problem.INITIAL-STATE),problem,limit)
functionRECURSIVE-DLS(node,problem,limit)returnsasolution,orfailure/cutoff
ifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)
elseiflimit =0thenreturncutoff
else
cutoff occurred?←false
foreachaction inproblem.ACTIONS(node.STATE)do
child←CHILD-NODE(problem,node,action)
result←RECURSIVE-DLS(child,problem,limit −1)
ifresult =cutoff thencutoff occurred?←true
elseifresult (cid:7)=failure thenreturnresult
ifcutoff occurred?thenreturncutoff elsereturnfailure
Figure3.17 Arecursiveimplementationofdepth-limitedtreesearch.
mapcarefully, wewoulddiscoverthatanycitycanbereached fromanyothercityinatmost
9steps. Thisnumber, knownasthediameterofthestatespace,givesusabetterdepthlimit,
DIAMETER
which leads to a more efficient depth-limited search. For most problems, however, we will
notknowagooddepthlimituntilwehavesolvedtheproblem.
Depth-limited search can beimplemented asasimple modification to thegeneral tree-
or graph-search algorithm. Alternatively, it can be implemented as a simple recursive al-
gorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two
kinds of failure: the standard failure value indicates no solution; the cutoff value indicates
nosolutionwithinthedepthlimit.
3.4.5 Iterativedeepening depth-first search
ITERATIVE Iterative deepening search (or iterative deepening depth-first search) is a general strategy,
DEEPENINGSEARCH
often used incombination withdepth-first tree search, that findsthe bestdepth limit. Itdoes
thisbygraduallyincreasingthelimit—first0,then1,then2,andsoon—untilagoalisfound.
This will occur when the depth limit reaches d, the depth of the shallowest goal node. The
algorithm is shown in Figure 3.18. Iterative deepening combines the benefits of depth-first
andbreadth-firstsearch. Likedepth-firstsearch,itsmemoryrequirementsaremodest: O(bd)
tobeprecise. Likebreadth-first search, itiscomplete when thebranching factorisfiniteand
optimalwhenthepathcostisanondecreasing function ofthedepthofthenode. Figure3.19
showsfouriterationsofITERATIVE-DEEPENING-SEARCH onabinarysearchtree,wherethe
solution isfoundonthefourthiteration.
Iterative deepening search may seem wasteful because states are generated multiple
times. Itturns out this isnot too costly. Thereason is that inasearch tree with thesame (or
nearly the same) branching factor at each level, most of the nodes are in the bottom level,
so it does not mattermuch that the upper levels are generated multiple times. In an iterative
deepening search, the nodes on the bottom level (depth d) are generated once, those on the
Section3.4. UninformedSearchStrategies 89
functionITERATIVE-DEEPENING-SEARCH(problem)returnsasolution,orfailure
fordepth =0to∞do
result←DEPTH-LIMITED-SEARCH(problem,depth)
ifresult (cid:7)=cutoffthenreturnresult
Figure 3.18 The iterative deepening search algorithm, which repeatedly applies depth-
limitedsearchwithincreasinglimits. Itterminateswhena solutionisfoundorifthedepth-
limitedsearchreturnsfailure,meaningthatnosolutionexists.
Limit = 0 A A
Limit = 1 A A A A
B C B C B C B C
Limit = 2 A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
Limit = 3 A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O
A A A A
B C B C B C B C
D E F G D E F G D E F G D E F G
H I J K L M N O H I J K L M N O H I J K L M N O H I J K L M N O
Figure3.19 Fouriterationsofiterativedeepeningsearchonabinarytree.
90 Chapter 3. SolvingProblemsbySearching
next-to-bottom levelaregenerated twice, andsoon, uptothechildren oftheroot, whichare
generated dtimes. Sothetotalnumberofnodesgenerated intheworstcaseis
N(IDS) = (d)b+(d−1)b2+···+(1)bd ,
which gives a time complexity of O(bd)—asymptotically the same as breadth-first search.
Thereissomeextracostforgeneratingtheupperlevelsmultipletimes,butitisnotlarge. For
example,ifb = 10andd = 5,thenumbersare
N(IDS) = 50+400+3,000+20,000+100,000 = 123,450
N(BFS) = 10+100+1,000+10,000+100,000 = 111,110.
If you are really concerned about repeating the repetition, you can use a hybrid approach
that runs breadth-first search until almost all the available memory is consumed, and then
runs iterative deepening from all the nodes in the frontier. In general, iterative deepening is
the preferred uninformed search method when thesearch space islarge and thedepth ofthe
solution isnotknown.
Iterativedeepeningsearchisanalogous tobreadth-first searchinthatitexploresacom-
plete layer of new nodes at each iteration before going on to the next layer. It would seem
worthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algo-
rithm’s optimality guarantees while avoiding its memory requirements. The idea is to use
increasing path-cost limitsinstead ofincreasing depth limits. Theresulting algorithm, called
ITERATIVE
iterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that
LENGTHENING
SEARCH
iterativelengthening incurssubstantial overheadcomparedtouniform-cost search.
3.4.6 Bidirectional search
Theideabehindbidirectional searchistoruntwosimultaneous searches—one forwardfrom
the initial state andthe otherbackward from thegoal—hoping that thetwosearches meetin
the middle (Figure 3.20). The motivation is that bd/2 +bd/2 is much less than bd, or in the
figure, the areaofthe twosmall circles isless thanthe areaofone bigcircle centered onthe
startandreaching tothegoal.
Bidirectional search is implemented by replacing the goal test with a check to see
whether the frontiers of the two searches intersect; if they do, a solution has been found.
(It is important to realize that the first such solution found may not be optimal, even if the
two searches are both breadth-first; some additional search is required to make sure there
isn’t another short-cut across the gap.) The check can be done when each node is generated
or selected for expansion and, with a hash table, will take constant time. For example, if a
problem has solution depth d=6, and each direction runs breadth-first search one node at a
time,thenintheworstcasethetwosearches meetwhentheyhavegenerated allofthenodes
atdepth3. Forb=10,thismeansatotalof2,220nodegenerations,comparedwith1,111,110
for a standard breadth-first search. Thus, the time complexity of bidirectional search using
breadth-first searches in both directions is O(bd/2). The space complexity is also O(bd/2).
Wecanreducethisbyroughly halfifoneofthetwosearches is donebyiterative deepening,
butatleastoneofthefrontiers mustbekeptinmemorysothat theintersection checkcanbe
done. Thisspacerequirement isthemostsignificant weaknessofbidirectional search.
Section3.4. UninformedSearchStrategies 91
Start Goal
Figure 3.20 A schematic view of a bidirectionalsearch thatis aboutto succeed when a
branchfromthestartnodemeetsabranchfromthegoalnode.
Thereduction intimecomplexity makesbidirectional search attractive, buthowdowe
search backward? This is not as easy as it sounds. Let the predecessors of a state x be all
PREDECESSOR
thosestatesthathavexasasuccessor. Bidirectional searchrequiresamethodforcomputing
predecessors. Whenalltheactionsinthestatespacearereversible, thepredecessors of xare
justitssuccessors. Othercasesmayrequiresubstantial ingenuity.
Considerthequestion ofwhatwemeanby“thegoal”insearching “backwardfromthe
goal.” Forthe8-puzzleandforfindingarouteinRomania,thereisjustonegoalstate,sothe
backward search is very much like the forward search. If there are several explicitly listed
goalstates—forexample,thetwodirt-freegoalstatesinFigure3.3—thenwecanconstructa
newdummygoalstatewhoseimmediatepredecessors arealltheactualgoalstates. Butifthe
goal isan abstract description, such as thegoal that “no queen attacks another queen” in the
n-queensproblem,thenbidirectional searchisdifficulttouse.
3.4.7 Comparing uninformed search strategies
Figure 3.21 compares search strategies in terms of the four evaluation criteria set forth in
Section 3.3.2. This comparison is for tree-search versions. For graph searches, the main
differencesarethatdepth-firstsearchiscompleteforfinitestatespacesandthatthespaceand
timecomplexities arebounded bythesizeofthestatespace.
Breadth- Uniform- Depth- Depth- Iterative Bidirectional
Criterion
First Cost First Limited Deepening (ifapplicable)
Complete? Yesa Yesa,b No No Yesa Yesa,d
Time O(bd) O(b1+(cid:2)C∗/(cid:2)(cid:3)) O(bm) O(b(cid:3)) O(bd) O(bd/2)
Space O(bd) O(b1+(cid:2)C∗/(cid:2)(cid:3)) O(bm) O(b(cid:3)) O(bd) O(bd/2)
Optimal? Yesc Yes No No Yesc Yesc,d
Figure3.21 Evaluationoftree-searchstrategies. bisthebranchingfactor; disthedepth
ofthe shallowestsolution; m isthe maximumdepthofthe searchtree; l isthe depthlimit.
Superscriptcaveatsareasfollows: a completeifbisfinite; b completeifstepcosts≥ (cid:2)for
positive(cid:2);coptimalifstepcostsareallidentical;difbothdirectionsusebreadth-firstsearch.
92 Chapter 3. SolvingProblemsbySearching
3.5 INFORMED (HEURISTIC) SEARCH STRATEGIES
Thissectionshowshowaninformedsearchstrategy—onethatusesproblem-specificknowl-
INFORMEDSEARCH
edgebeyondthedefinitionoftheproblemitself—canfindsolutionsmoreefficientlythancan
anuninformedstrategy.
The general approach we consider is called best-first search. Best-first search is an
BEST-FIRSTSEARCH
instance of the general TREE-SEARCH or GRAPH-SEARCH algorithm in which a node is
EVALUATION selected for expansion based on an evaluation function, f(n). The evaluation function is
FUNCTION
construed as a cost estimate, so the node with the lowest evaluation is expanded first. The
implementation of best-first graph search is identical to that for uniform-cost search (Fig-
ure3.14),exceptfortheuseoff insteadofg toorderthepriority queue.
Thechoice off determines thesearch strategy. (Forexample, asExercise 3.21shows,
best-first tree search includes depth-first search asaspecial case.) Mostbest-first algorithms
HEURISTIC includeasacomponent off aheuristicfunction,denoted h(n):
FUNCTION
h(n) = estimatedcostofthecheapest pathfromthestateatnode ntoagoalstate.
(Notice that h(n) takes anode as input, but, unlike g(n), itdepends only on the state at that
node.) Forexample,inRomania,onemightestimatethecostofthecheapestpathfromArad
toBucharest viathestraight-line distance fromAradtoBucharest.
Heuristic functions are the most common form in which additional knowledge of the
problemisimpartedtothesearchalgorithm. WestudyheuristicsinmoredepthinSection3.6.
Fornow,weconsiderthemtobearbitrary, nonnegative, problem-specificfunctions, withone
constraint: ifnisagoalnode,thenh(n)=0. Theremainderofthissection coverstwoways
touseheuristic information toguidesearch.
3.5.1 Greedy best-first search
GREEDYBEST-FIRST Greedybest-firstsearch8 triestoexpandthenodethatisclosest tothegoal, onthegrounds
SEARCH
that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the
heuristic function; thatis,f(n)= h(n).
Letusseehowthisworksforroute-finding problemsinRomania;weusethestraight-
STRAIGHT-LINE line distance heuristic, which we will call h . If the goal is Bucharest, we need to
DISTANCE SLD
know the straight-line distances to Bucharest, which are shown in Figure 3.22. For exam-
ple, h (In(Arad))=366. Notice that the values of h cannot be computed from the
SLD SLD
problem description itself. Moreover, it takes a certain amount of experience to know that
h iscorrelated withactualroaddistances andis,therefore, ausefulheuristic.
SLD
Figure 3.23 shows the progress of a greedy best-first search using h to find a path
SLD
from Arad to Bucharest. The first node to be expanded from Arad will be Sibiu because it
is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will
be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For
this particular problem, greedy best-first search using h finds a solution without ever
SLD
8 Ourfirsteditioncalledthisgreedysearch; otherauthorshavecalleditbest-firstsearch. Ourmoregeneral
usageofthelattertermfollowsPearl(1984).
Section3.5. Informed(Heuristic)SearchStrategies 93
Arad 366 Mehadia 241
Bucharest 0 Neamt 234
Craiova 160 Oradea 380
Drobeta 242 Pitesti 100
Eforie 161 Rimnicu Vilcea 193
Fagaras 176 Sibiu 253
Giurgiu 77 Timisoara 329
Hirsova 151 Urziceni 80
Iasi 226 Vaslui 199
Lugoj 244 Zerind 374
Figure3.22 ValuesofhSLD—straight-linedistancestoBucharest.
expanding a node that is not on the solution path; hence, its search cost is minimal. It is
not optimal, however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longer
than the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called
“greedy”—at eachstepittriestogetasclosetothegoalasitcan.
Greedy best-first tree search is also incomplete even in a finite state space, much like
depth-first search. Consider the problem of getting from Iasi to Fagaras. The heuristic sug-
gests that Neamt be expanded first because it is closest to Fagaras, but it is a dead end. The
solution is to go first to Vaslui—a step that is actually farther from the goal according to
the heuristic—and then to continue to Urziceni, Bucharest, and Fagaras. The algorithm will
never find this solution, however, because expanding Neamt puts Iasi back into the frontier,
IasiisclosertoFagarasthanVasluiis,andsoIasiwillbeexpanded again, leading toaninfi-
niteloop. (Thegraphsearchversion iscompleteinfinitespaces,butnotininfiniteones.) The
worst-casetimeandspacecomplexityforthetreeversionisO(bm),wheremisthemaximum
depth of the search space. With a good heuristic function, however, the complexity can be
reducedsubstantially. Theamountofthereductiondepends ontheparticularproblemandon
thequalityoftheheuristic.
3.5.2 A* search: Minimizing the totalestimated solutioncost
∗ The most widely known form of best-first search is called A ∗ search (pronounced “A-star
A SEARCH
search”). Itevaluatesnodesbycombiningg(n),thecosttoreachthenode,andh(n),thecost
togetfromthenodetothegoal:
f(n)= g(n)+h(n).
Since g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost
ofthecheapest pathfrom ntothegoal,wehave
f(n)= estimatedcostofthecheapest solution through n.
Thus, if we are trying to find the cheapest solution, a reasonable thing to try first is the
node with the lowest value of g(n)+ h(n). It turns out that this strategy is more than just
∗
reasonable: providedthattheheuristicfunction h(n)satisfiescertainconditions, A searchis
both complete and optimal. The algorithm is identical to UNIFORM-COST-SEARCH except
∗
thatA usesg+hinsteadofg.
94 Chapter 3. SolvingProblemsbySearching
(a) The initial state Arad
366
(b) After expanding Arad Arad
Sibiu Timisoara Zerind
253 329 374
(c) After expanding Sibiu Arad
Sibiu Timisoara Zerind
329 374
Arad Fagaras Oradea Rimnicu Vilcea
366 176 380 193
(d) After expanding Fagaras Arad
Sibiu Timisoara Zerind
329 374
Arad Fagaras Oradea Rimnicu Vilcea
366 380 193
Sibiu Bucharest
253 0
Figure3.23 Stages in a greedybest-first tree search forBucharestwith the straight-line
distanceheuristichSLD. Nodesarelabeledwiththeirh-values.
Conditionsforoptimality: Admissibilityandconsistency
ADMISSIBLE The first condition we require for optimality is that h(n) be an admissible heuristic. An
HEURISTIC
admissible heuristic isonethat never overestimates thecost toreach the goal. Because g(n)
is the actual cost to reach n along the current path, and f(n)=g(n)+h(n), we have as an
immediate consequence that f(n) never overestimates the true cost of a solution along the
currentpaththrough n.
Admissible heuristics are by nature optimistic because they think the cost of solving
the problem is less than it actually is. An obvious example of an admissible heuristic is the
straight-line distance h that we used in getting to Bucharest. Straight-line distance is
SLD
admissible because theshortest pathbetweenanytwopoints isastraight line, sothestraight
Section3.5. Informed(Heuristic)SearchStrategies 95
∗
linecannot beanoverestimate. InFigure3.24,weshowtheprogressofanA treesearchfor
Bucharest. Thevaluesof g arecomputed fromthestepcosts inFigure3.2,andthevalues of
h aregiveninFigure3.22. NoticeinparticularthatBucharestfirstappearsonthefrontier
SLD
at step (e), but it is not selected forexpansion because its f-cost (450) is higher than that of
Pitesti (417). Anotherwaytosay this isthatthere mightbeasolution through Pitesti whose
costisaslowas417,sothealgorithm willnotsettleforasolution thatcosts450.
Asecond, slightly strongercondition called consistency (orsometimes monotonicity)
CONSISTENCY
isrequired only forapplications ofA ∗ tograph search.9 Aheuristic h(n)isconsistent if, for
MONOTONICITY
(cid:2)
every node n and every successor n of n generated by any action a, the estimated cost of
(cid:2)
reaching the goal from n is no greater than the step cost of getting to n plus the estimated
(cid:2)
costofreachingthegoalfrom n:
h(n) ≤ c(n,a,n (cid:2) )+h(n (cid:2) ).
TRIANGLE Thisisaformofthegeneral triangleinequality,whichstipulatesthateachsideofatriangle
INEQUALITY
(cid:2)
cannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n,
andthegoalG closestton. Foranadmissibleheuristic, theinequality makesperfect sense:
n
(cid:2)
if there were a route from n to G via n that was cheaper than h(n), that would violate the
n
property that h(n)isalowerboundonthecosttoreach G .
n
Itisfairlyeasytoshow(Exercise3.29)thateveryconsistentheuristicisalsoadmissible.
Consistency is therefore a stricter requirement than admissibility, but one has to work quite
hardtoconcoctheuristicsthatareadmissiblebutnotconsistent. Alltheadmissibleheuristics
we discuss in this chapter are also consistent. Consider, for example, h . We know that
SLD
the general triangle inequality is satisfied when each side is measured by the straight-line
(cid:2) (cid:2)
distance and that the straight-line distance between n and n is no greater than c(n,a,n).
Hence,h isaconsistent heuristic.
SLD
OptimalityofA*
∗ ∗
As we mentioned earlier, A has the following properties: the tree-search version of A is
optimalifh(n)isadmissible, whilethegraph-search versionisoptimalifh(n)isconsistent.
We show the second of these two claims since it is more useful. The argument es-
sentially mirrors the argument for the optimality of uniform-cost search, with g replaced by
∗
f—justasintheA algorithm itself.
The first step is to establish the following: if h(n) is consistent, then the values of
f(n) along any path are nondecreasing. The proof follows directly from the definition of
(cid:2) (cid:2) (cid:2)
consistency. Suppose n isasuccessor of n; then g(n)=g(n)+c(n,a,n)forsome action
a,andwehave
f(n (cid:2) )= g(n (cid:2) )+h(n (cid:2) ) = g(n)+c(n,a,n (cid:2) )+h(n (cid:2) )≥ g(n)+h(n) = f(n).
∗
The next step is to prove that whenever A selects a node n for expansion, the optimal path
to that node has been found. Were this not the case, there would have to be another frontier
(cid:2)
node n on the optimal path from the start node to n, by the graph separation property of
9 Withanadmissiblebutinconsistentheuristic,A∗requiressomeextrabookkeepingtoensureoptimality.
96 Chapter 3. SolvingProblemsbySearching
(a) The initial state
Arad
366=0+366
(b) After expanding Arad
Arad
Sibiu Timisoara Zerind
393=140+253 447=118+329 449=75+374
(c) After expanding Sibiu
Arad
Sibiu Timisoara Zerind
447=118+329 449=75+374
Arad Fagaras Oradea Rimnicu Vilcea
646=280+366 415=239+176671=291+380 413=220+193
(d) After expanding Rimnicu Vilcea
Arad
Sibiu Timisoara Zerind
447=118+329 449=75+374
Arad Fagaras Oradea Rimnicu Vilcea
646=280+366 415=239+176671=291+380
Craiova Pitesti Sibiu
526=366+160417=317+100 553=300+253
(e) After expanding Fagaras
Arad
Sibiu Timisoara Zerind
447=118+329 449=75+374
Arad Fagaras Oradea Rimnicu Vilcea
646=280+366 671=291+380
Sibiu Bucharest Craiova Pitesti Sibiu
591=338+253 450=450+0 526=366+160 417=317+100 553=300+253
(f) After expanding Pitesti
Arad
Sibiu Timisoara Zerind
447=118+329 449=75+374
Arad Fagaras Oradea Rimnicu Vilcea
646=280+366 671=291+380
Sibiu Bucharest Craiova Pitesti Sibiu
591=338+253 450=450+0 526=366+160 553=300+253
Bucharest Craiova Rimnicu Vilcea
418=418+0 615=455+160 607=414+193
Figure3.24 StagesinanA∗searchforBucharest.Nodesarelabeledwithf =g+h. The
hvaluesarethestraight-linedistancestoBucharesttakenfromFigure3.22.
Section3.5. Informed(Heuristic)SearchStrategies 97
O
N
Z
A I
380 S
F
V
400
T R
P
L
H
M U
B
420
D
E
C
G
Figure3.25 MapofRomaniashowingcontoursatf =380,f =400,andf =420,with
Aradas the start state. Nodesinside a givencontourhave f-costsless than orequalto the
contourvalue.
(cid:2)
Figure 3.9; because f is nondecreasing along any path, n would have lower f-cost than n
andwouldhavebeenselectedfirst.
From the two preceding observations, it follows that the sequence of nodes expanded
∗
by A using GRAPH-SEARCH is in nondecreasing order of f(n). Hence, the first goal node
selected forexpansion must be an optimal solution because f is the true cost forgoal nodes
(whichhaveh=0)andalllatergoalnodeswillbeatleastasexpensive.
The fact that f-costs are nondecreasing along any path also means that we can draw
contours in the state space, just like the contours in a topographic map. Figure 3.25 shows
CONTOUR
an example. Inside the contour labeled 400, all nodes have f(n) less than or equal to 400,
∗
and so on. Then, because A expands the frontier node of lowest f-cost, we can see that an
∗
A searchfansoutfromthestartnode,addingnodesinconcentricbandsofincreasingf-cost.
∗
With uniform-cost search (A search using h(n) = 0), the bands will be “circular”
around the start state. With more accurate heuristics, the bands will stretch toward the goal
∗
state and become more narrowly focused around the optimal path. If C is the cost of the
optimalsolutionpath,thenwecansaythefollowing:
• A ∗ expandsallnodeswithf(n)< C ∗ .
• A ∗ mightthenexpandsomeofthenodesrightonthe“goalcontour”(wheref(n)= C ∗ )
beforeselecting agoalnode.
Completeness requires that there be only finitely many nodes with cost less than orequal to
∗
C ,acondition thatistrueifallstepcostsexceedsomefinite(cid:2)andifbisfinite.
∗ ∗
Notice that A expands no nodes with f(n) > C —for example, Timisoara is not
expanded in Figure 3.24 even though it is a child of the root. Wesay that the subtree below
98 Chapter 3. SolvingProblemsbySearching
Timisoaraispruned;becauseh isadmissible,thealgorithmcansafelyignorethissubtree
PRUNING SLD
while still guaranteeing optimality. The concept of pruning—eliminating possibilities from
consideration withouthavingtoexaminethem—isimportant formanyareasofAI.
One final observation is that among optimal algorithms of this type—algorithms that
∗
extend search paths from the root and use the same heuristic information—A is optimally
OPTIMALLY efficient for any given consistent heuristic. That is, no other optimal algorithm is guaran-
EFFICIENT
∗
teedtoexpand fewernodes thanA (except possibly through tie-breaking amongnodeswith
∗ ∗
f(n)=C ). This is because any algorithm that does not expand all nodes with f(n) < C
runstheriskofmissingtheoptimalsolution.
∗
ThatA search iscomplete, optimal, andoptimally efficientamongallsuchalgorithms
∗
israthersatisfying. Unfortunately, itdoesnotmeanthatA istheanswertoalloursearching
needs. The catch is that, for most problems, the number of states within the goal contour
search space is still exponential in the length of the solution. The details of the analysis are
beyondthescopeofthisbook,butthebasicresultsareasfollows. Forproblemswithconstant
stepcosts, thegrowthinruntimeasafunction oftheoptimal solution depthdisanalyzed in
terms of the the absolute error or the relative error of the heuristic. The absolute error is
ABSOLUTEERROR
defined as Δ ≡ h ∗ −h, where h ∗ is the actual cost of getting from the root to the goal, and
RELATIVEERROR
therelativeerrorisdefinedas (cid:2) ≡ (h ∗ −h)/h ∗ .
The complexity results depend very strongly on the assumptions made about the state
space. The simplest model studied is a state space that has a single goal and is essentially a
tree with reversible actions. (The 8-puzzle satisfies the first and third of these assumptions.)
∗
Inthiscase, thetimecomplexity ofA isexponential inthemaximum absolute error, that is,
O(bΔ). For constant step costs, we can write this as O(b(cid:2)d), where d is the solution depth.
Foralmostallheuristics inpracticaluse,theabsoluteerrorisatleastproportional tothepath
∗
cost h , so (cid:2) is constant or growing and the time complexity is exponential in d. We can
alsoseetheeffectofamoreaccurateheuristic: O(b(cid:2)d)=O((b(cid:2))d),sotheeffectivebranching
factor(definedmoreformallyinthenextsection) is b(cid:2).
Whenthestate spacehasmanygoalstates—particularly near-optimal goalstates—the
searchprocesscanbeledastrayfromtheoptimalpathandthereisanextracostproportional
to the number of goals whose cost is within a factor (cid:2) of the optimal cost. Finally, in the
general case ofa graph, the situation is even worse. There can be exponentially many states
∗
with f(n) < C even if the absolute error is bounded by a constant. Forexample, consider
aversionofthevacuum worldwheretheagentcanclean upanysquare forunitcostwithout
evenhavingtovisitit: inthatcase,squarescanbecleanedinanyorder. WithN initiallydirty
squares, there are 2N states where some subset has been cleaned and all of them are on an
∗
optimalsolutionpath—andhencesatisfyf(n)< C —eveniftheheuristichasanerrorof1.
∗
ThecomplexityofA oftenmakesitimpracticaltoinsistonfindinganoptimalsolution.
∗
One can use variants of A that find suboptimal solutions quickly, or one can sometimes
design heuristics that are more accurate but not strictly admissible. In any case, the use of a
goodheuristic stillprovidesenormoussavingscomparedto theuseofanuninformed search.
InSection3.6,welookatthequestion ofdesigning goodheuristics.
∗
Computation timeisnot, however,A ’smaindrawback. Becauseitkeepsallgenerated
∗
nodes inmemory (asdo all GRAPH-SEARCH algorithms), A usually runs outof space long
Section3.5. Informed(Heuristic)SearchStrategies 99
functionRECURSIVE-BEST-FIRST-SEARCH(problem)returnsasolution,orfailure
returnRBFS(problem,MAKE-NODE(problem.INITIAL-STATE),∞)
functionRBFS(problem,node,f limit)returnsasolution,orfailureandanewf-costlimit
ifproblem.GOAL-TEST(node.STATE)thenreturnSOLUTION(node)
successors←[]
foreachaction inproblem.ACTIONS(node.STATE)do
addCHILD-NODE(problem,node,action) intosuccessors
ifsuccessors isemptythenreturnfailure,∞
foreachs insuccessors do /*updatef withvaluefromprevioussearch,ifany*/
s.f ←max(s.g + s.h, node.f))
loopdo
best←thelowestf-valuenodeinsuccessors
ifbest.f > f limit thenreturnfailure,best.f
alternative←thesecond-lowestf-valueamongsuccessors
result,best.f←RBFS(problem,best,min(f limit,alternative))
ifresult (cid:7)=failure thenreturnresult
Figure3.26 Thealgorithmforrecursivebest-firstsearch.
∗
before it runs out of time. For this reason, A is not practical for many large-scale prob-
lems. There are, however, algorithms that overcome the space problem without sacrificing
optimality orcompleteness, atasmallcostinexecution time. Wediscussthesenext.
3.5.3 Memory-bounded heuristic search
∗
The simplest way to reduce memory requirements for A is to adapt the idea of iterative
ITERATIVE- ∗ ∗
D∗EEPENING deepening to the heuristic search context, resulting in the iterative-deepening A (IDA)al-
A gorithm. ThemaindifferencebetweenIDA ∗ andstandarditerativedeepeningisthatthecutoff
usedisthef-cost(g+h)ratherthanthedepth;ateachiteration,thecutoffvalueisthesmall-
∗
est f-cost of any node that exceeded the cutoff on the previous iteration. IDA is practical
for many problems with unit step costs and avoids the substantial overhead associated with
keepingasortedqueueofnodes. Unfortunately,itsuffersfromthesamedifficultieswithreal-
valued costs as does the iterative version of uniform-cost search described in Exercise 3.17.
∗
Thissectionbrieflyexaminestwoothermemory-boundedalgorithms,calledRBFSandMA.
RECURSIVE Recursive best-first search (RBFS) is a simple recursive algorithm that attempts to
BEST-FIRSTSEARCH
mimictheoperation ofstandard best-first search, but using only linearspace. Thealgorithm
is shown in Figure 3.26. Its structure is similar to that of a recursive depth-first search, but
ratherthan continuing indefinitely downthecurrent path, ituses thef limit variable tokeep
track of the f-value of the best alternative path available from any ancestor of the current
node. If the current node exceeds this limit, the recursion unwinds back to the alternative
path. As the recursion unwinds, RBFS replaces the f-value of each node along the path
withabacked-upvalue—thebestf-valueofitschildren. Inthisway,RBFSremembersthe
BACKED-UPVALUE
f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth
100 Chapter 3. SolvingProblemsbySearching
(a) After expanding Arad, Sibiu, ∞
and Rimnicu Vilcea Arad 366
447
Sibiu Timisoara Zerind
393
447 449
415
Arad Fagaras Oradea Rimnicu Vilcea
413
646 415 671
Craiova Pitesti Sibiu
526 417 553
(b) After unwinding back to Sibiu ∞
and expanding Fagaras Arad
366
447
Sibiu Timisoara Zerind
393 447 449
417
Arad Fagaras Oradea Rimnicu Vilcea
646 415 671 413 417
Sibiu Bucharest
591 450
(c) After switching back to Rimnicu Vilcea ∞
and expanding Pitesti Arad
366
447
Sibiu Timisoara Zerind
393 447 449
447
Arad Fagaras Oradea Rimnicu Vilcea
646 415 450 671 417
447
Craiova Pitesti Sibiu
526 417 553
Bucharest Craiova Rimnicu Vilcea
418 615 607
Figure 3.27 Stages in an RBFS search for the shortest route to Bucharest. The f-limit
valueforeachrecursivecallisshownontopofeachcurrentnode,andeverynodeislabeled
withitsf-cost.(a)ThepathviaRimnicuVilceaisfolloweduntilthecurrentbestleaf(Pitesti)
hasavaluethatisworsethanthebestalternativepath(Fagaras). (b)Therecursionunwinds
andthe bestleaf value of the forgottensubtree(417)is backedup to Rimnicu Vilcea; then
Fagarasis expanded,revealinga bestleafvalueof 450. (c) Therecursionunwindsandthe
bestleafvalueoftheforgottensubtree(450)isbackeduptoFagaras;thenRimnicuVilceais
expanded.Thistime,becausethebestalternativepath(throughTimisoara)costsatleast447,
theexpansioncontinuestoBucharest.
reexpandingthesubtreeatsomelatertime. Figure3.27showshowRBFSreachesBucharest.
∗
RBFS is somewhat more efficient than IDA, but still suffers from excessive node re-
generation. In the example in Figure 3.27, RBFS follows the path via Rimnicu Vilcea, then
Section3.5. Informed(Heuristic)SearchStrategies 101
“changes its mind” and tries Fagaras, and then changes its mind back again. These mind
changes occur because every time the current best path is extended, its f-value is likely to
increase—h is usually less optimistic for nodes closer to the goal. When this happens, the
second-best path might become the best path, so the search has to backtrack to follow it.
∗
Eachmind change corresponds to aniteration of IDA and could require manyreexpansions
offorgotten nodestorecreatethebestpathandextenditone morenode.
∗
Like A tree search, RBFS is an optimal algorithm if the heuristic function h(n) is
admissible. Its space complexity is linear in the depth of the deepest optimal solution, but
its time complexity is rather difficult to characterize: it depends both on the accuracy of the
heuristic function andonhowoftenthebestpathchanges asnodesareexpanded.
∗ ∗
IDA and RBFS suffer from using too little memory. Between iterations, IDA retains
only a single number: the current f-cost limit. RBFS retains more information in memory,
butitusesonlylinearspace: evenifmorememorywereavailable,RBFShasnowaytomake
useofit. Becausetheyforgetmostofwhattheyhavedone,bothalgorithmsmayendupreex-
pandingthesamestatesmanytimesover. Furthermore,theysufferthepotentiallyexponential
increase incomplexityassociated withredundant pathsingraphs(seeSection3.3).
It seems sensible, therefore, to use all available memory. Two algorithms that do this
∗ ∗ ∗ ∗ ∗
are MA (memory-bounded A ) and SMA (simplified MA). SMA is—well—simpler, so
MA*
∗ ∗
wewilldescribeit. SMA proceedsjustlikeA,expanding thebestleafuntilmemoryisfull.
SMA*
∗
Atthispoint,itcannotaddanewnodetothesearchtreewithoutdropping anoldone. SMA
∗
always drops the worst leaf node—the one with the highest f-value. Like RBFS, SMA
then backs up the value of the forgotten node to its parent. In this way, the ancestor of a
forgotten subtree knows the quality of the best path in that subtree. With this information,
∗
SMA regenerates thesubtree onlywhenallotherpaths havebeenshowntolookworsethan
thepathithasforgotten. Anotherwayofsayingthisisthat, ifallthedescendants ofanoden
are forgotten, then we will not know which way to go from n, but we will still have an idea
ofhowworthwhileitistogoanywherefrom n.
Thecompletealgorithmistoocomplicatedtoreproducehere,10 butthereisonesubtlety
∗
worthmentioning. WesaidthatSMA expands thebestleafanddeletestheworstleaf. What
if all the leaf nodes have the same f-value? To avoid selecting the same node for deletion
∗
and expansion, SMA expands the newest best leaf and deletes the oldest worst leaf. These
coincide whenthereisonlyoneleaf,butinthatcase,thecurrentsearchtreemustbeasingle
pathfromroottoleafthatfillsallofmemory. Iftheleafisnotagoalnode,thenevenifitison
anoptimalsolutionpath,thatsolutionisnotreachablewiththeavailablememory. Therefore,
thenodecanbediscarded exactlyasifithadnosuccessors.
∗
SMA is complete if there is any reachable solution—that is, if d, the depth of the
shallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any
optimal solution is reachable; otherwise, it returns the best reachable solution. In practical
∗
terms,SMA isafairlyrobustchoiceforfindingoptimalsolutions,particularlywhenthestate
space is a graph, step costs are not uniform, and node generation is expensive compared to
theoverheadofmaintaining thefrontierandtheexploredset.
10 Aroughsketchappearedinthefirsteditionofthisbook.
102 Chapter 3. SolvingProblemsbySearching
∗
Onveryhardproblems,however,itwilloftenbethecasethat SMA isforcedtoswitch
backandforthcontinuallyamongmanycandidatesolutionpaths,onlyasmallsubsetofwhich
canfitinmemory. (Thisresembles the problem of thrashingindisk paging systems.) Then
THRASHING
the extra time required for repeated regeneration of the same nodes means that problems
∗
that would be practically solvable by A , given unlimited memory, become intractable for
∗
SMA. That is to say, memory limitations can make a problem intractable from the point
of view of computation time. Although no current theory explains the tradeoff between time
and memory, it seems that this is an inescapable problem. The only way out is to drop the
optimality requirement.
3.5.4 Learning to search better
We have presented several fixed strategies—breadth-first, greedy best-first, and so on—that
have been designed by computer scientists. Could anagent learn how to search better? The
METALEVELSTATE answerisyes,andthemethodrestsonanimportantconceptcalledthemetalevelstatespace.
SPACE
Eachstateinametalevelstatespacecaptures theinternal (computational) stateofaprogram
OBJECT-LEVELSTATE that is searching in an object-level state space such as Romania. For example, the internal
SPACE
∗
stateoftheA algorithmconsistsofthecurrentsearchtree. Eachactioninthemetalevelstate
space is acomputation step that alters the internal state; forexample, each computation step
∗
inA expandsaleafnodeandaddsitssuccessors tothetree. Thus,Figure3.24,whichshows
asequence of larger and larger search trees, can beseen asdepicting apath inthe metalevel
statespacewhereeachstateonthepathisanobject-level searchtree.
Now,thepathinFigure3.24hasfivesteps,includingonestep,theexpansionofFagaras,
that is not especially helpful. Forharder problems, there will be many such missteps, and a
METALEVEL metalevellearningalgorithmcanlearnfromtheseexperiencestoavoidexploringunpromis-
LEARNING
ing subtrees. Thetechniques used forthis kind of learning are described inChapter21. The
goal of learning is to minimize the total cost of problem solving, trading off computational
expenseandpathcost.
3.6 HEURISTIC FUNCTIONS
In this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature of
heuristics ingeneral.
The 8-puzzle was one of the earliest heuristic search problems. As mentioned in Sec-
tion 3.2, the object of the puzzle is toslide the tiles horizontally orvertically into the empty
spaceuntiltheconfiguration matchesthegoalconfiguration (Figure3.28).
Theaveragesolutioncostforarandomlygenerated8-puzzle instanceisabout22steps.
The branching factor is about 3. (When the empty tile is in the middle, four moves are
possible; when it is in a corner, two; and when it is along an edge, three.) This means
that an exhaustive tree search to depth 22 would look at about 322 ≈ 3.1×1010 states.
A graph search would cut this down by a factor of about 170,000 because only 9!/2 =
181,440 distinct states are reachable. (See Exercise 3.4.) This is a manageable number, but
Section3.6. HeuristicFunctions 103
7 2 4 1 2
5 6 3 4 5
8 3 1 6 7 8
Start State Goal State
Figure3.28 Atypicalinstanceofthe8-puzzle.Thesolutionis26stepslong.
the corresponding numberforthe 15-puzzle isroughly 1013,so thenext orderof business is
∗
to find a good heuristic function. If we want to find the shortest solutions by using A, we
needaheuristic function thatneveroverestimates thenumberofstepstothegoal. Thereisa
longhistory ofsuchheuristics forthe15-puzzle; herearetwocommonlyusedcandidates:
• h = the number of misplaced tiles. For Figure 3.28, all of the eight tiles are out of
1
position, so the start state would have h = 8. h is an admissible heuristic because it
1 1
isclearthatanytilethatisoutofplacemustbemovedatleastonce.
• h = the sum of the distances of the tiles from their goal positions. Because tiles
2
cannot move along diagonals, the distance we will count is the sum of the horizontal
andverticaldistances. ThisissometimescalledthecityblockdistanceorManhattan
MANHATTAN distance. h is also admissible because all any move can do is move one tile one step
DISTANCE 2
closertothegoal. Tiles1to8inthestartstategiveaManhattandistance of
h = 3+1+2+2+2+3+3+2 =18.
2
Asexpected, neitheroftheseoverestimates thetruesolution cost,whichis26.
3.6.1 The effect ofheuristic accuracy onperformance
∗
EFFECTIVE Onewaytocharacterize thequalityofaheuristicisthe effective branchingfactorb . Ifthe
BRANCHINGFACTOR
∗
totalnumberofnodesgeneratedbyA foraparticularproblemis N andthesolutiondepthis
∗
d, then b is the branching factor that a uniform tree of depth d would have to have in order
tocontain N +1nodes. Thus,
N +1 = 1+b ∗ +(b ∗ )2+···+(b ∗ )d .
∗
For example, if A finds a solution at depth 5 using 52 nodes, then the effective branching
factor is 1.92. Theeffective branching factor can vary across problem instances, but usually
it is fairly constant for sufficiently hard problems. (The existence of an effective branching
∗
factor follows from the result, mentioned earlier, that the number of nodes expanded by A
∗
grows exponentially withsolution depth.) Therefore, experimental measurements of b on a
smallsetofproblems canprovide agood guide tothe heuristic’s overall usefulness. Awell-
∗
designed heuristic would have a value of b close to 1, allowing fairly large problems to be
solvedatreasonable computational cost.
104 Chapter 3. SolvingProblemsbySearching
To test the heuristic functions h and h , we generated 1200 random problems with
1 2
solution lengths from 2 to 24 (100 for each even number) and solved them with iterative
∗
deepeningsearchandwithA treesearchusingbothh andh . Figure3.29givestheaverage
1 2
number of nodes generated by each strategy and the effective branching factor. The results
suggestthath isbetterthanh ,andisfarbetterthanusingiterativedeepening search. Even
2 1
for small problems with d=12, A ∗ with h is 50,000 times more efficient than uninformed
2
iterativedeepening search.
SearchCost(nodes generated) EffectiveBranching Factor
∗ ∗ ∗ ∗
d IDS A(h 1 ) A(h 2 ) IDS A(h 1 ) A(h 2 )
2 10 6 6 2.45 1.79 1.79
4 112 13 12 2.87 1.48 1.45
6 680 20 18 2.73 1.34 1.30
8 6384 39 25 2.80 1.33 1.24
10 47127 93 39 2.79 1.38 1.22
12 3644035 227 73 2.78 1.42 1.24
14 – 539 113 – 1.44 1.23
16 – 1301 211 – 1.45 1.25
18 – 3056 363 – 1.46 1.26
20 – 7276 676 – 1.47 1.27
22 – 18094 1219 – 1.48 1.28
24 – 39135 1641 – 1.48 1.26
Figure 3.29 Comparison of the search costs and effective branching factors for the
ITERATIVE-DEEPENING-SEARCH and A∗ algorithmswith h
1
, h
2
. Data are averagedover
100instancesofthe8-puzzleforeachofvarioussolutionlengthsd.
Onemightaskwhetherh isalwaysbetterthanh . Theansweris“Essentially, yes.” It
2 1
iseasytoseefromthedefinitions ofthetwoheuristics that, foranynode n,h (n) ≥ h (n).
2 1
∗
We thus say that h dominates h . Domination translates directly into efficiency: A using
DOMINATION 2 1
∗
h will never expand more nodes than A using h (except possibly for some nodes with
2 1
∗
f(n)=C ). The argument is simple. Recall the observation on page 97 that every node
∗
with f(n) < C will surely be expanded. This is the same as saying that every node with
h(n) < C ∗ −g(n) will surely be expanded. But because h is at least as big as h for all
2 1
∗
nodes, everynodethat issurely expanded byA search withh willalsosurely beexpanded
2
with h , and h might cause other nodes to be expanded as well. Hence, it is generally
1 1
better to use a heuristic function with higher values, provided it is consistent and that the
computation timefortheheuristic isnottoolong.
3.6.2 Generating admissibleheuristics from relaxed problems
We have seen that both h (misplaced tiles) and h (Manhattan distance) are fairly good
1 2
heuristics forthe 8-puzzle and that h isbetter. Howmight one have comeup with h ? Isit
2 2
possible foracomputertoinventsuchaheuristicmechanically?
h andh areestimates oftheremaining pathlengthforthe8-puzzle, buttheyarealso
1 2
perfectlyaccurate pathlengthsfor simplifiedversionsofthepuzzle. Iftherulesofthepuzzle
Section3.6. HeuristicFunctions 105
werechangedsothatatilecouldmoveanywhereinsteadofjusttotheadjacentemptysquare,
thenh wouldgivetheexactnumberofstepsintheshortestsolution. Similarly,ifatilecould
1
moveonesquareinanydirection,evenontoanoccupiedsquare,thenh wouldgivetheexact
2
number of steps in the shortest solution. A problem with fewerrestrictions on the actions is
called a relaxed problem. The state-space graph of the relaxed problem is a supergraph of
RELAXEDPROBLEM
theoriginalstatespacebecause theremovalofrestrictions createsaddededgesinthegraph.
Because the relaxed problem adds edges to the state space, anyoptimal solution in the
original problem is, by definition, also a solution in the relaxed problem; but the relaxed
problem may have better solutions if the added edges provide short cuts. Hence, the cost of
anoptimal solution toarelaxedproblem isanadmissible heuristic fortheoriginal problem.
Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must
obeythetriangleinequality andistherefore consistent(seepage95).
Ifaproblem definition iswritten down inaformal language, itis possible to construct
relaxedproblemsautomatically.11 Forexample,ifthe8-puzzle actionsaredescribed as
AtilecanmovefromsquareAtosquareBif
Aishorizontally orvertically adjacent toB andBisblank,
wecangeneratethreerelaxedproblemsbyremovingoneorbothoftheconditions:
(a)AtilecanmovefromsquareAtosquareBifAisadjacenttoB.
(b)AtilecanmovefromsquareAtosquareBifBisblank.
(c)AtilecanmovefromsquareAtosquareB.
From (a), we can derive h (Manhattan distance). The reasoning is that h would be the
2 2
properscoreifwemovedeachtileinturntoitsdestination. Theheuristic derivedfrom(b)is
discussed inExercise3.31. From(c),wecanderive h (misplaced tiles)because itwouldbe
1
theproperscore iftilescouldmovetotheirintended destination inonestep. Noticethatitis
crucialthattherelaxedproblemsgeneratedbythistechniquecanbesolvedessentiallywithout
search,becausetherelaxedrulesallowtheproblemtobedecomposedintoeightindependent
subproblems. If the relaxed problem is hard to solve, then the values of the corresponding
heuristic willbeexpensivetoobtain.12
Aprogram called ABSOLVER cangenerate heuristics automatically from problem def-
initions, using the “relaxed problem” method and various other techniques (Prieditis, 1993).
ABSOLVER generated a new heuristic for the 8-puzzle that was better than any preexisting
heuristic andfoundthefirstusefulheuristicforthefamous Rubik’sCubepuzzle.
One problem with generating new heuristic functions is that one often fails to get a
single “clearly best” heuristic. If a collection of admissible heuristics h ...h is available
1 m
foraproblem andnoneofthemdominates anyoftheothers, whichshould wechoose? Asit
turnsout,weneednotmakeachoice. Wecanhavethebestofallworlds,bydefining
h(n) = max{h (n),...,h (n)}.
1 m
11 InChapters8and10,wedescribeformallanguagessuitableforthistask;withformaldescriptionsthatcanbe
manipulated,theconstructionofrelaxedproblemscanbeautomated.Fornow,weuseEnglish.
12 Notethataperfect heuristiccanbeobtainedsimplybyallowinghtorunafullbreadth-first search“onthe
sly.”Thus,thereisatradeoffbetweenaccuracyandcomputationtimeforheuristicfunctions.
106 Chapter 3. SolvingProblemsbySearching
2 4 1 2
5 6 3 54 6
8 3 1 7 8
Start State Goal State
Figure3.30 A subproblemofthe 8-puzzleinstance givenin Figure 3.28. Thetask is to
gettiles 1, 2, 3, and4 intotheircorrectpositions,without worryingaboutwhathappensto
theothertiles.
This composite heuristic uses whichever function is most accurate on the node in question.
Becausethecomponentheuristicsareadmissible, hisadmissible;itisalsoeasytoprovethat
hisconsistent. Furthermore, hdominatesallofitscomponent heuristics.
3.6.3 Generating admissibleheuristics from subproblems: Patterndatabases
Admissible heuristics canalsobederived fromthesolution costofasubproblemofagiven
SUBPROBLEM
problem. For example, Figure 3.30 shows a subproblem of the 8-puzzle instance in Fig-
ure3.28. Thesubproblem involves gettingtiles1,2,3,4intotheircorrectpositions. Clearly,
the cost of the optimal solution of this subproblem is a lower bound on the cost of the com-
pleteproblem. ItturnsouttobemoreaccuratethanManhattan distance insomecases.
Theideabehind patterndatabases istostore theseexactsolution costs foreverypos-
PATTERNDATABASE
sible subproblem instance—in our example, every possible configuration of the four tiles
and the blank. (The locations of the other four tiles are irrelevant for the purposes of solv-
ing the subproblem, but moves of those tiles do count toward the cost.) Then we compute
an admissible heuristic h for each complete state encountered during a search simply by
DB
lookingupthecorrespondingsubproblemconfigurationinthedatabase. Thedatabaseitselfis
constructed bysearchingback13 fromthegoalandrecordingthecostofeachnewpatternen-
countered; theexpenseofthissearchisamortized overmany subsequent problem instances.
Thechoiceof1-2-3-4 isfairlyarbitrary; wecouldalsoconstruct databases for5-6-7-8,
for2-4-6-8, andsoon. Eachdatabaseyieldsanadmissibleheuristic, andtheseheuristics can
be combined, as explained earlier, by taking the maximum value. A combined heuristic of
thiskindismuchmoreaccuratethantheManhattandistance; thenumberofnodesgenerated
whensolvingrandom 15-puzzles canbereducedbyafactorof1000.
One might wonder whether the heuristics obtained from the 1-2-3-4 database and the
5-6-7-8couldbeadded,sincethetwosubproblems seemnottooverlap. Wouldthisstillgive
an admissible heuristic? The answer is no, because the solutions of the 1-2-3-4 subproblem
and the 5-6-7-8 subproblem for a given state will almost certainly share some moves—it is
13 By working backward from the goal, the exact solution cost of every instance encountered is immediately
available.Thisisanexampleofdynamicprogramming,whichwediscussfurtherinChapter17.
Section3.6. HeuristicFunctions 107
unlikely that1-2-3-4 can bemovedinto place without touching 5-6-7-8, andvice versa. But
whatifwedon’t count those moves? Thatis, werecord notthetotal costofsolving the1-2-
3-4 subproblem, but just the number of moves involving 1-2-3-4. Then it is easy to see that
thesumofthetwocostsisstillalowerboundonthecostofsolvingtheentireproblem. This
DISJOINTPATTERN is the idea behind disjoint pattern databases. With such databases, it is possible to solve
DATABASES
random 15-puzzles in a few milliseconds—the number of nodes generated is reduced by a
factorof10,000compared withtheuseofManhattan distance. For24-puzzles, aspeedup of
roughly afactorofamillioncanbeobtained.
Disjoint pattern databases work for sliding-tile puzzles because the problem can be
dividedupinsuchawaythateachmoveaffectsonlyonesubproblem—because onlyonetile
ismoved atatime. Foraproblem such as Rubik’s Cube, this kind ofsubdivision isdifficult
because each move affects 8 or 9 of the 26 cubies. More general ways of defining additive,
admissible heuristics have been proposed that do apply to Rubik’s cube (Yang et al., 2008),
buttheyhavenotyieldedaheuristicbetterthanthebestnonadditiveheuristicfortheproblem.
3.6.4 Learning heuristics from experience
A heuristic function h(n) is supposed to estimate the cost of a solution beginning from the
state at node n. How could an agent construct such a function? One solution was given in
the preceding sections—namely, to devise relaxed problems for which an optimal solution
can befound easily. Anothersolution isto learn from experience. “Experience” here means
solvinglotsof8-puzzles,forinstance. Eachoptimalsolutiontoan8-puzzleproblemprovides
examples from which h(n) can be learned. Each example consists of a state from the solu-
tionpathandtheactualcostofthesolution fromthatpoint. Fromtheseexamples, alearning
algorithm canbeusedtoconstruct afunction h(n)thatcan(withluck)predictsolution costs
forotherstates thatarise during search. Techniques fordoing just thisusing neural nets, de-
cisiontrees,andothermethodsaredemonstrated inChapter 18. (Thereinforcement learning
methodsdescribed inChapter21arealsoapplicable.)
Inductive learning methods work best when supplied with features of a state that are
FEATURE
relevant to predicting the state’s value, rather than with just the raw state description. For
example, the feature “number of misplaced tiles” might be helpful in predicting the actual
distance of a state from the goal. Let’s call this feature x (n). Wecould take 100 randomly
1
generated 8-puzzle configurations and gather statistics on their actual solution costs. We
might find that when x (n) is 5, the average solution cost is around 14, and so on. Given
1
thesedata,thevalueofx canbeusedtopredicth(n). Ofcourse,wecanuseseveralfeatures.
1
Asecondfeaturex (n)mightbe“numberofpairsofadjacenttilesthatarenotadjacentinthe
2
goalstate.” Howshouldx (n)andx (n)becombinedtopredicth(n)? Acommonapproach
1 2
istousealinearcombination:
h(n) = c x (n)+c x (n).
1 1 2 2
The constants c and c are adjusted to give the best fit to the actual data on solution costs.
1 2
Oneexpectsbothc andc tobepositivebecausemisplacedtilesandincorrectadjacentpairs
1 2
make the problem harder to solve. Notice that this heuristic does satisfy the condition that
h(n)=0forgoalstates,butitisnotnecessarily admissibleorconsistent.
108 Chapter 3. SolvingProblemsbySearching
3.7 SUMMARY
This chapter has introduced methods that an agent can use to select actions in environments
thataredeterministic, observable, static,andcompletelyknown. Insuchcases,theagentcan
construct sequences ofactionsthatachieveitsgoals;thisprocess iscalled search.
• Before anagent can start searching forsolutions, a goal must beidentified and awell-
definedproblemmustbeformulated.
• Aproblem consists of fiveparts: the initial state, aset ofactions, atransition model
describing the results of those actions, a goal test function, and a path cost function.
The environment of the problem is represented by a state space. A path through the
statespacefromtheinitialstatetoagoalstateisasolution.
• Search algorithms treat states and actions as atomic: theydo not consider anyinternal
structuretheymightpossess.
• A general TREE-SEARCH algorithm considers all possible paths to find a solution,
whereasaGRAPH-SEARCH algorithm avoids consideration ofredundant paths.
• Searchalgorithmsarejudgedonthebasisofcompleteness,optimality,timecomplex-
ity, and space complexity. Complexity depends onb, the branching factor inthe state
space,andd,thedepthoftheshallowestsolution.
• Uninformed search methods have access only to the problem definition. The basic
algorithmsareasfollows:
– Breadth-first search expands the shallowest nodes first; it is complete, optimal
forunitstepcosts, buthasexponential spacecomplexity.
– Uniform-costsearchexpandsthenodewithlowestpathcost,g(n),andisoptimal
forgeneralstepcosts.
– Depth-firstsearch expands the deepest unexpanded node first. It is neither com-
plete nor optimal, but has linear space complexity. Depth-limited search adds a
depthbound.
– Iterative deepening search calls depth-first search with increasing depth limits
untilagoalisfound. Itiscomplete,optimalforunitstepcosts,hastimecomplexity
comparable tobreadth-first search,andhaslinearspacecomplexity.
– Bidirectionalsearchcanenormously reducetimecomplexity,butitisnotalways
applicable andmayrequiretoomuchspace.
• Informedsearchmethodsmayhaveaccesstoaheuristicfunctionh(n)thatestimates
thecostofasolution fromn.
– Thegeneric best-firstsearch algorithm selectsanodeforexpansion according to
anevaluationfunction.
– Greedybest-first search expands nodes withminimalh(n). Itisnot optimal but
isoftenefficient.
Bibliographical andHistorical Notes 109
∗ ∗
– A search expands nodes withminimal f(n) = g(n)+h(n). A iscomplete and
optimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for
∗
GRAPH-SEARCH). ThespacecomplexityofA isstillprohibitive.
∗ ∗
– RBFS (recursive best-first search) and SMA (simplified memory-bounded A )
are robust, optimal search algorithms that use limited amounts of memory; given
∗
enough time, they can solve problems that A cannot solve because it runs out of
memory.
• Theperformance ofheuristic search algorithms depends on the quality ofthe heuristic
function. One can sometimes construct good heuristics by relaxing the problem defi-
nition, bystoring precomputed solution costs forsubproblems inapattern database, or
bylearning fromexperience withtheproblemclass.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Thetopicofstate-spacesearchoriginatedinmoreorlessitscurrentformintheearlyyearsof
AI.NewellandSimon’sworkontheLogicTheorist(1957)andGPS(1961)ledtotheestab-
lishmentofsearchalgorithms astheprimaryweaponsinthearmoryof1960sAIresearchers
and to the establishment of problem solving as the canonical AI task. Work in operations
research by Richard Bellman (1957) showed the importance of additive path costs in sim-
plifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson
(1971)established theareaonasolidtheoretical footing.
Most of the state-space search problems analyzed in this chapter have a long history
in the literature and are less trivial than they might seem. The missionaries and cannibals
problem used in Exercise 3.9 was analyzed in detail by Amarel (1968). It had been consid-
ered earlier—in AIby Simonand Newell(1961) and inoperations research by Bellman and
Dreyfus(1962).
The8-puzzle isasmallercousin ofthe15-puzzle, whosehistory isrecounted atlength
by Slocum and Sonneveld (2006). It was widely believed to have been invented by the fa-
mous American game designer Sam Loyd, based on his claims to that effect from 1891 on-
ward(Loyd, 1959). Actually itwasinvented byNoyesChapman, apostmasterinCanastota,
New York, in the mid-1870s. (Chapman was unable to patent his invention, as a generic
patentcoveringslidingblockswithletters,numbers,orpictureswasgrantedtoErnestKinsey
in1878.) Itquickly attracted theattention ofthepublic andofmathematicians (Johnson and
Story, 1879; Tait, 1880). The editors of the American Journal of Mathematics stated, “The
‘15’puzzleforthelastfewweekshasbeenprominentlybeforetheAmericanpublic,andmay
safely be said to have engaged the attention of nine out of ten persons of both sexes and all
agesandconditions ofthecommunity.” RatnerandWarmuth(1986)showedthatthegeneral
n×nversionofthe15-puzzle belongs totheclassofNP-complete problems.
The 8-queens problem was first published anonymously in the German chess maga-
zine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850
and at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who
110 Chapter 3. SolvingProblemsbySearching
attempted to enumerate all possible solutions; initially he found only 72, but eventually he
found the correct answer of 92, although Nauck published all 92 solutions first, in 1850.
Netto(1901)generalized theproblem tonqueens, andAbramsonandYung(1989)foundan
O(n)algorithm.
Each of the real-world search problems listed in the chapter has been the subject of a
gooddeal ofresearch effort. Methods forselecting optimal airline flightsremainproprietary
forthemostpart,butCarldeMarcken(personalcommunication)hasshownthatairlineticket
pricing and restrictions have become so convoluted that the problem of selecting an optimal
flight is formally undecidable. The traveling-salesperson problem is a standard combinato-
rial problem in theoretical computer science (Lawler et al., 1992). Karp (1972) proved the
TSPto be NP-hard, but effective heuristic approximation methods were developed (Lin and
Kernighan, 1973). Arora (1998) devised a fully polynomial approximation scheme for Eu-
clidean TSPs. VLSIlayout methods are surveyed by Shahookar and Mazumder (1991), and
manylayout optimization papers appearinVLSIjournals. Robotic navigation and assembly
problemsarediscussed inChapter25.
Uninformedsearchalgorithmsforproblemsolvingareacentraltopicofclassicalcom-
puterscience(HorowitzandSahni,1978)andoperations research(Dreyfus, 1969). Breadth-
first search was formulated for solving mazes by Moore (1959). The method of dynamic
programming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records
solutions for all subproblems of increasing lengths, can be seen as a form of breadth-first
search on graphs. The two-point shortest-path algorithm of Dijkstra (1959) is the origin
of uniform-cost search. These works also introduced the idea of explored and frontier sets
(closedandopenlists).
A version of iterative deepening designed to make efficient use of the chess clock was
first used by Slate and Atkin (1977) in the CHESS 4.5 game-playing program. Martelli’s
∗
algorithmB(1977)includesaniterativedeepeningaspectandalsodominatesA’sworst-case
performance with admissible but inconsistent heuristics. The iterative deepening technique
came to the fore in work by Korf (1985a). Bidirectional search, which was introduced by
Pohl(1971), canalsobeeffectiveinsomecases.
Theuseofheuristicinformationinproblemsolvingappears inanearlypaperbySimon
and Newell (1958), but the phrase “heuristic search” and the use of heuristic functions that
estimate the distance to the goal came somewhat later (Newell and Ernst, 1965; Lin, 1965).
Doran and Michie (1966) conducted extensive experimental studies of heuristic search. Al-
thoughtheyanalyzed pathlengthand“penetrance” (theratioofpathlengthtothetotalnum-
ber of nodes examined so far), they appear to have ignored the information provided by the
∗
path cost g(n). The A algorithm, incorporating the current path cost into heuristic search,
wasdevelopedbyHart,Nilsson,andRaphael(1968),withsomelatercorrections(Hartetal.,
∗
1972). DechterandPearl(1985)demonstrated theoptimalefficiencyofA.
∗
Theoriginal A paperintroduced theconsistency condition onheuristic functions. The
monotoneconditionwasintroducedbyPohl(1977)asasimplerreplacement,butPearl(1984)
showedthatthetwowereequivalent.
Pohl(1977) pioneered thestudyoftherelationship between theerrorinheuristic func-
∗
tionsandthetimecomplexityofA. Basicresultswereobtainedfortreesearchwithunitstep
Bibliographical andHistorical Notes 111
costsandasinglegoalnode(Pohl,1977;Gaschnig,1979;Huynetal.,1980;Pearl,1984)and
withmultiple goalnodes (Dinhetal., 2007). The“effective branching factor” wasproposed
by Nilsson (1971) as an empirical measure of the efficiency; it is equivalent to assuming a
timecostofO((b ∗ )d). Fortreesearchappliedtoagraph,Korfetal.(2001)arguethatthetime
cost is better modeled as
O(bd−k),
where k depends on the heuristic accuracy; this analysis
has elicited some controversy, however. For graph search, Helmert and Ro¨ger (2008) noted
that several well-known problems contained exponentially many nodes on optimal solution
∗
paths,implyingexponential timecomplexity forA evenwithconstant absolute errorin h.
∗
TherearemanyvariationsontheA algorithm. Pohl(1973)proposedtheuseofdynamic
weighting, whichusesaweightedsumf (n)=w g(n)+w h(n)ofthecurrentpathlength
w g h
andtheheuristicfunctionasanevaluationfunction,ratherthanthesimplesumf(n)=g(n)+
∗
h(n)used inA. Theweights w and w are adjusted dynamically asthe search progresses.
g h
Pohl’salgorithmcanbeshowntobe(cid:2)-admissible—thatis,guaranteedtofindsolutionswithin
afactor 1+(cid:2)of the optimal solution, where (cid:2) isaparameter supplied tothe algorithm. The
∗
samepropertyisexhibitedbytheA algorithm(Pearl,1984),whichcanselectanynodefrom
(cid:2)
thefrontierprovideditsf-costiswithinafactor1+(cid:2)ofthelowest-f-costfrontiernode. The
selection canbedonesoastominimizesearchcost.
∗ ∗
Bidirectional versions of A have been investigated; a combination of bidirectional A
and known landmarks was used to efficiently find driving routes for Microsoft’s online map
service(Goldbergetal.,2006). Aftercachingasetofpathsbetweenlandmarks,thealgorithm
canfindanoptimalpathbetweenanypairofpointsina24millionpointgraphoftheUnited
States, searching less than 0.1% of the graph. Others approaches to bidirectional search
include a breadth-first search backward from the goal up to a fixed depth, followed by a
∗
forwardIDA search(Dillenburg andNelson,1994;Manzini,1995).
∗
A andotherstate-space searchalgorithms areclosely relatedtothebranch-and-bound
techniques that are widely used in operations research (Lawler and Wood, 1966). The
relationships between state-space search and branch-and-bound have been investigated in
depth (Kumarand Kanal, 1983; Nauet al., 1984; Kumaret al., 1988). Martelli and Monta-
nari (1978) demonstrate a connection between dynamic programming (see Chapter 17) and
certaintypesofstate-spacesearch. KumarandKanal(1988)attempta“grandunification”of
heuristic search, dynamic programming, and branch-and-bound techniques under the name
ofCDP—the“composite decision process.”
Becausecomputersinthelate1950sandearly1960shadatmostafewthousandwords
of main memory, memory-bounded heuristic search wasan early research topic. The Graph
Traverser (Doran and Michie, 1966), one of the earliest search programs, commits to an
∗
operatoraftersearchingbest-firstuptothememorylimit. IDA (Korf,1985a,1985b)wasthe
first widely used optimal, memory-bounded heuristic search algorithm, and a large number
∗
of variants have been developed. Ananalysis of the efficiency of IDA and of its difficulties
withreal-valued heuristics appearsinPatrick etal.(1992).
RBFS (Korf, 1993) is actually somewhat more complicated than the algorithm shown
in Figure 3.26, which is closer to anindependently developed algorithm called iterative ex-
ITERATIVE pansion (Russell, 1992). RBFSuses a lower bound as well as the upper bound; the two al-
EXPANSION
gorithmsbehaveidentically withadmissible heuristics, butRBFSexpandsnodesinbest-first
112 Chapter 3. SolvingProblemsbySearching
order even with an inadmissible heuristic. The idea of keeping track of the best alternative
∗ ∗
pathappearedearlierinBratko’s(1986)elegantPrologimplementationofA andintheDTA
algorithm (Russell and Wefald, 1991). Thelatter work also discusses metalevel state spaces
andmetalevellearning.
∗ ∗ ∗
The MA algorithm appeared in Chakrabarti et al. (1989). SMA, or Simplified MA,
∗
emergedfromanattempttoimplementMA asacomparisonalgorithmforIE(Russell,1992).
∗
KaindlandKhorsand (1994) have applied SMA toproduce abidirectional search algorithm
thatissubstantiallyfasterthanpreviousalgorithms. KorfandZhang(2000)describeadivide-
∗
and-conquer approach, and Zhou and Hansen (2002) introduce memory-bounded A graph
search and a strategy for switching to breadth-first search to increase memory-efficiency
(ZhouandHansen,2006). Korf(1995)surveysmemory-bounded searchtechniques.
Theideathatadmissibleheuristics canbederivedbyproblem relaxation appearsinthe
seminal paper by Held and Karp (1970), who used the minimum-spanning-tree heuristic to
solvetheTSP.(SeeExercise3.30.)
The automation of the relaxation process was implemented successfully by Priedi-
tis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989). Holte and
Hernadvolgyi (2001) describe morerecent steps towardsautomating theprocess. Theuseof
pattern databases to derive admissible heuristics is due to Gasser (1995) and Culberson and
Schaeffer (1996, 1998); disjoint pattern databases are described by Korf and Felner (2002);
a similar method using symbolic patterns is due to Edelkamp (2009). Felner et al. (2007)
show how to compress pattern databases to save space. The probabilistic interpretation of
heuristics wasinvestigated indepthbyPearl(1984)andHanssonandMayer(1989).
By far the most comprehensive source on heuristics and heuristic search algorithms
is Pearl’s (1984) Heuristics text. This book provides especially good coverage of the wide
∗
varietyofoffshootsandvariationsofA ,includingrigorousproofsoftheirformalproperties.
Kanal and Kumar(1988) present an anthology of important articles on heuristic search, and
Rayward-Smithetal.(1996)coverapproaches fromOperations Research. Papersaboutnew
search algorithms—which, remarkably, continue to be discovered—appear in journals such
asArtificialIntelligence andJournal oftheACM.
Thetopic ofparallel search algorithms wasnot coveredinthechapter, partly because
PARALLELSEARCH
it requires a lengthy discussion of parallel computer architectures. Parallel search became a
populartopicinthe1990sinbothAIandtheoreticalcomputerscience(MahantiandDaniels,
1993; Grama and Kumar, 1995; Crauser et al., 1998) and is making a comeback in the era
of new multicore and cluster architectures (Ralphs et al., 2004; Korf and Schultze, 2005).
Also of increasing importance are search algorithms for very large graphs that require disk
storage(Korf,2008).
EXERCISES
3.1 Explainwhyproblem formulation mustfollowgoalformulation.
3.2 Yourgoalistonavigatearobotoutofamaze. Therobotstarts inthecenterofthemaze
Exercises 113
facing north. You can turn the robot to face north, east, south, or west. You can direct the
robottomoveforwardacertaindistance, although itwillstopbeforehittingawall.
a. Formulatethisproblem. Howlargeisthestatespace?
b. In navigating a maze, the only place we need to turn is at the intersection of two or
morecorridors. Reformulatethisproblemusingthisobservation. Howlargeisthestate
spacenow?
c. Fromeachpointinthemaze,wecanmoveinanyofthefourdirectionsuntilwereacha
turningpoint, andthisistheonlyactionweneedtodo. Reformulatetheproblemusing
theseactions. Doweneedtokeeptrackoftherobot’sorientation now?
d. In our initial description of the problem we already abstracted from the real world,
restricting actionsandremovingdetails. Listthreesuchsimplifications wemade.
3.3 Suppose twofriends livein different cities ona map, such as the Romania map shown
in Figure 3.2. Onevery turn, wecan simultaneously move each friend to a neighboring city
onthemap. Theamountoftimeneededtomovefromcityitoneighborj isequaltotheroad
distanced(i,j)betweenthecities,butoneachturnthefriendthatarrivesfirstmustwaituntil
theotheronearrives(andcallsthefirstonhis/hercellphone) before thenextturncanbegin.
Wewantthetwofriendstomeetasquicklyaspossible.
a. Writeadetailed formulation forthissearch problem. (You willfindithelpful todefine
someformalnotation here.)
b. LetD(i,j) bethestraight-line distance between cities iandj. Whichofthefollowing
heuristicfunctions areadmissible? (i)D(i,j);(ii)2·D(i,j);(iii)D(i,j)/2.
c. Aretherecompletely connected mapsforwhichnosolution exists?
d. Aretheremapsinwhichallsolutions requireonefriendtovisitthesamecitytwice?
3.4 Show that the 8-puzzle states are divided into two disjoint sets, such that any state is
reachable from any other state in the same set, while no state is reachable from any state in
theotherset. (Hint: SeeBerlekamp etal. (1982).) Deviseaprocedure todecide whichset a
givenstateisin,andexplainwhythisisusefulforgenerating random states.
3.5 Consider the n-queens problem using the “effi√cient” incremental formulation given on
page 72. Explain why the state space has at least 3 n! states and estimate the largest n for
whichexhaustiveexplorationisfeasible. (Hint: Derivealowerboundonthebranchingfactor
byconsideringthemaximumnumberofsquaresthataqueencanattackinanycolumn.)
3.6 Givea complete problem formulation foreach of the following. Choose aformulation
thatisprecise enoughtobeimplemented.
a. Using only four colors, you have to color a planar map in such a way that no two
adjacentregionshavethesamecolor.
b. A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot
ceiling. Hewould like to get the bananas. Theroom contains twostackable, movable,
climbable3-foot-high crates.
114 Chapter 3. SolvingProblemsbySearching
G
S
Figure3.31 Ascenewithpolygonalobstacles. S andGarethestartandgoalstates.
c. Youhave aprogram that outputs the message “illegal input record” when feda certain
file of input records. You know that processing of each record is independent of the
otherrecords. Youwanttodiscoverwhatrecordisillegal.
d. Youhavethreejugs,measuring12gallons,8gallons,and3gallons,andawaterfaucet.
Youcanfillthejugsuporemptythemoutfromonetoanotherorontotheground. You
needtomeasureoutexactlyonegallon.
3.7 Considertheproblemoffindingtheshortestpathbetweentwopointsonaplanethathas
convex polygonal obstacles as shown in Figure 3.31. This is an idealization of the problem
thatarobothastosolvetonavigateinacrowdedenvironment.
a. Suppose the state space consists of all positions (x,y) in the plane. How many states
arethere? Howmanypathsaretheretothegoal?
b. Explainbrieflywhytheshortestpathfromonepolygonvertextoanyotherinthescene
must consist of straight-line segments joining some of the vertices of the polygons.
Defineagoodstatespacenow. Howlargeisthisstatespace?
c. Definethenecessaryfunctionstoimplementthesearchproblem,includinganACTIONS
functionthattakesavertexasinputandreturnsasetofvectors,eachofwhichmapsthe
currentvertextooneoftheverticesthatcanbereachedinastraightline. (Donotforget
the neighbors on the same polygon.) Use the straight-line distance for the heuristic
function.
d. Applyoneormoreofthealgorithms inthischaptertosolvea rangeofproblemsinthe
domain,andcommentontheirperformance.
3.8 Onpage 68, wesaid that wewould not consider problems withnegative path costs. In
thisexercise, weexplorethisdecision inmoredepth.
a. Suppose that actions can have arbitrarily large negative costs; explain why this possi-
bilitywouldforceanyoptimalalgorithm toexploretheentirestatespace.
Exercises 115
b. Does it help if we insist that step costs must be greater than or equal to some negative
constantc? Considerbothtreesandgraphs.
c. Supposethatasetofactionsformsaloopinthestatespacesuchthatexecutingthesetin
someorderresultsinnonetchangetothestate. Ifalloftheseactionshavenegativecost,
whatdoesthisimplyabouttheoptimalbehaviorforanagentinsuchanenvironment?
d. Onecan easily imagine actions withhigh negative cost, even indomains such asroute
finding. For example, some stretches of road might have such beautiful scenery as to
far outweigh the normal costs in terms of time and fuel. Explain, in precise terms,
withinthecontext ofstate-space search, whyhumans donotdrivearound scenic loops
indefinitely, and explain how to define the state space and actions for route finding so
thatartificialagentscanalsoavoid looping.
e. Canyouthinkofarealdomaininwhichstepcostsaresuchastocauselooping?
3.9 Themissionaries and cannibals problem is usually stated as follows. Three mission-
aries and three cannibals are on one side of a river, along with a boat that can hold one or
twopeople. Findawaytogeteveryonetotheothersidewithouteverleavingagroupofmis-
sionariesinoneplaceoutnumbered bythecannibals inthatplace. Thisproblemisfamousin
AIbecauseitwasthesubjectofthefirstpaperthatapproached problemformulation froman
analytical viewpoint(Amarel,1968).
a. Formulate the problem precisely, making only those distinctions necessary to ensure a
validsolution. Drawadiagram ofthecompletestatespace.
b. Implementandsolvetheproblemoptimallyusinganappropriatesearchalgorithm. Isit
agoodideatocheckforrepeated states?
c. Whydoyouthinkpeoplehaveahardtimesolvingthispuzzle,giventhatthestatespace
issosimple?
3.10 Define in your own words the following terms: state, state space, search tree, search
node,goal,action, transition model,andbranching factor.
3.11 What’s the difference between a world state, a state description, and a search node?
Whyisthisdistinction useful?
3.12 AnactionsuchasGo(Sibiu)reallyconsistsofalongsequenceoffiner-grainedactions:
turn on the car, release the brake, accelerate forward, etc. Having composite actions of this
kind reduces the number of steps in a solution sequence, thereby reducing the search time.
Supposewetakethistothelogicalextreme,bymakingsuper-composite actions outofevery
possible sequence of Go actions. Then every problem instance is solved by a single super-
composite action, such as Go(Sibiu)Go(Rimnicu Vilcea)Go(Pitesti)Go(Bucharest). Explain
how search would work in this formulation. Is this a practical approach for speeding up
problem solving?
3.13 Prove that GRAPH-SEARCH satisfies the graph separation property illustrated in Fig-
ure3.9. (Hint: Beginbyshowingthatthepropertyholdsatthestart,thenshowthatifitholds
before an iteration of the algorithm, it holds afterwards.) Describe a search algorithm that
violates theproperty.
116 Chapter 3. SolvingProblemsbySearching
x 12
x 2 x 2
x 16
Figure3.32 Thetrackpiecesinawoodenrailwayset;eachislabeledwiththenumberof
copiesintheset. Notethatcurvedpiecesand“fork”pieces(“switches”or“points”)canbe
flippedoversotheycancurveineitherdirection.Eachcurvesubtends45degrees.
3.14 Whichofthefollowingaretrueandwhicharefalse? Explainyouranswers.
∗
a. Depth-firstsearchalwaysexpandsatleastasmanynodesasA searchwithanadmissi-
bleheuristic.
b. h(n)= 0isanadmissibleheuristic forthe8-puzzle.
∗
c. A isofnouseinrobotics becausepercepts, states, andactionsarecontinuous.
d. Breadth-firstsearchiscompleteevenifzerostepcostsare allowed.
e. Assumethatarookcanmoveonachessboard anynumberofsquaresinastraight line,
vertically orhorizontally, but cannot jump overother pieces. Manhattan distance is an
admissible heuristic for the problem of moving the rook from square A to square B in
thesmallestnumberofmoves.
3.15 Consider a state space where the start state is number 1 and each state k has two
successors: numbers 2k and2k+1.
a. Drawtheportionofthestatespaceforstates1to15.
b. Suppose the goal state is 11. List the order in which nodes will be visited forbreadth-
firstsearch,depth-limited searchwithlimit3,anditerativedeepening search.
c. How well would bidirectional search work on this problem? What is the branching
factorineachdirection ofthebidirectional search?
d. Doestheanswerto(c)suggest areformulation oftheproblem thatwouldallow youto
solvetheproblemofgettingfromstate1toagivengoalstate withalmostnosearch?
e. Calltheactiongoingfrom k to2k Left,andtheactiongoingto2k+1Right. Canyou
findanalgorithm thatoutputsthesolution tothisproblem withoutanysearchatall?
3.16 A basic wooden railway set contains the pieces shown in Figure 3.32. The task is to
connect these piecesintoarailwaythathasnooverlapping tracksandnoloose endswherea
traincouldrunoffontothefloor.
a. Supposethatthepiecesfittogetherexactlywithnoslack. Giveapreciseformulationof
thetaskasasearchproblem.
b. Identifyasuitable uninformed searchalgorithm forthistaskandexplainyourchoice.
c. Explainwhyremovinganyoneofthe“fork”piecesmakestheproblem unsolvable.
Exercises 117
d. Give an upper bound on the total size of the state space defined by your formulation.
(Hint: think about the maximum branching factor for the construction process and the
maximumdepth, ignoring theproblem ofoverlapping pieces andloose ends. Beginby
pretending thateverypieceisunique.)
3.17 On page 90, we mentioned iterative lengthening search, an iterative analog of uni-
form cost search. The idea is to use increasing limits on path cost. If a node is generated
whose path cost exceeds the current limit, it is immediately discarded. For each new itera-
tion,thelimitissettothelowestpathcostofanynodediscarded inthepreviousiteration.
a. Showthatthisalgorithm isoptimalforgeneralpathcosts.
b. Consider a uniform tree with branching factor b, solution depth d, and unit step costs.
Howmanyiterations williterativelengthening require?
c. Nowconsiderstepcostsdrawnfromthecontinuousrange [(cid:2),1],where0 < (cid:2) < 1. How
manyiterations arerequiredintheworstcase?
d. Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales-
personproblems. Comparethealgorithm’sperformance tothatofuniform-cost search,
andcommentonyourresults.
3.18 Describe astate space inwhichiterative deepening search performs muchworsethan
depth-firstsearch(forexample, O(n2)vs. O(n)).
3.19 Write a program that will take as input two Web page URLs and find a path of links
from onetotheother. Whatisanappropriate search strategy? Isbidirectional search agood
idea? Couldasearchenginebeusedtoimplementapredecessor function?
3.20 Considerthevacuum-world problemdefinedinFigure2.2.
a. Whichofthealgorithms definedinthischapterwouldbeappropriate forthisproblem?
Shouldthealgorithm usetreesearchorgraphsearch?
b. Apply your chosen algorithm to compute an optimal sequence of actions for a 3×3
worldwhoseinitialstatehasdirtinthethreetopsquares andtheagentinthecenter.
c. Constructasearchagentforthevacuumworld,andevaluate itsperformance inasetof
3×3worldswithprobability 0.2ofdirtineachsquare. Include thesearchcostaswell
aspathcostintheperformance measure,usingareasonable exchange rate.
d. Compare your best search agent with a simple randomized reflex agent that sucks if
thereisdirtandotherwise movesrandomly.
e. Consider what would happen if the world were enlarged to n×n. How does the per-
formanceofthesearchagentandofthereflexagentvarywith n?
3.21 Proveeachofthefollowingstatements, orgiveacounterexample:
a. Breadth-firstsearchisaspecialcaseofuniform-cost search.
b. Depth-firstsearchisaspecialcaseofbest-firsttreesearch.
∗
c. Uniform-costsearchisaspecial caseofA search.
118 Chapter 3. SolvingProblemsbySearching
∗
3.22 Compare the performance of A and RBFS on a set of randomly generated problems
inthe8-puzzle(withManhattandistance)andTSP(withMST—seeExercise3.30)domains.
Discussyourresults. Whathappenstotheperformance ofRBFSwhenasmallrandomnum-
berisaddedtotheheuristic valuesinthe8-puzzle domain?
∗
3.23 Trace the operation of A search applied to the problem of getting to Bucharest from
Lugojusing thestraight-line distance heuristic. Thatis, show thesequence ofnodes that the
algorithm willconsiderandthe f,g,andhscoreforeachnode.
∗
3.24 DeviseastatespaceinwhichA usingGRAPH-SEARCH returnsasuboptimalsolution
withanh(n)function thatisadmissible butinconsistent.
HEURISTICPATH 3.25 The heuristic path algorithm (Pohl, 1977) is a best-first search in which the evalu-
ALGORITHM
ation function is f(n) = (2 − w)g(n) + wh(n). For what values of w is this complete?
Forwhat values is it optimal, assuming that h is admissible? What kind of search does this
perform forw = 0,w = 1,andw = 2?
3.26 Considertheunbounded version oftheregular2DgridshowninFigure3.9. Thestart
stateisattheorigin, (0,0),andthegoalstateisat(x,y).
a. Whatisthebranching factorbinthisstatespace?
b. Howmanydistinct statesarethereatdepth k(fork > 0)?
c. Whatisthemaximumnumberofnodesexpandedbybreadth-first treesearch?
d. Whatisthemaximumnumberofnodesexpandedbybreadth-first graphsearch?
e. Ish = |u−x|+|v−y|anadmissible heuristic forastateat(u,v)? Explain.
∗
f. HowmanynodesareexpandedbyA graphsearchusing h?
g. Doeshremainadmissible ifsomelinksareremoved?
h. Doeshremainadmissible ifsomelinksareaddedbetweennonadjacent states?
3.27 nvehiclesoccupysquares(1,1)through(n,1)(i.e.,thebottomrow)ofann×ngrid.
Thevehicles mustbemovedtothetoprowbutinreverseorder; sothevehicleithatstartsin
(i,1)mustendupin(n−i+1,n). Oneachtimestep,everyoneofthenvehiclescanmove
one square up, down, left, or right, orstay put; but if a vehicle stays put, one other adjacent
vehicle(butnotmorethanone)canhopoverit. Twovehiclescannotoccupythesamesquare.
a. Calculatethesizeofthestatespaceasafunction ofn.
b. Calculatethebranching factorasafunction ofn.
c. Suppose that vehicle i is at (x ,y ); write a nontrivial admissible heuristic h for the
i i i
numberof moves itwillrequire to get toits goal location (n−i+1,n), assuming no
othervehiclesareonthegrid.
d. Whichofthefollowingheuristics areadmissible fortheproblem ofmovingall nvehi-
clestotheirdestinations? Explain.
(cid:2)
(i) n h .
i=1 i
(ii) max{h ,...,h }.
1 n
(iii) min{h ,...,h }.
1 n
Exercises 119
3.28 Invent a heuristic function for the 8-puzzle that sometimes overestimates, and show
howitcanleadtoasuboptimalsolution onaparticularproblem. (Youcanuseacomputerto
∗
help if you want.) Prove that if h never overestimates by more than c, A using h returns a
solution whosecostexceedsthatoftheoptimalsolution bynomorethan c.
3.29 Prove that if a heuristic is consistent, it must be admissible. Construct an admissible
heuristic thatisnotconsistent.
3.30 The traveling salesperson problem (TSP)can be solved with the minimum-spanning-
tree (MST)heuristic, which estimates the cost of completing atour, given that a partial tour
has already been constructed. TheMSTcost ofaset ofcities isthe smallest sum of thelink
costsofanytreethatconnects allthecities.
a. Showhowthisheuristic canbederivedfromarelaxedversionoftheTSP.
b. ShowthattheMSTheuristicdominates straight-line distance.
c. Write a problem generator for instances of the TSP where cities are represented by
randompointsintheunitsquare.
∗
d. Findanefficientalgorithmintheliteratureforconstructing theMST,anduseitwithA
graphsearchtosolveinstances oftheTSP.
3.31 Onpage 105,wedefinedtherelaxation ofthe8-puzzle inwhich atilecanmovefrom
square A to square B if B is blank. The exact solution of this problem defines Gaschnig’s
heuristic (Gaschnig, 1979). Explain why Gaschnig’s heuristic is at least as accurate as h
1
(misplaced tiles), and show cases where it is moreaccurate than both h and h (Manhattan
1 2
distance). Explainhowtocalculate Gaschnig’s heuristicefficiently.
3.32 We gave two simple heuristics for the 8-puzzle: Manhattan distance and misplaced
tiles. Several heuristics in the literature purport to improve on this—see, for example, Nils-
son (1971), Mostow and Prieditis (1989), and Hansson et al. (1992). Test these claims by
implementingtheheuristicsandcomparingtheperformance oftheresultingalgorithms.
4
BEYOND CLASSICAL
SEARCH
In which we relax the simplifying assumptions of the previous chapter, thereby
gettingclosertotherealworld.
Chapter 3 addressed a single category of problems: observable, deterministic, known envi-
ronmentswherethesolutionisasequenceofactions. Inthischapter,welookatwhathappens
whentheseassumptionsarerelaxed. Webeginwithafairlysimplecase: Sections4.1and4.2
coveralgorithms thatperform purely localsearch inthestate space, evaluating andmodify-
ingoneormorecurrentstatesratherthansystematically exploringpathsfromaninitialstate.
Thesealgorithms are suitable forproblems inwhich allthat mattersisthe solution state, not
thepathcost toreach it. Thefamilyoflocal search algorithms includes methods inspired by
statistical physics(simulatedannealing)andevolutionary biology (geneticalgorithms).
Then, in Sections 4.3–4.4, we examine what happens when we relax the assumptions
ofdeterminismandobservability. Thekeyideaisthatifanagentcannotpredictexactlywhat
percept it will receive, then it will need to consider what to do under each contingency that
its percepts may reveal. With partial observability, the agent will also need to keep track of
thestatesitmightbein.
Finally, Section 4.5investigates onlinesearch, inwhichtheagent isfaced withastate
spacethatisinitially unknownandmustbeexplored.
4.1 LOCAL SEARCH ALGORITHMS AND OPTIMIZATION PROBLEMS
The search algorithms that we have seen so far are designed to explore search spaces sys-
tematically. Thissystematicity isachieved by keeping one ormore paths in memory and by
recordingwhichalternativeshavebeenexploredateachpointalongthepath. Whenagoalis
found,thepathtothatgoalalsoconstitutesasolutiontotheproblem. Inmanyproblems,how-
ever, the path to the goal is irrelevant. Forexample, in the 8-queens problem (see page 71),
whatmattersisthefinalconfiguration ofqueens, nottheorderinwhichthey areadded. The
same general property holds for many important applications such as integrated-circuit de-
sign,factory-floorlayout,job-shopscheduling,automaticprogramming,telecommunications
networkoptimization, vehiclerouting, andportfolio management.
120
Section4.1. LocalSearchAlgorithmsandOptimization Problems 121
If the path to the goal does not matter, we might consider a different class of algo-
rithms, ones that do not worry about paths at all. Local search algorithms operate using
LOCALSEARCH
a single current node (rather than multiple paths) and generally move only to neighbors
CURRENTNODE
of that node. Typically, the paths followed by the search are not retained. Although local
search algorithms are not systematic, they have two key advantages: (1) they use very little
memory—usuallyaconstantamount;and(2)theycanoftenfindreasonablesolutionsinlarge
orinfinite(continuous) statespacesforwhichsystematic algorithmsareunsuitable.
In addition to finding goals, local search algorithms are useful for solving pure op-
OPTIMIZATION timization problems, in which the aim is to find the best state according to an objective
PROBLEM
OBJECTIVE function. Many optimization problems do not fitthe “standard” search model introduced in
FUNCTION
Chapter 3. For example, nature provides an objective function—reproductive fitness—that
Darwinian evolution could be seen as attempting tooptimize, but there isno “goal test” and
no“pathcost”forthisproblem.
STATE-SPACE Tounderstand localsearch, wefindituseful toconsiderthe state-space landscape(as
LANDSCAPE
inFigure4.1). Alandscapehasboth“location”(definedbythestate)and“elevation”(defined
by the value ofthe heuristic cost function orobjective function). Ifelevation corresponds to
cost, then the aim is to find the lowest valley—a global minimum; if elevation corresponds
GLOBALMINIMUM
toanobjective function, thentheaim istofindthehighest peak—a globalmaximum. (You
GLOBALMAXIMUM
can convert from one to the other just by inserting a minus sign.) Local search algorithms
explore this landscape. A complete local search algorithm always finds a goal ifone exists;
anoptimalalgorithm alwaysfindsaglobalminimum/maximum.
objective function
global maximum
shoulder
local maximum
“flat” local maximum
state space
current
state
Figure4.1 Aone-dimensionalstate-spacelandscapeinwhichelevationcorrespondstothe
objectivefunction. The aim is to find the globalmaximum. Hill-climbingsearch modifies
thecurrentstatetotrytoimproveit,asshownbythearrow.Thevarioustopographicfeatures
aredefinedinthetext.
122 Chapter 4. BeyondClassicalSearch
functionHILL-CLIMBING(problem)returnsastatethatisalocalmaximum
current←MAKE-NODE(problem.INITIAL-STATE)
loopdo
neighbor←ahighest-valuedsuccessorofcurrent
ifneighbor.VALUE ≤current.VALUEthenreturncurrent.STATE
current←neighbor
Figure4.2 Thehill-climbingsearchalgorithm,whichisthemostbasiclocalsearchtech-
nique. At each step the currentnode is replaced by the best neighbor; in this version, that
means the neighborwith the highest VALUE, but if a heuristic cost estimate h is used, we
wouldfindtheneighborwiththelowesth.
4.1.1 Hill-climbingsearch
The hill-climbing search algorithm (steepest-ascent version) is shown in Figure 4.2. It is
HILLCLIMBING
simply a loop that continually moves in the direction of increasing value—that is, uphill. It
STEEPESTASCENT
terminates when it reaches a “peak” where no neighbor has a higher value. The algorithm
does not maintain a search tree, so the data structure for the current node need only record
the state and the value of the objective function. Hill climbing does not look ahead beyond
the immediate neighbors of thecurrent state. Thisresembles trying tofind thetop ofMount
Everestinathickfogwhilesufferingfromamnesia.
To illustrate hill climbing, we will use the 8-queens problem introduced on page 71.
Local search algorithms typically use a complete-state formulation, where each state has
8 queens on the board, one per column. The successors of a state are all possible states
generated bymoving asingle queen toanothersquare inthesamecolumn (soeachstate has
8×7=56 successors). The heuristic cost function h is the number of pairs of queens that
are attacking each other, either directly or indirectly. The global minimum of this function
iszero, whichoccurs only atperfect solutions. Figure 4.3(a) showsastate withh=17. The
figure also shows the values of all its successors, with the best successors having h=12.
Hill-climbingalgorithmstypicallychooserandomlyamongthesetofbestsuccessorsifthere
ismorethanone.
GREEDYLOCAL Hillclimbingissometimescalledgreedylocalsearchbecauseitgrabsagoodneighbor
SEARCH
statewithoutthinkingaheadaboutwheretogonext. Althoughgreedisconsideredoneofthe
sevendeadly sins,itturnsoutthatgreedy algorithms often perform quitewell. Hillclimbing
oftenmakesrapidprogresstowardasolutionbecauseitisusuallyquiteeasytoimproveabad
state. For example, from the state in Figure 4.3(a), it takes just five steps to reach the state
in Figure 4.3(b), which has h=1 and is very nearly a solution. Unfortunately, hill climbing
oftengetsstuckforthefollowingreasons:
• Local maxima: a local maximum is a peak that is higher than each of its neighboring
LOCALMAXIMUM
states but lower than the global maximum. Hill-climbing algorithms that reach the
vicinity of a local maximum will be drawn upward toward the peak but will then be
stuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More
Section4.1. LocalSearchAlgorithmsandOptimization Problems 123
18 12 14 13 13 12 14 14
14 16 13 15 12 14 12 16
14 12 18 13 15 12 14 14
15 14 14 13 16 13 16
14 17 15 14 16 16
17 16 18 15 15
18 14 15 15 14 16
14 14 13 17 12 14 12 18
(a) (b)
Figure4.3 (a)An8-queensstatewithheuristiccostestimateh=17,showingthevalueof
hforeachpossiblesuccessorobtainedbymovingaqueenwithinitscolumn.Thebestmoves
aremarked. (b)Alocalminimuminthe8-queensstatespace;thestatehash=1butevery
successorhasahighercost.
concretely, thestate inFigure 4.3(b)isalocal maximum(i.e.,alocalminimum forthe
costh);everymoveofasinglequeenmakesthesituationworse.
• Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maxima
RIDGE
thatisverydifficultforgreedyalgorithms tonavigate.
• Plateaux: a plateau is a flat area of the state-space landscape. It can be a flat local
PLATEAU
maximum, from which no uphill exit exists, or a shoulder, from which progress is
SHOULDER
possible. (SeeFigure4.1.) Ahill-climbing searchmightgetlostontheplateau.
Ineachcase,thealgorithmreachesapointatwhichnoprogressisbeingmade. Startingfrom
arandomlygenerated8-queensstate,steepest-ascenthillclimbinggetsstuck86%ofthetime,
solvingonly14%ofprobleminstances. Itworksquickly,takingjust4stepsonaveragewhen
itsucceeds and3whenitgetsstuck—not badforastatespacewith88 ≈ 17millionstates.
The algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has
the same value as the current state. Might it not be a good idea to keep going—to allow a
sidewaysmoveinthehopethattheplateauisreallyashoulder, asshowninFigure4.1? The
SIDEWAYSMOVE
answerisusuallyyes,butwemusttakecare. Ifwealwaysallowsidewaysmoveswhenthere
are no uphill moves, an infinite loop will occur whenever the algorithm reaches a flat local
maximumthatisnotashoulder. Onecommonsolutionistoputalimitonthenumberofcon-
secutive sideways moves allowed. Forexample, we could allow up to, say, 100 consecutive
sideways moves in the 8-queens problem. This raises the percentage of problem instances
solved by hill climbing from 14% to 94%. Success comes at a cost: the algorithm averages
roughly 21stepsforeachsuccessful instance and64foreach failure.
124 Chapter 4. BeyondClassicalSearch
Figure4.4 Illustrationofwhyridgescausedifficultiesforhillclimbing.Thegridofstates
(darkcircles)issuperimposedonaridgerisingfromlefttoright,creatingasequenceoflocal
maxima that are not directly connected to each other. From each local maximum, all the
availableactionspointdownhill.
STOCHASTICHILL Manyvariantsofhillclimbinghavebeeninvented. Stochastichillclimbingchoosesat
CLIMBING
randomfromamongtheuphillmoves;theprobabilityofselectioncanvarywiththesteepness
of the uphill move. This usually converges more slowly than steepest ascent, but in some
FIRST-CHOICEHILL state landscapes, it finds better solutions. First-choice hill climbing implements stochastic
CLIMBING
hillclimbing bygenerating successors randomly until oneisgenerated thatisbetterthanthe
currentstate. Thisisagoodstrategywhenastatehasmany(e.g.,thousands) ofsuccessors.
The hill-climbing algorithms described so far are incomplete—they often fail to find
a goal when one exists because they can get stuck on local maxima. Random-restart hill
RANDOM-RESTART climbingadopts thewell-known adage, “Ifatfirstyou don’t succeed, try, try again.” Itcon-
HILLCLIMBING
ducts a series of hill-climbing searches from randomly generated initial states,1 until a goal
is found. It is trivially complete with probability approaching 1, because it will eventually
generate a goal state as the initial state. If each hill-climbing search has a probability p of
success, then the expected number of restarts required is 1/p. For 8-queens instances with
nosideways movesallowed, p ≈ 0.14,soweneed roughly 7iterations tofindagoal (6fail-
uresand1success). Theexpectednumberofstepsisthecostofonesuccessful iterationplus
(1−p)/ptimesthecostoffailure,orroughly22stepsinall. Whenweallowsidewaysmoves,
1/0.94 ≈ 1.06iterationsareneededonaverageand (1×21)+(0.06/0.94)×64 ≈25steps.
For8-queens, then, random-restart hillclimbingisveryeffectiveindeed. Evenforthreemil-
lionqueens, theapproach canfindsolutions inunderaminute.2
1 Generatingarandomstatefromanimplicitlyspecifiedstatespacecanbeahardprobleminitself.
2 Lubyetal.(1993)provethatitisbest,insomecases,torestartarandomizedsearchalgorithmafteraparticular,
fixed amount of time and that this can be much more efficient than letting each search continue indefinitely.
Disallowingorlimitingthenumberofsidewaysmovesisanexampleofthisidea.
Section4.1. LocalSearchAlgorithmsandOptimization Problems 125
The success of hill climbing depends very much on the shape of the state-space land-
scape: if there are few local maxima and plateaux, random-restart hill climbing will find a
good solution very quickly. On the other hand, many real problems have a landscape that
looksmorelikeawidelyscatteredfamilyofbaldingporcupinesonaflatfloor,withminiature
porcupines living on the tip of each porcupine needle, ad infinitum. NP-hard problems typi-
callyhaveanexponential numberoflocalmaximatogetstuckon. Despitethis,areasonably
goodlocalmaximumcanoftenbefoundafterasmallnumberofrestarts.
4.1.2 Simulated annealing
Ahill-climbingalgorithmthatnevermakes“downhill”movestowardstateswithlowervalue
(or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maxi-
mum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly
at random from the set of successors—is complete but extremely inefficient. Therefore, it
seemsreasonabletotrytocombinehillclimbingwitharandomwalkinsomewaythatyields
SIMULATED bothefficiencyandcompleteness. Simulatedannealingissuchanalgorithm. Inmetallurgy,
ANNEALING
annealing is the process used to temper or harden metals and glass by heating them to a
hightemperature andthengradually coolingthem,thusallowingthematerialtoreachalow-
energy crystalline state. To explain simulated annealing, we switch our point of view from
hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a
GRADIENTDESCENT
ping-pong ball into thedeepest crevice inabumpy surface. Ifwejustlet theball roll, itwill
come to rest at a local minimum. If weshake the surface, wecan bounce the ball out of the
local minimum. The trick is to shake just hard enough to bounce the ball out of local min-
ima but not hard enough to dislodge it from the global minimum. The simulated-annealing
solution istostartbyshaking hard(i.e.,atahightemperature) andthengradually reduce the
intensity oftheshaking (i.e.,lowerthetemperature).
Theinnermostloopofthesimulated-annealing algorithm(Figure4.5)isquitesimilarto
hillclimbing. Insteadofpickingthebestmove,however,itpicksarandommove. Ifthemove
improvesthesituation,itisalwaysaccepted. Otherwise,thealgorithmacceptsthemovewith
some probability less than 1. The probability decreases exponentially with the “badness” of
the move—the amount ΔE by which the evaluation is worsened. The probability also de-
creasesasthe“temperature” T goesdown: “bad”movesaremorelikelytobeallowedatthe
startwhenT ishigh,andtheybecomemoreunlikely asT decreases. Iftheschedule lowers
T slowlyenough, thealgorithm willfindaglobaloptimumwithprobability approaching 1.
Simulated annealing was first used extensively to solve VLSI layout problems in the
early1980s. Ithasbeenapplied widelytofactoryscheduling andotherlarge-scale optimiza-
tiontasks. InExercise4.4,youareaskedtocompareitsperformancetothatofrandom-restart
hillclimbingonthe8-queens puzzle.
4.1.3 Local beam search
Keeping just one node in memory might seem to be an extreme reaction to the problem of
LOCALBEAM memory limitations. The local beam search algorithm3 keeps track of k states rather than
SEARCH
3 Localbeamsearchisanadaptationofbeamsearch,whichisapath-basedalgorithm.
126 Chapter 4. BeyondClassicalSearch
functionSIMULATED-ANNEALING(problem,schedule)returnsasolutionstate
inputs:problem,aproblem
schedule,amappingfromtimeto“temperature”
current←MAKE-NODE(problem.INITIAL-STATE)
fort =1to∞do
T←schedule(t)
ifT =0thenreturncurrent
next←arandomlyselectedsuccessorofcurrent
ΔE←next.VALUE–current.VALUE
ifΔE >0thencurrent←next
elsecurrent←next onlywithprobabilityeΔE/T
Figure4.5 Thesimulatedannealingalgorithm,aversionofstochastichillclimbingwhere
somedownhillmovesareallowed. Downhillmovesareacceptedreadilyearlyintheanneal-
ingscheduleandthenlessoftenastimegoeson.Theschedule inputdeterminesthevalueof
thetemperatureT asafunctionoftime.
justone. Itbegins withk randomly generated states. Ateach step, allthesuccessors ofallk
states aregenerated. Ifanyoneisagoal, thealgorithm halts. Otherwise, itselects the k best
successors fromthecompletelistandrepeats.
At first sight, a local beam search with k states might seem to be nothing more than
running k random restarts in parallel instead of in sequence. In fact, the two algorithms
are quite different. In a random-restart search, each search process runs independently of
the others. In a local beam search, useful information is passed among the parallel search
threads. In effect, the states that generate the best successors say to the others, “Come over
here, the grass is greener!” The algorithm quickly abandons unfruitful searches and moves
itsresources towherethemostprogress isbeingmade.
In its simplest form, local beam search can suffer from a lack of diversity among the
k states—they canquickly becomeconcentrated inasmallregionofthestate space, making
thesearch little morethananexpensive version ofhillclimbing. Avariant called stochastic
STOCHASTICBEAM beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead
SEARCH
of choosing the best k from the the pool of candidate successors, stochastic beam search
chooses k successors at random, with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the “successors” (offspring) of a “state” (organism)
populate thenextgeneration according toits“value”(fitness).
4.1.4 Geneticalgorithms
GENETIC Ageneticalgorithm(orGA)isavariantofstochasticbeamsearchinwhichsuccessorstates
ALGORITHM
are generated by combining two parent states rather than by modifying a single state. The
analogy tonatural selection isthesameasinstochastic beamsearch, exceptthatnowweare
dealingwithsexualratherthanasexualreproduction.
Section4.1. LocalSearchAlgorithmsandOptimization Problems 127
24748552 32752411 32748552 32748152
24 31%
32752411 24748552 24752411 24752411
23 29%
24415124 32752411 32752124 32252124
20 26%
32543213 24415124 24415411 24415417
11 14%
(a) (b) (c) (d) (e)
Initial Population Fitness Function Selection Crossover Mutation
Figure4.6 Thegeneticalgorithm,illustratedfordigitstringsrepresenting8-queensstates.
The initial population in (a) is ranked by the fitness function in (b), resulting in pairs for
matingin(c). Theyproduceoffspringin(d),whicharesubjecttomutationin(e).
+ =
Figure4.7 The8-queensstatescorrespondingtothefirsttwoparentsinFigure4.6(c)and
thefirstoffspringinFigure4.6(d).Theshadedcolumnsarelostinthecrossoverstepandthe
unshadedcolumnsareretained.
Like beam searches, GAs begin with a set of k randomly generated states, called the
population. Eachstate,orindividual,isrepresented asastringoverafinitealphabet—most
POPULATION
commonly,astringof0sand1s. Forexample,an8-queensstatemustspecifythepositionsof
INDIVIDUAL
8queens, each inacolumn of 8squares, and so requires 8× log 8=24 bits. Alternatively,
2
thestatecouldberepresentedas8digits,eachintherangefrom1to8. (Wedemonstratelater
that the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit
stringsrepresenting 8-queens states.
The production of the next generation of states is shown in Figure 4.6(b)–(e). In (b),
each state is rated by the objective function, or(in GAterminology) the fitnessfunction. A
FITNESSFUNCTION
fitness function should return higher values for better states, so, for the 8-queens problem
we use the number of nonattacking pairs of queens, which has a value of 28 for a solution.
The values of the four states are 24, 23, 20, and 11. In this particular variant of the genetic
algorithm, the probability of being chosen for reproducing is directly proportional to the
fitnessscore,andthepercentages areshownnexttotherawscores.
In (c), two pairs are selected at random for reproduction, in accordance with the prob-
128 Chapter 4. BeyondClassicalSearch
abilities in (b). Notice that one individual is selected twice and one not at all.4 For each
pair to be mated, a crossover point is chosen randomly from the positions in the string. In
CROSSOVER
Figure4.6,thecrossoverpointsareafterthethirddigitin thefirstpairandafterthefifthdigit
inthesecondpair.5
In (d), the offspring themselves are created by crossing over the parent strings at the
crossoverpoint. Forexample,thefirstchildofthefirstpair getsthefirstthreedigitsfromthe
first parent and the remaining digits from the second parent, whereas the second child gets
the first three digits from the second parent and the rest from the first parent. The 8-queens
states involved in this reproduction step are shown in Figure 4.7. The example shows that
whentwoparent statesarequitedifferent, thecrossover operation canproduce astatethatis
a long way from either parent state. It is often the case that the population is quite diverse
earlyonintheprocess,socrossover(likesimulatedannealing)frequentlytakeslargestepsin
the state space early in the search process and smaller steps later on when most individuals
arequitesimilar.
Finally, in (e), each location is subject to random mutation with a small independent
MUTATION
probability. One digit was mutated in the first, third, and fourth offspring. In the 8-queens
problem, this corresponds to choosing a queen at random and moving it to a random square
initscolumn. Figure4.8describes analgorithm thatimplementsallthesesteps.
Like stochastic beam search, genetic algorithms combine an uphill tendency with ran-
dom exploration and exchange of information among parallel search threads. The primary
advantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be
shown mathematically that, if the positions of the genetic code are permuted initially in a
random order, crossover conveys no advantage. Intuitively, the advantage comes from the
abilityofcrossovertocombinelargeblocksoflettersthat haveevolvedindependently toper-
form useful functions, thus raising the level of granularity at which the search operates. For
example,itcouldbethatputtingthefirstthreequeensinpositions 2,4,and6(wheretheydo
not attack each other) constitutes a useful block that can be combined with other blocks to
construct asolution.
The theory of genetic algorithms explains how this works using the idea of a schema,
SCHEMA
which is a substring in which some of the positions can be left unspecified. For example,
the schema 246***** describes all 8-queens states in which the first three queens are in
positions 2, 4, and 6, respectively. Strings that match the schema (such as 24613578) are
calledinstancesoftheschema. Itcanbeshownthatiftheaveragefitnessoftheinstances of
INSTANCE
aschemaisabovethemean,thenthenumberofinstancesoftheschemawithinthepopulation
willgrowovertime. Clearly,thiseffectisunlikelytobesignificantifadjacentbitsaretotally
unrelated to each other, because then there will be few contiguous blocks that provide a
consistent benefit. Genetic algorithms work best when schemata correspond to meaningful
componentsofasolution. Forexample,ifthestringisarepresentationofanantenna,thenthe
schematamayrepresentcomponentsoftheantenna,suchasreflectorsanddeflectors. Agood
4 Therearemanyvariantsofthisselectionrule. Themethodofculling,inwhichallindividualsbelowagiven
thresholdarediscarded,canbeshowntoconvergefasterthantherandomversion(Baumetal.,1995).
5 Itisherethattheencodingmatters. Ifa24-bitencodingisusedinsteadof8digits,thenthecrossoverpoint
hasa2/3chanceofbeinginthemiddleofadigit,whichresultsinanessentiallyarbitrarymutationofthatdigit.
Section4.2. LocalSearchinContinuous Spaces 129
functionGENETIC-ALGORITHM(population,FITNESS-FN)returnsanindividual
inputs:population,asetofindividuals
FITNESS-FN,afunctionthatmeasuresthefitnessofanindividual
repeat
new population←emptyset
fori =1toSIZE(population)do
x←RANDOM-SELECTION(population,FITNESS-FN)
y←RANDOM-SELECTION(population,FITNESS-FN)
child←REPRODUCE(x,y)
if(smallrandomprobability)thenchild←MUTATE(child)
addchild tonew population
population←new population
untilsomeindividualisfitenough,orenoughtimehaselapsed
returnthebestindividualinpopulation,accordingtoFITNESS-FN
functionREPRODUCE(x,y)returnsanindividual
inputs:x,y,parentindividuals
n←LENGTH(x);c←randomnumberfrom1ton
returnAPPEND(SUBSTRING(x,1,c),SUBSTRING(y,c+1,n))
Figure 4.8 A genetic algorithm. The algorithm is the same as the one diagrammed in
Figure 4.6, with one variation: in this more popular version, each mating of two parents
producesonlyoneoffspring,nottwo.
componentislikelytobegoodinavarietyofdifferentdesigns. Thissuggeststhatsuccessful
useofgenetic algorithmsrequires carefulengineering oftherepresentation.
Inpractice,geneticalgorithmshavehadawidespreadimpactonoptimizationproblems,
such ascircuit layout and job-shop scheduling. Atpresent, itisnot clearwhetherthe appeal
ofgeneticalgorithmsarisesfromtheirperformanceorfrom theiræstheticallypleasingorigins
in the theory of evolution. Much work remains to be done to identify the conditions under
whichgeneticalgorithms performwell.
4.2 LOCAL SEARCH IN CONTINUOUS SPACES
In Chapter 2, we explained the distinction between discrete and continuous environments,
pointing out that most real-world environments are continuous. Yet none of the algorithms
wehavedescribed (exceptforfirst-choice hillclimbingand simulated annealing) canhandle
continuous stateandactionspaces,becausetheyhaveinfinitebranchingfactors. Thissection
provides a very brief introduction to some local search techniques for finding optimal solu-
tions in continuous spaces. Theliterature on this topic is vast; many of the basic techniques
130 Chapter 4. BeyondClassicalSearch
EVOLUTION AND SEARCH
The theory of evolution was developed in Charles Darwin’s On the Origin of
SpeciesbyMeansofNaturalSelection(1859)andindependently byAlfredRussel
Wallace (1858). The central idea is simple: variations occur in reproduction and
will be preserved in successive generations approximately in proportion to their
effectonreproductive fitness.
Darwin’stheorywasdevelopedwithnoknowledgeofhowthetraitsoforgan-
isms can be inherited and modified. The probabilistic laws governing these pro-
cesses were first identified by Gregor Mendel (1866), a monk who experimented
withsweetpeas. Muchlater,WatsonandCrick(1953)identifiedthestructureofthe
DNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In
thestandardmodel,variationoccursbothbypointmutationsinthelettersequence
and by “crossover” (in which the DNAof an offspring is generated by combining
longsections ofDNAfromeachparent).
Theanalogytolocalsearchalgorithmshasalreadybeendescribed; theprinci-
paldifferencebetweenstochasticbeamsearchandevolutionistheuseofsexualre-
production, wherein successors are generated from multiple organisms rather than
just one. The actual mechanisms of evolution are, however, far richer than most
genetic algorithms allow. For example, mutations can involve reversals, duplica-
tions,andmovementoflargechunksofDNA;somevirusesborrowDNAfromone
organism andinsert itinanother; and therearetransposable genes thatdonothing
but copy themselves many thousands of times within the genome. There are even
genesthatpoison cellsfrompotential matesthatdonotcarrythegene,thereby in-
creasingtheirownchancesofreplication. Mostimportantisthefactthatthegenes
themselves encode the mechanisms whereby the genome is reproduced and trans-
lated into an organism. In genetic algorithms, those mechanisms are a separate
programthatisnotrepresented withinthestrings beingmanipulated.
Darwinian evolution may appear inefficient, having generated blindly some
1045 or so organisms without improving its search heuristics one iota. Fifty
yearsbeforeDarwin,however,theotherwisegreatFrenchnaturalist JeanLamarck
(1809) proposed a theory of evolution whereby traits acquired by adaptation dur-
ing an organism’s lifetime would be passed on to its offspring. Such a process
would be effective but does not seem to occur in nature. Much later, James Bald-
win(1896)proposedasuperficiallysimilartheory: thatbehaviorlearnedduringan
organism’slifetimecouldacceleratetherateofevolution. UnlikeLamarck’s,Bald-
win’stheoryisentirelyconsistentwithDarwinianevolutionbecauseitreliesonse-
lection pressures operating onindividuals that have found local optimaamong the
set of possible behaviors allowed by their genetic makeup. Computer simulations
confirm that the “Baldwin effect” is real, once “ordinary” evolution has created
organismswhoseinternal performance measurecorrelates withactualfitness.
Section4.2. LocalSearchinContinuous Spaces 131
originatedinthe17thcentury,afterthedevelopmentofcalculusbyNewtonandLeibniz.6 We
findusesforthesetechniquesatseveralplacesinthebook,includingthechaptersonlearning,
vision, androbotics.
We begin with an example. Suppose we want to place three new airports anywhere
in Romania, such that the sum of squared distances from each city on the map (Figure 3.2)
to its nearest airport is minimized. The state space is then defined by the coordinates of
the airports: (x ,y ), (x ,y ), and (x ,y ). This is a six-dimensional space; we also say
1 1 2 2 3 3
that states are defined by six variables. (In general, states are defined by an n-dimensional
VARIABLE
vector of variables, x.) Moving around in this space corresponds to moving one or more of
the airports on the map. The objective function f(x ,y ,x ,y ,x ,y ) is relatively easy to
1 1 2 2 3 3
compute for any particular state once we compute the closest cities. Let C be the set of
i
citieswhoseclosestairport(inthecurrentstate)isairporti. Then,intheneighborhood ofthe
currentstate,wheretheC sremainconstant, wehave
i
(cid:12)3 (cid:12)
f(x ,y ,x ,y ,x ,y ) = (x −x )2+(y −y )2 . (4.1)
1 1 2 2 3 3 i c i c
i=1c∈Ci
This expression is correct locally, but not globally because the sets C are (discontinuous)
i
functions ofthestate.
Onewaytoavoidcontinuousproblemsissimplytodiscretizetheneighborhoodofeach
DISCRETIZATION
state. Forexample, we can move only one airport at a time in either the x or y direction by
a fixed amount ±δ. With 6 variables, this gives 12 possible successors for each state. We
can then apply any of the local search algorithms described previously. We could also ap-
ply stochastic hill climbing and simulated annealing directly, without discretizing the space.
Thesealgorithmschoosesuccessorsrandomly,whichcanbedonebygeneratingrandomvec-
torsoflength δ.
Many methods attempt to use the gradient of the landscape to find a maximum. The
GRADIENT
gradientoftheobjectivefunctionisavector∇f thatgivesthemagnitudeanddirectionofthe
steepest slope. Forourproblem,wehave
(cid:13) (cid:14)
∂f ∂f ∂f ∂f ∂f ∂f
∇f = , , , , , .
∂x ∂y ∂x ∂y ∂x ∂y
1 1 2 2 3 3
Insomecases,wecanfindamaximumbysolvingtheequation∇f=0. (Thiscouldbedone,
forexample,ifwewereplacingjustoneairport;thesolutionisthearithmeticmeanofallthe
cities’ coordinates.) In many cases, however, this equation cannot be solved in closed form.
For example, with three airports, the expression for the gradient depends on what cities are
closest to each airport in the current state. This means we can compute the gradient locally
(butnotglobally); forexample,
(cid:12)
∂f
= 2 (x −x ). (4.2)
i c
∂x
1 c∈C1
Givenalocallycorrectexpressionforthegradient,wecanperformsteepest-ascenthillclimb-
6 Abasicknowledgeofmultivariatecalculusandvectorarithmeticisusefulforreadingthissection.
132 Chapter 4. BeyondClassicalSearch
ingbyupdatingthecurrentstateaccording totheformula
x ← x+α∇f(x),
where α is a small constant often called the step size. In other cases, the objective function
STEPSIZE
mightnotbeavailableinadifferentiableformatall—forexample,thevalueofaparticularset
of airport locations might be determined by running some large-scale economic simulation
EMPIRICAL package. In those cases, we can calculate a so-called empirical gradient by evaluating the
GRADIENT
response to small increments and decrements in each coordinate. Empirical gradient search
isthesameassteepest-ascent hillclimbinginadiscretized versionofthestatespace.
Hidden beneath the phrase “α is a small constant” lies a huge variety of methods for
adjusting α. The basic problem is that, if α is too small, too many steps are needed; if α
is too large, the search could overshoot the maximum. The technique of line search tries to
LINESEARCH
overcome this dilemma by extending the current gradient direction—usually by repeatedly
doublingα—untilf startstodecreaseagain. Thepointatwhichthisoccursbecomesthenew
current state. There are several schools of thought about how the new direction should be
chosenatthispoint.
For many problems, the most effective algorithm is the venerable Newton–Raphson
NEWTON–RAPHSON
method. Thisisageneral technique forfindingrootsoffunctions—that is,solvingequations
of the form g(x)=0. It works by computing a new estimate for the root x according to
Newton’sformula
x← x−g(x)/g (cid:2) (x).
To find a maximum or minimum of f, we need to find x such that the gradient is zero (i.e.,
∇f(x)=0). Thus, g(x) in Newton’s formula becomes ∇f(x), and the update equation can
bewritteninmatrix–vector formas
x ← x−H −1(x)∇f(x),
f
where H (x) is the Hessian matrix of second derivatives, whose elements H are given
HESSIAN f ij
by ∂2f/∂x ∂x . For our airport example, we can see from Equation (4.2) that H (x) is
i j f
particularly simple: theoff-diagonal elements are zero and thediagonal elements forairport
i are just twice the number of cities in C . A moment’s calculation shows that one step of
i
the update moves airport i directly to the centroid of C , which is the minimum of the local
i
expression forf fromEquation (4.1).7 Forhigh-dimensional problems, however, computing
then2entriesoftheHessianandinvertingitmaybeexpensive,somanyapproximateversions
oftheNewton–Raphson methodhavebeendeveloped.
Local search methods suffer from local maxima, ridges, and plateaux in continuous
statespacesjustasmuchasindiscrete spaces. Randomrestartsandsimulatedannealing can
be used and are often helpful. High-dimensional continuous spaces are, however, big places
inwhichitiseasytogetlost.
CONSTRAINED Afinaltopicwithwhichapassingacquaintance isusefulisconstrainedoptimization.
OPTIMIZATION
Anoptimizationproblemisconstrainedifsolutionsmustsatisfysomehardconstraintsonthe
values of the variables. Forexample, in ourairport-siting problem, wemight constrain sites
7 Ingeneral,theNewton–Raphsonupdatecanbeseenasfittingaquadraticsurfacetof atxandthenmoving
directlytotheminimumofthatsurface—whichisalsotheminimumoff iff isquadratic.
Section4.3. SearchingwithNondeterministic Actions 133
to be inside Romania and on dry land (rather than in the middle of lakes). The difficulty of
constrained optimization problemsdepends onthenatureof theconstraints andtheobjective
LINEAR function. Thebest-known category isthat of linear programming problems, in which con-
PROGRAMMING
straints must be linear inequalities forming a convex set 8 and the objective function is also
CONVEXSET
linear. Thetimecomplexityoflinearprogrammingispolynomialinthenumberofvariables.
Linear programming is probably the most widely studied and broadly useful class of
optimization problems. It is a special case of the more general problem of convex opti-
CONVEX mization, which allows the constraint region to be any convex region and the objective to
OPTIMIZATION
beanyfunction thatisconvexwithintheconstraint region. Undercertainconditions, convex
optimization problems are also polynomially solvable and may be feasible in practice with
thousands of variables. Several important problems in machine learning and control theory
canbeformulated asconvexoptimization problems(seeChapter20).
4.3 SEARCHING WITH NONDETERMINISTIC ACTIONS
InChapter3,weassumedthattheenvironment isfullyobservable anddeterministic andthat
theagentknowswhattheeffectsofeachactionare. Therefore,theagentcancalculateexactly
which state results from any sequence of actions and always knows which state it is in. Its
percepts provide nonewinformation aftereachaction, although ofcourse theytelltheagent
theinitialstate.
Whentheenvironment iseitherpartially observable ornondeterministic (orboth), per-
ceptsbecomeuseful. Inapartiallyobservableenvironment, everypercepthelpsnarrowdown
thesetofpossible states theagent mightbein, thus makingiteasierfortheagent toachieve
itsgoals. Whentheenvironmentisnondeterministic, perceptstelltheagentwhichofthepos-
sible outcomes ofits actions hasactually occurred. In both cases, the future percepts cannot
bedeterminedinadvanceandtheagent’sfutureactionswilldependonthosefuturepercepts.
Sothesolutiontoaproblemisnotasequencebutacontingencyplan(alsoknownasastrat-
CONTINGENCYPLAN
egy) that specifies what to do depending on what percepts are received. In this section, we
STRATEGY
examinethecaseofnondeterminism, deferring partialobservability toSection4.4.
4.3.1 The erraticvacuum world
As an example, we use the vacuum world, first introduced in Chapter 2 and defined as a
search problem in Section 3.2.1. Recall that the state space has eight states, as shown in
Figure 4.9. There are three actions—Left, Right, and Suck—and the goal is to clean up all
the dirt (states 7 and 8). If the environment is observable, deterministic, and completely
known, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the
solution isanaction sequence. Forexample, iftheinitial state is1, then theaction sequence
[Suck,Right,Suck]willreachagoalstate, 8.
8 AsetofpointsSisconvexifthelinejoininganytwopointsinSisalsocontainedinS.Aconvexfunctionis
oneforwhichthespace“above”itformsaconvexset;bydefinition,convexfunctionshavenolocal(asopposed
toglobal)minima.
134 Chapter 4. BeyondClassicalSearch
1 2
3 4
5 6
7 8
Figure4.9 Theeightpossiblestatesofthevacuumworld;states7and8aregoalstates.
Now suppose that we introduce nondeterminism in the form of a powerful but erratic
ERRATICVACUUM vacuumcleaner. Intheerraticvacuumworld,theSuckactionworksasfollows:
WORLD
• When applied to a dirty square the action cleans the square and sometimes cleans up
dirtinanadjacentsquare, too.
• Whenapplied toacleansquaretheactionsometimesdeposits dirtonthecarpet.9
Toprovideapreciseformulation ofthisproblem,weneedtogeneralize thenotionofatran-
sitionmodelfromChapter3. Insteadofdefiningthetransition modelbyaRESULT function
that returns asingle state, weuse a RESULTS function thatreturns a setofpossible outcome
states. Forexample, intheerratic vacuum world, the Suck action instate1leads toastatein
theset{5,7}—thedirtintheright-hand squaremayormaynotbevacuumedup.
Wealsoneed togeneralize thenotion ofasolution totheproblem. Forexample, ifwe
start in state 1, there is no single sequence of actions that solves the problem. Instead, we
needacontingency plansuchasthefollowing:
[Suck,ifState=5then[Right,Suck]else[]]. (4.3)
Thus, solutions for nondeterministic problems can contain nested if–then–else statements;
this means that they are trees rather than sequences. This allows the selection of actions
based oncontingencies arising during execution. Manyproblems in thereal, physical world
are contingency problems because exact prediction is impossible. For this reason, many
peoplekeeptheireyesopenwhilewalkingaroundordriving.
9 We assume that most readers face similar problems and can sympathize with our agent. We apologize to
ownersofmodern,efficienthomeapplianceswhocannottakeadvantageofthispedagogicaldevice.
Section4.3. SearchingwithNondeterministic Actions 135
4.3.2 AND–OR searchtrees
The next question is how to find contingent solutions to nondeterministic problems. As in
Chapter3,webeginbyconstructingsearchtrees,butherethetreeshaveadifferentcharacter.
In adeterministic environment, the only branching isintroduced by the agent’s ownchoices
ORNODE in each state. We call these nodes OR nodes. In the vacuum world, for example, at an OR
node the agent chooses Left orRight orSuck. In anondeterministic environment, branching
is also introduced by the environment’s choice of outcome for each action. We call these
ANDNODE nodes AND nodes. For example, the Suck action in state 1 leads to a state in the set {5,7},
so the agent would need to find a plan for state 5 and for state 7. These two kinds of nodes
AND–ORTREE
alternate, leading toan AND–OR treeasillustrated inFigure4.10.
Asolution foran AND–OR searchproblemisasubtreethat(1)hasagoalnodeatevery
leaf, (2) specifies one action ateach of its OR nodes, and (3) includes every outcome branch
at each of its AND nodes. The solution is shown in bold lines in the figure; it corresponds
to the plan given in Equation (4.3). (The plan uses if–then–else notation to handle the AND
branches,butwhentherearemorethantwobranchesatanode, itmightbebettertouseacase
1
Suck Right
7 5 2
GOAL
Suck Right Left Suck
5 1 6 1 8 4
Suck Left
LOOP LOOP LOOP GOAL
8 5
GOAL LOOP
Figure 4.10 The first two levels of the search tree for the erratic vacuum world. State
nodesareORnodeswheresomeactionmustbechosen.AttheANDnodes,shownascircles,
everyoutcomemustbehandled,asindicatedbythearclinkingtheoutgoingbranches. The
solutionfoundisshowninboldlines.
136 Chapter 4. BeyondClassicalSearch
functionAND-OR-GRAPH-SEARCH(problem)returnsa conditional plan, or failure
OR-SEARCH(problem.INITIAL-STATE,problem,[])
functionOR-SEARCH(state,problem,path)returnsa conditional plan, or failure
ifproblem.GOAL-TEST(state)thenreturntheemptyplan
ifstate isonpath thenreturnfailure
foreachaction inproblem.ACTIONS(state)do
plan←AND-SEARCH(RESULTS(state,action),problem,[state |path])
ifplan (cid:7)=failure thenreturn[action | plan]
returnfailure
functionAND-SEARCH(states,problem,path)returnsa conditional plan, or failure
foreachsiinstates do
plan
i
←OR-SEARCH(si,problem,path)
ifplan =failure thenreturnfailure
i
return[ifs
1
thenplan
1
elseifs
2
thenplan
2
else ...ifsn−1 thenplan
n−1
elseplan
n
]
Figure 4.11 An algorithmforsearching AND–OR graphsgeneratedby nondeterministic
environments.Itreturnsaconditionalplanthatreachesagoalstateinallcircumstances.(The
notation[x|l]referstothelistformedbyaddingobjectxtothefrontoflistl.)
construct.) Modifying the basic problem-solving agent shown in Figure 3.1 to execute con-
tingentsolutionsofthiskindisstraightforward. Onemayalsoconsiderasomewhatdifferent
agentdesign, inwhichtheagentcanactbeforeithasfoundaguaranteed plananddealswith
some contingencies only as they arise during execution. Thistype of interleaving of search
INTERLEAVING
andexecution isalsousefulforexploration problems (seeSection4.5)andforgameplaying
(seeChapter5).
Figure 4.11 gives a recursive, depth-first algorithm for AND–OR graph search. One
key aspect of the algorithm is the way in which it deals with cycles, which often arise in
nondeterministic problems (e.g., if an action sometimes has no effect or if an unintended
effect can be corrected). If the current state is identical to a state on the path from the root,
thenitreturnswithfailure. Thisdoesn’tmeanthatthereis nosolutionfromthecurrentstate;
it simply means that if there is a noncyclic solution, it must be reachable from the earlier
incarnation ofthecurrentstate,sothenewincarnation can bediscarded. Withthischeck,we
ensurethatthealgorithmterminatesineveryfinitestatespace,becauseeverypathmustreach
agoal, adead end, orarepeated state. Notice that the algorithm does notcheck whetherthe
currentstateisarepetitionofastateonsomeotherpathfromtheroot,whichisimportantfor
efficiency. Exercise4.5investigates thisissue.
AND–ORgraphscanalsobeexploredbybreadth-firstorbest-firstmethods. Theconcept
of a heuristic function must be modified to estimate the cost of a contingent solution rather
∗
than asequence, but the notion of admissibility carries over and there is an analog of the A
algorithm forfindingoptimal solutions. Pointers aregiven inthebibliographical notes atthe
endofthechapter.
Section4.3. SearchingwithNondeterministic Actions 137
1
Suck Right
5 2
Right
6
Figure4.12 Partofthesearchgraphfortheslipperyvacuumworld,wherewehaveshown
(some)cyclesexplicitly. Allsolutionsforthisproblemarecyclicplansbecausethereis no
waytomovereliably.
4.3.3 Try,try again
Consider the slippery vacuum world, which is identical to the ordinary (non-erratic) vac-
uumworldexcept thatmovementactionssometimes fail,leaving theagentinthesameloca-
tion. For example, moving Right in state 1 leads to the state set {1,2}. Figure 4.12 shows
part of the search graph; clearly, there are no longer any acyclic solutions from state 1, and
CYCLICSOLUTION
AND-OR-GRAPH-SEARCH would return with failure. There is, however, a cyclic solution,
whichistokeeptryingRight untilitworks. Wecanexpressthissolutionbyaddingalabelto
LABEL
denote someportion oftheplanandusing thatlabel laterinstead ofrepeating theplanitself.
Thus,ourcyclicsolutionis
[Suck,L : Right,ifState=5thenL elseSuck].
1 1
(A better syntax for the looping part of this plan would be “while State=5 do Right.”)
In general a cyclic plan may be considered a solution provided that every leaf is a goal
state and that a leaf is reachable from every point in the plan. The modifications needed
to AND-OR-GRAPH-SEARCH arecoveredinExercise4.6. Thekeyrealization isthataloop
inthestatespacebacktoastateLtranslates toaloopintheplanbacktothepointwherethe
subplan forstate Lisexecuted.
Giventhedefinitionofacyclicsolution,anagentexecutingsuchasolutionwilleventu-
allyreachthegoalprovidedthateachoutcomeofanondeterministicactioneventuallyoccurs.
Is this condition reasonable? It depends on the reason forthe nondeterminism. If the action
rollsadie,then it’sreasonable tosuppose thateventually asixwillberolled. Iftheaction is
toinsertahotelcardkeyintothedoorlock,butitdoesn’t workthefirsttime,thenperhaps it
willeventuallywork,orperhapsonehasthewrongkey(orthe wrongroom!). Aftersevenor
138 Chapter 4. BeyondClassicalSearch
eighttries,mostpeoplewillassumetheproblem iswiththekeyandwillgobacktothefront
desk to getanew one. Onewayto understand this decision isto saythat the initial problem
formulation (observable, nondeterministic) is abandoned in favor of a different formulation
(partially observable, deterministic) where the failure is attributed to an unobservable prop-
ertyofthekey. WehavemoretosayonthisissueinChapter13.
4.4 SEARCHING WITH PARTIAL OBSERVATIONS
We now turn to the problem of partial observability, where the agent’s percepts do not suf-
fice to pin down the exact state. As noted at the beginning of the previous section, if the
agent is in one of several possible states, then an action may lead to one of several possible
outcomes—even if the environment is deterministic. The key concept required for solving
partiallyobservableproblemsisthe beliefstate,representing theagent’scurrentbeliefabout
BELIEFSTATE
the possible physical states it might be in, given the sequence of actions and percepts up to
that point. Webegin withthe simplest scenario forstudying belief states, which iswhen the
agenthasnosensorsatall;thenweaddinpartialsensingaswellasnondeterministic actions.
4.4.1 Searching withno observation
When the agent’s percepts provide no information at all, we have what is called a sensor-
less problem or sometimes a conformant problem. At first, one might think the sensorless
SENSORLESS
agent has no hope ofsolving aproblem if ithas no idea whatstate it’s in; in fact, sensorless
CONFORMANT
problems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,
primarily because they don’t rely on sensors working properly. In manufacturing systems,
forexample,manyingeniousmethodshavebeendevelopedfororientingpartscorrectlyfrom
an unknown initial position by using a sequence of actions with no sensing at all. The high
cost of sensing is another reason to avoid it: for example, doctors often prescribe a broad-
spectrum antibiotic rather than using the contingent plan of doing an expensive blood test,
thenwaitingfortheresults tocomeback, andthenprescribing amorespecificantibiotic and
perhapshospitalization becausetheinfection hasprogressed toofar.
Wecan make a sensorless version of the vacuum world. Assume that the agent knows
the geography of its world, but doesn’t know its location or the distribution of dirt. In that
case,itsinitialstatecouldbeanyelementoftheset{1,2,3,4,5,6,7,8}. Now,considerwhat
happensifittriestheactionRight. Thiswillcauseittobeinoneofthestates{2,4,6,8}—the
agentnowhasmoreinformation! Furthermore, theactionsequence [Right,Suck]willalways
endup inone ofthestates {4,8}. Finally, thesequence [Right,Suck,Left,Suck]isguaranteed
to reach the goal state 7 no matter what the start state. We say that the agent can coerce the
COERCION
worldintostate7.
Tosolvesensorlessproblems,wesearchinthespaceofbeliefstatesratherthanphysical
states.10 Notice that in belief-state space, the problem is fully observable because the agent
10 Inafullyobservableenvironment,eachbeliefstatecontainsonephysicalstate. Thus,wecanviewthealgo-
rithmsinChapter3assearchinginabelief-statespaceofsingletonbeliefstates.
Section4.4. SearchingwithPartialObservations 139
alwaysknowsitsownbeliefstate. Furthermore, thesolution (ifany)isalwaysasequence of
actions. Thisisbecause,asintheordinaryproblemsofChapter3,theperceptsreceivedafter
eachactionarecompletelypredictable—they’re alwaysempty! Sotherearenocontingencies
toplanfor. Thisistrue eveniftheenvironment isnondeterminstic.
It is instructive to see how the belief-state search problem is constructed. Suppose
theunderlying physical problem P isdefined by ACTIONSP , RESULTP , GOAL-TESTP ,and
STEP-COSTP . Thenwecandefinethecorresponding sensorless problem asfollows:
• Beliefstates: Theentirebelief-statespacecontainseverypossiblesetofphysicalstates.
IfP hasN states,thenthesensorless problemhasupto2N states,although manymay
beunreachable fromtheinitialstate.
• Initial state: Typically the set of allstates in P, although insome cases the agent will
havemoreknowledge thanthis.
• Actions: This is slightly tricky. Suppose the agent is in belief state b={s ,s }, but
1 2
ACTIONSP (s
1
) (cid:7)= ACTIONSP (s
2
);thentheagent isunsure ofwhichactions arelegal.
If we assume that illegal actions have no effect on the environment, then it is safe to
taketheunionofalltheactionsinanyofthephysicalstatesinthecurrent beliefstateb:
(cid:15)
ACTIONS(b) = ACTIONSP (s).
s∈b
Ontheotherhand, ifanillegalactionmightbetheendoftheworld, itissafertoallow
only the intersection, that is, the set of actions legal in all the states. For the vacuum
world,everystatehasthesamelegalactions, sobothmethodsgivethesameresult.
• Transition model: The agent doesn’t know which state in the belief state is the right
one; so as far as it knows, it might get to any of the states resulting from applying the
action tooneofthe physical states inthebelief state. Fordeterministic actions, theset
ofstatesthatmightbereached is
b (cid:2) = RESULT(b,a)= {s (cid:2) : s (cid:2) =RESULTP (s,a)ands ∈ b}. (4.4)
(cid:2)
Withdeterministic actions, b isneverlargerthan b. Withnondeterminism, wehave
b (cid:2) = RESULT(b,a) = {
(cid:15)
s (cid:2) : s (cid:2) ∈ RESULTSP (s,a)ands ∈ b}
= RESULTSP (s,a),
s∈b
which may be larger than b, as shown in Figure 4.13. The process of generating
(cid:2)
the new belief state after the action is called the prediction step; the notation b =
PREDICTION
PREDICTP (b,a)willcomeinhandy.
• Goaltest: The agent wants a plan that is sure to work, which means that a belief state
satisfies the goal only if all the physical states in it satisfy GOAL-TESTP . The agent
mayaccidentally achievethegoalearlier, butitwon’t knowthatithasdoneso.
• Path cost: This is also tricky. If the same action can have different costs in different
states, then the cost of taking an action in a given belief state could be one of several
values. (Thisgives risetoanew class ofproblems, which weexplore inExercise 4.9.)
For now we assume that the cost of an action is the same in all states and so can be
transferred directlyfromtheunderlying physical problem.
140 Chapter 4. BeyondClassicalSearch
1
1 2 1 2
3 4 3 4
3
(a) (b)
Figure 4.13 (a) Predicting the next belief state for the sensorless vacuum world with a
deterministicaction,Right. (b)Predictionforthesamebeliefstateandactionintheslippery
versionofthesensorlessvacuumworld.
Figure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum
world. Thereareonly12reachable beliefstatesoutof 28=256possible beliefstates.
Thepreceding definitions enabletheautomaticconstruction ofthebelief-state problem
formulation from the definition of the underlying physical problem. Once this is done, we
can apply any of the search algorithms of Chapter 3. In fact, we can do a little bit more
than that. In “ordinary” graph search, newly generated states are tested to see if they are
identical toexisting states. Thisworksforbelief states, too; forexample, inFigure 4.14, the
action sequence [Suck,Left,Suck] starting at the initial state reaches the same belief state as
[Right,Left,Suck], namely, {5,7}. Now, consider the belief state reached by [Left], namely,
{1,3,5,7}. Obviously, this is not identical to {5,7}, but it is a superset. It is easy to prove
(Exercise4.8)thatifanactionsequenceisasolutionforabeliefstateb,itisalsoasolutionfor
anysubset ofb. Hence, wecandiscardapathreaching {1,3,5,7} if{5,7} hasalready been
generated. Conversely, if {1,3,5,7} has already been generated and found to be solvable,
thenanysubset, suchas{5,7}, isguaranteed tobesolvable. Thisextralevelofpruningmay
dramatically improvetheefficiencyofsensorless problem solving.
Evenwiththisimprovement,however,sensorlessproblem-solvingaswehavedescribed
itisseldom feasible inpractice. Thedifficulty isnotsomuchthevastness ofthebelief-state
space—even though it is exponentially larger than the underlying physical state space; in
most cases the branching factor and solution length in the belief-state space and physical
state space are not so different. The real difficulty lies with the size of each belief state. For
example, the initial belief state forthe 10×10 vacuum worldcontains 100×2100 oraround
1032 physical states—far too many if we use the atomic representation, which is an explicit
listofstates.
One solution is to represent the belief state by some more compact description. In
English, we could say the agent knows “Nothing” in the initial state; after moving Left, we
could say, “Not in the rightmost column,” and so on. Chapter 7 explains how to do this in a
formal representation scheme. Another approach is to avoid the standard search algorithms,
whichtreatbeliefstatesasblackboxesjustlikeanyotherproblemstate. Instead,wecanlook
Section4.4. SearchingwithPartialObservations 141
L
R
1 3 1 2 3 2 4
L R
5 7 4 5 6 6 8
7 8
S
4 5
S S
7 8
L R
L
5 S 5 3 6 4 S 4
7 7 8 8
R
R L L R
L
6 S S 3
8 7
8 R 7
Figure4.14 Thereachableportionofthebelief-statespaceforthedeterministic,sensor-
lessvacuumworld.Eachshadedboxcorrespondstoasinglebeliefstate. Atanygivenpoint,
the agentis in a particularbeliefstate butdoesnotknowwhichphysicalstate it is in. The
initial belief state (complete ignorance) is the top center box. Actions are represented by
labeledlinks. Self-loopsareomittedforclarity.
INCREMENTAL
insidethebeliefstatesanddevelopincrementalbelief-statesearch algorithms thatbuildup
BELIEF-STATE
SEARCH
the solution one physical state at a time. For example, in the sensorless vacuum world, the
initial belief state is{1,2,3,4,5,6,7,8}, and wehave tofindan action sequence that works
inall8states. Wecandothisbyfirstfindingasolution thatworksforstate1;thenwecheck
ifitworksforstate 2;ifnot, goback andfindadifferent solution forstate1, andsoon. Just
asan AND–OR search hastofindasolution forevery branch at an AND node, thisalgorithm
hastofindasolution forevery stateinthebelief state; thedifference isthat AND–OR search
can find a different solution for each branch, whereas an incremental belief-state search has
tofindonesolutionthatworksforallthestates.
The main advantage of the incremental approach is that it is typically able to detect
failurequickly—when abeliefstateisunsolvable, itisusuallythecasethatasmallsubsetof
thebeliefstate,consisting ofthefirstfewstatesexamined, isalsounsolvable. Insomecases,
142 Chapter 4. BeyondClassicalSearch
thisleads toaspeedup proportional tothesizeofthebelief states, whichmaythemselves be
aslargeasthephysicalstatespaceitself.
Even the most efficient solution algorithm is not of much use when no solutions exist.
Many things just cannot be done without sensing. For example, the sensorless 8-puzzle is
impossible. On the other hand, a little bit of sensing can go a long way. Forexample, every
8-puzzle instanceissolvableifjustonesquareisvisible—the solution involves movingeach
tileinturnintothevisiblesquareandthenkeeping trackof itslocation.
4.4.2 Searching withobservations
Forageneralpartiallyobservableproblem,wehavetospecifyhowtheenvironmentgenerates
percepts for the agent. Forexample, we might define the local-sensing vacuum world to be
oneinwhichtheagenthasapositionsensorandalocaldirtsensorbuthasnosensorcapable
of detecting dirt in other squares. The formal problem specification includes a PERCEPT(s)
function that returns the percept received in a given state. (If sensing is nondeterministic,
thenweuseaPERCEPTS functionthatreturnsasetofpossiblepercepts.) Forexample,inthe
local-sensingvacuumworld,thePERCEPTinstate1is[A,Dirty]. Fullyobservableproblems
are aspecial case in which PERCEPT(s)=sforevery state s, whilesensorless problems are
aspecialcaseinwhich PERCEPT(s)=null.
Whenobservations are partial, itwillusually be the case that several states could have
produced any given percept. For example, the percept [A,Dirty] is produced by state 3 as
well as by state 1. Hence, given this as the initial percept, the initial belief state for the
local-sensing vacuum world will be {1,3}. The ACTIONS, STEP-COST, and GOAL-TEST
areconstructedfromtheunderlying physicalproblemjustasforsensorlessproblems,butthe
transition model isabitmorecomplicated. Wecan think of transitions from one belief state
tothenextforaparticularactionasoccurring inthreestages, asshowninFigure4.15:
• Thepredictionstageisthesameasforsensorlessproblems: giventheactionainbelief
stateb,thepredicted beliefstateis ˆb=PREDICT(b,a).11
• The observation prediction stage determines the set of percepts o that could be ob-
servedinthepredicted beliefstate:
POSSIBLE-PERCEPTS(ˆb) = {o: o=PERCEPT(s)ands ∈ˆb}.
• The update stage determines, for each possible percept, the belief state that would
result from the percept. The new belief state b is just the set of states in ˆb that could
o
haveproduced thepercept:
b
o
= UPDATE(ˆb,o) = {s :o=PERCEPT(s)ands ∈ˆb}.
Noticethateachupdatedbeliefstateb canbenolargerthanthepredictedbeliefstate ˆb;
o
observations can only help reduce uncertainty compared to the sensorless case. More-
over, for deterministic sensing, the belief states for the different possible percepts will
bedisjoint, formingapartition oftheoriginal predicted beliefstate.
11 Here,andthroughoutthebook,the“hat”inˆbmeansanestimatedorpredictedvalueforb.
Section4.4. SearchingwithPartialObservations 143
[B,Dirty] 2
Right
1 2
(a)
3 4
[B,Clean]
4
[B,Dirty] 2
Right 2
1 1 [A,Dirty] 1
(b)
3 3 3
4
[B,Clean]
4
Figure 4.15 Two example of transitions in local-sensing vacuum worlds. (a) In the de-
terministic world, Right is applied in the initial belief state, resulting in a new belief state
with two possible physicalstates; forthose states, the possible perceptsare [B,Dirty] and
[B,Clean], leading to two belief states, each of which is a singleton. (b) In the slippery
world, Right is applied in the initial belief state, giving a new belief state with four physi-
calstates; forthosestates, thepossibleperceptsare [A,Dirty], [B,Dirty], and[B,Clean],
leadingtothreebeliefstatesasshown.
Puttingthesethreestagestogether, weobtainthepossible beliefstatesresultingfromagiven
actionandthesubsequent possible percepts:
RESULTS(b,a) = {b
o
:b
o
= UPDATE(PREDICT(b,a),o)and
o ∈ POSSIBLE-PERCEPTS(PREDICT(b,a))}. (4.5)
Again,thenondeterminisminthepartiallyobservableproblemcomesfromtheinability
topredict exactly whichpercept willbereceived afteracting; underlying nondeterminism in
the physical environment may contribute to this inability by enlarging the belief state at the
prediction stage, leadingtomorepercepts attheobservation stage.
4.4.3 Solvingpartially observableproblems
The preceding section showed how to derive the RESULTS function for a nondeterministic
belief-state problem fromanunderlying physicalproblem andthe PERCEPT function. Given
144 Chapter 4. BeyondClassicalSearch
1
3
Suck Right
[A,Clean] [B,Dirty] [B,Clean]
5
2 4
7
Figure4.16 Thefirstlevelofthe AND–OR searchtreeforaprobleminthelocal-sensing
vacuumworld;Suck isthefirststepofthesolution.
such a formulation, the AND–OR search algorithm of Figure 4.11 can be applied directly to
derive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum
world,assuminganinitialpercept [A,Dirty]. Thesolution istheconditional plan
[Suck,Right,ifBstate={6}thenSuckelse[]].
Notice that, because we supplied a belief-state problem to the AND–OR search algorithm, it
returned a conditional plan that tests the belief state rather than the actual state. This isas it
shouldbe: inapartiallyobservableenvironmenttheagentwon’tbeabletoexecuteasolution
thatrequires testingtheactualstate.
Asinthecaseofstandard search algorithms applied tosensorless problems, the AND–
OR search algorithm treats belief states as black boxes, just like any other states. One can
improveonthisbycheckingforpreviouslygeneratedbeliefstatesthataresubsetsorsupersets
of the current state, just as for sensorless problems. One can also derive incremental search
algorithms, analogous to those described for sensorless problems, that provide substantial
speedups overtheblack-box approach.
4.4.4 Anagentforpartiallyobservable environments
Thedesign ofaproblem-solving agent forpartially observable environments isquitesimilar
to the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls a
searchalgorithm (such as AND-OR-GRAPH-SEARCH)tosolveit,andexecutes thesolution.
There are two main differences. First, the solution to a problem will be a conditional plan
rather than a sequence; if the first step is an if–then–else expression, the agent will need to
testtheconditionintheif-partandexecutethethen-partortheelse-partaccordingly. Second,
the agent will need to maintain its belief state as it performs actions and receives percepts.
This process resembles the prediction–observation–update process in Equation (4.5) but is
actuallysimplerbecausetheperceptisgivenbytheenvironmentratherthancalculatedbythe
Section4.4. SearchingwithPartialObservations 145
Suck [A,Clean] Right 2 [B,Dirty]
1 5 5 6 2
3 7 7 4 6
8
Figure4.17 Twoprediction–updatecyclesofbelief-statemaintenanceinthekindergarten
vacuumworldwithlocalsensing.
agent. Givenaninitialbeliefstateb,anactiona,andapercept o,thenewbeliefstateis:
(cid:2)
b = UPDATE(PREDICT(b,a),o). (4.6)
Figure 4.17 shows the belief state being maintained in the kindergarten vacuum world with
local sensing, wherein any square may become dirty at any time unless the agent is actively
cleaning itatthatmoment.12
In partially observable environments—which include the vast majority of real-world
environments—maintaining one’s belief state is a core function of any intelligent system.
This function goes under various names, including monitoring, filtering and state estima-
MONITORING
tion. Equation (4.6) iscalled arecursive state estimator because it computes the new belief
FILTERING
statefromthepreviousoneratherthanbyexaminingtheentireperceptsequence. Iftheagent
STATEESTIMATION
is not to “fall behind,” the computation has to happen as fast as percepts are coming in. As
RECURSIVE
the environment becomes more complex, the exact update computation becomes infeasible
and the agent willhave to compute an approximate belief state, perhaps focusing on the im-
plications ofthe percept fortheaspects ofthe environment that are ofcurrent interest. Most
work on this problem has been done for stochastic, continuous-state environments with the
tools of probability theory, as explained in Chapter 15. Here we will show an example in a
discreteenvironment withdetrministic sensorsandnondeterministic actions.
The example concerns a robot with the task of localization: working out where it is,
LOCALIZATION
given amap of the world and a sequence of percepts and actions. Ourrobot is placed in the
maze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that
tell whether there is an obstacle—the outer wall or a black square in the figure—in each of
thefourcompassdirections. Weassumethatthesensors give perfectly correct data, andthat
the robot has a correct map of the enviornment. But unfortunately the robot’s navigational
systemisbroken,sowhenitexecutesaMoveaction,itmovesrandomlytooneoftheadjacent
squares. Therobot’staskistodetermineitscurrentlocation.
Suppose the robot has just been switched on, so it does not know where it is. Thus its
initial belief state b consists of the set of all locations. The the robot receives the percept
12 Theusualapologiestothosewhoareunfamiliarwiththeeffectofsmallchildrenontheenvironment.
146 Chapter 4. BeyondClassicalSearch
(a)Possiblelocations ofrobotafterE = NSW
1
(b)Possiblelocations ofrobotAfterE = NSW,E = NS
1 2
Figure4.18 Possiblepositionsoftherobot,(cid:13),(a)afteroneobservationE =NSW and
1
(b)afterasecondobservationE =NS.Whensensorsarenoiselessandthetransitionmodel
2
isaccurate,therearenootherpossiblelocationsfortherobotconsistentwiththissequence
oftwoobservations.
NSW,meaningthereareobstaclestothenorth,west,andsouth,anddoesanupdateusingthe
equation b
o
=UPDATE(b),yielding the4locations showninFigure4.18(a). Youcaninspect
themazetoseethatthosearetheonlyfourlocations thatyieldthepercept NWS.
Nexttherobot executes aMove action, buttheresultisnondeterministic. Thenewbe-
liefstate,b
a
=PREDICT(b
o
,Move),containsallthelocationsthatareonestepawayfromthe
locations in b
o
. When the second percept, NS, arrives, the robot does UPDATE(b
a
,NS) and
finds that the belief state has collapsed down to the single location shown in Figure 4.18(b).
That’stheonlylocation thatcouldbetheresultof
UPDATE(PREDICT(UPDATE(b,NSW),Move),NS).
With nondetermnistic actions the PREDICT step grows the belief state, but the UPDATE step
shrinks it back down—as long as the percepts provide some useful identifying information.
Sometimes the percepts don’t help much for localization: If there were one or more long
east-west corridors, then a robot could receive a long sequence of NS percepts, but never
knowwhereinthecorridor(s) itwas.
Section4.5. OnlineSearchAgentsandUnknownEnvironments 147
4.5 ONLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS
So far we have concentrated on agents that use offline search algorithms. They compute
OFFLINESEARCH
a complete solution before setting foot in the real world and then execute the solution. In
contrast,anonlinesearch13agentinterleavescomputationandaction: firstittakesanaction,
ONLINESEARCH
then itobserves theenvironment and computes the nextaction. Online search isagoodidea
in dynamic or semidynamic domains—domains where there is a penalty for sitting around
and computing too long. Online search is also helpful in nondeterministic domains because
it allows the agent to focus its computational efforts on the contingencies that actually arise
rather than those that might happen but probably won’t. Of course, there is a tradeoff: the
moreanagentplansahead, thelessoftenitwillfinditselfupthecreekwithoutapaddle.
Onlinesearchisanecessary ideaforunknownenvironments, wheretheagentdoesnot
know what states exist or what its actions do. In this state of ignorance, the agent faces an
EXPLORATION exploration problem and must use its actions as experiments in order to learn enough to
PROBLEM
makedeliberation worthwhile.
Thecanonical example of online search is a robot that is placed in anew building and
mustexplore ittobuild amapthatitcanuse forgetting from AtoB. Methods forescaping
fromlabyrinths—required knowledge foraspiring heroesof antiquity—are alsoexamplesof
online search algorithms. Spatial exploration is not the only form of exploration, however.
Consider a newborn baby: it has many possible actions but knows the outcomes of none of
them, and it has experienced only a few of the possible states that it can reach. The baby’s
gradualdiscovery ofhowtheworldworksis,inpart,anonlinesearchprocess.
4.5.1 Onlinesearchproblems
Anonline search problem mustbesolved byan agent executing actions, rather than bypure
computation. We assume a deterministic and fully observable environment (Chapter 17 re-
laxestheseassumptions), butwestipulate thattheagentknowsonlythefollowing:
• ACTIONS(s),whichreturnsalistofactions allowedinstate s;
• The step-cost function c(s,a,s (cid:2) )—note that this cannot be used until the agent knows
(cid:2)
thats istheoutcome; and
• GOAL-TEST(s).
Note in particular that the agent cannot determine RESULT(s,a) except by actually being
in s and doing a. For example, in the maze problem shown in Figure 4.19, the agent does
not know that going Up from (1,1) leads to (1,2); nor, having done that, does it know that
going Down will take it back to (1,1). This degree of ignorance can be reduced in some
applications—for example,arobotexplorermightknowhowitsmovementactionsworkand
beignorantonlyofthelocations ofobstacles.
13 Theterm“online”iscommonlyusedincomputersciencetorefertoalgorithmsthatmustprocessinputdata
astheyarereceivedratherthanwaitingfortheentireinputdatasettobecomeavailable.
148 Chapter 4. BeyondClassicalSearch
3 G
2
1 S
1 2 3
Figure4.19 Asimplemazeproblem. TheagentstartsatS andmustreachGbutknows
nothingoftheenvironment.
G
S A
S G
S A
G
(a) (b)
Figure4.20 (a)Twostatespacesthatmightleadanonlinesearchagentintoadeadend.
Anygivenagentwillfailinatleastoneofthesespaces. (b)Atwo-dimensionalenvironment
that can cause an online search agent to follow an arbitrarily inefficient route to the goal.
Whicheverchoice the agentmakes, the adversaryblocksthat route with anotherlong, thin
wall,sothatthepathfollowedismuchlongerthanthebestpossiblepath.
Finally, the agent might have access to an admissible heuristic function h(s) that es-
timates the distance from the current state to a goal state. For example, in Figure 4.19, the
agentmightknowthelocationofthegoalandbeabletousetheManhattan-distanceheuristic.
Typically,theagent’sobjectiveistoreachagoalstatewhileminimizingcost. (Another
possibleobjectiveissimplytoexploretheentireenvironment.) Thecostisthetotalpathcost
of the path that the agent actually travels. It is common to compare this cost with the path
cost of the path the agent would follow if it knew the search space in advance—that is, the
actual shortest path (orshortest complete exploration). Inthelanguage ofonline algorithms,
thisiscalledthecompetitiveratio;wewouldlikeittobeassmallaspossible.
COMPETITIVERATIO
Section4.5. OnlineSearchAgentsandUnknownEnvironments 149
Althoughthissounds likeareasonable request, itiseasytoseethatthebestachievable
competitive ratio is infinite in some cases. For example, if some actions are irreversible—
IRREVERSIBLE
i.e., they lead to a state from which no action leads back to the previous state—the online
searchmightaccidentally reachadead-endstatefromwhichnogoalstateisreachable. Per-
DEADEND
haps the term “accidentally” is unconvincing—after all, there might be an algorithm that
happensnottotakethedead-endpathasitexplores. Ourclaim,tobemoreprecise,isthatno
algorithm canavoiddeadendsinallstatespaces. Considerthetwodead-end statespacesin
Figure 4.20(a). To an online search algorithm that has visited states S and A, the two state
spaces look identical, so it must make the same decision in both. Therefore, it will fail in
ADVERSARY one of them. This isan example of an adversary argument—wecan imagine an adversary
ARGUMENT
constructing the state space while the agent explores it and putting the goals and dead ends
whereveritchooses.
Deadendsarearealdifficultyforrobotexploration—staircases, ramps,cliffs,one-way
streets, andallkindsofnaturalterrainpresentopportunities forirreversibleactions. Tomake
progress,wesimplyassumethatthestatespaceissafelyexplorable—thatis,somegoalstate
SAFELYEXPLORABLE
is reachable from every reachable state. State spaces with reversible actions, such as mazes
and8-puzzles, canbeviewedasundirected graphsandareclearlysafelyexplorable.
Even in safely explorable environments, no bounded competitive ratio can be guaran-
teed if there are paths of unbounded cost. This is easy to show in environments with irre-
versible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)
shows. Forthisreason,itiscommontodescribetheperformanceofonlinesearchalgorithms
intermsofthesizeoftheentirestatespaceratherthanjust thedepthoftheshallowest goal.
4.5.2 Onlinesearchagents
Aftereachaction, anonlineagentreceivesapercepttelling itwhatstateithasreached; from
this information, it can augment its map of the environment. The current map is used to
decide where to go next. This interleaving of planning and action means that online search
algorithmsarequitedifferentfromtheofflinesearchalgorithmswehaveseenpreviously. For
∗
example, offline algorithms such as A can expand a node in one part of the space and then
immediately expand a node in another part of the space, because node expansion involves
simulated rather than real actions. An online algorithm, on the other hand, can discover
successors only for a node that it physically occupies. To avoid traveling all the way across
thetreetoexpandthenextnode,itseemsbettertoexpandnodesinalocalorder. Depth-first
searchhasexactlythisproperty because(exceptwhenbacktracking) thenextnodeexpanded
isachildoftheprevious nodeexpanded.
An online depth-first search agent is shown in Figure 4.21. This agent stores its map
in a table, RESULT[s,a], that records the state resulting from executing action a in state s.
Whenever an action from the current state has not been explored, the agent tries that action.
The difficulty comes when the agent has tried all the actions in a state. In offline depth-first
search, the state is simply dropped from the queue; in an online search, the agent has to
backtrack physically. Indepth-firstsearch,thismeansgoingbacktothestatefromwhichthe
agentmostrecentlyenteredthecurrentstate. Toachievethat,thealgorithmkeepsatablethat
150 Chapter 4. BeyondClassicalSearch
functionONLINE-DFS-AGENT(s(cid:5))returnsanaction
inputs:s(cid:5),aperceptthatidentifiesthecurrentstate
persistent: result,atableindexedbystateandaction,initiallyempty
untried,atablethatlists,foreachstate,theactionsnotyettried
unbacktracked,atablethatlists,foreachstate,thebacktracksnotyettried
s,a,thepreviousstateandaction,initiallynull
ifGOAL-TEST(s(cid:5))thenreturnstop
ifs(cid:5)isanewstate(notinuntried)thenuntried[s(cid:5)]←ACTIONS(s(cid:5))
ifs isnotnullthen
result[s,a]←s(cid:5)
adds tothefrontofunbacktracked[s(cid:5)]
ifuntried[s(cid:5)]isemptythen
ifunbacktracked[s(cid:5)]isemptythenreturnstop
elsea←anactionb suchthatresult[s(cid:5),b]=POP(unbacktracked[s(cid:5)])
elsea←POP(untried[s(cid:5)])
s←s(cid:5)
returna
Figure4.21 Anonlinesearchagentthatusesdepth-firstexploration. Theagentisappli-
cableonlyinstatespacesinwhicheveryactioncanbe“undone”bysomeotheraction.
lists, foreach state, the predecessor states to which the agent has not yet backtracked. If the
agenthasrunoutofstatestowhichitcanbacktrack, thenits searchiscomplete.
We recommend that the reader trace through the progress of ONLINE-DFS-AGENT
when applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will, in
theworstcase, enduptraversing everylinkinthestatespace exactly twice. Forexploration,
this is optimal; for finding a goal, on the other hand, the agent’s competitive ratio could be
arbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial
state. Anonlinevariantofiterativedeepeningsolvesthis problem;foranenvironment thatis
auniform tree,thecompetitive ratioofsuchanagentisasmallconstant.
Because of its method of backtracking, ONLINE-DFS-AGENT works only in state
spaces where the actions are reversible. There are slightly more complex algorithms that
workingeneralstatespaces,butnosuchalgorithm hasabounded competitiveratio.
4.5.3 Onlinelocalsearch
Like depth-first search, hill-climbing search has the property of locality in its node expan-
sions. In fact, because it keeps just one current state in memory, hill-climbing search is
already an online search algorithm! Unfortunately, it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go. Moreover, random
restartscannotbeused,becausetheagentcannottransport itselftoanewstate.
Instead of random restarts, one might consider using a random walk to explore the
RANDOMWALK
environment. Arandom walksimply selects atrandom one oftheavailable actions from the
Section4.5. OnlineSearchAgentsandUnknownEnvironments 151
S G
Figure4.22 Anenvironmentinwhicharandomwalkwilltakeexponentiallymanysteps
tofindthegoal.
current state; preference can be given to actions that have not yet been tried. It is easy to
prove that a random walk will eventually find a goal or complete its exploration, provided
thatthespaceisfinite.14 Ontheotherhand, theprocess canbeveryslow. Figure4.22shows
an environment in which a random walk will take exponentially many steps to find the goal
because, ateachstep,backwardprogressistwiceaslikelyasforwardprogress. Theexample
is contrived, of course, but there are many real-world state spaces whose topology causes
thesekindsof“traps”forrandomwalks.
Augmenting hill climbing withmemoryrather than randomness turns out to beamore
effective approach. The basic idea is to store a “current best estimate” H(s) of the cost to
reach the goal from each state that has been visited. H(s) starts out being just the heuristic
estimate h(s) and is updated as the agent gains experience in the state space. Figure 4.23
shows a simple example in a one-dimensional state space. In (a), the agent seems to be
stuck in a flat local minimum at the shaded state. Rather than staying where it is, the agent
should follow what seems to bethe best path tothe goal given the current cost estimates for
(cid:2)
its neighbors. The estimated cost to reach the goal through a neighbor s is the cost to get
(cid:2) (cid:2) (cid:2)
to s plus the estimated cost to get to a goal from there—that is, c(s,a,s)+H(s). In the
example,therearetwoactions,withestimatedcosts1+9and1+2,soitseemsbesttomove
right. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.
Since the best move cost 1 and led to a state that is at least 2 steps from a goal, the shaded
state must be at least 3 steps from agoal, so its H should be updated accordingly, as shown
in Figure 4.23(b). Continuing this process, the agent will move back and forth twice more,
updating H eachtimeand“flattening out”thelocalminimumuntilitescapes totheright.
∗ ∗
Anagent implementing this scheme, whichiscalled learning real-time A (LRTA), is
LRTA*
shown in Figure 4.24. Like ONLINE-DFS-AGENT, it builds a map of the environment in
theresult table. Itupdates thecostestimate forthestate ithas justleft andthen chooses the
“apparently best” move according to its current cost estimates. One important detail is that
actionsthathavenotyetbeentriedinastatesarealwaysassumedtoleadimmediatelytothe
OPTIMISMUNDER goalwiththeleastpossiblecost,namelyh(s). Thisoptimismunderuncertaintyencourages
UNCERTAINTY
theagenttoexplorenew,possiblypromising paths.
∗
AnLRTA agentisguaranteedtofindagoalinanyfinite,safelyexplorableenvironment.
∗
UnlikeA,however,itisnotcompleteforinfinitestatespaces—therearecaseswhereitcanbe
ledinfinitelyastray. ItcanexploreanenvironmentofnstatesinO(n2)stepsintheworstcase,
14 Randomwalksarecompleteoninfiniteone-dimensionalandtwo-dimensionalgrids. Onathree-dimensional
grid,theprobabilitythatthewalkeverreturnstothestartingpointisonlyabout0.3405(Hughes,1995).
152 Chapter 4. BeyondClassicalSearch
1 1 1 1 1 1 1
(a) 8 9 2 2 4 3
1 1 1 1 1 1 1
(b) 8 9 3 2 4 3
1 1 1 1 1 1 1
(c) 8 9 3 4 4 3
1 1 1 1 1 1 1
(d) 8 9 5 4 4 3
1 1 1 1 1 1 1
(e) 8 9 5 5 4 3
Figure 4.23 Five iterations of LRTA∗ on a one-dimensional state space. Each state is
labeledwithH(s),thecurrentcostestimatetoreachagoal,andeachlinkislabeledwithits
stepcost. Theshadedstatemarksthelocationoftheagent,andtheupdatedcostestimatesat
eachiterationarecircled.
functionLRTA*-AGENT(s(cid:5))returnsanaction
inputs:s(cid:5),aperceptthatidentifiesthecurrentstate
persistent: result,atable,indexedbystateandaction,initiallyempty
H,atableofcostestimatesindexedbystate,initiallyempty
s,a,thepreviousstateandaction,initiallynull
ifGOAL-TEST(s(cid:5))thenreturnstop
ifs(cid:5)isanewstate(notinH)thenH[s(cid:5)]←h(s(cid:5))
ifs isnotnull
result[s,a]←s(cid:5)
H[s]← min LRTA*-COST(s,b,result[s,b],H)
b∈ACTIONS(s)
a←anactionb inACTIONS(s(cid:5))thatminimizesLRTA*-COST(s(cid:5),b,result[s(cid:5),b],H)
s←s(cid:5)
returna
functionLRTA*-COST(s,a,s(cid:5),H)returnsacostestimate
ifs(cid:5)isundefinedthenreturnh(s)
elsereturnc(s,a,s(cid:5)) + H[s(cid:5)]
Figure 4.24 LRTA*-AGENT selects an action according to the values of neighboring
states,whichareupdatedastheagentmovesaboutthestatespace.
Section4.6. Summary 153
∗
butoftendoesmuchbetter. TheLRTA agentisjustoneofalargefamilyofonlineagentsthat
one can define by specifying the action selection rule and the update rule in different ways.
Wediscussthisfamily,developed originally forstochastic environments, inChapter21.
4.5.4 Learning inonlinesearch
Theinitialignoranceofonlinesearchagentsprovidesseveralopportunitiesforlearning. First,
theagents learn a“map”ofthe environment—more precisely, theoutcome ofeach action in
each state—simply by recording each of their experiences. (Notice that the assumption of
deterministic environments means that one experience is enough for each action.) Second,
thelocalsearchagentsacquiremoreaccurateestimatesofthecostofeachstatebyusinglocal
∗
updating rules, as inLRTA. InChapter 21, weshow that these updates eventually converge
to exact values for every state, provided that the agent explores the state space in the right
way. Onceexact values are known, optimal decisions can be taken simply by moving to the
lowest-cost successor—that is,purehillclimbingisthenanoptimalstrategy.
If you followed our suggestion to trace the behavior of ONLINE-DFS-AGENT in the
environment of Figure 4.19, you will have noticed that the agent is not very bright. For
example, after it has seen that the Up action goes from (1,1) to (1,2), the agent still has no
idea that the Down action goes back to (1,1) or that the Up action also goes from (2,1) to
(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up
increases they-coordinate unless thereisawallintheway,that Down reduces it,andsoon.
For this to happen, we need two things. First, we need a formal and explicitly manipulable
representation forthese kinds ofgeneral rules; sofar, wehavehidden the information inside
theblack boxcalled the RESULT function. PartIII isdevoted tothis issue. Second, weneed
algorithms that can construct suitable general rules from the specific observations made by
theagent. ThesearecoveredinChapter18.
4.6 SUMMARY
This chapter has examined search algorithms for problems beyond the “classical” case of
findingtheshortest pathtoagoalinanobservable, deterministic, discreteenvironment.
• Local search methods such as hill climbing operate on complete-state formulations,
keeping only a small number of nodes in memory. Several stochastic algorithms have
beendeveloped,includingsimulatedannealing,whichreturnsoptimalsolutionswhen
givenanappropriate coolingschedule.
• Many local search methods apply also to problems in continuous spaces. Linear pro-
gramming and convex optimization problems obey certain restrictions on the shape
of the state space and the nature of the objective function, and admit polynomial-time
algorithmsthatareoftenextremelyefficientinpractice.
• Ageneticalgorithmisastochastic hill-climbing searchinwhichalargepopulation of
states is maintained. New states are generated by mutation and by crossover, which
combinespairsofstatesfromthepopulation.
154 Chapter 4. BeyondClassicalSearch
• Innondeterministicenvironments, agents canapply AND–OR search togenerate con-
tingentplansthatreachthegoalregardlessofwhichoutcomesoccur duringexecution.
• When the environment is partially observable, the belief state represents the set of
possiblestatesthattheagentmightbein.
• Standardsearchalgorithmscanbeapplieddirectlytobelief-statespacetosolvesensor-
less problems, and belief-state AND–OR search can solve general partially observable
problems. Incremental algorithms thatconstruct solutions state-by-state withinabelief
stateareoftenmoreefficient.
• Explorationproblemsarisewhentheagenthasnoideaaboutthestatesandactionsof
itsenvironment. Forsafely explorable environments, onlinesearch agents can build a
mapandfindagoalifoneexists. Updatingheuristicestimatesfromexperienceprovides
aneffectivemethodtoescapefromlocalminima.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Local search techniques have a long history in mathematics and computer science. Indeed,
the Newton–Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very effi-
cient local search method for continuous spaces in which gradient information is available.
Brent (1973) is a classic reference for optimization algorithms that do not require such in-
formation. Beam search, which we have presented as a local search algorithm, originated
as a bounded-width variant of dynamic programming for speech recognition in the HARPY
system(Lowerre,1976). Arelatedalgorithm isanalyzed indepthbyPearl(1984,Ch.5).
Thetopic oflocal search wasreinvigorated intheearly 1990s bysurprisingly good re-
sults for large constraint-satisfaction problems such as n-queens (Minton et al., 1992) and
logical reasoning (Selman et al., 1992) and by the incorporation of randomness, multiple
simultaneoussearches,andotherimprovements. ThisrenaissanceofwhatChristosPapadim-
itriou has called “New Age” algorithms also sparked increased interest among theoretical
computer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994). In
thefieldofoperationsresearch,avariantofhillclimbingcalledtabusearchhasgainedpopu-
TABUSEARCH
larity(GloverandLaguna,1997). Thisalgorithmmaintains atabulistofkpreviouslyvisited
statesthatcannotberevisited;aswellasimprovingefficiencywhensearchinggraphs,thislist
canallow the algorithm toescape from somelocal minima. Anotheruseful improvement on
hill climbing is the STAGE algorithm (Boyan and Moore, 1998). Theidea is touse the local
maximafoundbyrandom-restart hillclimbingtogetanideaoftheoverallshapeoftheland-
scape. Thealgorithm fitsasmoothsurface tothesetoflocalmaximaandthencalculates the
global maximum of that surface analytically. This becomes the new restart point. The algo-
rithmhasbeenshowntoworkinpracticeonhardproblems. Gomesetal.(1998)showedthat
HEAVY-TAILED the runtimes ofsystematic backtracking algorithms often have aheavy-tailed distribution,
DISTRIBUTION
which means that the probability of a very long run time is more than would be predicted if
theruntimes wereexponentially distributed. Whentherun timedistribution isheavy-tailed,
randomrestarts findasolution faster, onaverage, thanasingleruntocompletion.
Bibliographical andHistorical Notes 155
Simulated annealing was first described by Kirkpatrick et al. (1983), who borrowed
directly from the Metropolis algorithm (which is used to simulate complex systems in
physics(Metropolisetal.,1953)andwassupposedlyinventedataLosAlamosdinnerparty).
Simulatedannealing isnowafieldinitself,withhundreds ofpaperspublished everyyear.
Finding optimal solutions in continuous spaces is the subject matter of several fields,
includingoptimizationtheory,optimalcontroltheory,andthecalculusofvariations. The
basictechniques areexplained wellbyBishop(1995);Pressetal.(2007)coverawiderange
ofalgorithms andprovideworkingsoftware.
As Andrew Moore points out, researchers have taken inspiration for search and opti-
mizationalgorithmsfromawidevarietyoffieldsofstudy: metallurgy(simulatedannealing),
biology (geneticalgorithms), economics(market-based algorithms), entomology (antcolony
optimization),neurology(neuralnetworks),animalbehavior(reinforcementlearning),moun-
taineering (hillclimbing), andothers.
Linearprogramming(LP)wasfirststudiedsystematically bytheRussianmathemati-
cian Leonid Kantorovich (1939). It was one of the first applications of computers; the sim-
plexalgorithm(Dantzig,1949)isstilluseddespiteworst-caseexponential complexity. Kar-
markar(1984)developedthefarmoreefficientfamilyof interior-pointmethods, whichwas
showntohavepolynomialcomplexityforthemoregeneralclassofconvexoptimizationprob-
lemsbyNesterovandNemirovski(1994). Excellentintroductionstoconvexoptimizationare
provided byBen-TalandNemirovski(2001)andBoydandVandenberghe (2004).
Work by Sewall Wright (1931) on the concept of a fitness landscape was an impor-
tant precursor to the development of genetic algorithms. In the 1950s, several statisticians,
including Box (1957) and Friedman (1959), used evolutionary techniques for optimization
EVOLUTION problems,butitwasn’tuntilRechenberg(1965)introduced evolutionstrategies tosolveop-
STRATEGY
timization problemsforairfoils thattheapproach gainedpopularity. Inthe1960s and1970s,
John Holland (1975) championed genetic algorithms, both as a useful tool and as a method
toexpand ourunderstanding ofadaptation, biological orotherwise (Holland, 1995). Thear-
tificial life movement (Langton, 1995) takes this idea one step further, viewing the products
ARTIFICIALLIFE
of genetic algorithms as organisms rather than solutions to problems. Work in this field by
HintonandNowlan(1987) andAckleyandLittman(1991) hasdonemuchtoclarify theim-
plicationsoftheBaldwineffect. Forgeneralbackground onevolution, werecommendSmith
andSzathma´ry(1999),Ridley(2004), andCarroll(2007).
Mostcomparisons ofgenetic algorithms tootherapproaches (especially stochastic hill
climbing) have found that the genetic algorithms are slower to converge (O’Reilly and Op-
pacher,1994;Mitchelletal.,1996;JuelsandWattenberg,1996;Baluja,1997). Suchfindings
are not universally popular within the GA community, but recent attempts within that com-
munity to understand population-based search as an approximate form of Bayesian learning
(see Chapter 20) might help close the gap between the field and its critics (Pelikan et al.,
1999). The theory of quadratic dynamical systems may also explain the performance of
GAs(Rabani etal.,1998). SeeLohnetal.(2001) foranexampleofGAsapplied toantenna
design, andRennerandEkart(2003)foranapplication tocomputer-aided design.
GENETIC Thefieldofgeneticprogrammingiscloselyrelatedtogeneticalgorithms. Theprinci-
PROGRAMMING
pal difference is that the representations that are mutated and combined are programs rather
156 Chapter 4. BeyondClassicalSearch
thanbitstrings. Theprogramsarerepresentedintheformof expressiontrees;theexpressions
canbeinastandard language suchasLisporcanbespecially designed torepresent circuits,
robot controllers, and so on. Crossover involves splicing together subtrees rather than sub-
strings. This form of mutation guarantees that the offspring are well-formed expressions,
whichwouldnotbethecaseifprogramsweremanipulated asstrings.
InterestingeneticprogrammingwasspurredbyJohnKoza’swork(Koza,1992,1994),
but it goes back at least to early experiments with machine code by Friedberg (1958) and
withfinite-state automata byFogeletal. (1966). Aswithgenetic algorithms, there isdebate
about the effectiveness of the technique. Kozaet al. (1999) describe experiments in the use
ofgenetic programmingtodesigncircuitdevices.
Thejournals Evolutionary Computation andIEEETransactions onEvolutionary Com-
putation covergenetic algorithms and genetic programming; articles are alsofound in Com-
plex Systems, Adaptive Behavior, and Artificial Life. The main conference is the Genetic
andEvolutionary Computation Conference(GECCO).Goodoverviewtextsongeneticalgo-
rithmsaregivenbyMitchell (1996), Fogel(2000), andLangdon andPoli(2002), andbythe
freeonlinebookbyPolietal.(2008).
The unpredictability and partial observability of real environments were recognized
early on in robotics projects that used planning techniques, including Shakey (Fikes et al.,
1972) and FREDDY (Michie, 1974). Theproblems received moreattention after thepublica-
tionofMcDermott’s(1978a) influentialarticle, PlanningandActing.
ThefirstworktomakeexplicituseofAND–ORtreesseemstohavebeenSlagle’sSAINT
program for symbolic integration, mentioned in Chapter 1. Amarel (1967) applied the idea
to propositional theorem proving, a topic discussed in Chapter 7, and introduced a search
algorithm similar to AND-OR-GRAPH-SEARCH. The algorithm was further developed and
∗
formalized by Nilsson (1971), who also described AO —which, as its name suggests, finds
∗
optimalsolutionsgivenanadmissibleheuristic. AO wasanalyzedandimprovedbyMartelli
∗ ∗
and Montanari (1973). AO is a top-down algorithm; a bottom-up generalization of A is
∗ ∗
ALD,forA LightestDerivation (Felzenszwalb andMcAllester, 2007). Interest in AND–OR
search has undergone a revival in recent years, with new algorithms for finding cyclic solu-
tions(JimenezandTorras,2000;HansenandZilberstein, 2001)andnewtechniques inspired
bydynamicprogramming (BonetandGeffner,2005).
Theideaoftransforming partially observable problemsintobelief-state problemsorig-
inated withAstrom(1965) forthemuchmorecomplexcaseofprobabilistic uncertainty (see
Chapter17). Erdmann andMason (1988) studied theproblem of robotic manipulation with-
outsensors, usingacontinuous formofbelief-state search. Theyshowedthatitwaspossible
toorientapartonatablefromanarbitraryinitialposition byawell-designedsequenceoftilt-
ingactions. Morepracticalmethods,basedonaseriesofpreciselyorienteddiagonalbarriers
acrossaconveyorbelt,usethesamealgorithmic insights(Wiegleyetal.,1996).
The belief-state approach was reinvented in the context of sensorless and partially ob-
servable searchproblems byGenesereth andNourbakhsh (1993). Additional workwasdone
onsensorless problems inthe logic-based planning community (Goldman and Boddy, 1996;
Smithand Weld, 1998). Thiswork has emphasized concise representations forbelief states,
asexplainedinChapter11. BonetandGeffner(2000)introduced thefirsteffectiveheuristics
Exercises 157
for belief-state search; these were refined by Bryce et al. (2006). The incremental approach
to belief-state search, in which solutions are constructed incrementally for subsets of states
withineachbeliefstate,wasstudiedintheplanningliteraturebyKurienetal.(2002);several
newincrementalalgorithms wereintroduced fornondeterministic, partiallyobservable prob-
lemsbyRussellandWolfe(2005). Additional references for planning instochastic, partially
observable environments appearinChapter17.
Algorithmsforexploringunknownstatespaceshavebeenofinterestformanycenturies.
Depth-firstsearchinamazecanbeimplementedbykeepingone’slefthandonthewall;loops
can be avoided by marking each junction. Depth-first search fails with irreversible actions;
themoregeneralproblemofexploring Euleriangraphs(i.e.,graphsinwhicheachnodehas
EULERIANGRAPH
equalnumbersofincomingandoutgoingedges)wassolvedbyanalgorithmduetoHierholzer
(1873). Thefirst thorough algorithmic study of the exploration problem forarbitrary graphs
was carried out by Deng and Papadimitriou (1990), who developed a completely general
algorithm but showed that no bounded competitive ratio is possible for exploring a general
graph. PapadimitriouandYannakakis(1991)examinedthequestionoffindingpathstoagoal
ingeometricpath-planningenvironments(whereallactionsarereversible). Theyshowedthat
a small competitive ratio is achievable with square obstacles, but with general rectangular
obstacles noboundedratiocanbeachieved. (SeeFigure4.20.)
∗
The LRTA algorithm was developed by Korf (1990) as part of an investigation into
real-time search for environments in which the agent must act after searching for only a
REAL-TIMESEARCH
∗
fixed amount of time (a common situation in two-player games). LRTA is in fact a special
caseofreinforcementlearningalgorithmsforstochasticenvironments(Bartoetal.,1995). Its
policyofoptimismunderuncertainty—alwaysheadfortheclosestunvisitedstate—canresult
in an exploration pattern that is less efficient in the uninformed case than simple depth-first
search (Koenig, 2000). Dasguptaetal.(1994) show thatonline iterative deepening search is
optimally efficient for finding a goal in a uniform tree with no heuristic information. Sev-
∗
eral informed variants on the LRTA theme have been developed with different methods for
searching and updating within the known portion of the graph (Pemberton and Korf, 1992).
As yet, there is no good understanding of how to find goals with optimal efficiency when
usingheuristic information.
EXERCISES
4.1 Givethenameofthealgorithm thatresults fromeachofthefollowingspecialcases:
a. Localbeamsearchwithk = 1.
b. Localbeamsearchwithoneinitialstateandnolimitonthenumberofstatesretained.
c. Simulatedannealing withT = 0atalltimes(andomittingthetermination test).
d. Simulatedannealing withT = ∞atalltimes.
e. Geneticalgorithm withpopulation sizeN = 1.
158 Chapter 4. BeyondClassicalSearch
4.2 Exercise 3.16 considers the problem of building railway tracks under the assumption
that pieces fit exactly with no slack. Now consider the real problem, in which pieces don’t
fitexactly butallow forupto10degrees ofrotation toeither sideofthe“proper” alignment.
Explainhowtoformulatetheproblem soitcouldbesolvedbysimulatedannealing.
4.3 In this exercise, we explore the use of local search methods to solve TSPs of the type
definedinExercise3.30.
a. Implementandtestahill-climbingmethodtosolveTSPs. Comparetheresultswithop-
∗
timalsolutions obtained fromtheA algorithm withtheMSTheuristic(Exercise3.30).
b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to
consultLarran˜aga etal.(1999)forsomesuggestions forrepresentations.
4.4 Generatealargenumberof8-puzzleand8-queensinstancesandsolvethem(wherepos-
sible) by hill climbing (steepest-ascent and first-choice variants), hill climbing with random
restart, andsimulated annealing. Measurethesearchcostandpercentage ofsolvedproblems
andgraphtheseagainsttheoptimalsolution cost. Commentonyourresults.
4.5 The AND-OR-GRAPH-SEARCH algorithm in Figure 4.11 checks for repeated states
only on the path from the root to the current state. Suppose that, in addition, the algorithm
weretostore every visited state and check against that list. (See BREADTH-FIRST-SEARCH
inFigure3.11foranexample.) Determinetheinformation thatshouldbestoredandhowthe
algorithmshouldusethatinformationwhenarepeatedstate isfound. (Hint: Youwillneedto
distinguish atleastbetweenstatesforwhichasuccessfulsubplanwasconstructed previously
and states for which no subplan could be found.) Explain how to use labels, as defined in
Section4.3.3,toavoidhaving multiplecopiesofsubplans.
4.6 Explainpreciselyhowtomodifythe AND-OR-GRAPH-SEARCH algorithm togenerate
acyclicplanifnoacyclicplanexists. Youwillneedtodealwiththreeissues: labelingtheplan
stepssothatacyclicplancanpointbacktoanearlierpartoftheplan,modifyingOR-SEARCH
so that it continues to look for acyclic plans after finding a cyclic plan, and augmenting the
plan representation toindicate whether aplan iscyclic. Showhow your algorithm works on
(a)theslipperyvacuumworld,and(b)theslippery,erratic vacuumworld. Youmightwishto
useacomputerimplementation tocheckyourresults.
4.7 In Section 4.4.1 we introduced belief states to solve sensorless search problems. A
sequence of actions solves a sensorless problem if it maps every physical state in the initial
∗
beliefstatebtoagoalstate. Supposetheagentknowsh (s),thetrueoptimalcostofsolving
thephysicalstatesinthefullyobservable problem,foreverystate sinb. Findanadmissible
heuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.
Commentontheaccuracyofthisheuristiconthesensorless vacuumproblemofFigure4.14.
∗
HowwelldoesA perform?
4.8 This exercise explores subset–superset relations between belief states in sensorless or
partially observable environments.
a. Provethatifanactionsequence isasolution forabeliefstateb,itisalsoasolution for
anysubsetofb. Cananythingbesaidaboutsupersets ofb?
Exercises 159
b. Explainindetailhowtomodifygraphsearchforsensorless problemstotakeadvantage
ofyouranswersin(a).
c. Explain in detail how to modify AND–OR search for partially observable problems,
beyondthemodifications youdescribe in(b).
4.9 On page 139 it was assumed that a given action would have the same cost when ex-
ecuted in any physical state within a given belief state. (This leads to a belief-state search
problem with well-defined step costs.) Now consider what happens when the assumption
doesnothold. Doesthenotionofoptimalitystillmakesenseinthiscontext,ordoesitrequire
modification? Consideralso various possible definitions of the“cost” ofexecuting anaction
in a belief state; for example, we could use the minimum of the physical costs; or the maxi-
mum; or a cost interval with the lower bound being the minimum cost and the upper bound
beingthemaximum;orjustkeepthesetofallpossiblecostsforthataction. Foreachofthese,
∗
explorewhetherA (withmodifications ifnecessary) canreturnoptimalsolutions.
4.10 Consider the sensorless version of the erratic vacuum world. Draw the belief-state
spacereachablefromtheinitialbeliefstate{1,2,3,4,5,6,7,8},andexplainwhytheproblem
isunsolvable.
4.11 Wecanturnthenavigation problem inExercise3.7intoanenvironment asfollows:
• The percept will be a list of the positions, relative to the agent, of the visible vertices.
Theperceptdoesnotincludethepositionoftherobot! Therobotmustlearnitsownpo-
sitionfromthemap;fornow,youcanassumethateachlocation hasadifferent “view.”
• Each action will be a vector describing a straight-line path to follow. If the path is
unobstructed, the action succeeds; otherwise, the robot stops at the point where its
path first intersects an obstacle. If the agent returns a zero motion vector and is at the
goal(which isfixedand known), then theenvironment teleports theagent toarandom
location(notinsideanobstacle).
• Theperformance measure charges the agent 1point foreach unit ofdistance traversed
andawards1000pointseachtimethegoalisreached.
a. Implement this environment and a problem-solving agent for it. After each teleporta-
tion,theagentwillneedtoformulateanewproblem,whichwillinvolvediscoveringits
currentlocation.
b. Documentyouragent’sperformance(byhavingtheagentgeneratesuitablecommentary
asitmovesaround)andreportitsperformance over100episodes.
c. Modify the environment so that 30% of the time the agent ends up at an unintended
destination(chosenrandomlyfromtheothervisibleverticesifany;otherwise,nomove
at all). This is a crude model of the motion errors of a real robot. Modify the agent
so that when such an error is detected, it finds out where it is and then constructs a
plan to get back to where it was and resume the old plan. Remember that sometimes
gettingbacktowhereitwasmightalsofail! Showanexampleoftheagentsuccessfully
overcomingtwosuccessive motionerrorsandstillreaching thegoal.
160 Chapter 4. BeyondClassicalSearch
d. Nowtrytwodifferentrecoveryschemesafteranerror: (1)headfortheclosestvertexon
theoriginalroute;and(2)replanaroutetothegoalfromthe newlocation. Comparethe
performance ofthe three recovery schemes. Would theinclusion ofsearch costs affect
thecomparison?
e. Now suppose that there are locations from which the view is identical. (For example,
supposetheworldisagridwithsquareobstacles.) Whatkindofproblemdoestheagent
nowface? Whatdosolutions looklike?
4.12 Suppose that an agent is in a 3×3 maze environment like the one shown in Fig-
ure4.19. Theagentknowsthatitsinitiallocationis(1,1),thatthegoalisat(3,3),andthatthe
actions Up, Down, Left,Right have theirusual effects unless blocked byawall. Theagent
does notknow wheretheinternal wallsare. Inanygiven state, theagent perceives thesetof
legalactions;itcanalsotellwhetherthestateisoneithasvisitedbefore.
a. Explainhowthisonlinesearchproblemcanbeviewedasanofflinesearchinbelief-state
space, where the initial belief state includes all possible environment configurations.
Howlargeistheinitialbeliefstate? Howlargeisthespaceofbeliefstates?
b. Howmanydistinct perceptsarepossible intheinitialstate?
c. Describe the first few branches of a contingency plan for this problem. How large
(roughly) isthecompleteplan?
Noticethatthiscontingencyplanisasolutionforeverypossibleenvironmentfittingthegiven
description. Therefore, interleaving of search and execution is not strictly necessary even in
unknownenvironments.
4.13 Inthisexercise,weexaminehillclimbinginthecontextofrobotnavigation, usingthe
environment inFigure3.31asanexample.
a. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local
minimum? Isitpossible forittogetstuckwithconvexobstacles?
b. Constructanonconvex polygonal environment inwhichtheagentgetsstuck.
c. Modifythehill-climbing algorithm sothat, instead ofdoing adepth-1 search todecide
where to go next, it does a depth-k search. It should find the best k-step path and do
onestepalongit,andthenrepeattheprocess.
d. Istheresomekforwhichthenewalgorithmisguaranteedtoescapefromlocalminima?
∗
e. ExplainhowLRTA enablestheagenttoescapefromlocalminimainthiscase.
4.14 LikeDFS,onlineDFSisincompleteforreversiblestatespaceswithinfinitepaths. For
example, suppose that states are points on the infinite two-dimensional grid and actions are
unitvectors (1,0), (0,1), (−1,0), (0,−1), triedinthatorder. ShowthatonlineDFSstarting
at (0,0) will not reach (1,−1). Suppose the agent can observe, in addition to its current
state, all successor states and the actions that would lead to them. Write an algorithm that
is complete even for bidirected state spaces with infinite paths. What states does it visit in
reaching (1,−1)?
5
ADVERSARIAL SEARCH
Inwhichweexaminetheproblemsthatarisewhenwetrytoplanaheadinaworld
whereotheragentsareplanning againstus.
5.1 GAMES
Chapter 2 introduced multiagent environments, in which each agent needs to consider the
actions of other agents and how they affect its own welfare. The unpredictability of these
other agents can introduce contingencies into the agent’s problem-solving process, as dis-
cussedinChapter4. Inthischapterwecovercompetitiveenvironments,inwhichtheagents’
goalsareinconflict,givingrisetoadversarial search problems—often knownasgames.
GAME
Mathematicalgametheory,abranchofeconomics,viewsanymultiagentenvironment
as a game, provided that the impact of each agent on the others is “significant,” regardless
of whether the agents are cooperative or competitive.1 In AI, the most common games are
ofaratherspecialized kind—whatgametheorists calldeterministic, turn-taking, two-player,
zero-sum games of perfect information (such as chess). In our terminology, this means
ZERO-SUMGAMES
PERFECT deterministic,fullyobservableenvironmentsinwhichtwoagentsactalternatelyandinwhich
INFORMATION
the utility values at the end of the game are always equal and opposite. Forexample, if one
playerwinsagameofchess, theotherplayernecessarily loses. Itisthisopposition between
theagents’utilityfunctions thatmakesthesituation adversarial.
Games have engaged the intellectual faculties of humans—sometimes to an alarming
degree—for as long as civilization has existed. For AI researchers, the abstract nature of
games makes them an appealing subject for study. The state of a game is easy to represent,
andagentsareusuallyrestrictedtoasmallnumberofactionswhoseoutcomesaredefinedby
precise rules. Physicalgames,such ascroquet andicehockey, havemuchmorecomplicated
descriptions, a much larger range of possible actions, and rather imprecise rules defining
the legality of actions. With the exception of robot soccer, these physical games have not
attracted muchinterest intheAIcommunity.
1 Environmentswithverymanyagentsareoftenviewedaseconomiesratherthangames.
161
162 Chapter 5. Adversarial Search
Games, unlike most of the toy problems studied in Chapter 3, are interesting because
they are too hard to solve. For example, chess has an average branching factor of about 35,
and games often go to 50 moves by each player, so the search tree has about 35100 or 10154
nodes(although thesearchgraphhas“only” about 1040 distinct nodes). Games,likethereal
world,thereforerequiretheabilitytomake somedecision evenwhencalculating theoptimal
decisionisinfeasible. Gamesalsopenalizeinefficiencyseverely. Whereasanimplementation
∗
ofA searchthatishalfasefficientwillsimplytaketwiceaslongtoruntocompletion,achess
program that is half as efficient in using its available time probably will be beaten into the
ground, otherthingsbeingequal. Game-playingresearchhasthereforespawnedanumberof
interesting ideasonhowtomakethebestpossible useoftime.
We begin with a definition of the optimal move and an algorithm for finding it. We
then look at techniques for choosing a good move when time is limited. Pruningallows us
PRUNING
toignore portions ofthesearch treethatmakenodifference tothefinalchoice, andheuristic
evaluationfunctionsallowustoapproximate thetrueutilityofastatewithoutdoingacom-
plete search. Section 5.5 discusses games such as backgammon that include an element of
IMPERFECT chance; wealso discuss bridge, which includes elements of imperfect information because
INFORMATION
notallcardsarevisible toeachplayer. Finally,welookathowstate-of-the-art game-playing
programsfareagainsthumanopposition andatdirections forfuturedevelopments.
Wefirstconsidergameswithtwoplayers,whomwecall MAXandMINforreasonsthat
willsoonbecomeobvious. MAX movesfirst,andthentheytaketurnsmovinguntilthegame
is over. At the end of the game, points are awarded to the winning player and penalties are
given to the loser. A game can be formally defined as a kind of search problem with the
followingelements:
• S : Theinitialstate, whichspecifieshowthegameissetupatthestart.
0
• PLAYER(s): Defineswhichplayerhasthemoveinastate.
• ACTIONS(s): Returnsthesetoflegalmovesinastate.
• RESULT(s,a): Thetransitionmodel,whichdefinestheresultofamove.
TERMINALTEST
• TERMINAL-TEST(s): A terminal test, which is true when the game is over and false
otherwise. Stateswherethegamehasendedarecalled terminalstates.
TERMINALSTATES
• UTILITY(s,p): Autilityfunction(alsocalledanobjectivefunctionorpayofffunction),
definesthefinalnumericvalueforagamethatendsinterminal statesforaplayerp. In
chess,theoutcomeisawin,loss,ordraw,withvalues +1,0,or 1. Somegameshavea
2
widervarietyofpossibleoutcomes;thepayoffsinbackgammonrangefrom0to+192.
Azero-sum game is (confusingly) defined as one where the total payoff toall players
isthe sameforevery instance ofthegame. Chessiszero-sum because everygamehas
payoffofeither0+1,1+0or 1 + 1. “Constant-sum” wouldhavebeenabetterterm,
2 2
but zero-sum is traditional and makes sense if you imagine each player is charged an
entryfeeof 1.
2
GAMETREE
The initial state, ACTIONS function, and RESULT function define the game tree for the
game—a tree where the nodes are game states and the edges are moves. Figure 5.1 shows
part of the game tree for tic-tac-toe (noughts and crosses). From the initial state, MAX has
nine possible moves. Play alternates between MAX’s placing an X and MIN’s placing an O
Section5.2. OptimalDecisions inGames 163
until we reach leaf nodes corresponding to terminal states such that one player has three in
a row or all the squares are filled. The number on each leaf node indicates the utility value
ofthe terminal state from the point ofview of MAX; high values are assumed tobe good for
MAX andbadfor MIN(whichishowtheplayersgettheirnames).
For tic-tac-toe the game tree is relatively small—fewer than 9! = 362,880 terminal
nodes. But for chess there are over 1040 nodes, so the game tree is best thought of as a
theoretical construct that we cannot realize in the physical world. But regardless of the size
SEARCHTREE
ofthegametree,itisMAX’sjobtosearchforagoodmove. Weusethetermsearchtreefora
treethatissuperimposed onthefullgametree,andexamines enoughnodestoallowaplayer
todetermine whatmovetomake.
MAX (X)
X X X
MIN (O)
X X X
X X X
XO X O X . . .
MAX (X) O
X O X XO X O . . .
MIN (O) X X
. . . . . . . . . . . .
X O X X O X X O X . . .
TERMINAL O X OO X X
O X X O X O O
Utility –1 0 +1
Figure 5.1 A (partial) game tree forthe game of tic-tac-toe. The top node is the initial
state,andMAXmovesfirst,placinganXinanemptysquare.Weshowpartofthetree,giving
alternatingmovesbyMIN(O)andMAX(X),untilweeventuallyreachterminalstates,which
canbeassignedutilitiesaccordingtotherulesofthegame.
5.2 OPTIMAL DECISIONS IN GAMES
In a normal search problem, the optimal solution would be a sequence of actions leading to
a goal state—a terminal state that is a win. In adversarial search, MIN has something to say
STRATEGY about it. MAX therefore must find a contingent strategy, which specifies MAX’s move in
the initial state, then MAX’s moves in the states resulting from every possible response by
164 Chapter 5. Adversarial Search
MAX 3 A
a a
1 3
a
2
MIN 3 B 2 C 2 D
b b c c d d
1 3 1 3 1 3
b c d
2 2 2
3 12 8 2 4 6 14 5 2
Figure 5.2 A two-ply game tree. The (cid:14) nodes are “MAX nodes,” in which it is MAX’s
(cid:15)
turntomove,andthe nodesare“MINnodes.” Theterminalnodesshowtheutilityvalues
forMAX;theothernodesarelabeledwiththeirminimaxvalues. MAX’sbestmoveattheroot
isa
1
,becauseitleadstothestatewiththehighestminimaxvalue,andMIN’sbestreplyisb
1
,
becauseitleadstothestatewiththelowestminimaxvalue.
MIN,then MAX’smovesinthestates resulting fromeverypossible response by MIN tothose
moves, and so on. This is exactly analogous to the AND–OR search algorithm (Figure 4.11)
with MAX playing theroleof OR and MIN equivalent to AND. Roughly speaking, anoptimal
strategy leads to outcomes at least as good as any other strategy when one is playing an
infallible opponent. Webeginbyshowinghowtofindthisoptimalstrategy.
Even a simple game like tic-tac-toe is too complex for us to draw the entire game tree
ononepage,sowewillswitchtothetrivialgameinFigure5.2. ThepossiblemovesforMAX
at the root node are labeled a 1 , a 2 , and a 3 . The possible replies to a 1 for MIN are b 1 , b 2 ,
b 3 , and so on. This particular game ends after one move each by MAX and MIN. (In game
parlance,wesaythatthistreeisonemovedeep,consistingoftwohalf-moves,eachofwhich
iscalledaply.) Theutilitiesoftheterminalstatesinthisgamerangefrom2to14.
PLY
Given a game tree, the optimal strategy can be determined from the minimax value
MINIMAXVALUE
of each node, which we write as MINIMAX(n). The minimax value of a node is the utility
(for MAX) of being in the corresponding state, assuming that both players play optimally
from there to the end of the game. Obviously, the minimax value of a terminal state is just
its utility. Furthermore, given a choice, MAX prefers to move to a state of maximum value,
whereas MIN prefersastateofminimumvalue. Sowehavethefollowing:
MINIMAX(s) =
⎧
⎨ UTILITY(s) if TERMINAL-TEST(s)
⎩ max a∈Actions(s) MINIMAX(RESULT(s,a)) if PLAYER(s) =MAX
min a∈Actions(s) MINIMAX(RESULT(s,a)) if PLAYER(s) =MIN
LetusapplythesedefinitionstothegametreeinFigure5.2. Theterminalnodesonthebottom
level get their utility values from the game’s UTILITY function. The first MIN node, labeled
B, has three successor states with values 3, 12, and 8, so its minimax value is 3. Similarly,
the othertwo MIN nodes have minimaxvalue 2. Theroot node isa MAX node; its successor
states have minimax values 3, 2, and2; soithas aminimax value of3. Wecanalso identify
Section5.2. OptimalDecisions inGames 165
MINIMAXDECISION
theminimaxdecisionattheroot: actiona
1
istheoptimalchoiceforMAXbecauseitleadsto
thestatewiththehighest minimaxvalue.
This definition of optimal play for MAX assumes that MIN also plays optimally—it
maximizestheworst-caseoutcomeforMAX. WhatifMINdoesnotplayoptimally? Thenitis
easytoshow(Exercise5.7)thatMAXwilldoevenbetter. Otherstrategiesagainstsuboptimal
opponents maydobetterthantheminimaxstrategy,butthese strategiesnecessarily doworse
againstoptimalopponents.
5.2.1 The minimaxalgorithm
Theminimaxalgorithm(Figure5.3)computestheminimaxdecisionfromthecurrent state.
MINIMAXALGORITHM
Itusesasimplerecursivecomputationoftheminimaxvaluesofeachsuccessorstate,directly
implementing thedefining equations. Therecursion proceeds allthewaydowntotheleaves
of the tree, and then the minimax values are backed up through the tree as the recursion
unwinds. For example, in Figure 5.2, the algorithm first recurses down to the three bottom-
leftnodes andusesthe UTILITY function onthemtodiscoverthattheirvaluesare3,12,and
8, respectively. Then it takes the minimum of these values, 3, and returns it as the backed-
up value of node B. A similar process gives the backed-up values of 2 for C and 2 for D.
Finally,wetakethemaximumof3,2,and2togetthebacked-upvalueof3fortherootnode.
The minimax algorithm performs a complete depth-first exploration of the game tree.
If the maximum depth of the tree is m and there are b legal moves at each point, then the
timecomplexityoftheminimaxalgorithm isO(bm). ThespacecomplexityisO(bm)foran
algorithm that generates allactions atonce, or O(m)foran algorithm that generates actions
one at a time (see page 87). For real games, of course, the time cost is totally impractical,
but this algorithm serves as the basis for the mathematical analysis of games and for more
practical algorithms.
5.2.2 Optimaldecisions inmultiplayergames
Manypopulargamesallowmorethantwoplayers. Letusexaminehowtoextendtheminimax
idea to multiplayer games. This is straightforward from the technical viewpoint, but raises
someinteresting newconceptual issues.
First, we need to replace the single value for each node with a vector of values. For
example,inathree-playergamewithplayersA,B,andC,avector(cid:16)v ,v ,v (cid:17)isassociated
A B C
witheachnode. Forterminalstates,thisvectorgivestheutilityofthestatefromeachplayer’s
viewpoint. (Intwo-player,zero-sumgames,thetwo-elementvectorcanbereducedtoasingle
valuebecausethevaluesarealwaysopposite.) Thesimplestwaytoimplementthisistohave
the UTILITY function returnavectorofutilities.
Nowwehavetoconsidernonterminalstates. ConsiderthenodemarkedX inthegame
tree shown in Figure 5.4. In that state, player C chooses what to do. The two choices lead
toterminal states with utility vectors (cid:16)v =1,v =2,v =6(cid:17)and (cid:16)v =4,v =2,v =3(cid:17).
A B C A B C
Since6isbiggerthan3,Cshouldchoosethefirstmove. ThismeansthatifstateX isreached,
subsequent play will lead to a terminal state with utilities (cid:16)v =1,v =2,v =6(cid:17). Hence,
A B C
thebacked-upvalueofX isthisvector. Thebacked-upvalueofanodenisalwaystheutility
166 Chapter 5. Adversarial Search
functionMINIMAX-DECISION(state)returnsan action
returnargmax
a ∈ ACTIONS(s)
MIN-VALUE(RESULT(state,a))
functionMAX-VALUE(state)returnsa utility value
ifTERMINAL-TEST(state)thenreturnUTILITY(state)
v←−∞
foreacha inACTIONS(state)do
v←MAX(v,MIN-VALUE(RESULT(s,a)))
returnv
functionMIN-VALUE(state)returnsa utility value
ifTERMINAL-TEST(state)thenreturnUTILITY(state)
v←∞
foreacha inACTIONS(state)do
v←MIN(v,MAX-VALUE(RESULT(s,a)))
returnv
Figure 5.3 An algorithm forcalculating minimax decisions. It returns the action corre-
sponding to the best possible move, that is, the move that leads to the outcome with the
bestutility,undertheassumptionthattheopponentplaystominimizeutility. Thefunctions
MAX-VALUE and MIN-VALUE go throughthe whole game tree, all the way to the leaves,
to determine the backed-up value of a state. The notation argmax f(a) computes the
a∈S
elementaofsetS thathasthemaximumvalueoff(a).
to move
A (1, 2, 6)
B (1, 2, 6) (1, 5, 2)
C (1, 2, 6) X (6, 1, 2) (1, 5, 2) (5, 4, 5)
A
(1, 2, 6) (4, 2, 3) (6, 1, 2) (7, 4,1) (5,1,1) (1, 5, 2) (7, 7,1) (5, 4, 5)
Figure5.4 Thefirstthreepliesofagametreewiththreeplayers(A,B,C). Eachnodeis
labeledwithvaluesfromtheviewpointofeachplayer. Thebestmoveismarkedattheroot.
vector of the successor state with the highest value for the player choosing at n. Anyone
who plays multiplayer games, such as Diplomacy, quickly becomes aware that much more
isgoing onthan intwo-player games. Multiplayer gamesusually involve alliances, whether
ALLIANCE
formalorinformal,amongtheplayers. Alliancesaremadeandbrokenasthegameproceeds.
How are we to understand such behavior? Are alliances a natural consequence of optimal
strategies foreach player in a multiplayer game? It turns out that they can be. Forexample,
Section5.3. Alpha–BetaPruning 167
suppose A and B are in weak positions and C is in a stronger position. Then it is often
optimal for both A and B to attack C rather than each other, lest C destroy each of them
individually. In this way, collaboration emerges from purely selfish behavior. Of course,
as soon as C weakens under the joint onslaught, the alliance loses its value, and either A
or B could violate the agreement. In some cases, explicit alliances merely make concrete
what would have happened anyway. In other cases, a social stigma attaches to breaking an
alliance,soplayersmustbalancetheimmediateadvantageofbreakinganallianceagainstthe
long-term disadvantage of being perceived as untrustworthy. See Section 17.5 for more on
thesecomplications.
If the game is not zero-sum, then collaboration can also occur with just two players.
Suppose,forexample,thatthereisaterminalstatewithutilities(cid:16)v =1000,v =1000(cid:17)and
A B
that1000 isthehighest possible utility foreachplayer. Thentheoptimalstrategy isforboth
players to do everything possible to reach this state—that is, the players will automatically
cooperate toachieveamutuallydesirable goal.
5.3 ALPHA–BETA PRUNING
The problem with minimax search is that the number of game states it has to examine is
exponential in the depth of the tree. Unfortunately, we can’t eliminate the exponent, but it
turnsoutwecaneffectivelycutitinhalf. Thetrickisthatitispossibletocomputethecorrect
minimaxdecisionwithoutlookingateverynodeinthegametree. Thatis,wecanborrowthe
idea of pruningfrom Chapter 3 to eliminate large parts of the tree from consideration. The
ALPHA–BETA particular technique weexamine iscalled alpha–beta pruning. Whenapplied toastandard
PRUNING
minimax tree, it returns the same move as minimax would, but prunes away branches that
cannotpossibly influencethefinaldecision.
Consideragainthetwo-plygametreefromFigure5.2. Let’sgothroughthecalculation
of the optimal decision once more, this time paying careful attention to what we know at
eachpoint intheprocess. Thestepsareexplained inFigure5.5. Theoutcome isthatwecan
identify theminimaxdecision withouteverevaluating twooftheleafnodes.
Anotherwaytolookatthisisasasimplification oftheformula forMINIMAX. Letthe
two unevaluated successors of node C in Figure 5.5 have values x and y. Then the value of
therootnodeisgivenby
MINIMAX(root) = max(min(3,12,8),min(2,x,y),min(14,5,2))
= max(3,min(2,x,y),2)
= max(3,z,2) wherez = min(2,x,y) ≤ 2
= 3.
In otherwords, thevalue ofthe root and hence the minimax decision are independent of the
valuesoftheprunedleaves xandy.
Alpha–beta pruning can be applied to trees of any depth, and it is often possible to
prune entire subtrees ratherthanjust leaves. Thegeneral principle isthis: consider anode n
168 Chapter 5. Adversarial Search
(a) [−∞, +∞] A (b) [−∞, +∞] A
[−∞, 3] B [−∞, 3] B
3 3 12
(c)
[3, +∞]
A (d)
[3, +∞]
A
[3, 3] B [3, 3] B [−∞, 2] C
3 12 8 3 12 8 2
(e) [3, 14] A (f) [3, 3] A
[3, 3] B [−∞, 2] C [−∞, 14] D [3, 3] B [−∞, 2] C [2, 2] D
3 12 8 2 14 3 12 8 2 14 5 2
Figure5.5 StagesinthecalculationoftheoptimaldecisionforthegametreeinFigure5.2.
Ateachpoint,weshowtherangeofpossiblevaluesforeachnode.(a)ThefirstleafbelowB
hasthevalue3. Hence,B,whichisaMINnode,hasavalueofatmost3.(b)Thesecondleaf
belowB hasavalueof12; MIN wouldavoidthismove,sothevalueofB isstillatmost3.
(c) The third leaf below B has a value of 8; we have seen all B’s successor states, so the
value of B is exactly 3. Now, we can inferthat the value of the rootis at least 3, because
MAX has a choice worth 3 at the root. (d) The first leaf below C has the value 2. Hence,
C, whichisa MIN node,hasavalueofatmost2. ButweknowthatB isworth3,so MAX
wouldneverchooseC. Therefore,thereisnopointinlookingattheothersuccessorstates
ofC. Thisisanexampleofalpha–betapruning. (e)ThefirstleafbelowDhasthevalue14,
soDisworthatmost14. ThisisstillhigherthanMAX’sbestalternative(i.e.,3),soweneed
to keep exploringD’s successorstates. Notice also that we nowhave boundson all of the
successorsoftheroot,sotheroot’svalueisalsoatmost14. (f)ThesecondsuccessorofD
isworth5,soagainweneedtokeepexploring. Thethirdsuccessorisworth2,sonowDis
worthexactly2. MAX’sdecisionattherootistomovetoB,givingavalueof3.
somewhereinthetree(seeFigure5.6),suchthatPlayerhasa choice ofmovingtothatnode.
IfPlayerhasabetterchoicemeitherattheparentnodeofnoratanychoicepointfurtherup,
thennwillneverbereached inactualplay. Sooncewehavefoundoutenough aboutn(by
examiningsomeofitsdescendants) toreachthisconclusion, wecanpruneit.
Remember that minimax search is depth-first, so at any one time we just have to con-
sider the nodes along a single path in the tree. Alpha–beta pruning gets its name from the
followingtwoparametersthatdescribeboundsonthebacked-upvaluesthatappearanywhere
alongthepath:
Section5.3. Alpha–BetaPruning 169
Player
Opponent m
•
•
•
Player
Opponent n
Figure5.6 Thegeneralcaseforalpha–betapruning. If misbetterthannforPlayer,we
willnevergettoninplay.
α= thevalueofthebest(i.e.,highest-value)choicewehavefoundsofaratanychoicepoint
alongthepathfor MAX.
β = thevalueofthebest(i.e.,lowest-value)choicewehavefoundsofaratanychoicepoint
alongthepathfor MIN.
Alpha–beta search updates the values of α and β as it goes along and prunes the remaining
branches at a node (i.e., terminates the recursive call) as soon as the value of the current
node is known to be worse than the current α or β value for MAX or MIN, respectively. The
complete algorithm is given in Figure 5.7. We encourage you to trace its behavior when
appliedtothetreeinFigure5.5.
5.3.1 Moveordering
Theeffectiveness ofalpha–beta pruning ishighly dependent ontheorderinwhich thestates
areexamined. Forexample,inFigure5.5(e)and(f),wecould notpruneanysuccessors ofD
at all because the worst successors (from the point of view of MIN) were generated first. If
thethirdsuccessorofDhadbeengeneratedfirst,wewouldhavebeenabletoprunetheother
two. Thissuggests that it might beworthwhile to try toexamine firstthe successors that are
likelytobebest.
If this can be done,2 then it turns out that alpha–beta needs to examine only O(bm/2)
nodes to pick the best mov√e, instead of O(bm) for minimax. This means that the effective
branching factor becomes b instead of b—for chess, about 6 instead of 35. Put another
way, alpha–beta can solve a tree roughly twice as deep as minimax in the same amount of
time. If successors are examined in random order rather than best-first, the total number of
nodesexaminedwillberoughlyO(b3m/4)formoderateb. Forchess,afairlysimpleordering
function (such as trying captures first, then threats, then forward moves, and then backward
moves)getsyoutowithinaboutafactorof2ofthebest-case O(bm/2)result.
2 Obviously,itcannotbedoneperfectly;otherwise,theorderingfunctioncouldbeusedtoplayaperfectgame!
170 Chapter 5. Adversarial Search
functionALPHA-BETA-SEARCH(state)returnsanaction
v←MAX-VALUE(state,−∞,+∞)
returntheaction inACTIONS(state)withvaluev
functionMAX-VALUE(state,α,β)returnsa utility value
ifTERMINAL-TEST(state)thenreturnUTILITY(state)
v←−∞
foreacha inACTIONS(state)do
v←MAX(v,MIN-VALUE(RESULT(s,a),α,β))
ifv ≥ β thenreturnv
α←MAX(α,v)
returnv
functionMIN-VALUE(state,α,β)returnsa utility value
ifTERMINAL-TEST(state)thenreturnUTILITY(state)
v←+∞
foreacha inACTIONS(state)do
v←MIN(v,MAX-VALUE(RESULT(s,a),α,β))
ifv ≤ αthenreturnv
β←MIN(β,v)
returnv
Figure 5.7 The alpha–betasearch algorithm. Notice that these routinesare the same as
the MINIMAX functionsinFigure5.3,exceptforthetwolinesineachof MIN-VALUE and
MAX-VALUEthatmaintainαandβ(andthebookkeepingtopasstheseparametersalong).
Addingdynamicmove-orderingschemes,suchastryingfirstthemovesthatwerefound
to be best in the past, brings us quite close to the theoretical limit. The past could be the
previous move—often thesame threats remain—or itcould come from previous exploration
of the current move. One way to gain information from the current move is with iterative
deepening search. First, search 1 ply deep and record the best path of moves. Then search
1 ply deeper, but use the recorded path to inform move ordering. As we saw in Chapter 3,
iterative deepening on an exponential game tree adds only a constant fraction to the total
searchtime,whichcanbemorethanmadeupfrombettermoveordering. Thebestmovesare
oftencalledkillermovesandtotrythemfirstiscalledthekillermoveheuristic.
KILLERMOVES
In Chapter 3, we noted that repeated states in the search tree can cause an exponential
increaseinsearchcost. Inmanygames,repeatedstatesoccurfrequentlybecauseoftranspo-
sitions—different permutations of the move sequence that end up in the same position. For
TRANSPOSITION
example, if White has one move, a , that can be answered by Black with b and an unre-
1 1
lated movea on the other side of the board that can be answered by b , then the sequences
2 2
[a ,b ,a ,b ] and [a ,b ,a ,b ] both end up in the same position. It is worthwhile to store
1 1 2 2 2 2 1 1
the evaluation of the resulting position in a hash table the first time it is encountered so that
wedon’t havetorecompute itonsubsequent occurrences. The hashtableofpreviously seen
TRANSPOSITION positionsistraditionallycalledatranspositiontable;itisessentiallyidenticaltotheexplored
TABLE
Section5.4. ImperfectReal-TimeDecisions 171
listinGRAPH-SEARCH (Section3.3). Usingatransposition tablecanhaveadramaticeffect,
sometimesasmuchasdoublingthereachablesearchdepthinchess. Ontheotherhand,ifwe
areevaluatingamillionnodespersecond,atsomepointitisnotpracticaltokeepallofthem
in the transposition table. Various strategies have been used to choose which nodes to keep
andwhichtodiscard.
5.4 IMPERFECT REAL-TIME DECISIONS
Theminimaxalgorithmgeneratestheentiregamesearchspace,whereasthealpha–betaalgo-
rithm allowsustoprune large partsofit. However, alpha–beta stillhastosearch alltheway
toterminalstatesforatleastaportionofthesearchspace. Thisdepthisusuallynotpractical,
because moves must be made in a reasonable amount of time—typically a few minutes at
most. ClaudeShannon’spaperProgrammingaComputerforPlayingChess(1950)proposed
insteadthatprogramsshouldcutoffthesearchearlierandapplyaheuristic evaluationfunc-
EVALUATION tion to states in the search, effectively turning nonterminal nodes into terminal leaves. In
FUNCTION
otherwords,thesuggestion istoalterminimaxoralpha–beta intwoways: replacetheutility
function by a heuristic evaluation function EVAL, which estimates the position’s utility, and
CUTOFFTEST
replace theterminaltestbya cutofftestthatdecides whentoapply EVAL. Thatgivesusthe
followingforheuristic minimaxforstate sandmaximumdepthd:
H-MINIMAX(s,d) =
⎧
⎨ EVAL(s) ifCUTOFF-TEST(s,d)
⎩
max
a∈Actions(s)
H-MINIMAX(RESULT(s,a),d+1) ifPLAYER(s) =MAX
min
a∈Actions(s)
H-MINIMAX(RESULT(s,a),d+1) ifPLAYER(s) =MIN.
5.4.1 Evaluationfunctions
An evaluation function returns an estimate of the expected utility of the game from a given
position, just as the heuristic functions of Chapter 3 return an estimate of the distance to
the goal. The idea of an estimator was not new when Shannon proposed it. For centuries,
chess players (and aficionados of othergames) have developed ways ofjudging the value of
a position because humans are even more limited in the amount of search they can do than
are computer programs. It should be clear that the performance of a game-playing program
depends strongly onthequality ofitsevaluation function. Aninaccurate evaluation function
willguideanagenttowardpositions thatturnouttobelost. Howexactlydowedesigngood
evaluation functions?
First, the evaluation function should order the terminal states in the same way as the
trueutility function: states thatarewinsmustevaluate betterthan draws,whichinturnmust
be better than losses. Otherwise, an agent using the evaluation function might err even if it
can see ahead all the way to the end of the game. Second, the computation must not take
too long! (The whole point is to search faster.) Third, for nonterminal states, the evaluation
function shouldbestrongly correlated withtheactualchances ofwinning.
172 Chapter 5. Adversarial Search
Onemightwellwonderaboutthephrase“chancesofwinning.” Afterall,chessisnota
gameofchance: weknowthecurrentstatewithcertainty,andnodiceareinvolved. Butifthe
searchmustbecutoffatnonterminal states, thenthealgorithm willnecessarily beuncertain
aboutthefinaloutcomesofthosestates. Thistypeofuncertaintyisinducedbycomputational,
rather than informational, limitations. Given the limited amount of computation that the
evaluationfunctionisallowedtodoforagivenstate,thebestitcandoismakeaguessabout
thefinaloutcome.
Let us make this idea more concrete. Most evaluation functions work by calculating
various features ofthestate—for example, inchess, wewouldhave features forthenumber
of white pawns, black pawns, white queens, black queens, and so on. The features, taken
together,definevariouscategoriesorequivalenceclassesofstates: thestatesineachcategory
have the same values for all the features. For example, one category contains all two-pawn
vs. one-pawn endgames. Any given category, generally speaking, will contain some states
that lead to wins, some that lead to draws, and some that lead to losses. The evaluation
function cannot knowwhichstatesarewhich,butitcanreturnasinglevaluethatreflectsthe
proportion of states with each outcome. Forexample, suppose our experience suggests that
72% ofthe states encountered inthetwo-pawns vs. one-pawn category lead toawin(utility
+1); 20% to a loss (0), and 8% to a draw (1/2). Then a reasonable evaluation for states in
the category is the expected value: (0.72 ×+1)+(0.20 ×0)+(0.08 ×1/2) = 0.76. In
EXPECTEDVALUE
principle, the expected value canbe determined foreach category, resulting inan evaluation
function that works for any state. As with terminal states, the evaluation function need not
returnactualexpectedvaluesaslongasthe orderingofthestatesisthesame.
In practice, this kind of analysis requires too many categories and hence too much
experience to estimate all the probabilities of winning. Instead, most evaluation functions
compute separate numerical contributions from each feature and then combine them to find
the total value. Forexample, introductory chess books give an approximate material value
MATERIALVALUE
foreachpiece: eachpawnisworth1,aknightorbishopisworth3,arook5,andthequeen9.
Otherfeatures suchas“good pawnstructure” and“kingsafety” mightbeworthhalfapawn,
say. Thesefeaturevaluesarethensimplyaddeduptoobtaintheevaluation oftheposition.
Asecure advantage equivalent toapawngivesasubstantial likelihood ofwinning, and
asecureadvantageequivalenttothreepawnsshouldgivealmostcertainvictory,asillustrated
inFigure5.8(a). Mathematically, thiskindofevaluation functioniscalledaweightedlinear
WEIGHTEDLINEAR functionbecauseitcanbeexpressed as
FUNCTION
(cid:12)n
EVAL(s) = w
1
f
1
(s)+w
2
f
2
(s)+···+w
n
f
n
(s) = w
i
f
i
(s),
i=1
whereeach w isaweight andeach f isafeature oftheposition. Forchess, the f could be
i i i
thenumbers ofeach kind ofpiece ontheboard, andthe w could bethevalues ofthe pieces
i
(1forpawn,3forbishop, etc.).
Adding up the values of features seems like a reasonable thing to do, but in fact it
involves a strong assumption: that the contribution of each feature is independent of the
values of the other features. Forexample, assigning the value 3 to a bishop ignores the fact
that bishops are more powerful in the endgame, when they have a lot of space to maneuver.
Section5.4. ImperfectReal-TimeDecisions 173
(a) White to move (b) White to move
Figure5.8 Twochesspositionsthatdifferonlyinthepositionoftherookatlowerright.
In(a), Black has an advantageofa knightand two pawns, whichshouldbe enoughto win
thegame. In(b),Whitewillcapturethequeen,givingitanadvantagethatshouldbestrong
enoughtowin.
Forthisreason,currentprogramsforchessandothergamesalsousenonlinearcombinations
offeatures. Forexample,apairofbishopsmightbeworthslightlymorethantwicethevalue
ofasinglebishop,andabishopisworthmoreintheendgame(thatis,whenthemovenumber
featureishighorthe numberofremainingpiecesfeatureislow).
Theastutereaderwillhavenoticedthatthefeaturesandweightsarenotpartoftherules
ofchess! Theycomefromcenturiesofhumanchess-playingexperience. Ingameswherethis
kind of experience is not available, the weights of the evaluation function can be estimated
by the machine learning techniques of Chapter 18. Reassuringly, applying these techniques
tochesshasconfirmedthatabishopisindeedworthaboutthreepawns.
5.4.2 Cutting offsearch
The next step is to modify ALPHA-BETA-SEARCH so that it will call the heuristic EVAL
function when it is appropriate to cut off the search. We replace the two lines in Figure 5.7
thatmention TERMINAL-TEST withthefollowingline:
ifCUTOFF-TEST(state,depth)thenreturn EVAL(state)
Wealsomustarrangeforsomebookkeepingsothatthecurrent depth isincrementedoneach
recursivecall. Themoststraightforwardapproachtocontrollingtheamountofsearchistoset
afixeddepthlimitsothatCUTOFF-TEST(state,depth)returnstrue foralldepth greaterthan
somefixeddepthd. (Itmustalsoreturn true forallterminalstates,justasTERMINAL-TEST
did.) The depth d is chosen so that a move is selected within the allocated time. A more
robust approach is to apply iterative deepening. (See Chapter 3.) When time runs out, the
program returns the move selected by the deepest completed search. As a bonus, iterative
deepening alsohelpswithmoveordering.
174 Chapter 5. Adversarial Search
These simple approaches can lead to errors due to the approximate nature of the eval-
uation function. Consider again the simple evaluation function for chess based on material
advantage. Suppose the program searches to the depth limit, reaching the position in Fig-
ure 5.8(b), where Black is ahead by a knight and two pawns. It would report this as the
heuristic value of the state, thereby declaring that the state is a probable win by Black. But
White’s next move captures Black’s queen with no compensation. Hence, the position is
reallywonforWhite,butthiscanbeseenonlybylooking aheadonemoreply.
Obviously,amoresophisticatedcutofftestisneeded. Theevaluationfunctionshouldbe
appliedonlytopositions thatarequiescent—thatis,unlikely toexhibitwildswingsinvalue
QUIESCENCE
in the nearfuture. Inchess, forexample, positions inwhich favorable captures can be made
arenotquiescentforanevaluation functionthatjustcountsmaterial. Nonquiescent positions
can be expanded further until quiescent positions are reached. This extra search is called a
QUIESCENCE quiescence search; sometimes it is restricted to consider only certain types of moves, such
SEARCH
ascapture moves,thatwillquickly resolvetheuncertainties intheposition.
The horizon effect is more difficult to eliminate. It arises when the program is facing
HORIZONEFFECT
an opponent’s move that causes serious damage and is ultimately unavoidable, but can be
temporarily avoided by delaying tactics. Consider the chess game in Figure 5.9. It is clear
that there isno wayforthe black bishop toescape. Forexample, the white rook cancapture
itbymovingtoh1,thena1,thena2;acaptureatdepth6ply. ButBlackdoeshaveasequence
of moves that pushes the capture of the bishop “over the horizon.” Suppose Black searches
todepth8ply. MostmovesbyBlackwillleadtotheeventualcaptureofthebishop, andthus
will be marked as “bad” moves. But Black will consider checking the white king with the
pawnate4. Thiswillleadtothekingcapturingthepawn. NowBlackwillconsiderchecking
again, with the pawn at f5, leading to another pawn capture. That takes up 4 ply, and from
there the remaining 4 ply is not enough to capture the bishop. Black thinks that the line of
playhassavedthebishop atthepriceoftwopawns,whenactually allithasdoneispushthe
inevitable captureofthebishopbeyondthehorizonthatBlackcansee.
SINGULAR One strategy to mitigate the horizon effect is the singular extension, a move that is
EXTENSION
“clearly better” than all other moves in a given position. Once discovered anywhere in the
treeinthecourseofasearch,thissingularmoveisremembered. Whenthesearchreachesthe
normaldepth limit,thealgorithm checks toseeifthesingularextension isalegalmove;ifit
is, the algorithm allows the moveto be considered. This makes the tree deeper, but because
therewillbefewsingularextensions, itdoesnotaddmanytotalnodestothetree.
5.4.3 Forwardpruning
So far, we have talked about cutting off search at a certain level and about doing alpha–
beta pruning that provably has no effect on the result (at least with respect to the heuristic
evaluation values). It is also possible to do forward pruning, meaning that some moves at
FORWARDPRUNING
a given node are pruned immediately without further consideration. Clearly, most humans
playing chess consider only a few moves from each position (at least consciously). One
approach to forward pruning is beam search: on each ply, consider only a “beam” of the n
BEAMSEARCH
bestmoves(according totheevaluation function)ratherthanconsidering allpossible moves.
Section5.4. ImperfectReal-TimeDecisions 175
1
2
3
4
5
6
7
8
a b c d e f g h
Figure5.9 Thehorizoneffect. With Black tomove,the blackbishopissurelydoomed.
ButBlackcanforestallthateventbycheckingthewhitekingwithitspawns,forcingtheking
tocapturethepawns.Thispushestheinevitablelossofthebishopoverthehorizon,andthus
thepawnsacrificesareseenbythesearchalgorithmasgoodmovesratherthanbadones.
Unfortunately, this approach is rather dangerous because there is no guarantee that the best
movewillnotbeprunedaway.
The PROBCUT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning ver-
sionofalpha–betasearchthatusesstatisticsgainedfrompriorexperiencetolessenthechance
that the best move will be pruned. Alpha–beta search prunes any node that is provably out-
side the current (α,β) window. PROBCUT also prunes nodes that are probably outside the
window. It computes this probability by doing a shallow search to compute the backed-up
valuev ofanodeandthenusing pastexperience toestimatehowlikely itisthatascoreof v
atdepthdinthetreewouldbeoutside (α,β). BuroappliedthistechniquetohisOthellopro-
gram,LOGISTELLO,andfoundthataversionofhisprogramwithPROBCUTbeattheregular
version64%ofthetime,evenwhentheregularversionwasgiventwiceasmuchtime.
Combining all the techniques described here results in a program that can play cred-
itablechess(orothergames). Letusassumewehaveimplementedanevaluationfunctionfor
chess, a reasonable cutoff test with a quiescence search, and a large transposition table. Let
usalsoassumethat,aftermonthsoftediousbit-bashing,wecangenerateandevaluatearound
amillionnodespersecondonthelatestPC,allowingustosearchroughly 200millionnodes
per move under standard time controls (three minutes per move). The branching factor for
chess is about 35, on average, and 355 is about 50 million, so if we used minimax search,
we could look ahead only about fiveplies. Though not incompetent, such a program can be
fooledeasilybyanaverage humanchessplayer, whocanoccasionally plansixoreightplies
ahead. With alpha–beta search we get to about 10 plies, which results in an expert level of
play. Section5.8describes additional pruningtechniques thatcanextendtheeffectivesearch
depth to roughly 14 plies. To reach grandmaster status we would need an extensively tuned
evaluation function andalargedatabaseofoptimalopening andendgamemoves.
176 Chapter 5. Adversarial Search
5.4.4 Search versus lookup
Somehowitseemslikeoverkillforachessprogramtostartagamebyconsidering atreeofa
billiongamestates,onlytoconclude thatitwillmoveitspawntoe4. Booksdescribing good
playintheopeningandendgameinchesshavebeenavailableforaboutacentury(Tattersall,
1911). It is not surprising, therefore, that many game-playing programs use table lookup
ratherthansearchfortheopening andendingofgames.
Forthe openings, thecomputer ismostly relying onthe expertise ofhumans. Thebest
advice ofhumanexpertsonhowtoplayeachopening iscopied frombooks andentered into
tables forthecomputer’s use. However, computers canalso gatherstatistics from adatabase
of previously played games to see which opening sequences most often lead to a win. In
theearly movestherearefewchoices, andthusmuchexpert commentaryandpastgameson
whichtodraw. Usuallyaftertenmovesweendupinararelyseenposition, andtheprogram
mustswitchfromtablelookuptosearch.
Neartheendofthegamethereareagainfewerpossiblepositions,andthusmorechance
to do lookup. But here it is the computer that has the expertise: computer analysis of
endgames goes far beyond anything achieved by humans. A human can tell you the gen-
eral strategy forplaying aking-and-rook-versus-king (KRK)endgame: reduce the opposing
king’s mobility bysqueezing ittowardone edgeoftheboard, using yourking toprevent the
opponent fromescaping thesqueeze. Otherendings, such asking, bishop, andknight versus
king (KBNK),aredifficult tomasterandhave nosuccinct strategy description. Acomputer,
ontheotherhand,cancompletely solvetheendgamebyproducing apolicy,whichisamap-
POLICY
pingfromeverypossiblestatetothebestmoveinthatstate. Thenwecanjustlookupthebest
move rather than recompute it anew. How big will the KBNK lookup table be? It turns out
there are 462 ways that two kings can be placed on the board without being adjacent. After
the kings are placed, there are 62 empty squares for the bishop, 61 for the knight, and two
possible players to move next, so there are just 462 × 62×61 ×2 = 3,494,568 possible
positions. Someofthesearecheckmates;markthemassuchinatable. Thendoaretrograde
RETROGRADE
minimax search: reverse the rules of chess to do unmoves rather than moves. Any move by
Whitethat,nomatterwhatmoveBlackrespondswith,endsupinapositionmarkedasawin,
must also be a win. Continue this search until all 3,494,568 positions are resolved as win,
loss,ordraw,andyouhaveaninfallible lookuptableforall KBNKendgames.
Using this technique and a tour de force of optimization tricks, Ken Thompson (1986,
1996) and Lewis Stiller (1992, 1996) solved all chess endgames with up to five pieces and
some with six pieces, making them available on the Internet. Stiller discovered one case
whereaforcedmateexistedbutrequired262moves;thiscausedsomeconsternation because
the rules of chess require a capture or pawn move to occur within 50 moves. Laterwork by
Marc Bourzutschky and Yakov Konoval (Bourzutschky, 2006) solved all pawnless six-piece
andsomeseven-pieceendgames;thereisaKQNKRBNendgamethatwithbestplayrequires
517movesuntilacapture, whichthenleadstoamate.
If we could extend the chess endgame tables from 6 pieces to 32, then White would
know onthe opening movewhether itwould be awin, loss, ordraw. This has not happened
sofarforchess,butithashappenedforcheckers, asexplained inthehistoricalnotessection.
Section5.5. StochasticGames 177
5.5 STOCHASTIC GAMES
In real life, many unpredictable external events can put us into unforeseen situations. Many
games mirror this unpredictability by including a random element, such as the throwing of
dice. We call these stochastic games. Backgammon is a typical game that combines luck
STOCHASTICGAMES
andskill. Dicearerolledatthebeginning ofaplayer’s turn todetermine thelegalmoves. In
the backgammon position of Figure 5.10, for example, White has rolled a 6–5 and has four
possible moves.
0 1 2 3 4 5 6 7 8 9 10 11 12
25 24 23 22 21 20 19 18 17 16 15 14 13
Figure5.10 A typicalbackgammonposition. Thegoalofthegameistomoveallone’s
piecesofftheboard. Whitemovesclockwisetoward25,andBlackmovescounterclockwise
toward0.Apiececanmovetoanypositionunlessmultipleopponentpiecesarethere;ifthere
isoneopponent,itiscapturedandmuststartover. Inthepositionshown,White hasrolled
6–5 and must choose among four legal moves: (5–10,5–11), (5–11,19–24),(5–10,10–16),
and(5–11,11–16),wherethenotation(5–11,11–16)meansmoveonepiecefromposition5
to11,andthenmoveapiecefrom11to16.
AlthoughWhiteknowswhathisorherownlegalmovesare,Whitedoesnotknowwhat
Blackisgoing torollandthus doesnotknow whatBlack’s legalmoveswillbe. Thatmeans
White cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe. A
CHANCENODES game tree in backgammon must include chance nodes in addition to MAX and MIN nodes.
Chance nodes are shown as circles in Figure 5.11. The branches leading from each chance
node denote the possible dice rolls; each branch is labeled with the roll and its probability.
Thereare36waystorolltwodice,eachequallylikely;butbecausea6–5isthesameasa5–6,
thereareonly21distinctrolls. Thesixdoubles(1–1through 6–6)eachhaveaprobability of
1/36,sowesayP(1–1) = 1/36. Theother15distinctrollseachhavea1/18probability.
178 Chapter 5. Adversarial Search
MAX
CHANCE . . .
B
... ... ... ...
1/36 1/18 1/18 1/36
1,1 1,2 6,5 6,6
MIN . . .
... ... ...
CHANCE C . . .
... ... ...
1/36 1/18 1/18 1/36
1,1 1,2 6,5 6,6
MAX . . .
... ... ...
TERMINAL 2 –1 1 –1 1
Figure5.11 Schematicgametreeforabackgammonposition.
Thenextstepistounderstand howtomakecorrect decisions. Obviously, westillwant
to pick the move that leads to the best position. However, positions do not have definite
minimaxvalues. Instead,wecanonlycalculatetheexpectedvalueofaposition: theaverage
EXPECTEDVALUE
overallpossible outcomesofthechancenodes.
This leads us to generalize the minimax value for deterministic games to an expecti-
EXPECTIMINIMAX minimaxvalueforgameswithchance nodes. Terminalnodes and MAX and MIN nodes (for
VALUE
which the dice roll is known) work exactly the same way as before. For chance nodes we
compute the expected value, which is the sum of the value over all outcomes, weighted by
theprobability ofeachchanceaction:
EXPECTIMINIMAX(s)=
⎧
⎪ ⎪ UTILITY(s) ifTERMINAL-TEST(s)
⎨
max a EXPECTIMINIMAX(RESULT(s,a)) ifPLAYER(s)= MAX
⎪ ⎪ ⎩ m (cid:2) in a EXPECTIMINIMAX(RESULT(s,a)) ifPLAYER(s)= MIN
P(r)EXPECTIMINIMAX(RESULT(s,r)) ifPLAYER(s)= CHANCE
r
where r represents apossible diceroll(orotherchance event)and RESULT(s,r)isthesame
stateass,withtheadditional factthattheresultofthedicerollisr.
5.5.1 Evaluationfunctions forgames ofchance
As with minimax, the obvious approximation to make with expectiminimax is to cut the
search offat some point and apply an evaluation function to each leaf. Onemight think that
evaluation functions forgamessuchasbackgammonshould bejustlikeevaluation functions
Section5.5. StochasticGames 179
forchess—theyjustneedtogivehigherscorestobetterpositions. Butinfact,thepresenceof
chance nodes means that one has tobe more careful about what the evaluation values mean.
Figure 5.12 shows what happens: with an evaluation function that assigns the values [1, 2,
3, 4] to the leaves, move a is best; with values [1, 20, 30, 400], move a is best. Hence,
1 2
the program behaves totally differently if we make a change in the scale of some evaluation
values! It turns out that to avoid this sensitivity, the evaluation function must be a positive
lineartransformation oftheprobabilityofwinningfromaposition(or,moregenerally, ofthe
expected utility of the position). This is an important and general property of situations in
whichuncertainty isinvolved, andwediscussitfurtherinChapter16.
MAX
a a a a
1 2 1 2
CHANCE 2.1 1.3 21 40.9
.9 .1 .9 .1 .9 .1 .9 .1
MIN 2 3 1 4 20 30 1 400
2 2 3 3 1 1 4 4 20 20 30 30 1 1 400 400
Figure5.12 Anorder-preservingtransformationonleafvalueschangesthebestmove.
If the program knew in advance all the dice rolls that would occur for the rest of the
game,solving agamewithdicewouldbejustlikesolving agamewithout dice,whichmini-
maxdoesinO(bm)time,wherebisthebranchingfactorandmisthemaximumdepthofthe
game tree. Because expectiminimax is also considering all the possible dice-roll sequences,
itwilltakeO(bmnm),wherenisthenumberofdistinctrolls.
Evenifthesearchdepthislimitedtosomesmalldepth d,theextracostcomparedwith
that of minimax makes it unrealistic to consider looking ahead very far in most games of
chance. Inbackgammon nis21andb isusually around 20, butinsome situations canbeas
highas4000fordicerollsthataredoubles. Threepliesisprobably allwecouldmanage.
Another way to think about the problem is this: the advantage of alpha–beta is that
it ignores future developments that just are not going to happen, given best play. Thus, it
concentrates on likely occurrences. In games with dice, there are no likely sequences of
moves,because forthosemovestotakeplace, thedicewouldfirsthavetocomeouttheright
way to make them legal. This is a general problem whenever uncertainty enters the picture:
the possibilities are multiplied enormously, and forming detailed plans of action becomes
pointless becausetheworldprobably willnotplayalong.
It may have occurred to you that something like alpha–beta pruning could be applied
180 Chapter 5. Adversarial Search
to game trees with chance nodes. It turns out that it can. The analysis for MIN and MAX
nodes is unchanged, but wecan also prune chance nodes, using a bit of ingenuity. Consider
thechance node C inFigure5.11and whathappens toitsvalue asweexamine and evaluate
its children. Is it possible to find an upper bound on the value of C before we have looked
atallitschildren? (Recall that thisiswhatalpha–beta needs inordertoprune anode andits
subtree.) Atfirstsight, itmightseem impossible because thevalueofC istheaverage ofits
children’s values, and in order to compute the average of a set of numbers, we must look at
all the numbers. But ifweput bounds on the possible values of the utility function, then we
can arrive atbounds forthe average without looking atevery number. Forexample, say that
allutilityvaluesarebetween−2and+2;thenthevalueofleafnodesisbounded, andinturn
wecanplaceanupperboundonthevalueofachancenodewithoutlookingatallitschildren.
MONTECARLO An alternative is to do Monte Carlo simulation to evaluate a position. Start with
SIMULATION
an alpha–beta (or other) search algorithm. From a start position, have the algorithm play
thousands of games against itself, using random dice rolls. In the case of backgammon, the
resulting win percentage has been shown to be a good approximation of the value of the
position, even if the algorithm has an imperfect heuristic and is searching only a few plies
(Tesauro,1995). Forgameswithdice,thistypeofsimulation iscalledarollout.
ROLLOUT
5.6 PARTIALLY OBSERVABLE GAMES
Chess has often been described as war in miniature, but it lacks at least one major charac-
teristic of real wars, namely, partial observability. In the “fog of war,” the existence and
disposition of enemy units is often unknown until revealed by direct contact. As a result,
warfareincludestheuseofscoutsandspiestogatherinformationandtheuseofconcealment
and bluff to confuse the enemy. Partially observable games share these characteristics and
arethusqualitatively differentfromthegamesdescribed inthepreceding sections.
5.6.1 Kriegspiel: Partiallyobservablechess
Indeterministicpartiallyobservablegames,uncertaintyaboutthestateoftheboardarisesen-
tirelyfromlackofaccesstothechoicesmadebytheopponent. Thisclassincludeschildren’s
gamessuchasBattleships(whereeachplayer’sshipsareplacedinlocationshiddenfromthe
opponentbutdonotmove)andStratego(wherepiecelocationsareknownbutpiecetypesare
hidden). Wewill examine the game of Kriegspiel, apartially observable variant of chess in
KRIEGSPIEL
whichpiecescanmovebutarecompletelyinvisible totheopponent.
The rules of Kriegspiel are as follows: White and Black each see a board containing
onlytheirownpieces. Areferee,whocanseeallthepieces,adjudicatesthegameandperiod-
ically makes announcements that are heard by both players. On his turn, White proposes to
thereferee anymovethatwouldbelegalifthere werenoblack pieces. Ifthemoveisinfact
not legal (because of the black pieces), the referee announces “illegal.” In this case, White
maykeepproposing movesuntilalegaloneisfound—and learnsmoreaboutthelocation of
Black’s pieces in the process. Once a legal move is proposed, the referee announces one or
Section5.6. PartiallyObservable Games 181
more of the following: “Capture on square X”if there is a capture, and “Check by D” if the
black king is in check, where D is the direction of the check, and can be one of “Knight,”
“Rank,” “File,” “Long diagonal,” or “Short diagonal.” (In case of discovered check, the ref-
eree may make two “Check” announcements.) If Black is checkmated or stalemated, the
refereesaysso;otherwise, itisBlack’sturntomove.
Kriegspielmayseemterrifyinglyimpossible,buthumansmanageitquitewellandcom-
puter programs are beginning to catch up. It helps to recall the notion of a belief state as
defined in Section 4.4 and illustrated in Figure 4.14—the set of all logically possible board
states given the complete history of percepts to date. Initially, White’s belief state is a sin-
gleton because Black’s pieces haven’t moved yet. After White makes a move and Black re-
sponds, White’s belief state contains 20positions because Blackhas20replies toanyWhite
move. Keepingtrackofthebeliefstateasthegameprogressesisexactlytheproblemofstate
estimation, for which the update step is given in Equation (4.6). We can map Kriegspiel
state estimation directly onto the partially observable, nondeterministic framework of Sec-
tion 4.4 if we consider the opponent as the source of nondeterminism; that is, the RESULTS
ofWhite’smovearecomposedfromthe(predictable) outcome ofWhite’sownmoveandthe
unpredictable outcomegivenbyBlack’sreply.3
Given a current belief state, White may ask, “Can I win the game?” For a partially
observable game, the notion of a strategy is altered; instead of specifying a move to make
foreachpossible movetheopponent mightmake,weneedamoveforeverypossible percept
sequence that might be received. For Kriegspiel, a winning strategy, or guaranteed check-
GUARANTEED mate,isonethat, foreachpossible percept sequence, leadstoan actual checkmate forevery
CHECKMATE
possible board state in the current belief state, regardless of how the opponent moves. With
this definition, the opponent’s belief state is irrelevant—the strategy has to work even if the
opponent can see all the pieces. This greatly simplifies the computation. Figure 5.13 shows
part of a guaranteed checkmate for the KRK (king and rook against king) endgame. In this
case, Blackhasjustonepiece(theking), soabelief stateforWhitecanbeshowninasingle
boardbymarkingeachpossible position oftheBlackking.
The general AND-OR search algorithm can be applied to the belief-state space to find
guaranteed checkmates, just as in Section 4.4. The incremental belief-state algorithm men-
tioned inthatsection oftenfindsmidgamecheckmates uptodepth9—probably wellbeyond
theabilitiesofhumanplayers.
In addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that
PROBABILISTIC makes no sense in fully observable games: probabilistic checkmate. Such checkmates are
CHECKMATE
stillrequiredtoworkineveryboardstateinthebeliefstate;theyareprobabilisticwithrespect
torandomization ofthewinning player’s moves. Togetthebasic idea, consider theproblem
of finding a lone black king using just the white king. Simply by moving randomly, the
white king will eventually bump into the black king even if the latter tries to avoid this fate,
since Blackcannot keep guessing therightevasivemovesindefinitely. Intheterminology of
probability theory, detection occurs with probability 1. The KBNK endgame—king, bishop
3 Sometimes,thebeliefstatewillbecometoolargetorepresentjustasalistofboardstates,butwewillignore
thisissuefornow;Chapters7and8suggestmethodsforcompactlyrepresentingverylargebeliefstates.
182 Chapter 5. Adversarial Search
4
3
2
1
a b c d
Kc3 ?
“OK” “Illegal”
Rc3 ?
“OK” “Check”
Figure 5.13 Part of a guaranteedcheckmatein the KRK endgame, shown on a reduced
board. In the initial belief state, Black’s king is in one of three possible locations. By a
combination of probing moves, the strategy narrows this down to one. Completion of the
checkmateisleftasanexercise.
andknight against king—is woninthis sense; Whitepresents Blackwithaninfiniterandom
sequence of choices, for one of which Black will guess incorrectly and reveal his position,
leadingtocheckmate. TheKBBKendgame,ontheotherhand,iswonwithprobability1−(cid:2).
White can force a win only by leaving one of his bishops unprotected for one move. If
Blackhappens tobeintherightplaceandcaptures thebishop (amovethatwouldloseifthe
bishopsareprotected), thegameisdrawn. Whitecanchoosetomaketheriskymoveatsome
randomlychosenpointinthemiddleofaverylongsequence,thusreducing(cid:2)toanarbitrarily
smallconstant, butcannotreduce (cid:2)tozero.
It is quite rare that a guaranteed or probabilistic checkmate can be found within any
reasonabledepth,exceptintheendgame. Sometimesacheckmatestrategyworksforsomeof
theboardstatesinthecurrentbeliefstatebutnotothers. Tryingsuchastrategymaysucceed,
ACCIDENTAL leadingtoanaccidentalcheckmate—accidental inthesensethatWhitecouldnotknowthat
CHECKMATE
itwouldbecheckmate—ifBlack’spieceshappentobeintherightplaces. (Mostcheckmates
in games between humans are of this accidental nature.) This idea leads naturally to the
question ofhow likely itis that agiven strategy willwin, whichleads inturn tothe question
ofhowlikelyitisthateachboardstateinthecurrentbeliefstateisthetrueboardstate.
Section5.6. PartiallyObservable Games 183
One’sfirstinclinationmightbetoproposethatallboardstatesinthecurrentbeliefstate
are equally likely—but this can’t be right. Consider, for example, White’s belief state after
Black’s first move of the game. By definition (assuming that Black plays optimally), Black
musthaveplayedanoptimalmove,soallboardstatesresultingfromsuboptimalmovesought
tobeassignedzeroprobability. Thisargumentisnotquiterighteither,because eachplayer’s
goal is not just to move pieces tothe right squares but also tominimize the information that
the opponent has about their location. Playing any predictable “optimal” strategy provides
the opponent with information. Hence, optimal play in partially observable games requires
a willingness to play somewhat randomly. (This is why restaurant hygiene inspectors do
randominspection visits.) Thismeansoccasionally selecting movesthatmayseem“intrinsi-
cally”weak—buttheygainstrengthfromtheirveryunpredictability, becausetheopponentis
unlikely tohaveprepared anydefenseagainstthem.
From these considerations, it seems that the probabilities associated with the board
states in the current belief state can only be calculated given an optimal randomized strat-
egy; in turn, computing that strategy seems to require knowing the probabilities of the var-
ious states the board might be in. This conundrum can be resolved by adopting the game-
theoretic notion of an equilibrium solution, which we pursue further in Chapter 17. An
equilibrium specifies an optimal randomized strategy for each player. Computing equilib-
ria is prohibitively expensive, however, even for small games, and is out of the question for
Kriegspiel. At present, the design of effective algorithms for general Kriegspiel play is an
open research topic. Most systems perform bounded-depth lookahead in their own belief-
statespace, ignoring theopponent’s beliefstate. Evaluation functions resemble those forthe
observable gamebutincludeacomponent forthesizeofthebeliefstate—smallerisbetter!
5.6.2 Cardgames
Card games provide many examples of stochastic partial observability, where the missing
informationisgeneratedrandomly. Forexample,inmanygames,cardsaredealtrandomlyat
the beginning of the game, with each player receiving a hand that is not visible to the other
players. Suchgamesincludebridge, whist,hearts, andsome formsofpoker.
Atfirstsight,itmightseemthatthesecardgamesarejustlikedicegames: thecardsare
dealtrandomlyanddeterminethemovesavailabletoeachplayer,butallthe“dice”arerolled
atthe beginning! Even though this analogy turns out tobe incorrect, itsuggests an effective
algorithm: consider all possible deals of the invisible cards; solve each one as if it were a
fullyobservablegame;andthenchoosethemovethathasthebestoutcomeaveragedoverall
thedeals. Supposethateachdealsoccurswithprobability P(s);thenthemovewewantis
(cid:12)
argmax P(s)MINIMAX(RESULT(s,a)). (5.1)
a
s
Here,werunexact MINIMAX ifcomputationally feasible;otherwise, werun H-MINIMAX.
Now, in most card games, the number of possible deals is rather large. For example,
inbridge play, each player sees just two(cid:20)of(cid:21)the fourhands; there aretwounseen hands of13
cards each, so the number of deals is 26 = 10,400,600. Solving even one deal is quite
13
difficult, so solving ten million is out of the question. Instead, we resort to a Monte Carlo
184 Chapter 5. Adversarial Search
approximation: instead of adding up all the deals, we take a random sample of N deals,
wheretheprobability ofdeal sappearing inthesampleisproportional to P(s):
(cid:12)N
1
argmax MINIMAX(RESULT(s
i
,a)). (5.2)
N
a
i=1
(Notice that P(s) does not appear explicitly in the summation, because the samples are al-
ready drawn according to P(s).) As N grows large, the sum over the random sample tends
totheexact value, but evenforfairly small N—say, 100to1,000—the method gives agood
approximation. Itcanalso beapplied todeterministic gamessuchasKriegspiel, givensome
reasonable estimateofP(s).
Forgameslikewhistandhearts,wherethereisnobidding orbetting phasebeforeplay
commences, each deal will be equally likely and so the values of P(s) are all equal. For
bridge, play ispreceded byabidding phase inwhicheachteam indicates howmanytricks it
expects towin. Since players bid based on the cards they hold, the other players learn more
abouttheprobability ofeachdeal. Takingthisintoaccount indeciding howtoplaythehand
istricky, forthereasons mentioned inourdescription ofKriegspiel: players maybidinsuch
awayastominimize theinformation conveyed totheiropponents. Evenso, the approach is
quiteeffectiveforbridge, asweshowinSection5.7.
The strategy described in Equations 5.1 and 5.2 is sometimes called averaging over
clairvoyance because it assumes that the game will become observable to both players im-
mediately after the first move. Despite its intuitive appeal, the strategy can lead one astray.
Considerthefollowingstory:
Day 1: Road A leads to a heap of gold; Road B leads to a fork. Take the left fork and
you’llfindabiggerheapofgold,buttaketherightforkandyou’llberunoverbyabus.
Day2: RoadA leadsto aheapofgold; RoadBleadstoa fork. Takethe rightforkand
you’llfindabiggerheapofgold,buttaketheleftforkandyou’llberunoverbyabus.
Day 3: Road A leads to a heap of gold; Road B leads to a fork. One branch of the
forkleads to a biggerheap of gold, but take the wrong forkand you’llbe hit by a bus.
Unfortunatelyyoudon’tknowwhichforkiswhich.
Averagingoverclairvoyanceleadstothefollowingreasoning: onDay1,Bistherightchoice;
onDay2,Bistheright choice; onDay3,thesituation isthesameaseitherDay1orDay2,
soBmuststillbetherightchoice.
Now we can see how averaging overclairvoyance fails: it does not consider the belief
statethattheagentwillbeinafteracting. Abeliefstateoftotalignoranceisnotdesirable,es-
pecially whenonepossibility iscertain death. Becauseitassumesthateveryfuturestatewill
automatically beoneofperfectknowledge, theapproach neverselectsactionsthatgatherin-
formation(likethefirstmoveinFigure5.13);norwillitchooseactionsthathideinformation
from the opponent or provide information to a partner because it assumes that they already
knowtheinformation; anditwillneverbluffinpoker,4 because itassumestheopponent can
BLUFF
seeitscards. InChapter17, weshowhowtoconstruct algorithms thatdoallthese things by
virtueofsolving thetruepartiallyobservable decisionproblem.
4 Bluffing—bettingasifone’shandisgood,evenwhenit’snot—isacorepartofpokerstrategy.
Section5.7. State-of-the-Art GamePrograms 185
5.7 STATE-OF-THE-ART GAME PROGRAMS
In 1965, the Russian mathematician Alexander Kronrod called chess “the Drosophila of ar-
tificial intelligence.” John McCarthy disagrees: whereas geneticists use fruit flies to make
discoveries that apply to biology more broadly, AI has used chess to do the equivalent of
breeding very fast fruit flies. Perhaps a better analogy is that chess is to AI as Grand Prix
motorracingistothecarindustry: state-of-the-art gameprogramsareblindingly fast,highly
optimized machines that incorporate the latest engineering advances, but they aren’t much
use for doing the shopping or driving off-road. Nonetheless, racing and game-playing gen-
erate excitement and a steady stream of innovations that have been adopted by the wider
community. Inthissectionwelookatwhatittakestocomeoutontopinvariousgames.
CHESS
Chess: IBM’s DEEP BLUE chess program, now retired, is well known for defeating world
champion GarryKasparov inawidely publicized exhibition match. DeepBlueranonapar-
allel computer with 30 IBM RS/6000 processors doing alpha–beta search. The unique part
was a configuration of 480 custom VLSI chess processors that performed move generation
andmoveorderingforthelastfewlevelsofthetree,andevaluatedtheleafnodes. DeepBlue
searched up to 30 billion positions per move, reaching depth 14 routinely. The key to its
success seemstohavebeen itsability togenerate singular extensions beyond thedepth limit
forsufficiently interesting lines offorcing/forced moves. Insomecases thesearch reached a
depth of 40 plies. Theevaluation function had over8000 features, many of them describing
highly specific patterns of pieces. An “opening book” of about 4000 positions was used, as
well as a database of 700,000 grandmaster games from which consensus recommendations
could beextracted. Thesystem also used alarge endgame database of solved positions con-
taining all positions with five pieces and many with six pieces. This database had the effect
ofsubstantially extending theeffectivesearchdepth,allowingDeepBluetoplayperfectlyin
somecasesevenwhenitwasmanymovesawayfromcheckmate.
ThesuccessofDEEP BLUEreinforcedthewidelyheldbeliefthatprogressincomputer
game-playing has come primarily from ever-more-powerful hardware—a view encouraged
by IBM. But algorithmic improvements have allowed programs running on standard PCs
to win World Computer Chess Championships. A variety of pruning heuristics are used to
reducetheeffectivebranchingfactortolessthan3(comparedwiththeactualbranchingfactor
ofabout35). Themostimportantoftheseisthenullmoveheuristic, whichgenerates agood
NULLMOVE
lower bound on the value of a position, using a shallow search in which the opponent gets
to move twice at the beginning. This lower bound often allows alpha–beta pruning without
theexpenseofafull-depth search. Alsoimportant is futilitypruning,whichhelpsdecidein
FUTILITYPRUNING
advancewhichmoveswillcauseabetacutoffinthesuccessor nodes.
HYDRA can be seen as the successor to DEEP BLUE. HYDRA runs on a 64-processor
cluster with1gigabyte perprocessor andwithcustom hardware intheform ofFPGA(Field
ProgrammableGateArray)chips. HYDRAreaches200millionevaluationspersecond,about
the same as Deep Blue, but HYDRA reaches 18 plies deep rather than just 14 because of
aggressive useofthenullmoveheuristic andforwardpruning.
186 Chapter 5. Adversarial Search
RYBKA, winnerof the 2008 and 2009 World Computer Chess Championships, iscon-
sidered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 GHz Intel
Xeon processor, but little is known about the design of the program. RYBKA’s main ad-
vantage appears to be its evaluation function, which has been tuned by its main developer,
International MasterVasikRajlich,andatleastthreeothergrandmasters.
The most recent matches suggest that the top computer chess programs have pulled
aheadofallhumancontenders. (Seethehistorical notesfordetails.)
CHECKERS
Checkers: Jonathan Schaeffer and colleagues developed CHINOOK, which runs on regular
PCsand uses alpha–beta search. Chinook defeated the long-running human champion in an
abbreviatedmatchin1990,andsince2007CHINOOKhasbeenabletoplayperfectlybyusing
alpha–beta searchcombined withadatabase of39trillionendgamepositions.
Othello, also called Reversi, is probably more popular as a computer game than as a board
OTHELLO
game. It has a smaller search space than chess, usually 5 to 15 legal moves, but evaluation
expertisehadtobedevelopedfromscratch. In1997,theLOGISTELLO program(Buro,2002)
defeatedthehumanworldchampion,TakeshiMurakami,bysixgamestonone. Itisgenerally
acknowledged thathumansarenomatchforcomputers atOthello.
Backgammon: Section5.5explainedwhytheinclusionofuncertaintyfromdicerollsmakes
BACKGAMMON
deep search an expensive luxury. Most work on backgammon has gone into improving the
evaluation function. Gerry Tesauro (1992) combined reinforcement learning with neural
networks to develop a remarkably accurate evaluator that is used with a search to depth 2
or 3. After playing more than a million training games against itself, Tesauro’s program,
TD-GAMMON,iscompetitivewithtophumanplayers. Theprogram’sopinionsontheopen-
ingmovesofthegamehaveinsomecasesradicallyalteredthe receivedwisdom.
Go is the most popular board game in Asia. Because the board is 19 × 19 and moves are
GO
allowed into (almost) every empty square, the branching factor starts at 361, which is too
daunting for regular alpha–beta search methods. In addition, it is difficult to write an eval-
uation function because control of territory is often very unpredictable until the endgame.
Therefore thetopprograms, suchas MOGO,avoid alpha–beta search andinstead useMonte
Carlorollouts. Thetrickistodecidewhatmovestomakeinthecourseoftherollout. Thereis
noaggressive pruning; allmovesarepossible. TheUCT(upperconfidence bounds ontrees)
method works by making random moves in the first few iterations, and over time guiding
thesampling process toprefermovesthathaveledtowinsinprevious samples. Sometricks
are added, including knowledge-based rules that suggest particular moves whenever a given
pattern isdetected andlimitedlocal searchtodecide tactical questions. Someprogramsalso
COMBINATORIAL include special techniques from combinatorial game theory to analyze endgames. These
GAMETHEORY
techniques decompose aposition intosub-positions thatcanbeanalyzed separately andthen
combined (Berlekamp and Wolfe, 1994; Mu¨ller, 2003). The optimal solutions obtained in
this way have surprised many professional Go players, who thought they had been playing
optimally allalong. Current Goprograms play atthemasterlevelonareduced 9×9board,
butarestillatadvanced amateurlevelonafullboard.
Bridge is a card game of imperfect information: a player’s cards are hidden from the other
BRIDGE
players. Bridge is also a multiplayer game with four players instead of two, although the
Section5.8. AlternativeApproaches 187
players are paired into two teams. As in Section 5.6, optimal play in partially observable
gameslikebridgecanincludeelementsofinformationgathering,communication,andcareful
weighing of probabilities. Many of these techniques are used in the Bridge Baron program
(Smith et al., 1998), which won the 1997 computer bridge championship. While it does
not play optimally, Bridge Baron is one of the few successful game-playing systems to use
complex,hierarchicalplans(seeChapter11)involvinghigh-levelideas,suchasfinessingand
squeezing,thatarefamiliartobridgeplayers.
TheGIBprogram(Ginsberg,1999)wonthe2000computerbridgechampionshipquite
decisivelyusingtheMonteCarlomethod. Sincethen,otherwinningprogramshavefollowed
EXPLANATION-
GIB’slead. GIB’smajorinnovation isusingexplanation-based generalization tocompute
BASED
GENERALIZATION
and cache general rules foroptimal play in various standard classes of situations rather than
evaluating each situation individually. For example, in a situation where one player has the
cards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5, there are 7×6 = 42
ways that the first player can lead from that suit and the second player can follow. But GIB
treats these situations as just two: the first player can lead either a high card or a low card;
theexactcardsplayeddon’tmatter. Withthisoptimization (andafewothers), GIBcansolve
a52-card, fullyobservable deal exactly inabout asecond. GIB’stactical accuracy makesup
foritsinabilitytoreasonaboutinformation. Itfinished12thinafieldof35intheparcontest
(involving just play of the hand, not bidding) at the 1998 human world championship, far
exceeding theexpectations ofmanyhumanexperts.
Thereare several reasons why GIB plays atexpert level withMonte Carlosimulation,
whereas Kriegspiel programs donot. First, GIB’s evaluation ofthe fully observable version
ofthegameisexact,searching thefullgametree, whileKriegspiel programs relyoninexact
heuristics. But far more important is the fact that in bridge, most of the uncertainty in the
partially observable information comesfromtherandomness ofthedeal,notfromtheadver-
sarial play of the opponent. Monte Carlo simulation handles randomness well, but does not
alwayshandlestrategy well,especially whenthestrategy involves thevalueofinformation.
Scrabble: MostpeoplethinkthehardpartaboutScrabbleiscomingupwithgoodwords,but
SCRABBLE
giventheofficialdictionary, itturnsouttoberathereasytoprogramamovegeneratortofind
the highest-scoring move (Gordon, 1994). That doesn’t mean the game is solved, however:
merely taking the top-scoring move each turn results in a good but not expert player. The
problem is that Scrabble is both partially observable and stochastic: you don’t know what
letters the other player has or what letters you will draw next. So playing Scrabble well
combines the difficulties of backgammon and bridge. Nevertheless, in 2006, the QUACKLE
program defeatedtheformerworldchampion, DavidBoys,3–2.
5.8 ALTERNATIVE APPROACHES
Because calculating optimal decisions in games is intractable in most cases, all algorithms
must make some assumptions and approximations. The standard approach, based on mini-
max,evaluationfunctions, andalpha–beta, isjustonewaytodothis. Probablybecauseithas
188 Chapter 5. Adversarial Search
MAX
MIN 99 100
99 1000 1000 1000 100 101 102 100
Figure5.14 Atwo-plygametreeforwhichheuristicminimaxmaymakeanerror.
been worked on for so long, the standard approach dominates other methods in tournament
play. Some believe that this has caused game playing to become divorced from the main-
stream ofAIresearch: thestandard approach nolongerprovides muchroom fornewinsight
intogeneralquestions ofdecision making. Inthissection, welookatthealternatives.
First, let us consider heuristic minimax. It selects an optimal move in a given search
tree provided that the leaf node evaluations are exactly correct. In reality, evaluations are
usually crude estimates of the value of a position and can be considered to have large errors
associated with them. Figure 5.14 shows a two-ply game tree for which minimax suggests
taking the right-hand branch because 100 > 99. That is the correct move if the evaluations
are all correct. But of course the evaluation function is only approximate. Suppose that
the evaluation of each node has an error that is independent of other nodes and is randomly
distributed with mean zero and standard deviation of σ. Then when σ = 5, the left-hand
branch is actually better 71% of the time, and 58% of the time when σ = 2. The intuition
behind this is that the right-hand branch has four nodes that are close to 99; if an error in
the evaluation of any one of the four makes the right-hand branch slip below 99, then the
left-hand branchisbetter.
Inreality,circumstances areactuallyworsethanthisbecausetheerrorintheevaluation
functionisnotindependent. Ifwegetonenodewrong,thechancesarehighthatnearbynodes
in the tree will also be wrong. The fact that the node labeled 99 has siblings labeled 1000
suggests that in fact it might have a higher true value. We can use an evaluation function
thatreturns aprobability distribution overpossible values, butitisdifficulttocombinethese
distributions properly, because wewon’thaveagoodmodeloftheverystrongdependencies
thatexistbetweenthevaluesofsiblingnodes
Next,weconsiderthesearchalgorithmthatgeneratesthetree. Theaimofanalgorithm
designeristospecifyacomputationthatrunsquicklyandyieldsagoodmove. Thealpha–beta
algorithmisdesignednotjusttoselectagoodmovebutalsotocalculateboundsonthevalues
of all the legal moves. Tosee whythis extra information is unnecessary, consider aposition
in which there is only one legal move. Alpha–beta search still will generate and evaluate a
large search tree, telling usthatthe onlymoveisthebest moveand assigning itavalue. But
since we have to make the move anyway, knowing the move’s value is useless. Similarly, if
thereisoneobviouslygoodmoveandseveralmovesthatarelegalbutleadtoaquickloss,we
Section5.9. Summary 189
wouldnotwantalpha–betatowastetimedeterminingaprecisevalueforthelonegoodmove.
Bettertojustmakethemovequicklyandsavethetimeforlater. Thisleadstotheideaofthe
utility of a node expansion. A good search algorithm should select node expansions of high
utility—that is, ones that are likely tolead tothe discovery ofa significantly better move. If
there are no node expansions whose utility is higher than their cost (in terms of time), then
the algorithm should stop searching and make a move. Notice that this works not only for
clear-favorite situations but also forthe case of symmetrical moves, forwhich no amount of
searchwillshowthatonemoveisbetterthananother.
This kind of reasoning about what computations to do is called metareasoning (rea-
METAREASONING
soning about reasoning). It applies not just to game playing but to any kind of reasoning
at all. All computations are done in the service of trying to reach better decisions, all have
costs, andallhave somelikelihood ofresulting inacertain improvement indecision quality.
Alpha–beta incorporates thesimplest kindofmetareasoning, namely, atheorem totheeffect
thatcertainbranches ofthetreecanbeignored withoutloss. Itispossible todomuchbetter.
InChapter16,weseehowtheseideascanbemadepreciseandimplementable.
Finally, let us reexamine the nature of search itself. Algorithms for heuristic search
and for game playing generate sequences of concrete states, starting from the initial state
and then applying an evaluation function. Clearly, this is not how humans play games. In
chess,oneoftenhasaparticulargoalinmind—forexample,trappingtheopponent’squeen—
and can use this goal to selectively generate plausible plans for achieving it. This kind of
goal-directed reasoning or planning sometimes eliminates combinatorial search altogether.
David Wilkins’ (1980) PARADISE is the only program to have used goal-directed reasoning
successfully in chess: it was capable of solving some chess problems requiring an 18-move
combination. As yet there is no good understanding of how to combine the two kinds of
algorithms into a robust and efficient system, although Bridge Baron might be a step in the
right direction. A fully integrated system would be a significant achievement not just for
game-playing research but also forAI research in general, because it would be a good basis
forageneralintelligent agent.
5.9 SUMMARY
We have looked at a variety of games to understand what optimal play means and to under-
standhowtoplaywellinpractice. Themostimportant ideasareasfollows:
• A game can be defined by the initial state (how the board is set up), the legal actions
in each state, the result of each action, a terminal test (which says when the game is
over),andautilityfunctionthatapplies toterminalstates.
• In two-player zero-sum games with perfect information, the minimax algorithm can
selectoptimalmovesbyadepth-firstenumeration ofthegametree.
• The alpha–beta search algorithm computes the same optimal move as minimax, but
achievesmuchgreaterefficiencybyeliminating subtreesthatareprovablyirrelevant.
• Usually,itisnotfeasibletoconsiderthewholegametree(evenwithalpha–beta),sowe
190 Chapter 5. Adversarial Search
need tocutthe search offatsomepoint andapply aheuristic evaluation function that
estimatestheutilityofastate.
• Manygameprogramsprecompute tablesofbestmovesintheopening andendgameso
thattheycanlookupamoveratherthansearch.
• Games of chance can be handled by an extension to the minimax algorithm that eval-
uates a chance node by taking the average utility of all its children, weighted by the
probability ofeachchild.
• Optimal play in games of imperfect information, such as Kriegspiel and bridge, re-
quires reasoning about the current and future belief states of each player. A simple
approximation can be obtained by averaging the value of an action over each possible
configuration ofmissinginformation.
• Programshavebestedevenchampionhumanplayers atgamessuchaschess,checkers,
and Othello. Humans retain the edge in several games of imperfect information, such
as poker, bridge, and Kriegspiel, and in games with very large branching factors and
littlegoodheuristicknowledge, suchasGo.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The early history of mechanical game playing was marred by numerous frauds. The most
notoriousofthesewasBaronWolfgangvonKempelen’s(1734–1804)“TheTurk,”asupposed
chess-playing automaton that defeated Napoleon before being exposed as amagician’s trick
cabinet housing a human chess expert (see Levitt, 2000). It played from 1769 to 1854. In
1846, Charles Babbage (who had been fascinated by the Turk) appears to have contributed
the first serious discussion of the feasibility of computer chess and checkers (Morrison and
Morrison,1961). Hedidnotunderstandtheexponentialcomplexityofsearchtrees,claiming
“the combinations involved in the Analytical Engine enormously surpassed any required,
even by the game of chess.” Babbage also designed, but did not build, a special-purpose
machine for playing tic-tac-toe. The first true game-playing machine was built around 1890
by theSpanish engineer Leonardo TorresyQuevedo. Itspecialized inthe “KRK”(king and
rookvs.king)chessendgame,guaranteeing awinwithkingandrookfromanyposition.
The minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the developer of
modernsettheory. Thepaperunfortunatelycontainedseveralerrorsanddidnotdescribemin-
imaxcorrectly. Ontheotherhand,itdidlayouttheideasofretrogradeanalysisandproposed
(but did not prove) what became known as Zermelo’s theorem: that chess is determined—
Whitecan force awinorBlack canoritisadraw; wejust don’t know which. Zermelosays
thatshould weeventually know,“Chesswouldofcourse losethecharacterofagameatall.”
A solid foundation for game theory was developed in the seminal work Theory of Games
andEconomicBehavior (vonNeumannandMorgenstern, 1944), whichincluded ananalysis
showingthatsomegamesrequirestrategiesthatarerandomized(orotherwiseunpredictable).
SeeChapter17formoreinformation.
Bibliographical andHistorical Notes 191
John McCarthy conceived the idea of alpha–beta search in 1956, although he did not
publish it. TheNSSchess program (Newell etal.,1958) used asimplified version ofalpha–
beta; itwasthe firstchess program todo so. Alpha–beta pruning wasdescribed by Hartand
Edwards(1961)andHartetal.(1972). Alpha–betawasusedbythe“Kotok–McCarthy”chess
program written by a student of John McCarthy (Kotok, 1962). Knuth and Moore (1975)
proved the correctness ofalpha–beta and analysed its timecomplexity. Pearl(1982b) shows
alpha–beta tobeasymptotically optimalamongallfixed-depthgame-treesearchalgorithms.
Several attempts have been made to overcome the problems with the “standard ap-
proach” thatwereoutlined inSection5.8. Thefirstnonexhaustive heuristic searchalgorithm
∗
withsome theoretical grounding wasprobably B (Berliner, 1979), which attempts tomain-
tain interval bounds on the possible value of a node in the game tree rather than giving it
a single point-valued estimate. Leaf nodes are selected for expansion in an attempt to re-
∗
fine the top-level bounds until one move is “clearly best.” Palay (1985) extends the B idea
using probability distributions on values in place of intervals. David McAllester’s (1988)
conspiracy number search expands leaf nodes that, by changing their values, could cause
∗
the program to prefer a new move at the root. MGSS (Russell and Wefald, 1989) uses the
decision-theoretic techniques of Chapter 16 to estimate the value of expanding each leaf in
terms of the expected improvement in decision quality at the root. It outplayed an alpha–
∗
betaalgorithm atOthellodespite searching anorderofmagnitude fewernodes. TheMGSS
approach is,inprinciple, applicable tothecontrolofanyformofdeliberation.
Alpha–beta search is in many ways the two-player analog of depth-first branch-and-
∗ ∗
bound, which is dominated by A in the single-agent case. The SSS algorithm (Stockman,
∗
1979) can be viewed as a two-player A and never expands more nodes than alpha–beta to
reachthesamedecision. Thememoryrequirementsandcomputationaloverheadofthequeue
∗
make SSS in its original form impractical, but a linear-space version has been developed
from the RBFSalgorithm (Korf and Chickering, 1996). Plaat et al. (1996) developed a new
∗
viewofSSS asacombination ofalpha–beta andtransposition tables, showinghowtoover-
come the drawbacks of the original algorithm and developing a new variant called MTD(f)
thathasbeenadopted byanumberoftopprograms.
D. F.Beal (1980) and Dana Nau (1980, 1983) studied the weaknesses of minimax ap-
pliedtoapproximate evaluations. Theyshowedthatundercertainassumptions aboutthedis-
tributionofleafvaluesinthetree,minimaxingcanyieldvaluesattherootthatareactuallyless
reliable than the direct use of the evaluation function itself. Pearl’s book Heuristics (1984)
partially explains this apparent paradox and analyzes many game-playing algorithms. Baum
and Smith (1997) propose a probability-based replacement for minimax, showing that it re-
sults in better choices in certain games. The expectiminimax algorithm was proposed by
Donald Michie (1966). Bruce Ballard (1983) extended alpha–beta pruning to cover trees
withchancenodesandHauk(2004)reexaminesthisworkandprovides empiricalresults.
Koller and Pfeffer (1997) describe a system for completely solving partially observ-
able games. The system is quite general, handling games whose optimal strategy requires
randomized moves and games that are more complex than those handled by any previous
system. Still, it can’t handle games as complex as poker, bridge, and Kriegspiel. Frank
et al. (1998) describe several variants of Monte Carlo search, including one where MIN has
192 Chapter 5. Adversarial Search
complete information but MAX does not. Among deterministic, partially observable games,
Kriegspiel has received the most attention. Ferguson demonstrated hand-derived random-
izedstrategiesforwinningKriegspielwithabishopandknight(1992)ortwobishops(1995)
against a king. The first Kriegspiel programs concentrated on finding endgame checkmates
and performed AND–OR search in belief-state space (Sakuta and Iida, 2002; Bolognesi and
Ciancarini,2003). Incrementalbelief-statealgorithmsenabledmuchmorecomplexmidgame
checkmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efficient
stateestimation remainstheprimaryobstacletoeffective generalplay(Parker etal.,2005).
ChesswasoneofthefirsttasksundertakeninAI,withearlyefforts bymanyofthepio-
neersofcomputing,including KonradZusein1945,NorbertWienerinhisbookCybernetics
(1948), and Alan Turing in 1950 (see Turing et al., 1953). But it was Claude Shannon’s
article Programming a Computer for Playing Chess (1950) that had the most complete set
of ideas, describing a representation for board positions, an evaluation function, quiescence
search, andsomeideasforselective (nonexhaustive) game-treesearch. Slater(1950)andthe
commentators onhisarticlealsoexplored thepossibilities forcomputerchessplay.
D. G. Prinz (1952) completed a program that solved chess endgame problems but did
not play a full game. Stan Ulam and a group at the Los Alamos National Lab produced a
program that played chess on a 6×6 board with no bishops (Kister et al., 1957). It could
search 4pliesdeepinabout12minutes. AlexBernstein wrote thefirstdocumented program
toplayafullgameofstandard chess(BernsteinandRoberts, 1958).5
ThefirstcomputerchessmatchfeaturedtheKotok–McCarthyprogramfromMIT(Ko-
tok, 1962) and the ITEP program written in the mid-1960s at Moscow’s Institute of Theo-
retical and Experimental Physics (Adelson-Velsky etal., 1970). Thisintercontinental match
wasplayedbytelegraph. Itendedwitha3–1victoryfortheITEPprogramin1967. Thefirst
chess program to compete successfully with humans was MIT’s MACHACK-6 (Greenblatt
etal.,1967). ItsEloratingofapproximately 1400waswellabovethenovicelevelof1000.
The Fredkin Prize, established in 1980, offered awards for progressive milestones in
chess play. The$5,000 prize forthe firstprogram toachieve a masterrating wentto BELLE
(Condon and Thompson, 1982), which achieved a rating of2250. The$10,000 prize forthe
first program to achieve a USCF (United States Chess Federation) rating of 2500 (near the
grandmaster level) was awarded to DEEP THOUGHT (Hsu et al., 1990) in 1989. The grand
prize, $100,000, went to DEEP BLUE (Campbell et al., 2002; Hsu, 2004) for its landmark
victoryoverworldchampionGarryKasparovina1997exhibition match. Kasparovwrote:
ThedecisivegameofthematchwasGame2,whichleftascarinmymemory...wesaw
somethingthatwentwellbeyondourwildestexpectationsofhowwellacomputerwould
be able to foresee the long-termpositionalconsequencesof its decisions. The machine
refusedtomovetoapositionthathadadecisiveshort-termadvantage—showingavery
humansenseofdanger.(Kasparov,1997)
Probably the most complete description of a modern chess program is provided by Ernst
Heinz (2000), whose DARKTHOUGHT program was the highest-ranked noncommercial PC
program atthe1999worldchampionships.
5 ARussianprogram,BESMmayhavepredatedBernstein’sprogram.
Bibliographical andHistorical Notes 193
(a) (b)
Figure5.15 Pioneersincomputerchess:(a)HerbertSimonandAllenNewell,developers
of the NSS program (1958); (b) John McCarthy and the Kotok–McCarthy program on an
IBM7090(1967).
In recent years, chess programs are pulling ahead of even the world’s best humans.
In 2004–2005 HYDRA defeated grand master Evgeny Vladimirov 3.5–0.5, world champion
RuslanPonomariov2–0,andseventh-ranked MichaelAdams5.5–0.5. In2006, DEEP FRITZ
beat world champion Vladimir Kramnik 4–2, and in 2007 RYBKA defeated several grand
masters in games in which it gave odds (such as a pawn) to the human players. Asof 2009,
the highest Elorating everrecorded wasKasparov’s 2851. HYDRA (Donninger and Lorenz,
2004)isratedsomewherebetween2850and3000, basedmostly onitstrouncing ofMichael
Adams. The RYBKA program is rated between 2900 and 3100, but this is based on a small
numberofgamesandisnotconsideredreliable. Ross(2004)showshowhumanplayershave
learnedtoexploitsomeoftheweaknessesofthecomputerprograms.
Checkers was the first of the classic games fully played by a computer. Christopher
Strachey (1952) wrote the first working program for checkers. Beginning in 1952, Arthur
Samuel of IBM, working in his spare time, developed a checkers program that learned its
own evaluation function by playing itself thousands of times (Samuel, 1959, 1967). We
describe this idea in more detail in Chapter 21. Samuel’s program began as a novice but
after only a few days’ self-play had improved itself beyond Samuel’s own level. In 1962 it
defeated Robert Nealy, a champion at “blind checkers,” through an error on his part. When
one considers that Samuel’s computing equipment (an IBM 704) had 10,000 words of main
memory,magnetictapeforlong-termstorage,anda.000001GHzprocessor,thewinremains
agreataccomplishment.
Thechallenge started bySamuelwastaken upbyJonathan SchaefferoftheUniversity
of Alberta. His CHINOOK program came in second in the 1990 U.S. Open and earned the
right to challenge forthe world championship. It then ran up against a problem, in the form
of Marion Tinsley. Dr. Tinsley had been world champion for over 40 years, losing only
three gamesinallthat time. Inthefirstmatchagainst CHINOOK,Tinsleysuffered hisfourth
194 Chapter 5. Adversarial Search
and fifth losses, but won the match 20.5–18.5. A rematch at the 1994 world championship
ended prematurely when Tinsley had to withdraw forhealth reasons. CHINOOK became the
official world champion. Schaeffer kept on building on his database of endgames, and in
2007“solved”checkers(Schaeffer etal.,2007;Schaeffer,2008). Thishadbeenpredictedby
Richard Bellman (1965). In the paper that introduced the dynamic programming approach
to retrograde analysis, he wrote, “In checkers, the number of possible moves in any given
situation is so small that we can confidently expect a complete digital computer solution to
the problem of optimal play in this game.” Bellman did not, however, fully appreciate the
size of the checkers game tree. There are about 500 quadrillion positions. After 18 years
of computation on a cluster of 50 or more machines, Jonathan Schaeffer’s team completed
an endgame table forall checkers positions with 10 orfewerpieces: over39 trillion entries.
From there, they were able to do forward alpha–beta search to derive a policy that proves
that checkers is in fact a draw with best play by both sides. Note that this is an application
of bidirectional search (Section 3.4.6). Building an endgame table forall ofcheckers would
be impractical: it would require a billion gigabytes of storage. Searching without any table
wouldalsobeimpractical: thesearch treehasabout 847 positions, andwouldtakethousands
of years to search with today’s technology. Only a combination of clever search, endgame
data,andadropinthepriceofprocessors andmemorycouldsolvecheckers. Thus,checkers
joins Qubic (Patashnik, 1980), Connect Four(Allis, 1988), and Nine-Men’s Morris (Gasser,
1998)asgamesthathavebeensolvedbycomputeranalysis.
Backgammon,agameofchance,wasanalyzed mathematically byGerolamoCardano
(1663), but only taken up for computer play in the late 1970s, first with the BKG pro-
gram (Berliner, 1980b); it used a complex, manually constructed evaluation function and
searchedonlytodepth1. Itwasthefirstprogramtodefeatahumanworldchampionatama-
jorclassic game(Berliner, 1980a). Berlinerreadily acknowledged that BKGwasverylucky
withthedice. GerryTesauro’s(1995) TD-GAMMON played consistently atworldchampion
level. TheBGBLITZ program wasthewinnerofthe2008ComputerOlympiad.
Goisadeterministic game,butthelargebranchingfactormakesitchalleging. Thekey
issuesandearlyliteratureincomputerGoaresummarizedbyBouzyandCazenave(2001)and
Mu¨ller (2002). Up to 1997 there were no competent Go programs. Now the best programs
play most of their moves at the master level; the only problem is that over the course of a
game they usually make at least one serious blunder that allows a strong opponent to win.
Whereas alpha–beta search reigns in most games, many recent Go programs have adopted
MonteCarlomethodsbasedontheUCT(upperconfidence bounds ontrees)scheme(Kocsis
and Szepesvari, 2006). The strongest Go program as of 2009 is Gelly and Silver’s MOGO
(WangandGelly,2007;GellyandSilver,2008). InAugust2008, MOGOscoredasurprising
win against top professional Myungwan Kim, albeit with MOGO receiving a handicap of
nine stones (about the equivalent of a queen handicap in chess). Kim estimated MOGO’s
strength at 2–3 dan, the low end of advanced amateur. For this match, MOGO was run on
an 800-processor 15 teraflop supercomputer (1000 times Deep Blue). A few weeks later,
MOGO,withonlyafive-stonehandicap, wonagainsta6-danprofessional. Inthe9×9form
of Go, MOGO is at approximately the 1-dan professional level. Rapid advances are likely
as experimentation continues with new forms of Monte Carlo search. The Computer Go
Exercises 195
Newsletter,published bytheComputerGoAssociation, describes currentdevelopments.
Bridge: Smithetal.(1998) reportonhowtheirplanning-based program wonthe1998
computerbridgechampionship,and(Ginsberg,2001)describeshowhisGIBprogram,based
on Monte Carlo simulation, wonthe following computer championship and did surprisingly
wellagainsthumanplayersandstandard bookproblem sets. From2001–2007, thecomputer
bridge championship was won five times by JACK and twice by WBRIDGE5. Neither has
hadacademicarticlesexplainingtheirstructure,butboth arerumoredtousetheMonteCarlo
technique, whichwasfirstproposed forbridgebyLevy(1989).
Scrabble: Agood description ofatopprogram, MAVEN, isgivenby itscreator, Brian
Sheppard (2002). Generating the highest-scoring move is described by Gordon (1994), and
modelingopponents iscoveredbyRichardsandAmir(2007).
Soccer (Kitano et al., 1997b; Visser et al., 2008) and billiards (Lam and Greenspan,
2008; Archibald et al., 2009) and other stochastic games with a continuous space of actions
arebeginning toattractattention inAI,bothinsimulation andwithphysicalrobotplayers.
Computergamecompetitions occurannually, andpapersappearinavarietyofvenues.
The rather misleadingly named conference proceedings Heuristic Programming in Artificial
Intelligence reportontheComputerOlympiads, whichincludeawidevarietyofgames. The
General GameCompetition (Love etal.,2006) tests programs that mustlearn toplay anun-
knowngamegivenonlyalogicaldescription oftherulesofthegame. Therearealsoseveral
editedcollections ofimportantpapersongame-playing research(Levy,1988a,1988b;Mars-
land and Schaeffer, 1990). The International Computer Chess Association (ICCA), founded
in 1977, publishes the ICGA Journal (formerly the ICCA Journal). Important papers have
been published in the serial anthology Advances in Computer Chess, starting with Clarke
(1977). Volume 134 of the journal Artificial Intelligence (2002) contains descriptions of
state-of-the-art programs forchess, Othello, Hex, shogi, Go, backgammon, poker, Scrabble,
andothergames. Since1998,abiennial ComputersandGamesconference hasbeenheld.
EXERCISES
5.1 Suppose you have an oracle, OM(s), that correctly predicts the opponent’s move in
any state. Using this, formulate the definition of a game as a (single-agent) search problem.
Describeanalgorithm forfindingtheoptimalmove.
5.2 Considertheproblem ofsolvingtwo8-puzzles.
a. Giveacompleteproblem formulation inthestyleofChapter 3.
b. Howlargeisthereachable statespace? Giveanexactnumericalexpression.
c. Suppose wemake the problem adversarial as follows: the two players take turns mov-
ing;acoinisflippedtodeterminethepuzzleonwhichtomakeamoveinthatturn;and
the winner is the first to solve one puzzle. Which algorithm can be used to choose a
moveinthissetting?
d. Giveaninformalproofthatsomeonewilleventuallywinifbothplayperfectly.
196 Chapter 5. Adversarial Search
e
(a) a b c d
P E
f
bd
cd ad
(b)
ce cf cc ae af ac
? ? ? ? ?
de df
dd dd
Figure5.16 (a)Amapwherethecostofeveryedgeis1.InitiallythepursuerP isatnode
bandtheevaderE isatnoded. (b)Apartialgametreeforthismap. Eachnodeislabeled
withtheP,E positions.P movesfirst. Branchesmarked“?” haveyettobeexplored.
5.3 Imagine that, inExercise 3.3, oneof thefriends wantstoavoid the other. Theproblem
then becomes a two-player pursuit–evasion game. We assume now that the players take
PURSUIT–EVASION
turns moving. The game ends only when the players are on the same node; the terminal
payoff tothepursuer isminusthe totaltimetaken. (Theevader“wins” byneverlosing.) An
exampleisshowninFigure5.16.
a. Copythegametreeandmarkthevaluesoftheterminalnodes.
b. Next to each internal node, write the strongest fact you can infer about its value (a
number,oneormoreinequalities suchas“≥ 14”,ora“?”).
c. Beneatheachquestion mark,writethenameofthenodereached bythatbranch.
d. Explainhowaboundonthevalueofthenodesin(c)canbederivedfromconsideration
ofshortest-pathlengthsonthemap,andderivesuchboundsforthesenodes. Remember
thecosttogettoeachleafaswellasthecosttosolveit.
e. Nowsupposethatthetreeasgiven,withtheleafboundsfrom(d),isevaluatedfromleft
to right. Circle those “?” nodes that would not need to be expanded further, given the
boundsfrompart(d),andcrossoutthosethatneednotbeconsidered atall.
f. Canyouproveanythingingeneralaboutwhowinsthegameonamapthatisatree?
Exercises 197
5.4 Describeandimplementstatedescriptions,movegenerators,terminaltests,utilityfunc-
tions,andevaluationfunctionsforoneormoreofthefollowingstochasticgames: Monopoly,
Scrabble, bridgeplaywithagivencontract, orTexashold’em poker.
5.5 Describe and implement a real-time, multiplayer game-playing environment, where
timeispartoftheenvironment stateandplayersaregivenfixedtimeallocations.
5.6 Discusshowwellthestandardapproachtogameplayingwouldapplytogamessuchas
tennis, pool,andcroquet, whichtakeplaceinacontinuous physicalstatespace.
5.7 Prove the following assertion: Forevery game tree, the utility obtained by MAX using
minimaxdecisions against asuboptimal MIN willbeneverbelowerthan theutility obtained
playing against an optimal MIN. Can you come up with a game tree in which MAX can do
stillbetterusingasuboptimal strategyagainstasuboptimal MIN?
A B
1 2 3 4
Figure5.17 Thestartingpositionofasimplegame.PlayerAmovesfirst.Thetwoplayers
taketurnsmoving,andeachplayermustmovehistokentoanopenadjacentspaceineither
direction. If the opponent occupies an adjacent space, then a player may jump over the
opponenttothenextopenspaceifany.(Forexample,ifAison3andBison2,thenAmay
movebackto1.) Thegameendswhenoneplayerreachestheoppositeendoftheboard. If
player A reaches space 4 first, then the value of the game to A is +1; if player B reaches
space1first,thenthevalueofthegametoAis−1.
5.8 Considerthetwo-playergamedescribed inFigure5.17.
a. Drawthecompletegametree,usingthefollowingconventions:
• Writeeachstateas(s ,s ),wheres ands denotethetokenlocations.
A B A B
• Puteachterminalstateinasquareboxandwriteitsgamevalueinacircle.
• Putloopstates(statesthatalreadyappearonthepathtotheroot)indouble square
boxes. Sincetheirvalueisunclear, annotate eachwitha“?” inacircle.
b. Nowmarkeachnodewithitsbacked-up minimaxvalue(alsoinacircle). Explainhow
youhandledthe“?” valuesandwhy.
c. Explain why the standard minimax algorithm would fail on this game tree and briefly
sketch how you might fixit, drawing on youranswerto(b). Does yourmodified algo-
rithmgiveoptimaldecisions forallgameswithloops?
d. This4-square game can be generalized to n squares forany n > 2. Prove that Awins
ifnisevenandlosesifnisodd.
5.9 This problem exercises the basic concepts of game playing, using tic-tac-toe (noughts
and crosses) as an example. We define X as the number of rows, columns, or diagonals
n
198 Chapter 5. Adversarial Search
with exactly n X’s and no O’s. Similarly, O is the number of rows, columns, ordiagonals
n
withjust nO’s. Theutility function assigns +1toany position with X = 1 and −1toany
3
position withO = 1. Allotherterminal positions have utility 0. Fornonterminal positions,
3
weusealinearevaluationfunctiondefinedasEval(s)= 3X (s)+X (s)−(3O (s)+O (s)).
2 1 2 1
a. Approximately howmanypossible gamesoftic-tac-toe arethere?
b. Show the whole game tree starting from an empty board down to depth 2 (i.e., one X
andoneOontheboard),takingsymmetryintoaccount.
c. Markonyourtreetheevaluations ofallthepositions atdepth2.
d. Usingtheminimaxalgorithm,markonyourtreethebacked-up valuesforthepositions
atdepths1and0,andusethosevaluestochoosethebeststartingmove.
e. Circle the nodes at depth 2 that would not be evaluated if alpha–beta pruning were
applied, assumingthenodesaregenerated intheoptimalorderforalpha–beta pruning.
5.10 Considerthefamilyofgeneralized tic-tac-toe games,definedasfollows. Eachpartic-
ular game is specified by a set S of squares and a collection W of winning positions. Each
winningpositionisasubsetofS. Forexample,instandardtic-tac-toe, S isasetof9squares
andW isacollectionof8subsetsofW: thethreerows,thethreecolumns,andthetwodiag-
onals. Inotherrespects, thegameisidentical tostandard tic-tac-toe. Startingfromanempty
board, players alternate placing their marks on an empty square. A player who marks every
square in a winning position wins the game. It is a tie if all squares are marked and neither
playerhaswon.
a. LetN = |S|, the number of squares. Give an upper bound on the number of nodes in
thecompletegametreeforgeneralized tic-tac-toe asafunction ofN.
b. Givealowerboundonthesizeofthegametreefortheworstcase,whereW ={}.
c. Proposeaplausibleevaluationfunctionthatcanbeusedforanyinstanceofgeneralized
tic-tac-toe. Thefunction maydependonS andW.
d. Assume that it is possible to generate a new board and check whether it is a winning
position in 100N machine instructions and assume a 2 gigahertz processor. Ignore
memory limitations. Using your estimate in (a), roughly how large agame tree can be
completelysolvedbyalpha–beta inasecondofCPUtime? aminute? anhour?
5.11 Developageneralgame-playing program,capableofplaying avarietyofgames.
a. Implement move generators and evaluation functions forone ormore of the following
games: Kalah,Othello, checkers, andchess.
b. Constructageneralalpha–beta game-playing agent.
c. Compare theeffect ofincreasing search depth, improving moveordering, and improv-
ingtheevaluationfunction. Howclosedoesyoureffectivebranchingfactorcometothe
idealcaseofperfectmoveordering?
d. Implementaselectivesearchalgorithm,suchasB*(Berliner,1979),conspiracynumber
search (McAllester, 1988), or MGSS* (Russell and Wefald, 1989) and compare its
performance toA*.
Exercises 199
n
1
n
2
n
j
Figure5.18 Situationwhenconsideringwhethertoprunenodenj.
5.12 Describe how the minimax and alpha–beta algorithms change for two-player, non-
zero-sumgamesinwhicheachplayerhasadistinctutilityfunction andbothutilityfunctions
areknowntobothplayers. Iftherearenoconstraintsonthetwoterminalutilities,isitpossible
for any node to be pruned by alpha–beta? What if the player’s utility functions on any state
differbyatmostaconstant k,makingthegamealmostcooperative?
5.13 Developaformalproofofcorrectness foralpha–betapruning. Todothis,considerthe
situation shown in Figure 5.18. The question is whether to prune node n , which is a max-
j
node and a descendant of node n . The basic idea is to prune it if and only if the minimax
1
valueofn canbeshowntobeindependent ofthevalueofn .
1 j
a. Moden takesontheminimumvalueamongitschildren: n =min(n ,n ,...,n ).
1 1 2 21 2b2
Findasimilarexpression forn andhenceanexpression forn intermsofn .
2 1 j
b. Letl betheminimum(ormaximum)valueofthenodestotheleftofnoden atdepthi,
i i
whoseminimaxvalueisalreadyknown. Similarly,letr betheminimum(ormaximum)
i
valueoftheunexplored nodes totherightof n atdepthi. Rewriteyourexpression for
i
n intermsofthel andr values.
1 i i
c. Nowreformulate the expression to show that in orderto affect n , n must not exceed
1 j
acertainboundderivedfromthe l values.
i
d. Repeattheprocessforthecasewhere n isamin-node.
j
5.14 Provethatalpha–betapruningtakestimeO(2m/2)withoptimalmoveordering,where
misthemaximumdepthofthegametree.
5.15 Suppose you have a chess program that can evaluate 10 million nodes per second.
Decideonacompactrepresentationofagamestateforstorageinatranspositiontable. About
how many entries can you fit in a 2-gigabyte in-memory table? Will that be enough for the
200 Chapter 5. Adversarial Search
0.5 0.5 0.5 0.5
2 2 1 2 0 2 -1 0
Figure5.19 Thecompletegametreeforatrivialgamewithchancenodes.
three minutes of search allocated forone move? Howmany table lookups can you do in the
time it would take to do one evaluation? Now suppose the transposition table is stored on
disk. Abouthowmanyevaluations couldyoudointhetimeittakes todoonediskseekwith
standard diskhardware?
5.16 This question considers pruning in games with chance nodes. Figure 5.19 shows the
completegametreeforatrivialgame. Assumethattheleafnodesaretobeevaluated inleft-
to-rightorder,andthatbeforealeafnodeisevaluated, weknownothingaboutitsvalue—the
rangeofpossible valuesis−∞to∞.
a. Copythe figure, mark thevalue ofall theinternal nodes, and indicate the best moveat
therootwithanarrow.
b. Given the values of the first six leaves, do we need to evaluate the seventh and eighth
leaves? Given the values of the first seven leaves, do we need to evaluate the eighth
leaf? Explainyouranswers.
c. Suppose the leaf node values are known to lie between –2 and 2 inclusive. After the
firsttwoleavesareevaluated, whatisthevaluerangeforthe left-hand chancenode?
d. Circlealltheleavesthatneednotbeevaluatedundertheassumption in(c).
5.17 Implement the expectiminimax algorithm and the *-alpha–beta algorithm, which is
described byBallard(1983), forpruning gametreeswithchance nodes. Trythemonagame
suchasbackgammonandmeasurethepruning effectiveness of *-alpha–beta.
5.18 Prove that with a positive linear transformation of leaf values (i.e., transforming a
valuextoax+bwherea > 0),thechoiceofmoveremainsunchanged inagametree,even
whentherearechancenodes.
5.19 Considerthefollowingprocedure forchoosing movesingameswithchance nodes:
• Generatesomedice-roll sequences (say,50)downtoasuitabledepth(say,8).
• With known dice rolls, the game tree becomes deterministic. For each dice-roll se-
quence,solvetheresultingdeterministic gametreeusingalpha–beta.
Exercises 201
• Usetheresults toestimatethevalueofeachmoveandtochoosethebest.
Willthisprocedure workwell? Why(orwhynot)?
5.20 In the following, a “max” tree consists only of max nodes, whereas an “expectimax”
tree consists of a max node at the root with alternating layers of chance and max nodes. At
chance nodes, alloutcome probabilities arenonzero. Thegoal istofindthevalue oftheroot
withabounded-depth search. Foreachof(a)–(f), eithergiveanexampleorexplainwhythis
isimpossible.
a. Assuming that leaf values are finite but unbounded, is pruning (as in alpha–beta) ever
possibleinamaxtree?
b. Ispruningeverpossible inanexpectimaxtreeunderthesameconditions?
c. If leaf values are all nonnegative, is pruning ever possible in a max tree? Give an
example,orexplainwhynot.
d. Ifleaf values are allnonnegative, ispruning everpossible inanexpectimax tree? Give
anexample,orexplainwhynot.
e. Ifleaf values areallintherange [0,1], ispruning everpossible inamaxtree? Givean
example,orexplainwhynot.
f. Ifleafvaluesareallintherange [0,1],ispruning everpossibleinanexpectimax tree?
g. Considertheoutcomesofachancenodeinanexpectimaxtree. Whichofthefollowing
evaluationordersismostlikelytoyieldpruning opportunities?
(i) Lowestprobability first
(ii) Highestprobability first
(iii) Doesn’tmakeanydifference
5.21 Whichofthefollowingaretrueandwhicharefalse? Givebriefexplanations.
a. Inafully observable, turn-taking, zero-sum gamebetween twoperfectly rational play-
ers, itdoesnot helpthe firstplayer toknowwhatstrategy the second playerisusing—
thatis,whatmovethesecondplayerwillmake,giventhefirstplayer’smove.
b. In a partially observable, turn-taking, zero-sum game between two perfectly rational
players, it does not help the first player to know what move the second player will
make,giventhefirstplayer’s move.
c. Aperfectly rationalbackgammonagentneverloses.
5.22 Considercarefullytheinterplayofchanceeventsandpartialinformationineachofthe
gamesinExercise5.4.
a. Forwhichisthestandardexpectiminimaxmodelappropriate? Implementthealgorithm
and run it in your game-playing agent, with appropriate modifications to the game-
playingenvironment.
b. Forwhichwouldtheschemedescribed inExercise5.19beappropriate?
c. Discusshowyoumightdealwiththefactthatinsomeofthegames,theplayers donot
havethesameknowledgeofthecurrentstate.
6
CONSTRAINT
SATISFACTION PROBLEMS
Inwhichweseehowtreatingstatesasmorethanjustlittleblackboxesleadstothe
inventionofarangeofpowerfulnewsearchmethodsandadeeperunderstanding
ofproblem structureandcomplexity.
Chapters 3 and 4 explored the idea that problems can be solved by searching in a space of
states. Thesestatescanbeevaluated bydomain-specific heuristics andtestedtoseewhether
they are goal states. From the point of view of the search algorithm, however, each state is
atomic,orindivisible—a blackboxwithnointernalstructure.
This chapter describes away to solve a wide variety of problems more efficiently. We
use a factored representation for each state: a set of variables, each of which has a value.
A problem is solved when each variable has a value that satisfies all the constraints on the
CONSTRAINT
variable. Aproblem described thiswayiscalleda constraintsatisfaction problem,orCSP.
SATISFACTION
PROBLEM
CSPsearchalgorithmstakeadvantageofthestructureofstatesandusegeneral-purpose
ratherthanproblem-specific heuristicstoenablethesolutionofcomplexproblems. Themain
ideaistoeliminatelargeportionsofthesearchspaceallatoncebyidentifyingvariable/value
combinations thatviolatetheconstraints.
6.1 DEFINING CONSTRAINT SATISFACTION PROBLEMS
Aconstraint satisfaction problemconsists ofthreecomponents, X,D,andC:
X isasetofvariables, {X ,...,X }.
1 n
Disasetofdomains,{D ,...,D },oneforeachvariable.
1 n
C isasetofconstraints thatspecifyallowablecombinations ofvalues.
Each domain D consists of a set of allowable values, {v ,...,v } for variable X . Each
i 1 k i
constraintC consistsofapair(cid:16)scope,rel(cid:17),wherescope isatupleofvariablesthatparticipate
i
intheconstraintandrel isarelationthatdefinesthevaluesthatthosevariablescantakeon. A
relationcanberepresented asanexplicitlistofalltuples ofvaluesthatsatisfytheconstraint,
or as an abstract relation that supports two operations: testing if a tuple is a member of the
relation andenumerating themembersoftherelation. Forexample, ifX and X bothhave
1 2
202
Section6.1. DefiningConstraintSatisfaction Problems 203
the domain {A,B}, then the constraint saying the two variables must have different values
canbewrittenas(cid:16)(X ,X ),[(A,B),(B,A)](cid:17) oras(cid:16)(X ,X ),X (cid:7)= X (cid:17).
1 2 1 2 1 2
To solve a CSP, we need to define a state space and the notion of a solution. Each
state in a CSP is defined by an assignment of values to some or all of the variables, {X =
ASSIGNMENT i
v ,X = v ,...}. Anassignment that does not violate anyconstraints iscalled aconsistent
CONSISTENT i j j
COMPLETE orlegalassignment. Acompleteassignmentisoneinwhicheveryvariable isassigned, and
ASSIGNMENT
a solution to a CSP is a consistent, complete assignment. A partial assignment is one that
SOLUTION
PARTIAL assignsvaluestoonlysomeofthevariables.
ASSIGNMENT
6.1.1 Exampleproblem: Mapcoloring
Suppose that, having tired of Romania, we are looking at a map of Australia showing each
of its states and territories (Figure 6.1(a)). We are given the task of coloring each region
either red, green, orblue in such away that no neighboring regions have the samecolor. To
formulatethisasaCSP,wedefinethevariables tobetheregions
X = {WA,NT,Q,NSW,V,SA,T}.
The domain of each variable is the set D = {red,green,blue}. The constraints require
i
neighboring regionstohavedistinctcolors. Sincetherearenineplaceswhereregionsborder,
therearenineconstraints:
C = {SA (cid:7)= WA,SA (cid:7)= NT,SA (cid:7)= Q,SA (cid:7)= NSW,SA (cid:7)= V,
WA (cid:7)= NT,NT (cid:7)= Q,Q(cid:7)= NSW,NSW (cid:7)= V}.
Hereweareusingabbreviations; SA (cid:7)= WAisashortcutfor(cid:16)(SA,WA),SA (cid:7)= WA(cid:17),where
SA (cid:7)= WAcanbefullyenumerated inturnas
{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}.
Therearemanypossible solutions tothisproblem, suchas
{WA=red,NT =green,Q=red,NSW =green,V =red,SA=blue,T =red }.
It can be helpful to visualize a CSP as a constraint graph, as shown in Figure 6.1(b). The
CONSTRAINTGRAPH
nodes ofthegraph correspond tovariables oftheproblem, andalink connects anytwovari-
ablesthatparticipate inaconstraint.
Why formulate a problem as a CSP? One reason is that the CSPs yield a natural rep-
resentation for a wide variety of problems; if you already have a CSP-solving system, it is
ofteneasiertosolveaproblemusingitthantodesignacustomsolutionusinganothersearch
technique. Inaddition, CSPsolverscanbefasterthanstate-space searchers because theCSP
solver canquickly eliminate large swatches ofthe search space. Forexample, once wehave
chosen{SA=blue}intheAustraliaproblem,wecanconcludethatnoneofthefiveneighbor-
ingvariablescantakeonthevalueblue. Withouttakingadvantageofconstraintpropagation,
a search procedure would have to consider 35=243 assignments for the five neighboring
variables; withconstraint propagation weneverhavetoconsider blue asavalue, sowehave
only25=32assignments tolookat,areduction of87%.
In regular state-space search we can only ask: is this specific state a goal? No? What
aboutthisone? WithCSPs,oncewefindoutthatapartialassignmentisnotasolution,wecan
204 Chapter 6. ConstraintSatisfaction Problems
NT
Q
WA
Northern
Territory
Queensland
Western SA NSW
Australia
South
Australia New
South V
Wales
Victoria
T
Tasmania
(a) (b)
Figure 6.1 (a) The principal states and territories of Australia. Coloring this map can
be viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each
regionso that no neighboringregionshave the same color. (b) The map-coloringproblem
representedasaconstraintgraph.
immediately discard further refinements of the partial assignment. Furthermore, we can see
whytheassignmentisnotasolution—weseewhichvariablesviolateaconstraint—sowecan
focus attention on the variables that matter. As a result, many problems that are intractable
forregularstate-space searchcanbesolvedquickly whenformulated asaCSP.
6.1.2 Exampleproblem: Job-shopscheduling
Factorieshavetheproblemofschedulingaday’sworthofjobs,subjecttovariousconstraints.
Inpractice,manyoftheseproblemsaresolvedwithCSPtechniques. Considertheproblemof
schedulingtheassemblyofacar. Thewholejobiscomposedoftasks,andwecanmodeleach
task as avariable, where the value of each variable isthe time that the task starts, expressed
as an integer number of minutes. Constraints can assert that one task must occur before
another—for example, awheel must beinstalled before the hubcap isput on—and that only
so many tasks can go on at once. Constraints can also specify that a task takes a certain
amountoftimetocomplete.
Weconsiderasmallpartofthecarassembly, consisting of15 tasks: installaxles(front
and back), affix all four wheels (right and left, front and back), tighten nuts for each wheel,
affixhubcaps, andinspect thefinalassembly. Wecanrepresent thetaskswith15variables:
X = {Axle ,Axle ,Wheel ,Wheel ,Wheel ,Wheel ,Nuts ,
F B RF LF RB LB RF
Nuts ,Nuts ,Nuts ,Cap ,Cap ,Cap ,Cap ,Inspect}.
LF RB LB RF LF RB LB
The value of each variable is the time that the task starts. Next we represent precedence
PRECEDENCE constraints between individual tasks. Whenever a task T must occur before task T , and
CONSTRAINTS 1 2
taskT takesduration d tocomplete,weaddanarithmeticconstraint oftheform
1 1
T +d ≤ T .
1 1 2
Section6.1. DefiningConstraintSatisfaction Problems 205
In our example, the axles have to be in place before the wheels are put on, and it takes 10
minutestoinstallanaxle,sowewrite
Axle +10 ≤ Wheel ; Axle +10 ≤Wheel ;
F RF F LF
Axle +10 ≤ Wheel ; Axle +10 ≤ Wheel .
B RB B LB
Nextwesaythat,foreachwheel,wemustaffixthewheel(whichtakes1minute),thentighten
thenuts(2minutes),andfinallyattachthehubcap(1minute,butnotrepresented yet):
Wheel +1 ≤ Nuts ; Nuts +2 ≤ Cap ;
RF RF RF RF
Wheel +1≤ Nuts ; Nuts +2 ≤ Cap ;
LF LF LF LF
Wheel +1 ≤ Nuts ; Nuts +2 ≤ Cap ;
RB RB RB RB
Wheel +1 ≤Nuts ; Nuts +2≤ Cap .
LB LB LB LB
Supposewehavefourworkerstoinstallwheels,buttheyhavetoshareonetoolthathelpsput
DISJUNCTIVE the axle in place. We need a disjunctive constraint to say that Axle and Axle must not
CONSTRAINT F B
overlapintime;eitheronecomesfirstortheotherdoes:
(Axle +10 ≤ Axle ) or (Axle +10 ≤ Axle ).
F B B F
This looks like a more complicated constraint, combining arithmetic and logic. But it still
reducestoasetofpairsofvaluesthat Axle andAxle cantakeon.
F F
We also need to assert that the inspection comes last and takes 3 minutes. For every
variableexceptInspect weaddaconstraintoftheform X+d ≤ Inspect. Finally,suppose
X
thereisarequirement togetthewholeassembly donein30minutes. Wecanachievethatby
limitingthedomainofallvariables:
D = {1,2,3,...,27}.
i
This particular problem is trivial to solve, but CSPs have been applied to job-shop schedul-
ing problems like this with thousands of variables. In some cases, there are complicated
constraints that are difficult to specify in the CSP formalism, and more advanced planning
techniques areused,asdiscussed inChapter11.
6.1.3 Variationsonthe CSP formalism
The simplest kind of CSP involves variables that have discrete, finite domains. Map-
DISCRETEDOMAIN
coloring problems andscheduling withtimelimitsarebothofthiskind. The8-queens prob-
FINITEDOMAIN
lem described in Chapter 3 can also be viewed as a finite-domain CSP, where the variables
Q ,...,Q are the positions of each queen in columns 1,...,8 and each variable has the
1 8
domainD = {1,2,3,4,5,6,7,8}.
i
Adiscretedomaincanbeinfinite,suchasthesetofintegersorstrings. (Ifwedidn’tput
INFINITE
a deadline on the job-scheduling problem, there would be an infinite number of start times
for each variable.) With infinite domains, it is no longer possible to describe constraints by
CONSTRAINT enumerating all allowed combinations of values. Instead, a constraint language must be
LANGUAGE
used that understands constraints such as T + d ≤ T directly, without enumerating the
1 1 2
set of pairs of allowable values for (T ,T ). Special solution algorithms (which we do not
1 2
LINEAR discuss here) exist for linear constraints on integer variables—that is, constraints, such as
CONSTRAINTS
the one just given, in which each variable appears only in linear form. It can be shown that
NONLINEAR noalgorithm existsforsolving general nonlinearconstraintsonintegervariables.
CONSTRAINTS
206 Chapter 6. ConstraintSatisfaction Problems
CONTINUOUS Constraint satisfaction problems with continuous domains are common in the real
DOMAINS
worldandarewidelystudiedinthefieldofoperations research. Forexample,thescheduling
ofexperiments on theHubble Space Telescope requires very precise timingofobservations;
the start and finish of each observation and maneuver are continuous-valued variables that
must obey a variety of astronomical, precedence, and power constraints. The best-known
category of continuous-domain CSPs is that of linear programming problems, where con-
straintsmustbelinearequalitiesorinequalities. Linearprogrammingproblemscanbesolved
in time polynomial in the number of variables. Problems with different types of constraints
andobjectivefunctions havealsobeenstudied—quadratic programming, second-order conic
programming, andsoon.
In addition to examining the types of variables that can appear in CSPs, it is useful to
look at the types of constraints. The simplest type is the unary constraint, which restricts
UNARYCONSTRAINT
thevalueofasinglevariable. Forexample,inthemap-coloring problem itcouldbethecase
that South Australians won’t tolerate the color green; we can express that with the unary
constraint (cid:16)(SA),SA (cid:7)= green(cid:17)
A binary constraint relates two variables. For example, SA (cid:7)= NSW is a binary
BINARYCONSTRAINT
constraint. A binary CSP is one with only binary constraints; it can be represented as a
constraint graph, asinFigure6.1(b).
We can also describe higher-order constraints, such as asserting that the value of Y is
betweenX andZ,withtheternaryconstraint Between(X,Y,Z).
GLOBAL A constraint involving an arbitrary number of variables is called a global constraint.
CONSTRAINT
(Thenameistraditional butconfusing becauseitneednotinvolveallthevariablesinaprob-
lem). One of the most common global constraints is Alldiff, which says that all of the
variables involved in the constraint must have different values. In Sudoku problems (see
Section 6.2.6), all variables in a row or column must satisfy an Alldiff constraint. An-
other example isprovided by cryptarithmetic puzzles. (See Figure 6.2(a).) Each letter in a
CRYPTARITHMETIC
cryptarithmetic puzzle represents a different digit. For the case in Figure 6.2(a), this would
be represented as the global constraint Alldiff(F,T,U,W,R,O). The addition constraints
onthefourcolumnsofthepuzzlecanbewrittenasthefollowingn-aryconstraints:
O+O = R+10·C
10
C +W +W = U +10·C
10 100
C +T +T = O+10·C
100 1000
C = F ,
1000
whereC ,C ,andC areauxiliaryvariablesrepresentingthedigitcarriedoverintothe
10 100 1000
tens, hundreds, or thousands column. These constraints can be represented in a constraint
CONSTRAINT hypergraph,suchastheoneshowninFigure6.2(b). Ahypergraphconsistsofordinarynodes
HYPERGRAPH
(thecirclesinthefigure)andhypernodes (thesquares), whichrepresent n-aryconstraints.
Alternatively, as Exercise 6.6 asks you to prove, every finite-domain constraint can be
reducedtoasetofbinaryconstraintsifenoughauxiliaryvariablesareintroduced,sowecould
transform anyCSPinto one withonly binary constraints; thismakes the algorithms simpler.
Anotherwaytoconvertann-aryCSPtoabinaryoneisthedualgraphtransformation: create
DUALGRAPH
anewgraphinwhichtherewillbeonevariableforeachconstraint intheoriginal graph, and
Section6.1. DefiningConstraintSatisfaction Problems 207
T W O F T U W R O
+ T W O
F O U R
C C C
3 2 1
(a) (b)
Figure6.2 (a)Acryptarithmeticproblem.Eachletterstandsforadistinctdigit;theaimis
tofindasubstitutionofdigitsforletterssuchthattheresultingsumisarithmeticallycorrect,
withtheaddedrestrictionthatnoleadingzeroesareallowed. (b)Theconstrainthypergraph
for the cryptarithmetic problem, showing the Alldiff constraint (square box at the top) as
wellasthecolumnadditionconstraints(foursquareboxesinthemiddle). ThevariablesC ,
1
C ,andC representthecarrydigitsforthethreecolumns.
2 3
onebinaryconstraintforeachpairofconstraintsintheoriginalgraphthatsharevariables. For
example, if the original graph has variables {X,Y,Z} and constraints (cid:16)(X,Y,Z),C (cid:17) and
1
(cid:16)(X,Y),C (cid:17) then the dual graph would have variables {C ,C } with the binary constraint
2 1 2
(cid:16)(X,Y),R (cid:17),where(X,Y)arethesharedvariablesand R isanewrelationthatdefinesthe
1 1
constraint betweenthesharedvariables, asspecifiedbythe original C andC .
1 2
TherearehowevertworeasonswhywemightpreferaglobalconstraintsuchasAlldiff
rather than a set of binary constraints. First, it is easier and less error-prone to write the
problem description using Alldiff. Second,itispossible todesignspecial-purpose inference
algorithmsforglobalconstraintsthatarenotavailableforasetofmoreprimitiveconstraints.
Wedescribetheseinference algorithmsinSection6.2.5.
Theconstraintswehavedescribedsofarhaveallbeenabsoluteconstraints, violationof
PREFERENCE which rules out apotential solution. Many real-world CSPsinclude preference constraints
CONSTRAINTS
indicating whichsolutions arepreferred. Forexample,ina university class-scheduling prob-
lem there are absolute constraints that no professor can teach two classes at the same time.
Butwealsomayallowpreference constraints: Prof.Rmightpreferteaching inthemorning,
whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at
2p.m.wouldstillbeanallowablesolution(unlessProf.Rhappenstobethedepartmentchair)
butwouldnotbeanoptimalone. Preference constraints canoftenbeencoded ascostsonin-
dividual variable assignments—for example, assigning an afternoon slot for Prof. R costs
2 points against the overall objective function, whereas a morning slot costs 1. With this
formulation, CSPs with preferences can be solved with optimization search methods, either
CONSTRAINT
path-based or local. We call such a problem a constraint optimization problem, or COP.
OPTIMIZATION
PROBLEM
Linearprogramming problemsdothiskindofoptimization.
208 Chapter 6. ConstraintSatisfaction Problems
6.2 CONSTRAINT PROPAGATION: INFERENCE IN CSPS
Inregular state-space search, analgorithm can doonly one thing: search. In CSPsthere is a
choice: analgorithmcansearch(chooseanewvariableassignmentfromseveralpossibilities)
or do a specific type of inference called constraint propagation: using the constraints to
INFERENCE
CONSTRAINT reduce the number of legal values for a variable, which in turn can reduce the legal values
PROPAGATION
foranothervariable, andsoon. Constraint propagation may beintertwined withsearch, orit
maybedoneasapreprocessing step, before searchstarts. Sometimesthispreprocessing can
solvethewholeproblem,sonosearchisrequired atall.
LOCAL The key idea is local consistency. If we treat each variable as a node in a graph (see
CONSISTENCY
Figure 6.1(b)) and each binary constraint as an arc, then the process of enforcing local con-
sistency ineach part of the graph causes inconsistent values tobe eliminated throughout the
graph. Therearedifferent typesoflocalconsistency, whichwenowcoverinturn.
6.2.1 Nodeconsistency
A single variable (corresponding to a node in the CSP network) is node-consistent if all
NODECONSISTENCY
the values in the variable’s domain satisfy the variable’s unary constraints. For example,
in the variant of the Australia map-coloring problem (Figure 6.1) where South Australians
dislike green, the variable SA starts with domain {red,green,blue}, and we can make it
node consistent byeliminating green,leaving SAwiththe reduced domain {red,blue}. We
saythatanetworkisnode-consistent ifeveryvariableinthenetworkisnode-consistent.
It is always possible to eliminate all the unary constraints in a CSP by running node
consistency. It is also possible to transform all n-ary constraints into binary ones (see Ex-
ercise 6.6). Because of this, it is common to define CSP solvers that work with only binary
constraints; wemakethatassumption fortherestofthischapter, exceptwherenoted.
6.2.2 Arcconsistency
A variable in a CSP is arc-consistent if every value in its domain satisfies the variable’s
ARCCONSISTENCY
binaryconstraints. Moreformally, X isarc-consistent withrespecttoanothervariable X if
i j
forevery value in the current domain D there is some value in the domain D that satisfies
i j
thebinary constraint onthearc (X ,X ). Anetworkisarc-consistent ifeveryvariable isarc
i j
consistentwitheveryothervariable. Forexample,considertheconstraint Y =X2 wherethe
domainofbothX andY isthesetofdigits. Wecanwritethisconstraint explicitly as
(cid:16)(X,Y),{(0,0),(1,1),(2,4),(3,9))}(cid:17) .
To make X arc-consistent with respect to Y, we reduce X’s domain to {0,1,2,3}. If we
alsomakeY arc-consistent withrespect to X,thenY’sdomainbecomes {0,1,4,9} andthe
wholeCSPisarc-consistent.
Ontheotherhand,arcconsistency candonothingfortheAustraliamap-coloring prob-
lem. Considerthefollowinginequality constraint on(SA,WA):
{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}.
Section6.2. ConstraintPropagation: InferenceinCSPs 209
functionAC-3(csp)returnsfalseifaninconsistencyisfoundandtrueotherwise
inputs:csp,abinaryCSPwithcomponents(X, D, C)
localvariables: queue,aqueueofarcs,initiallyallthearcsincsp
whilequeue isnotemptydo
(Xi, Xj)←REMOVE-FIRST(queue)
ifREVISE(csp,Xi, Xj)then
ifsizeofDi = 0thenreturnfalse
foreachXk inXi.NEIGHBORS-{Xj }do
add(Xk, Xi)toqueue
returntrue
functionREVISE(csp,Xi, Xj)returnstrueiffwerevisethedomainofXi
revised←false
foreachx inDido
ifnovalueyinDj allows(x,y)tosatisfytheconstraintbetweenXiandXj then
deletex fromDi
revised←true
returnrevised
Figure 6.3 The arc-consistency algorithm AC-3. After applying AC-3, either every arc
isarc-consistent,orsomevariablehasanemptydomain,indicatingthattheCSP cannotbe
solved. Thename“AC-3”wasusedbythealgorithm’sinventor(Mackworth,1977)because
it’sthethirdversiondevelopedinthepaper.
No matter what value you choose for SA (or for WA), there is a valid value for the other
variable. Soapplying arcconsistency hasnoeffectonthedomainsofeithervariable.
The most popular algorithm for arc consistency is called AC-3 (see Figure 6.3). To
makeeveryvariablearc-consistent, theAC-3algorithmmaintainsaqueueofarcstoconsider.
(Actually, theorderofconsideration isnotimportant, sothedatastructure isreallyaset, but
traditioncallsitaqueue.) Initially,thequeuecontainsallthearcsintheCSP.AC-3thenpops
offanarbitraryarc (X ,X )fromthequeueandmakesX arc-consistent withrespecttoX .
i j i j
If this leaves D unchanged, the algorithm just moves on to the next arc. But if this revises
i
D (makes the domain smaller), then we add to the queue all arcs (X ,X ) where X is a
i k i k
neighborofX . WeneedtodothatbecausethechangeinD mightenablefurtherreductions
i i
in the domains of D , even if we have previously considered X . If D is revised down to
k k i
nothing, thenweknowthewholeCSPhasnoconsistentsolution, andAC-3canimmediately
return failure. Otherwise, we keep checking, trying to remove values from the domains of
variables until no more arcs are in the queue. At that point, we are left with a CSP that is
equivalent to the original CSP—they both have the same solutions—but the arc-consistent
CSPwillinmostcasesbefastertosearchbecauseitsvariables havesmallerdomains.
The complexity of AC-3 can be analyzed as follows. Assume a CSP with n variables,
eachwithdomainsizeatmostd,andwithcbinaryconstraints (arcs). Eacharc (X ,X )can
k i
be inserted in the queue only d times because X has at most d values to delete. Checking
i
210 Chapter 6. ConstraintSatisfaction Problems
consistency ofanarccanbedoneinO(d2)time,sowegetO(cd3)totalworst-casetime.1
It is possible to extend the notion of arc consistency to handle n-ary rather than just
binary constraints; this is called generalized arc consistency or sometimes hyperarc consis-
GENERALIZEDARC tency, depending onthe author. A variable X is generalized arc consistent with respect to
CONSISTENT i
ann-aryconstraint ifforeveryvalue v inthedomainofX thereexistsatupleofvaluesthat
i
isamemberoftheconstraint, hasallitsvalues takenfromthedomains ofthecorresponding
variables, and has its X component equal to v. For example, if all variables have the do-
i
main {0,1,2,3}, then to make the variable X consistent with the constraint X < Y < Z,
wewould have to eliminate 2 and 3from the domain of X because the constraint cannot be
satisfiedwhenX is2or3.
6.2.3 Pathconsistency
Arc consistency can go a long way toward reducing the domains of variables, sometimes
finding a solution (by reducing every domain to size 1) and sometimes finding that the CSP
cannotbesolved(byreducingsomedomaintosize0). Butforothernetworks,arcconsistency
fails to make enough inferences. Consider the map-coloring problem on Australia, but with
onlytwocolorsallowed,redandblue. Arcconsistencycandonothingbecauseeveryvariable
isalreadyarcconsistent: eachcanberedwithblueattheotherendofthearc(orviceversa).
Butclearlythereisnosolutiontotheproblem: becauseWesternAustralia,NorthernTerritory
andSouthAustraliaalltoucheachother,weneedatleastthreecolorsforthemalone.
Arc consistency tightens down the domains (unary constraints) using the arcs (binary
constraints). Tomakeprogress onproblems like mapcoloring, weneed astronger notion of
consistency. Path consistency tightens the binary constraints by using implicit constraints
PATHCONSISTENCY
thatareinferredbylookingattriplesofvariables.
A two-variable set {X ,X } is path-consistent with respect to a third variable X if,
i j m
foreveryassignment {X = a,X = b}consistent withtheconstraints on{X ,X },thereis
i j i j
anassignmenttoX thatsatisfiestheconstraintson{X ,X }and{X ,X }. Thisiscalled
m i m m j
path consistency because one can think of it aslooking at apath from X to X withX in
i j m
themiddle.
Let’sseehowpathconsistency faresincoloring theAustraliamapwithtwocolors. We
willmaketheset{WA,SA}pathconsistentwithrespecttoNT. Westartbyenumeratingthe
consistent assignments totheset. Inthis case, there areonly two: {WA = red,SA = blue}
and {WA = blue,SA = red}. We can see that with both of these assignments NT can be
neither red nor blue (because it would conflict with either WA or SA). Because there is no
validchoiceforNT,weeliminatebothassignments,andweendupwithnovalidassignments
for{WA,SA}. Therefore, weknow that there canbenosolution tothisproblem. ThePC-2
algorithm (Mackworth, 1977) achieves path consistency in much the same way that AC-3
achievesarcconsistency. Becauseitissosimilar, wedonot showithere.
1 TheAC-4algorithm(MohrandHenderson,1986)runsinO(cd2)worst-casetimebutcanbeslowerthanAC-3
onaveragecases.SeeExercise6.13.
Section6.2. ConstraintPropagation: InferenceinCSPs 211
6.2.4 K-consistency
Stronger forms of propagation can be defined with the notion of k-consistency. A CSP is
K-CONSISTENCY
k-consistent if, for any set of k − 1 variables and for any consistent assignment to those
variables, a consistent value can always be assigned to any kth variable. 1-consistency says
that, given the empty set, we can make any set of one variable consistent: this is what we
called node consistency. 2-consistency is the same as arc consistency. Forbinary constraint
networks, 3-consistency isthesameaspathconsistency.
STRONGLY A CSP is strongly k-consistent if it is k-consistent and is also (k − 1)-consistent,
K-CONSISTENT
(k −2)-consistent, ... all the way down to 1-consistent. Now suppose we have a CSPwith
n nodes and make it strongly n-consistent (i.e., strongly k-consistent for k=n). We can
then solve the problem as follows: First, we choose a consistent value for X . We are then
1
guaranteed to be able to choose a value for X because the graph is 2-consistent, for X
2 3
becauseitis3-consistent, andsoon. ForeachvariableX ,weneedonlysearchthroughthed
i
valuesinthedomaintofindavalueconsistent withX 1 ,...,X i−1 . Weareguaranteed tofind
a solution in time O(n2d). Ofcourse, there is no free lunch: any algorithm for establishing
n-consistency must take time exponential in n in the worst case. Worse, n-consistency also
requires space thatisexponential in n. Thememoryissue isevenmoreseverethanthetime.
Inpractice, determining theappropriate levelofconsistency checking ismostlyanempirical
science. It can be said practitioners commonly compute 2-consistency and less commonly
3-consistency.
6.2.5 Globalconstraints
Rememberthataglobalconstraintisoneinvolvinganarbitrarynumberofvariables(butnot
necessarily all variables). Global constraints occur frequently in real problems and can be
handledbyspecial-purpose algorithmsthataremoreefficientthanthegeneral-purpose meth-
ods described so far. Forexample, the Alldiff constraint says that all the variables involved
must have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles be-
low). One simple form of inconsistency detection for Alldiff constraints works as follows:
ifmvariables areinvolved inthe constraint, and iftheyhave npossible distinct values alto-
gether, andm > n,thentheconstraint cannotbesatisfied.
This leads to the following simple algorithm: First, remove any variable in the con-
straint that has a singleton domain, and delete that variable’s value from the domains of the
remainingvariables. Repeataslongastherearesingleton variables. Ifatanypointanempty
domainisproducedortherearemorevariablesthandomainvaluesleft,thenaninconsistency
hasbeendetected.
Thismethodcandetect theinconsistency intheassignment {WA=red,NSW =red}
for Figure 6.1. Notice that the variables SA, NT, and Q are effectively connected by an
Alldiff constraint because each pair must have two different colors. After applying AC-3
with the partial assignment, the domain of each variable is reduced to {green,blue}. That
is, we have three variables and only two colors, so the Alldiff constraint is violated. Thus,
a simple consistency procedure for a higher-order constraint is sometimes more effective
than applying arc consistency to an equivalent set of binary constraints. There are more
212 Chapter 6. ConstraintSatisfaction Problems
complex inference algorithms for Alldiff (see van Hoeve and Katriel, 2006) that propagate
moreconstraints butaremorecomputationally expensiveto run.
RESOURCE Anotherimportanthigher-orderconstraintisthe resourceconstraint,sometimescalled
CONSTRAINT
the atmost constraint. For example, in a scheduling problem, let P ,...,P denote the
1 4
numbers of personnel assigned to each of four tasks. The constraint that no more than 10
personnel are assigned in total is written as Atmost(10,P ,P ,P ,P ). We can detect an
1 2 3 4
inconsistency simply by checking the sum of the minimum values of the current domains;
for example, if each variable has the domain {3,4,5,6}, the Atmost constraint cannot be
satisfied. Wecanalsoenforceconsistencybydeletingthemaximumvalueofanydomainifit
isnotconsistent withtheminimumvaluesoftheotherdomains. Thus,ifeachvariableinour
examplehasthedomain{2,3,4,5,6}, thevalues5and6canbedeletedfromeachdomain.
For large resource-limited problems with integer values—such as logistical problems
involving moving thousands of people in hundreds of vehicles—it is usually not possible to
representthedomainofeachvariableasalargesetofintegersandgraduallyreducethatsetby
consistency-checking methods. Instead, domainsarerepresented byupperandlowerbounds
BOUNDS and are managed by bounds propagation. For example, in an airline-scheduling problem,
PROPAGATION
let’s suppose there are two flights, F and F , for which the planes have capacities 165 and
1 2
385,respectively. Theinitialdomainsforthenumbersofpassengers oneachflightarethen
D = [0,165] and D = [0,385].
1 2
Now suppose we have the additional constraint that the two flights together must carry 420
people: F +F = 420. Propagating boundsconstraints, wereducethedomainsto
1 2
D = [35,165] and D = [255,385].
1 2
BOUNDS We say that a CSP is bounds consistent if for every variable X, and for both the lower-
CONSISTENT
boundandupper-bound valuesofX,thereexistssomevalueofY thatsatisfiestheconstraint
between X and Y for every variable Y. This kind of bounds propagation is widely used in
practical constraint problems.
6.2.6 Sudoku example
ThepopularSudokupuzzlehasintroducedmillionsofpeopletoconstraintsatisfactionprob-
SUDOKU
lems, although they may not recognize it. A Sudoku board consists of 81 squares, some of
which are initially filled with digits from 1 to 9. The puzzle is to fill in all the remaining
squaressuchthatnodigitappearstwiceinanyrow,column,or3×3box(seeFigure6.4). A
row,column,orboxiscalledaunit.
TheSudokupuzzles thatareprintedinnewspapers andpuzzle bookshavetheproperty
that thereisexactly onesolution. Although somecanbetricky tosolve byhand, taking tens
ofminutes, eventhehardest SudokuproblemsyieldtoaCSPsolverinlessthan0.1second.
ASudoku puzzle can beconsidered a CSPwith 81 variables, one foreach square. We
usethevariable namesA1 through A9 forthetoprow(lefttoright), downto I1 through I9
forthebottom row. Theemptysquares have the domain {1,2,3,4,5,6,7,8,9} and the pre-
filled squares have a domain consisting of a single value. In addition, there are 27 different
Section6.2. ConstraintPropagation: InferenceinCSPs 213
1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9
A 3 2 6 A 4 8 3 9 2 1 6 5 7
B 9 3 5 1 B 9 6 7 3 4 5 8 2 1
C 1 8 6 4 C 2 5 1 8 7 6 4 9 3
D 8 1 2 9 D 5 4 8 1 3 2 9 7 6
E 7 8 E 7 2 9 5 6 4 1 3 8
F 6 7 8 2 F 1 3 6 7 9 8 2 4 5
G 2 6 9 5 G 3 7 2 6 8 9 5 1 4
H 8 2 3 9 H 8 1 4 2 5 3 7 6 9
I 5 1 3 I 6 9 5 4 1 7 3 8 2
(a) (b)
Figure6.4 (a)ASudokupuzzleand(b)itssolution.
Alldiff constraints: oneforeachrow,column, andboxof9squares.
Alldiff(A1,A2,A3,A4,A5,A6,A7,A8,A9)
Alldiff(B1,B2,B3,B4,B5,B6,B7,B8,B9)
···
Alldiff(A1,B1,C1,D1,E1,F1,G1,H1,I1)
Alldiff(A2,B2,C2,D2,E2,F2,G2,H2,I2)
···
Alldiff(A1,A2,A3,B1,B2,B3,C1,C2,C3)
Alldiff(A4,A5,A6,B4,B5,B6,C4,C5,C6)
···
Letusseehowfararcconsistencycantakeus. AssumethattheAlldiff constraintshavebeen
expandedintobinaryconstraints(suchasA1 (cid:7)= A2)sothatwecanapplytheAC-3algorithm
directly. Consider variable E6 from Figure 6.4(a)—the empty square between the 2 and the
8inthemiddlebox. Fromtheconstraintsinthebox,wecanremovenotonly2and8butalso
1 and 7 from E6’s domain. From the constraints in its column, we can eliminate 5, 6, 2, 8,
9,and3. Thatleaves E6 withadomainof{4};inotherwords, weknowtheanswerfor E6.
Now consider variable I6—the square in the bottom middle box surrounded by 1, 3, and 3.
Applyingarcconsistencyinitscolumn,weeliminate5,6,2,4(sincewenowknowE6 must
be 4), 8, 9, and 3. We eliminate 1 by arc consistency with I5, and we are left with only the
value 7 inthe domain of I6. Now there are 8known values in column 6, so arc consistency
caninferthatA6 mustbe1. Inference continues along theselines, andeventually, AC-3can
solve the entire puzzle—all the variables have their domains reduced to a single value, as
showninFigure6.4(b).
Of course, Sudoku would soon lose its appeal if every puzzle could be solved by a
214 Chapter 6. ConstraintSatisfaction Problems
mechanicalapplicationofAC-3,andindeedAC-3worksonlyfortheeasiestSudokupuzzles.
Slightly harder ones can be solved by PC-2, but at a greater computational cost: there are
255,960differentpathconstraintstoconsiderinaSudokupuzzle. Tosolvethehardestpuzzles
andtomakeefficientprogress, wewillhavetobemoreclever.
Indeed,theappealofSudokupuzzlesforthehumansolveristheneedtoberesourceful
in applying more complex inference strategies. Aficionados give them colorful names, such
as “naked triples.” That strategy works as follows: in any unit (row, column or box), find
three squares that each have a domain that contains the same three numbers or a subset of
those numbers. Forexample, thethree domains mightbe {1,8}, {3,8}, and {1,3,8}. From
that wedon’t know which square contains 1,3, or8, but wedoknow that thethree numbers
must be distributed among the three squares. Therefore we can remove 1, 3, and 8 from the
domainsofeveryothersquareintheunit.
It is interesting to note how far we can go without saying much that is specific to Su-
doku. Wedoofcoursehavetosaythatthereare81variables,thattheirdomainsarethedigits
1to9,andthatthereare27 Alldiff constraints. Butbeyond that, allthestrategies—arc con-
sistency, path consistency, etc.—apply generally to all CSPs, not just to Sudoku problems.
Even naked triples is really a strategy for enforcing consistency of Alldiff constraints and
hasnothingtodowithSudokuperse. ThisisthepoweroftheCSPformalism: foreachnew
problem area, we only need to define the problem in terms of constraints; then the general
constraint-solving mechanismscantakeover.
6.3 BACKTRACKING SEARCH FOR CSPS
Sudoku problems are designed to be solved by inference over constraints. But many other
CSPs cannot be solved by inference alone; there comes a time when we must search for a
solution. In this section we look at backtracking search algorithms that work on partial as-
signments;inthenextsectionwelookatlocalsearchalgorithmsovercompleteassignments.
We could apply a standard depth-limited search (from Chapter 3). A state would be a
partial assignment, andanaction wouldbeadding var = value totheassignment. Butfora
CSPwith nvariables of domain size d, wequickly notice something terrible: the branching
factoratthetoplevelisndbecauseanyofdvaluescanbeassignedtoanyofnvariables. At
the next level, the branching factor is (n −1)d, and so on for n levels. We generate a tree
withn!·dn leaves,eventhoughthereareonly dn possible completeassignments!
Our seemingly reasonable but naive formulation ignores crucial property common to
allCSPs: commutativity. Aproblemiscommutativeiftheorderofapplication ofanygiven
COMMUTATIVITY
set of actions has no effect on the outcome. CSPsare commutative because when assigning
values to variables, wereach the samepartial assignment regardless of order. Therefore, we
need onlyconsider a single variable ateach node inthe search tree. Forexample, attheroot
node of a search tree for coloring the map of Australia, we might make a choice between
SA=red, SA=green, and SA=blue, but we would never choose between SA=red and
WA=blue. Withthisrestriction, thenumberofleavesis dn,aswewouldhope.
Section6.3. Backtracking SearchforCSPs 215
functionBACKTRACKING-SEARCH(csp)returnsasolution,orfailure
returnBACKTRACK({},csp)
functionBACKTRACK(assignment,csp)returnsasolution,orfailure
ifassignment iscompletethenreturnassignment
var←SELECT-UNASSIGNED-VARIABLE(csp)
foreachvalue inORDER-DOMAIN-VALUES(var,assignment,csp)do
ifvalue isconsistentwithassignment then
add{var =value}toassignment
inferences←INFERENCE(csp,var,value)
ifinferences (cid:7)= failure then
addinferences toassignment
result←BACKTRACK(assignment,csp)
ifresult (cid:7)=failure then
returnresult
remove{var =value}andinferences fromassignment
returnfailure
Figure6.5 Asimplebacktrackingalgorithmforconstraintsatisfactionproblems. Theal-
gorithmismodeledontherecursivedepth-firstsearchofChapter3.Byvaryingthefunctions
SELECT-UNASSIGNED-VARIABLE and ORDER-DOMAIN-VALUES, we can implementthe
general-purposeheuristicsdiscussedinthetext. ThefunctionINFERENCEcanoptionallybe
used to impose arc-, path-, or k-consistency, as desired. If a value choice leads to failure
(noticedeitherbyINFERENCEorbyBACKTRACK),thenvalueassignments(includingthose
madebyINFERENCE)areremovedfromthecurrentassignmentandanewvalueistried.
BACKTRACKING The term backtracking search is used for a depth-first search that chooses values for
SEARCH
one variable atatime and backtracks when avariable has nolegal values left to assign. The
algorithmisshowninFigure6.5. Itrepeatedlychoosesanunassigned variable,andthentries
allvaluesinthedomainofthatvariableinturn,tryingtofindasolution. Ifaninconsistencyis
detected,thenBACKTRACKreturnsfailure,causingthepreviouscalltotryanothervalue. Part
of the search tree forthe Australia problem is shown in Figure 6.6, where wehave assigned
variables in the order WA,NT,Q,.... Because the representation of CSPs is standardized,
there is no need to supply BACKTRACKING-SEARCH with a domain-specific initial state,
actionfunction, transition model,orgoaltest.
NoticethatBACKTRACKING-SEARCH keepsonlyasinglerepresentation ofastateand
altersthatrepresentation ratherthancreating newones,asdescribed onpage87.
In Chapter 3 we improved the poor performance of uninformed search algorithms by
supplying them with domain-specific heuristic functions derived from ourknowledge of the
problem. ItturnsoutthatwecansolveCSPsefficientlywithoutsuchdomain-specific knowl-
edge. Instead, we can add some sophistication to the unspecified functions in Figure 6.5,
usingthemtoaddressthefollowingquestions:
1. Which variable should be assigned next (SELECT-UNASSIGNED-VARIABLE), and in
whatordershoulditsvaluesbetried(ORDER-DOMAIN-VALUES)?
216 Chapter 6. ConstraintSatisfaction Problems
WA=red WA=green WA=blue
WA=red WA=red
NT=green NT=blue
WA=red WA=red
NT=green NT=green
Q=red Q=blue
Figure6.6 Partofthesearchtreeforthemap-coloringprobleminFigure6.1.
2. Whatinferences should beperformed ateachstepinthesearch(INFERENCE)?
3. Whenthesearcharrivesatanassignmentthatviolatesaconstraint,canthesearchavoid
repeating thisfailure?
Thesubsections thatfollowanswereachofthesequestions inturn.
6.3.1 Variableand valueordering
Thebacktracking algorithm containstheline
var←SELECT-UNASSIGNED-VARIABLE(csp) .
ThesimpleststrategyforSELECT-UNASSIGNED-VARIABLE istochoosethenextunassigned
variableinorder, {X ,X ,...}. Thisstaticvariable ordering seldomresultsinthemosteffi-
1 2
cientsearch. Forexample,aftertheassignmentsforWA=red andNT =greeninFigure6.6,
thereisonlyonepossiblevalueforSA,soitmakessensetoassignSA=blue nextratherthan
assigningQ. Infact,afterSAisassigned,thechoicesforQ,NSW,andV areallforced. This
intuitiveidea—choosingthevariablewiththefewest“legal”values—iscalledtheminimum-
MINIMUM- remaining-values(MRV)heuristic. Italsohasbeencalledthe“mostconstrainedvariable”or
REMAINING-VALUES
“fail-first” heuristic, thelatterbecause itpicksavariablethatismostlikelytocauseafailure
soon, thereby pruning the search tree. If some variable X has no legal values left, the MRV
heuristicwillselectX andfailurewillbedetectedimmediately—avoiding pointlesssearches
through other variables. The MRV heuristic usually performs better than a random or static
ordering,sometimesbyafactorof1,000ormore,althoughtheresultsvarywidelydepending
ontheproblem.
TheMRVheuristic doesn’t help atallinchoosing thefirstregion tocolorinAustralia,
becauseinitiallyeveryregionhasthreelegalcolors. Inthiscase,thedegreeheuristiccomes
DEGREEHEURISTIC
in handy. It attempts to reduce the branching factor on future choices by selecting the vari-
able that is involved in the largest number of constraints on other unassigned variables. In
Figure 6.1, SAis the variable with highest degree, 5; the other variables have degree 2 or3,
except for T, which has degree 0. In fact, once SA is chosen, applying the degree heuris-
tic solves the problem without any false steps—you can choose any consistent color at each
choice point and still arrive at a solution with no backtracking. The minimum-remaining-
Section6.3. Backtracking SearchforCSPs 217
values heuristic isusually amorepowerful guide, but the degree heuristic canbe useful as a
tie-breaker.
Once a variable has been selected, the algorithm must decide on the order in which to
LEAST-
examine itsvalues. Forthis, the least-constraining-value heuristic can beeffective insome
CONSTRAINING-
VALUE
cases. It prefers the value that rules out the fewest choices for the neighboring variables in
the constraint graph. For example, suppose that in Figure 6.1 we have generated the partial
assignment with WA=red and NT =green and that our next choice is for Q. Blue would
be a bad choice because it eliminates the last legal value left for Q’s neighbor, SA. The
least-constraining-value heuristic therefore prefers red to blue. In general, the heuristic is
tryingtoleavethemaximumflexibilityforsubsequentvariableassignments. Ofcourse,ifwe
are trying to find all the solutions to a problem, not just the first one, then the ordering does
not matter because we have to consider every value anyway. The same holds if there are no
solutions totheproblem.
Whyshould variable selection be fail-first, but value selection be fail-last? It turns out
that, for a wide variety of problems, a variable ordering that chooses a variable with the
minimumnumberofremainingvalueshelpsminimizethenumberofnodesinthesearchtree
by pruning larger parts of the tree earlier. Forvalue ordering, the trick is that we only need
onesolution;thereforeitmakessensetolookforthemostlikelyvaluesfirst. Ifwewantedto
enumerateallsolutions ratherthanjustfindone,thenvalue orderingwouldbeirrelevant.
6.3.2 Interleaving searchand inference
So far we have seen how AC-3 and other algorithms can infer reductions in the domain of
variables before webeginthesearch. Butinference canbeevenmorepowerfulinthecourse
of a search: every time we make a choice of a value for a variable, we have a brand-new
opportunity toinfernewdomainreductions ontheneighboring variables.
FORWARD One of the simplest forms of inference is called forward checking. Whenever a vari-
CHECKING
able X isassigned, the forward-checking process establishes arc consistency forit: foreach
unassigned variable Y that is connected to X by a constraint, delete from Y’s domain any
value that is inconsistent withthe value chosen for X. Because forward checking only does
arcconsistencyinferences,thereisnoreasontodoforward checkingifwehavealreadydone
arcconsistency asapreprocessing step.
Figure 6.7 shows the progress of backtracking search on the Australia CSP with for-
ward checking. There are two important points to notice about this example. First, notice
that after WA=red and Q=green are assigned, the domains of NT and SA are reduced
to a single value; we have eliminated branching on these variables altogether by propagat-
ing information from WA and Q. A second point to notice is that after V =blue, the do-
main of SA is empty. Hence, forward checking has detected that the partial assignment
{WA=red,Q=green,V =blue} is inconsistent with the constraints of the problem, and
thealgorithm willtherefore backtrack immediately.
For many problems the search will be more effective if we combine the MRV heuris-
tic with forward checking. Consider Figure 6.7 after assigning {WA=red}. Intuitively, it
seems that that assignment constrains itsneighbors, NT and SA, soweshould handle those
218 Chapter 6. ConstraintSatisfaction Problems
WA NT Q NSW V SA T
Initial domains R G B R G B R G B R G B R G B R G B R G B
AfterWA=red R G B R G B R G B R G B G B R G B
AfterQ=green R B G R B R G B B R G B
AfterV=blue R B G R B R G B
Figure 6.7 The progress of a map-coloring search with forward checking. WA=red
is assigned first; then forward checking deletes red from the domains of the neighboring
variables NT and SA. After Q=green is assigned, green is deleted from the domainsof
NT,SA,andNSW. AfterV =blue isassigned,blue isdeletedfromthedomainsofNSW
andSA,leavingSAwithnolegalvalues.
variables next, and then all the other variables will fall into place. That’s exactly what hap-
pens with MRV:NT and SAhave two values, soone of them ischosen first, then the other,
then Q, NSW, and V in order. Finally T still has three values, and any one of them works.
Wecanviewforwardchecking asanefficientwaytoincrementally compute theinformation
thattheMRVheuristic needstodoitsjob.
Althoughforwardcheckingdetectsmanyinconsistencies, itdoesnotdetectallofthem.
The problem is that it makes the current variable arc-consistent, but doesn’t look ahead and
makealltheothervariablesarc-consistent. Forexample,considerthethirdrowofFigure6.7.
ItshowsthatwhenWAisred andQisgreen,bothNT andSAareforcedtobeblue. Forward
checking does notlook farenough ahead tonotice that thisis aninconsistency: NT andSA
areadjacentandsocannothavethesamevalue.
MAINTAININGARC The algorithm called MAC (for Maintaining Arc Consistency (MAC)) detects this
CONSISTENCY(MAC)
inconsistency. Afteravariable X
i
isassignedavalue,theINFERENCE procedurecallsAC-3,
but instead of a queue of all arcs in the CSP, we start with only the arcs (X ,X ) for all
j i
X thatareunassigned variables thatareneighbors of X . Fromthere, AC-3doesconstraint
j i
propagation intheusualway,andifanyvariablehasitsdomainreducedtotheemptyset,the
call to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly
morepowerfulthanforwardcheckingbecauseforwardcheckingdoesthesamethingasMAC
on the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursively
propagate constraints whenchanges aremadetothedomainsofvariables.
6.3.3 Intelligentbacktracking: Looking backward
The BACKTRACKING-SEARCH algorithm inFigure6.5hasaverysimple policy forwhatto
do when a branch of the search fails: back up to the preceding variable and try a different
CHRONOLOGICAL value for it. This is called chronological backtracking because the most recent decision
BACKTRACKING
pointisrevisited. Inthissubsection, weconsiderbetterpossibilities.
Consider what happens when we apply simple backtracking in Figure 6.1 with a fixed
variable ordering Q, NSW, V, T, SA, WA, NT. Suppose we have generated the partial
assignment {Q=red,NSW =green,V =blue,T =red}. When we try the next variable,
SA, we see that every value violates a constraint. We back up to T and try a new color for
Section6.3. Backtracking SearchforCSPs 219
Tasmania! Obviouslythisissilly—recoloring Tasmaniacannotpossiblyresolvetheproblem
withSouthAustralia.
Amoreintelligent approach tobacktracking istobacktrack toavariable thatmightfix
the problem—a variable that was responsible for making one of the possible values of SA
impossible. To do this, we will keep track of a set of assignments that are in conflict with
somevalueforSA. Theset(inthiscase{Q=red,NSW =green,V =blue,}),iscalledthe
conflict set for SA. The backjumpingmethod backtracks to the most recent assignment in
CONFLICTSET
the conflict set; in this case, backjumping would jump over Tasmania and try a new value
BACKJUMPING
for V. This method is easily implemented by a modification to BACKTRACK such that it
accumulates the conflict set while checking for a legal value to assign. If no legal value is
found, the algorithm should return the most recent element of the conflict setalong with the
failureindicator.
The sharp-eyed reader will have noticed that forward checking can supply the conflict
setwithnoextrawork: wheneverforwardchecking basedonan assignment X=xdeletes a
value from Y’s domain, it should add X=x to Y’s conflict set. If the last value is deleted
from Y’s domain, then the assignments in the conflict set of Y are added to the conflict set
ofX. Then,whenwegettoY,weknowimmediately wheretobacktrack ifneeded.
The eagle-eyed reader will have noticed something odd: backjumping occurs when
every value in a domain is in conflict with the current assignment; but forward checking
detects this event and prevents the search from ever reaching such a node! In fact, it can be
shownthateverybranchprunedbybackjumping isalsoprunedbyforwardchecking. Hence,
simple backjumping is redundant in a forward-checking search or, indeed, in a search that
usesstrongerconsistency checking, suchas MAC.
Despite the observations of the preceding paragraph, the idea behind backjumping re-
mainsagoodone: tobacktrackbasedonthereasonsforfailure. Backjumpingnoticesfailure
whenavariable’sdomainbecomesempty,butinmanycasesabranchisdoomedlongbefore
this occurs. Consider again the partial assignment {WA=red,NSW =red} (which, from
ourearlierdiscussion, isinconsistent). Supposewetry T =red nextandthenassignNT,Q,
V,SA. Weknowthatnoassignment canworkfortheselastfourvariables, soeventually we
runoutofvaluestotryatNT. Now,thequestionis,wheretobacktrack? Backjumpingcannot
work, because NT does have values consistent with the preceding assigned variables—NT
doesn’t have a complete conflict set of preceding variables that caused it to fail. We know,
however,thatthefourvariables NT,Q,V,andSA,takentogether, failedbecauseofasetof
preceding variables, which must be those variables that directly conflict with the four. This
leads toadeepernotion oftheconflict setforavariable such asNT: itisthat setofpreced-
ing variables that caused NT, together with any subsequent variables, to have no consistent
solution. In this case, the set is WA and NSW, so the algorithm should backtrack to NSW
andskipoverTasmania. Abackjumping algorithm thatusesconflictsets definedinthisway
CONFLICT-DIRECTED iscalledconflict-directed backjumping.
BACKJUMPING
We must now explain how these new conflict sets are computed. The method is in
fact quite simple. The “terminal” failure of a branch of the search always occurs because a
variable’s domain becomes empty; that variable has a standard conflict set. In our example,
SA fails, and its conflict set is (say) {WA,NT,Q}. We backjump to Q, and Q absorbs
220 Chapter 6. ConstraintSatisfaction Problems
the conflict set from SA (minus Q itself, of course) into its own direct conflict set, which is
{NT,NSW}; the new conflict set is {WA,NT,NSW}. That is, there is no solution from
Q onward, given the preceding assignment to {WA,NT,NSW}. Therefore, we backtrack
to NT, the most recent of these. NT absorbs {WA,NT,NSW} − {NT} into its own
direct conflict set {WA}, giving {WA,NSW} (as stated in the previous paragraph). Now
the algorithm backjumps to NSW, as wewould hope. Tosummarize: let X be the current
j
variable, and let conf(X ) be its conflict set. If every possible value for X fails, backjump
j j
tothemostrecentvariable X inconf(X ),andset
i j
conf(X ) ← conf(X )∪conf(X )−{X }.
i i j i
When we reach a contradiction, backjumping can tell us how far to back up, so we don’t
waste time changing variables that won’t fix the problem. But we would also like to avoid
running into the same problem again. When the search arrives at a contradiction, we know
CONSTRAINT thatsomesubsetoftheconflictsetisresponsiblefortheproblem. Constraintlearningisthe
LEARNING
ideaoffindingaminimumsetofvariablesfromtheconflictsetthatcausestheproblem. This
set of variables, along with their corresponding values, is called a no-good. Wethen record
NO-GOOD
the no-good, either byadding anew constraint tothe CSPorby keeping aseparate cache of
no-goods.
For example, consider the state {WA = red,NT = green,Q = blue} in the bottom
row of Figure 6.6. Forward checking can tell us this state is a no-good because there is no
validassignmenttoSA. Inthisparticularcase,recordingtheno-goodwouldnothelp,because
once we prune this branch from the search tree, we will never encounter this combination
again. ButsupposethatthesearchtreeinFigure6.6wereactuallypartofalargersearchtree
that started by first assigning values for V and T. Then it would be worthwhile to record
{WA = red,NT = green,Q = blue} as a no-good because we are going to run into the
sameproblemagainforeachpossible setofassignments toV andT.
No-goods can be effectively used by forward checking orby backjumping. Constraint
learning is one of the most important techniques used by modern CSP solvers to achieve
efficiencyoncomplexproblems.
6.4 LOCAL SEARCH FOR CSPS
Localsearchalgorithms(seeSection4.1)turnouttobeeffectiveinsolvingmanyCSPs. They
use a complete-state formulation: the initial state assigns a value to every variable, and the
searchchangesthevalueofonevariableatatime. Forexample,inthe8-queensproblem(see
Figure 4.3), the initial state might be a random configuration of 8 queens in 8 columns, and
each step moves a single queen to a new position in its column. Typically, the initial guess
violatesseveralconstraints. Thepointoflocalsearchistoeliminatetheviolatedconstraints.2
Inchoosing anewvalueforavariable, themostobvious heuristic istoselect thevalue
that results in the minimum number of conflicts with other variables—the min-conflicts
MIN-CONFLICTS
2 Localsearchcaneasilybeextendedtoconstraintoptimizationproblems(COPs).Inthatcase,allthetechniques
forhillclimbingandsimulatedannealingcanbeappliedtooptimizetheobjectivefunction.
Section6.4. LocalSearchforCSPs 221
functionMIN-CONFLICTS(csp,max steps)returnsasolutionorfailure
inputs:csp,aconstraintsatisfactionproblem
max steps,thenumberofstepsallowedbeforegivingup
current←aninitialcompleteassignmentforcsp
fori =1tomax steps do
ifcurrent isasolutionforcsp thenreturncurrent
var←arandomlychosenconflictedvariablefromcsp.VARIABLES
value←thevaluev forvar thatminimizesCONFLICTS(var,v,current,csp)
setvar=value incurrent
returnfailure
Figure6.8 TheMIN-CONFLICTSalgorithmforsolvingCSPsbylocalsearch. Theinitial
state may be chosen randomlyor by a greedy assignmentprocess that choosesa minimal-
conflict value for each variable in turn. The CONFLICTS function counts the number of
constraintsviolatedbyaparticularvalue,giventherestofthecurrentassignment.
2 3
2 3
1
2 2
3 3
1 2
2 3
0
Figure 6.9 A two-step solution using min-conflicts for an 8-queens problem. At each
stage, a queen is chosen for reassignment in its column. The number of conflicts (in this
case, the number of attacking queens) is shown in each square. The algorithm moves the
queentothemin-conflictssquare,breakingtiesrandomly.
heuristic. Thealgorithm isshowninFigure6.8anditsapplication toan8-queens problem is
diagrammedinFigure6.9.
Min-conflicts is surprisingly effective for many CSPs. Amazingly, on the n-queens
problem, if you don’t count the initial placement of queens, the run time of min-conflicts is
roughly independent of problem size. It solves even the million-queens problem in an aver-
age of 50 steps (after the initial assignment). This remarkable observation was the stimulus
leading to a great deal of research in the 1990s on local search and the distinction between
easy and hard problems, which we take up in Chapter 7. Roughly speaking, n-queens is
easy for local search because solutions are densely distributed throughout the state space.
Min-conflicts also works well for hard problems. Forexample, it has been used to schedule
observations fortheHubble SpaceTelescope, reducing the timetaken toschedule aweekof
observations fromthreeweeks(!) toaround10minutes.
222 Chapter 6. ConstraintSatisfaction Problems
AllthelocalsearchtechniquesfromSection4.1arecandidatesforapplicationtoCSPs,
and some of those have proved especially effective. The landscape of aCSPunder the min-
conflicts heuristic usually has a series of plateaux. There may be millions of variable as-
signments that are only one conflict away from a solution. Plateau search—allowing side-
waysmovestoanother state withthesamescore—can helplocal search finditswayoffthis
plateau. This wandering on the plateau can be directed with tabu search: keeping a small
listofrecentlyvisitedstatesandforbidding thealgorithm toreturntothosestates. Simulated
annealing canalsobeusedtoescapefromplateaux.
CONSTRAINT Anothertechnique,calledconstraintweighting,canhelpconcentratethesearchonthe
WEIGHTING
important constraints. Eachconstraint isgivenanumeric weight, W ,initially all1. Ateach
i
stepofthesearch,thealgorithmchoosesavariable/value pairtochangethatwillresultinthe
lowesttotalweightofallviolatedconstraints. Theweightsarethenadjustedbyincrementing
theweightofeachconstraintthatisviolatedbythecurrentassignment. Thishastwobenefits:
it adds topography to plateaux, making sure that it is possible to improve from the current
state,anditalso,overtime,addsweighttotheconstraints thatareprovingdifficulttosolve.
Another advantage of local search is that it can be used in an online setting when the
problem changes. This is particularly important in scheduling problems. A week’s airline
schedule may involve thousands of flights and tens of thousands of personnel assignments,
butbadweatheratoneairportcanrenderthescheduleinfeasible. Wewouldliketorepairthe
schedule with a minimum number of changes. This can be easily done with a local search
algorithm starting from the current schedule. A backtracking search with the new set of
constraints usually requires much more time and might find a solution with many changes
fromthecurrentschedule.
6.5 THE STRUCTURE OF PROBLEMS
In this section, we examine ways in which the structure of the problem, as represented by
theconstraint graph, can beused tofindsolutions quickly. Mostofthe approaches herealso
applytootherproblemsbesidesCSPs,suchasprobabilistic reasoning. Afterall,theonlyway
wecan possibly hope todeal withthe realworld istodecompose itintomany subproblems.
LookingagainattheconstraintgraphforAustralia(Figure6.1(b),repeatedasFigure6.12(a)),
onefactstandsout: Tasmaniaisnotconnectedtothemainland.3 Intuitively, itisobviousthat
INDEPENDENT coloring Tasmania and coloring the mainland are independentsubproblems—any solution
SUBPROBLEMS
for the mainland combined with any solution for Tasmania yields a solution for the whole
CONNECTED map. Independence can be ascertained simply by finding connected components of the
COMPONENT
constraint graph. Each component corresponds to a subproblem CSP . If assignment S is
(cid:22) (cid:22) i i
a solution of CSP , then S is a solution of CSP . Why is this important? Consider
i i i i i
the following: suppose each CSP has c variables from the total of n variables, where c is
i
a constant. Then there are n/c subproblems, each of which takes at most dc work to solve,
3 AcarefulcartographerorpatrioticTasmanianmightobject thatTasmaniashouldnotbecoloredthesameas
itsnearestmainlandneighbor,toavoidtheimpressionthatitmightbepartofthatstate.
Section6.5. TheStructureofProblems 223
where d is the size of the domain. Hence, the total work is O(dcn/c), which is linear in n;
without the decomposition, the total work is O(dn), which is exponential in n. Let’s make
thismoreconcrete: dividingaBooleanCSPwith80variables intofoursubproblems reduces
theworst-casesolution timefromthelifetimeoftheuniversedowntolessthanasecond.
Completely independent subproblems are delicious, then, but rare. Fortunately, some
other graph structures are also easy to solve. Forexample, a constraint graph isa tree when
anytwovariablesareconnectedbyonlyonepath. Weshowthatanytree-structured CSPcan
besolved intimelinear inthenumberofvariables.4 Thekeyisanewnotion ofconsistency,
DIRECTEDARC calleddirectedarcconsistencyorDAC.ACSPisdefinedtobedirectedarc-consistentunder
CONSISTENCY
an ordering of variables X ,X ,...,X if and only if every X is arc-consistent with each
1 2 n i
X forj > i.
j
To solve a tree-structured CSP, first pick any variable to be the root of the tree, and
chooseanorderingofthevariablessuchthateachvariableappearsafteritsparentinthetree.
Such an ordering is called a topological sort. Figure 6.10(a) shows a sample tree and (b)
TOPOLOGICALSORT
showsonepossibleordering. Anytreewithnnodeshasn−1arcs,sowecanmakethisgraph
directed arc-consistent in O(n) steps, each of which must compare up to d possible domain
values fortwo variables, foratotal time of O(nd2). Once we have a directed arc-consistent
graph, we can just march down the list of variables and choose any remaining value. Since
eachlinkfromaparenttoitschildisarcconsistent,weknowthatforanyvaluewechoosefor
theparent, there willbeavalidvalue lefttochoose forthechild. Thatmeanswewon’thave
to backtrack; we can move linearly through the variables. Thecomplete algorithm is shown
inFigure6.11.
A E
B D A B C D E F
C F
(a) (b)
Figure6.10 (a)Theconstraintgraphofatree-structuredCSP.(b)Alinearorderingofthe
variablesconsistentwiththetreewithAastheroot. Thisisknownasatopologicalsortof
thevariables.
Nowthatwehaveanefficientalgorithmfortrees,wecanconsiderwhethermoregeneral
constraint graphs can be reduced to trees somehow. There are two primary ways to do this,
onebasedonremovingnodesandonebasedoncollapsing nodes together.
The first approach involves assigning values to some variables so that the remaining
variables form a tree. Consider the constraint graph for Australia, shown again in Fig-
ure 6.12(a). If we could delete South Australia, the graph would become a tree, as in (b).
Fortunately, we can do this (in the graph, not the continent) by fixing a value for SA and
4 Sadly,veryfewregionsoftheworldhavetree-structuredmaps,althoughSulawesicomesclose.
224 Chapter 6. ConstraintSatisfaction Problems
functionTREE-CSP-SOLVER(csp)returnsasolution,orfailure
inputs: csp,aCSPwithcomponentsX, D, C
n←numberofvariablesinX
assignment←anemptyassignment
root←anyvariableinX
X ←TOPOLOGICALSORT(X,root)
forj =n downto2do
MAKE-ARC-CONSISTENT(PARENT(Xj),Xj)
ifitcannotbemadeconsistentthenreturnfailure
fori =1ton do
assignment[Xi]←anyconsistentvaluefromDi
ifthereisnoconsistentvaluethenreturnfailure
returnassignment
Figure6.11 The TREE-CSP-SOLVER algorithmforsolvingtree-structuredCSPs. Ifthe
CSPhasasolution,wewillfinditinlineartime;ifnot,wewilldetectacontradiction.
NT NT
Q Q
WA WA
SA NSW NSW
V V
T T
(a) (b)
Figure6.12 (a) TheoriginalconstraintgraphfromFigure6.1. (b)The constraintgraph
aftertheremovalofSA.
deleting from the domains of the other variables any values that are inconsistent with the
valuechosen forSA.
Now, any solution for the CSP after SA and its constraints are removed will be con-
sistent with the value chosen for SA. (This works for binary CSPs; the situation is more
complicated withhigher-order constraints.) Therefore, wecan solve theremaining treewith
the algorithm given above and thus solve the whole problem. Ofcourse, in the general case
(asopposed tomapcoloring), thevaluechosenforSAcouldbethewrongone, sowewould
needtotryeachpossible value. Thegeneralalgorithm isasfollows:
Section6.5. TheStructureofProblems 225
1. ChooseasubsetS oftheCSP’svariables suchthattheconstraint graphbecomesatree
afterremovalofS. S iscalledacyclecutset.
CYCLECUTSET
2. Foreachpossible assignment tothevariables inS thatsatisfiesallconstraints onS,
(a) remove from thedomains oftheremaining variables anyvalues that are inconsis-
tentwiththeassignment forS,and
(b) Iftheremaining CSPhasasolution, returnittogetherwiththeassignment forS.
Ifthecyclecutsethassizec,thenthetotalruntimeisO(dc·(n−c)d2): wehavetotryeach
of the dc combinations of values for the variables in S, and for each combination we must
solveatreeproblemofsize n−c. Ifthegraphis“nearlyatree,”then cwillbesmallandthe
savingsoverstraightbacktracking willbehuge. Intheworstcase,however,ccanbeaslarge
as(n−2). Finding the smallest cycle cutset isNP-hard, but several efficient approximation
CUTSET algorithms are known. The overall algorithmic approach is called cutset conditioning; it
CONDITIONING
comesupagaininChapter14,whereitisusedforreasoning aboutprobabilities.
TREE The second approach is based on constructing a tree decomposition of the constraint
DECOMPOSITION
graphintoasetofconnectedsubproblems. Eachsubproblemissolvedindependently, andthe
resulting solutions are then combined. Likemostdivide-and-conquer algorithms, this works
well if no subproblem is too large. Figure 6.13 shows a tree decomposition of the map-
coloring problem into five subproblems. A tree decomposition must satisfy the following
threerequirements:
• Everyvariableintheoriginalproblem appearsinatleastoneofthesubproblems.
• Iftwovariablesareconnectedbyaconstraint intheoriginalproblem,theymustappear
together(alongwiththeconstraint) inatleastoneofthesubproblems.
• Ifavariableappearsintwosubproblemsinthetree,itmustappearineverysubproblem
alongthepathconnecting thosesubproblems.
The first two conditions ensure that all the variables and constraints are represented in the
decomposition. Thethird condition seemsrathertechnical, butsimply reflectstheconstraint
that any given variable must have the same value in every subproblem in which it appears;
thelinksjoining subproblems inthetreeenforcethisconstraint. Forexample, SAappears in
all four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 that
thisdecomposition makessense.
Wesolve eachsubproblem independently; ifanyone hasnosolution, weknow theen-
tireproblemhasnosolution. Ifwecansolveallthesubproblems,thenweattempttoconstruct
aglobalsolutionasfollows. First,wevieweachsubproblem asa“mega-variable”whosedo-
main isthe set of all solutions forthe subproblem. Forexample, the leftmost subproblem in
Figure6.13isamap-coloring problemwiththreevariablesandhencehassixsolutions—one
is {WA = red,SA = blue,NT = green}. Then, we solve the constraints connecting the
subproblems, using the efficient algorithm for trees given earlier. The constraints between
subproblems simply insistthatthesubproblem solutions agreeontheirshared variables. For
example,giventhesolution{WA = red,SA = blue,NT = green}forthefirstsubproblem,
theonlyconsistentsolutionforthenextsubproblemis{SA = blue,NT = green,Q = red}.
A given constraint graph admits many tree decompositions; in choosing a decompo-
sition, the aim is to make the subproblems as small as possible. The tree width of a tree
TREEWIDTH
226 Chapter 6. ConstraintSatisfaction Problems
NT
NT
Q
WA
SA
SA
Q
SA NSW
SA NSW
T
V
Figure6.13 AtreedecompositionoftheconstraintgraphinFigure6.12(a).
decomposition of a graph is one less than the size of the largest subproblem; the tree width
ofthegraphitselfisdefinedtobetheminimumtreewidthamongallitstreedecompositions.
Ifagraph has treewidth w andwearegiven thecorresponding tree decomposition, then the
problem can be solved in O(ndw+1) time. Hence, CSPs with constraint graphs of bounded
tree width are solvable in polynomial time. Unfortunately, finding the decomposition with
minimaltreewidthisNP-hard,butthereareheuristic methodsthatworkwellinpractice.
Sofar, wehave looked atthestructure oftheconstraint graph. There canbeimportant
structureinthevaluesofvariablesaswell. Considerthemap-coloringproblemwithncolors.
Foreveryconsistent solution, there isactually aset of n!solutions formed bypermuting the
colornames. Forexample,ontheAustraliamapweknowthatWA,NT,andSAmustallhave
different colors, but there are 3! = 6 ways to assign the three colors to these three regions.
This is called value symmetry. We would like to reduce the search space by a factor of
VALUESYMMETRY
SYMMETRY-
n!by breaking the symmetry. Wedo this by introducing a symmetry-breaking constraint.
BREAKING
CONSTRAINT
Forourexample, we might impose an arbitrary ordering constraint, NT < SA < WA, that
requires the three values tobeinalphabetical order. Thisconstraint ensures that only one of
then!solutions ispossible: {NT = blue,SA = green,WA = red}.
For map coloring, it was easy to find a constraint that eliminates the symmetry, and
in general it is possible to find constraints that eliminate all but one symmetric solution in
polynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of
values during search. In practice, breaking value symmetry has proved to be important and
effectiveonawiderangeofproblems.
Section6.6. Summary 227
6.6 SUMMARY
• Constraintsatisfaction problems(CSPs)represent astatewithasetofvariable/value
pairsandrepresent theconditions forasolutionbyasetofconstraints onthevariables.
Manyimportantreal-world problemscanbedescribed asCSPs.
• Anumberofinferencetechniquesusetheconstraintstoinferwhichvariable/valuepairs
areconsistent andwhicharenot. Theseinclude node,arc,path,andk-consistency.
• Backtrackingsearch,aformofdepth-firstsearch,iscommonlyusedforsolvingCSPs.
Inferencecanbeinterwovenwithsearch.
• Theminimum-remaining-valuesanddegreeheuristicsaredomain-independentmeth-
ods for deciding which variable to choose next in a backtracking search. The least-
constraining-value heuristic helps in deciding which value to try first for a given
variable. Backtracking occurs when no legal assignment can be found for a variable.
Conflict-directedbackjumpingbacktracks directlytothesourceoftheproblem.
• Localsearchusingthemin-conflictsheuristic hasalsobeenappliedtoconstraint satis-
factionproblemswithgreatsuccess.
• The complexity of solving a CSP is strongly related to the structure of its constraint
graph. Tree-structured problemscanbesolvedinlineartime. Cutsetconditioningcan
reduceageneral CSPtoatree-structured oneandisquiteefficientifasmallcutset can
befound. TreedecompositiontechniquestransformtheCSPintoatreeofsubproblems
andareefficientifthetreewidthoftheconstraint graphissmall.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The earliest work related to constraint satisfaction dealt largely with numerical constraints.
Equationalconstraints withintegerdomainswerestudiedbytheIndianmathematicianBrah-
DIOPHANTINE maguptaintheseventhcentury;theyareoftencalledDiophantineequations,aftertheGreek
EQUATIONS
mathematician Diophantus (c. 200–284), whoactually considered thedomainofpositive ra-
tionals. Systematicmethodsforsolvinglinearequationsbyvariableeliminationwerestudied
byGauss(1829);thesolutionoflinearinequality constraints goesbacktoFourier(1827).
Finite-domain constraint satisfaction problems also have a long history. For example,
graph coloring (ofwhich mapcoloring isaspecial case) is anold problem in mathematics.
GRAPHCOLORING
Thefour-color conjecture (that every planar graph can be colored with fourorfewercolors)
was first made by Francis Guthrie, a student of De Morgan, in 1852. It resisted solution—
despite several published claims to the contrary—until a proof was devised by Appel and
Haken (1977) (see the book Four Colors Suffice (Wilson, 2004)). Purists were disappointed
that part of the proof relied on a computer, so Georges Gonthier (2008), using the COQ
theorem prover,derivedaformalproofthatAppelandHaken’sproofwascorrect.
Specific classes of constraint satisfaction problems occur throughout the history of
computer science. One of the most influential early examples was the SKETCHPAD sys-
228 Chapter 6. ConstraintSatisfaction Problems
tem (Sutherland, 1963), which solved geometric constraints in diagrams and was the fore-
runnerofmoderndrawingprogramsandCADtools. Theidentification ofCSPsasageneral
class is due to Ugo Montanari (1974). The reduction of higher-order CSPsto purely binary
CSPswithauxiliaryvariables(seeExercise6.6)isdueoriginallytothe19th-centurylogician
Charles Sanders Peirce. It was introduced into the CSP literature by Dechter (1990b) and
waselaboratedbyBacchusandvanBeek(1998). CSPswithpreferencesamongsolutionsare
studied widely in the optimization literature; see Bistarelli et al. (1997) for a generalization
of the CSPframework to allow forpreferences. The bucket-elimination algorithm (Dechter,
1999)canalsobeapplied tooptimization problems.
Constraint propagation methods were popularized by Waltz’s (1975) success on poly-
hedral line-labeling problems for computer vision. Waltz showed that, in many problems,
propagation completely eliminates the need for backtracking. Montanari (1974) introduced
the notion of constraint networks and propagation by path consistency. Alan Mackworth
(1977)proposedtheAC-3algorithmforenforcingarcconsistencyaswellasthegeneralidea
of combining backtracking with some degree of consistency enforcement. AC-4, a more
efficientarc-consistency algorithm,wasdevelopedbyMohrandHenderson(1986). Soonaf-
terMackworth’s paperappeared, researchers beganexperimenting withthetradeoffbetween
the cost of consistency enforcement and the benefits in terms of search reduction. Haralick
and Elliot (1980) favored the minimal forward-checking algorithm described by McGregor
(1979), whereas Gaschnig (1979) suggested full arc-consistency checking after each vari-
able assignment—an algorithm later called MAC by Sabin and Freuder (1994). The latter
paper provides somewhat convincing evidence that, on harder CSPs, full arc-consistency
checking pays off. Freuder (1978, 1982) investigated the notion of k-consistency and its
relationship to the complexity of solving CSPs. Apt (1999) describes a generic algorithmic
framework within which consistency propagation algorithms can be analyzed, and Bessie`re
(2006)presents acurrentsurvey.
Special methods for handling higher-order or global constraints were developed first
within the context of constraint logic programming. Marriott and Stuckey (1998) provide
excellent coverage of research in this area. The Alldiff constraint was studied by Regin
(1994),StergiouandWalsh(1999),andvanHoeve(2001). Boundsconstraintswereincorpo-
ratedintoconstraint logicprogrammingbyVanHentenryck etal.(1998). Asurveyofglobal
constraints isprovided byvanHoeveandKatriel(2006).
SudokuhasbecomethemostwidelyknownCSPandwasdescribedassuchbySimonis
(2005). Agerbeck and Hansen (2008) describe some of the strategies and show that Sudoku
on an n2 × n2 board is in the class of NP-hard problems. Reeson et al. (2007) show an
interactive solverbasedonCSPtechniques.
The idea of backtracking search goes back to Golomb and Baumert (1965), and its
applicationtoconstraintsatisfactionisduetoBitnerandReingold(1975),althoughtheytrace
the basic algorithm back to the 19th century. Bitner and Reingold also introduced the MRV
heuristic, which they called the most-constrained-variable heuristic. Brelaz (1979) used the
degree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,
despite its simplicity, is still the best method for k-coloring arbitrary graphs. Haralick and
Elliot(1980)proposed theleast-constraining-value heuristic.
Bibliographical andHistorical Notes 229
The basic backjumping method is due to John Gaschnig (1977, 1979). Kondrak and
van Beek (1997) showed that this algorithm is essentially subsumed by forward checking.
Conflict-directed backjumping was devised by Prosser (1993). The most general and pow-
erful form of intelligent backtracking was actually developed very early on by Stallman and
DEPENDENCY-
Sussman(1977). Theirtechniqueofdependency-directedbacktrackingledtothedevelop-
DIRECTED
BACKTRACKING
mentoftruthmaintenancesystems(Doyle,1979),whichwediscussinSection12.6.2. The
connection betweenthetwoareasisanalyzed bydeKleer(1989).
The work of Stallman and Sussman also introduced the idea of constraint learning,
in which partial results obtained by search can be saved and reused later in the search. The
ideawasformalized Dechter(1990a). Backmarking(Gaschnig, 1979) isaparticularly sim-
BACKMARKING
ple method in which consistent and inconsistent pairwise assignments are saved and used
to avoid rechecking constraints. Backmarking can be combined with conflict-directed back-
jumping; Kondrak and van Beek (1997) present a hybrid algorithm that provably subsumes
DYNAMIC eithermethodtaken separately. Themethodof dynamicbacktracking(Ginsberg, 1993) re-
BACKTRACKING
tains successful partial assignments from later subsets of variables when backtracking over
anearlierchoicethatdoesnotinvalidate thelatersuccess.
Empirical studies of several randomized backtracking methods were done by Gomes
etal.(2000)andGomesandSelman(2001). VanBeek(2006)surveysbacktracking.
Local search in constraint satisfaction problems was popularized by the work of Kirk-
patricketal.(1983)onsimulatedannealing(seeChapter4),whichiswidelyusedforschedul-
ingproblems. Themin-conflictsheuristicwasfirstproposed byGu(1989)andwasdeveloped
independently byMintonetal.(1992). SosicandGu(1994)showedhowitcouldbeapplied
to solve the 3,000,000 queens problem in less than a minute. The astounding success of
local search using min-conflicts on the n-queens problem led to a reappraisal of the nature
and prevalence of “easy” and “hard” problems. Peter Cheeseman et al. (1991) explored the
difficulty of randomly generated CSPs and discovered that almost all such problems either
are trivially easy or have no solutions. Only if the parameters of the problem generator are
setin acertain narrow range, within which roughly half oftheproblems aresolvable, dowe
find “hard” problem instances. We discuss this phenomenon further in Chapter 7. Konolige
(1994)showedthatlocalsearchisinferiortobacktracking searchonproblemswithacertain
degree of local structure; this led to work that combined local search and inference, such as
thatbyPinkasandDechter(1995). HoosandTsang(2006)surveylocalsearchtechniques.
WorkrelatingthestructureandcomplexityofCSPsoriginateswithFreuder(1985),who
showedthat search onarcconsistent treesworks without any backtracking. Asimilarresult,
with extensions to acyclic hypergraphs, was developed in the database community (Beeri
et al., 1983). Bayardo and Miranker (1994) present an algorithm for tree-structured CSPs
thatrunsinlineartimewithoutanypreprocessing.
Sincethosepaperswerepublished,therehasbeenagreatdealofprogressindeveloping
moregeneralresultsrelatingthecomplexityofsolvingaCSPtothestructureofitsconstraint
graph. ThenotionoftreewidthwasintroducedbythegraphtheoristsRobertsonandSeymour
(1986). Dechter and Pearl (1987, 1989), building on the work of Freuder, applied a related
notion (whichthey called inducedwidth)toconstraint satisfaction problems and developed
thetreedecompositionapproachsketchedinSection6.5. Drawingonthisworkandonresults
230 Chapter 6. ConstraintSatisfaction Problems
fromdatabasetheory,Gottlobetal.(1999a,1999b)developedanotion,hypertreewidth,that
isbased onthecharacterization of theCSPasahypergraph. Inaddition toshowing that any
CSP with hypertree width w can be solved in time O(nw+1logn), they also showed that
hypertree width subsumes all previously defined measures of “width” in the sense that there
arecaseswherethehypertreewidthisboundedandtheothermeasuresareunbounded.
Interestinlook-backapproachestobacktrackingwasrekindledbytheworkofBayardo
andSchrag(1997),whoseRELSAT algorithmcombinedconstraintlearningandbackjumping
and was shown to outperform many other algorithms of the time. This led to AND/OR
search algorithms applicable to both CSPs and probabilistic reasoning (Dechter and Ma-
teescu, 2007). Brown et al. (1988) introduce the idea of symmetry breaking in CSPs, and
Gentetal.(2006)givearecentsurvey.
DISTRIBUTED
Thefieldofdistributed constraint satisfaction looks atsolving CSPswhenthere isa
CONSTRAINT
SATISFACTION
collection of agents, each of which controls a subset of the constraint variables. There have
been annual workshops on this problem since 2000, and good coverage elsewhere (Collin
etal.,1999;Pearceetal.,2008;ShohamandLeyton-Brown,2009).
ComparingCSPalgorithmsismostlyanempiricalscience: fewtheoreticalresultsshow
that one algorithm dominates another on all problems; instead, we need to run experiments
to see which algorithms perform better on typical instances of problems. AsHooker (1995)
points out, we need to be careful to distinguish between competitive testing—as occurs in
competitions among algorithms based on run time—and scientific testing, whose goal is to
identify theproperties ofanalgorithm thatdetermineitsefficacyonaclassofproblems.
The recent textbooks by Apt (2003) and Dechter (2003), and the collection by Rossi
etal. (2006) are excellent resources on constraint processing. There are several good earlier
surveys,includingthosebyKumar(1992),DechterandFrost(2002),andBartak(2001);and
the encyclopedia articles by Dechter (1992) and Mackworth (1992). Pearson and Jeavons
(1997) survey tractable classes of CSPs, covering both structural decomposition methods
and methods that rely on properties of the domains or constraints themselves. Kondrak and
van Beek (1997) give an analytical survey of backtracking search algorithms, and Bacchus
andvanRun(1995) giveamoreempirical survey. Constraint programming iscoveredinthe
booksbyApt(2003)andFruhwirthandAbdennadher(2003). Severalinterestingapplications
aredescribedinthecollectioneditedbyFreuderandMackworth(1994). Papersonconstraint
satisfactionappearregularlyinArtificialIntelligenceandinthespecialistjournalConstraints.
The primary conference venue is the International Conference on Principles and Practice of
Constraint Programming,oftencalled CP.
EXERCISES
6.1 Howmanysolutions arethere forthemap-coloring problem in Figure6.1? Howmany
solutions iffourcolorsareallowed? Twocolors?
6.2 Consider the problem of placing k knights on an n×n chessboard such that no two
knightsareattacking eachother, where k isgivenandk ≤ n2.
Exercises 231
a. ChooseaCSPformulation. Inyourformulation, whatarethe variables?
b. Whatarethepossiblevaluesofeachvariable?
c. Whatsetsofvariablesareconstrained, andhow?
d. Now consider the problem of putting as many knights as possible on the board with-
out any attacks. Explain how to solve this with local search by defining appropriate
ACTIONS and RESULT functions andasensibleobjective function.
6.3 Consider the problem of constructing (not solving) crossword puzzles:5 fitting words
into a rectangular grid. The grid, which is given as part of the problem, specifies which
squares are blank and which are shaded. Assume that a list of words (i.e., a dictionary)
is provided and that the task is to fill in the blank squares by using any subset of the list.
Formulatethisproblem precisely intwoways:
a. As a general search problem. Choose an appropriate search algorithm and specify a
heuristicfunction. Isitbettertofillinblanksoneletteratatimeoronewordatatime?
b. Asaconstraint satisfaction problem. Shouldthevariablesbewordsorletters?
Whichformulation doyouthinkwillbebetter? Why?
6.4 Givepreciseformulationsforeachofthefollowingasconstraint satisfaction problems:
a. Rectilinearfloor-planning: findnon-overlappingplacesinalargerectangleforanumber
ofsmallerrectangles.
b. Classscheduling: Thereisafixednumberofprofessorsandclassrooms,alistofclasses
to be offered, and a list of possible time slots for classes. Each professor has a set of
classesthatheorshecanteach.
c. Hamiltoniantour: givenanetworkofcitiesconnectedbyroads,chooseanordertovisit
allcitiesinacountry withoutrepeating any.
6.5 Solve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of back-
tracking withforwardcheckingandtheMRVandleast-constraining-value heuristics.
6.6 Show how a single ternary constraint such as “A+B = C” can be turned into three
binary constraints by using an auxiliary variable. You may assume finite domains. (Hint:
Consider a new variable that takes on values that are pairs of other values, and consider
constraints such as “X is the first element of the pair Y.”) Next, show how constraints with
morethanthreevariablescanbetreatedsimilarly. Finally,showhowunaryconstraintscanbe
eliminated by altering the domains of variables. This completes the demonstration that any
CSPcanbetransformed intoaCSPwithonlybinaryconstraints.
6.7 Considerthefollowinglogicpuzzle: Infivehouses,eachwithadifferentcolor,livefive
personsofdifferentnationalities, eachofwhomprefersadifferentbrandofcandy,adifferent
drink,andadifferentpet. Giventhefollowingfacts,thequestionstoanswerare“Wheredoes
thezebralive,andinwhichhousedotheydrinkwater?”
5 Ginsbergetal.(1990)discussseveralmethodsforconstructingcrosswordpuzzles.Littmanetal.(1999)tackle
theharderproblemofsolvingthem.
232 Chapter 6. ConstraintSatisfaction Problems
TheEnglishmanlivesintheredhouse.
TheSpaniardownsthedog.
TheNorwegianlivesinthefirsthouseontheleft.
Thegreenhouseisimmediately totherightoftheivoryhouse.
ThemanwhoeatsHersheybarslivesinthehousenexttothemanwiththefox.
KitKatsareeatenintheyellowhouse.
TheNorwegianlivesnexttothebluehouse.
TheSmartieseaterownssnails.
TheSnickerseaterdrinksorangejuice.
TheUkrainiandrinkstea.
TheJapaneseeatsMilkyWays.
KitKatsareeateninahousenexttothehousewherethehorseiskept.
Coffeeisdrunkinthegreenhouse.
Milkisdrunkinthemiddlehouse.
Discussdifferent representations ofthisproblem asaCSP. Whywouldonepreferonerepre-
sentation overanother?
6.8 Consider the graph with 8 nodes A , A , A , A , H, T, F , F . A is connected to
1 2 3 4 1 2 i
A for all i, each A is connected to H, H is connected to T, and T is connected to each
i+1 i
F . Find a 3-coloring of this graph by hand using the following strategy: backtracking with
i
conflict-directed backjumping, the variable order A , H, A , F , A , F , A , T, and the
1 4 1 2 2 3
valueorder R,G,B.
6.9 Explainwhyitisagoodheuristictochoosethevariablethatismostconstrainedbutthe
valuethatisleastconstraining inaCSPsearch.
6.10 Generate random instances of map-coloring problems as follows: scatter n points on
theunit square; select apoint X atrandom, connect X byastraight linetothenearest point
Y such that X is not already connected to Y and the line crosses no other line; repeat the
previous step until no more connections are possible. The points represent regions on the
map and the lines connect neighbors. Now try to find k-colorings of each map, for both
k=3andk=4,usingmin-conflicts, backtracking, backtracking withforwardchecking, and
backtracking withMAC.Constructatableofaverageruntimesforeachalgorithmforvalues
ofnuptothelargestyoucanmanage. Commentonyourresults.
6.11 Use the AC-3 algorithm to show that arc consistency can detect the inconsistency of
thepartialassignment {WA=green,V =red}fortheproblem showninFigure6.1.
6.12 Whatistheworst-casecomplexity ofrunning AC-3onatree-structured CSP?
6.13 AC-3 puts back on the queue every arc (X ,X ) whenever any value is deleted from
k i
thedomainofX ,evenifeachvalueofX isconsistentwithseveralremainingvaluesofX .
i k i
Supposethat,foreveryarc(X ,X ),wekeeptrackofthenumberofremainingvaluesof X
k i i
that are consistent with each value of X . Explain how to update these numbers efficiently
k
andhenceshowthatarcconsistency canbeenforced intotaltimeO(n2d2).
Exercises 233
6.14 TheTREE-CSP-SOLVER (Figure6.10)makesarcsconsistentstartingattheleavesand
working backwards towards the root. Whydoes itdothat? What would happen ifitwentin
theopposite direction?
6.15 We introduced Sudoku as a CSP to be solved by search over partial assignments be-
causethatisthewaypeoplegenerallyundertakesolvingSudokuproblems. Itisalsopossible,
of course, to attack these problems with local search over complete assignments. How well
wouldalocalsolverusingthemin-conflictsheuristic doonSudokuproblems?
6.16 Define in your own words the terms constraint, backtracking search, arc consistency,
backjumping, min-conflicts, andcyclecutset.
6.17 Supposethatagraphisknowntohaveacyclecutsetofnomorethanknodes. Describe
asimple algorithm forfinding aminimalcyclecutset whoserun timeisnotmuchmorethan
O(nk)foraCSPwithnvariables. Searchtheliteratureformethodsforfindingapproximately
minimal cycle cutsets intimethat ispolynomial inthe size ofthe cutset. Doesthe existence
ofsuchalgorithmsmakethecyclecutsetmethodpractical?
7
LOGICAL AGENTS
Inwhichwedesignagentsthatcanformrepresentationsofacomplexworld,usea
processofinference toderivenewrepresentations abouttheworld,andusethese
newrepresentations todeduce whattodo.
Humans, it seems, know things; and what they know helps them do things. These are
not empty statements. They make strong claims about how the intelligence of humans is
achieved—not by purely reflex mechanisms but by processes of reasoning that operate on
REASONING
internal representations of knowledge. In AI, this approach to intelligence is embodied in
REPRESENTATION
KNOWLEDGE-BASED knowledge-basedagents.
AGENTS
Theproblem-solvingagentsofChapters3and4knowthings,butonlyinaverylimited,
inflexiblesense. Forexample, thetransition modelforthe8-puzzle—knowledge ofwhatthe
actions do—is hidden inside the domain-specific code of the RESULT function. It can be
used to predict the outcome of actions but not to deduce that two tiles cannot occupy the
samespaceorthatstateswithoddparitycannotbereachedfromstateswithevenparity. The
atomic representations used by problem-solving agents are also very limiting. In a partially
observable environment, an agent’s only choice for representing what it knows about the
currentstateistolistallpossibleconcretestates—ahopelessprospectinlargeenvironments.
Chapter 6 introduced the idea of representing states as assignments of values to vari-
ables; this is a step in the right direction, enabling some parts of the agent to work in a
domain-independent way and allowing for more efficient algorithms. In this chapter and
those that follow, we take this step to its logical conclusion, so to speak—we develop logic
LOGIC
as a general class of representations to support knowledge-based agents. Such agents can
combineandrecombineinformationtosuitmyriadpurposes. Often,thisprocesscanbequite
far removed from the needs of the moment—as when a mathematician proves a theorem or
anastronomercalculatestheearth’slifeexpectancy. Knowledge-basedagentscanacceptnew
tasksintheformofexplicitlydescribedgoals;theycanachievecompetencequicklybybeing
toldorlearning newknowledge about theenvironment; andtheycanadapttochanges inthe
environment byupdating therelevantknowledge.
We begin in Section 7.1 with the overall agent design. Section 7.2 introduces a sim-
plenewenvironment, thewumpusworld, andillustrates theoperation ofaknowledge-based
agentwithoutgoingintoanytechnicaldetail. Thenweexplainthegeneralprinciplesoflogic
234
Section7.1. Knowledge-Based Agents 235
in Section 7.3 and the specifics of propositional logic in Section 7.4. While less expressive
than first-order logic (Chapter 8), propositional logic illustrates all the basic concepts of
logic; it also comes with well-developed inference technologies, which we describe in sec-
tions7.5and7.6. Finally,Section7.7combinestheconceptofknowledge-based agentswith
thetechnology ofpropositional logictobuildsomesimpleagentsforthewumpusworld.
7.1 KNOWLEDGE-BASED AGENTS
Thecentralcomponent ofaknowledge-based agentisitsknowledgebase,orKB.Aknowl-
KNOWLEDGEBASE
edge base is a set of sentences. (Here “sentence” is used as a technical term. It is related
SENTENCE
but not identical to the sentences of English and other natural languages.) Each sentence is
KNOWLEDGE
expressed in a language called a knowledge representation language and represents some
REPRESENTATION
LANGUAGE
assertion about theworld. Sometimeswedignify asentence withthenameaxiom, whenthe
AXIOM
sentence istakenasgivenwithoutbeingderivedfromothersentences.
There must be a way to add new sentences to the knowledge base and a way to query
what is known. The standard names for these operations are TELL and ASK, respectively.
Bothoperations mayinvolve inference—that is,deriving newsentences fromold. Inference
INFERENCE
mustobeytherequirementthatwhenoneASKsaquestionoftheknowledgebase,theanswer
should follow from whathasbeen told(or TELLed)totheknowledge basepreviously. Later
in this chapter, we will be more precise about the crucial word “follow.” Fornow, take it to
meanthattheinferenceprocess shouldnotmakethingsupasitgoesalong.
Figure7.1showstheoutlineofaknowledge-based agentprogram. Likeallouragents,
ittakesaperceptasinputandreturnsanaction. Theagentmaintainsaknowledgebase, KB,
BACKGROUND whichmayinitially containsomebackgroundknowledge.
KNOWLEDGE
Each time the agent program is called, it does three things. First, it TELLs the knowl-
edge base what it perceives. Second, it ASKs the knowledge base what action it should
perform. In the process of answering this query, extensive reasoning may be done about
the current state of the world, about the outcomes of possible action sequences, and so on.
Third,theagentprogram TELLstheknowledgebasewhichactionwaschosen, andtheagent
executestheaction.
Thedetails oftherepresentation language arehidden insidethreefunctions thatimple-
menttheinterface between thesensors andactuators ononesideandthecorerepresentation
and reasoning system on the other. MAKE-PERCEPT-SENTENCE constructs a sentence as-
sertingthattheagentperceivedthegivenperceptatthegiventime. MAKE-ACTION-QUERY
constructs a sentence that asks what action should be done at the current time. Finally,
MAKE-ACTION-SENTENCE constructs a sentence asserting that the chosen action was ex-
ecuted. The details of the inference mechanisms are hidden inside TELL and ASK. Later
sections willrevealthesedetails.
TheagentinFigure7.1appearsquitesimilartotheagentswithinternalstatedescribed
in Chapter 2. Because of the definitions of TELL and ASK, however, the knowledge-based
agent is not an arbitrary program for calculating actions. It is amenable to a description at
236 Chapter 7. LogicalAgents
functionKB-AGENT(percept)returnsanaction
persistent: KB,aknowledgebase
t,acounter,initially0,indicatingtime
TELL(KB,MAKE-PERCEPT-SENTENCE(percept,t))
action←ASK(KB,MAKE-ACTION-QUERY(t))
TELL(KB,MAKE-ACTION-SENTENCE(action,t))
t←t +1
returnaction
Figure7.1 Agenericknowledge-basedagent.Givenapercept,theagentaddsthepercept
toitsknowledgebase,askstheknowledgebaseforthebestaction,andtellstheknowledge
basethatithasinfacttakenthataction.
the knowledge level, where we need specify only what the agent knows and what its goals
KNOWLEDGELEVEL
are, in order to fix its behavior. For example, an automated taxi might have the goal of
taking a passenger from San Francisco to Marin County and might know that the Golden
Gate Bridge is the only link between the two locations. Then we can expect it to cross the
GoldenGateBridgebecauseitknowsthatthatwillachieveitsgoal. Noticethatthisanalysis
IMPLEMENTATION isindependent ofhowthetaxiworksattheimplementationlevel. Itdoesn’t matterwhether
LEVEL
itsgeographicalknowledgeisimplementedaslinkedlistsorpixelmaps,orwhetheritreasons
by manipulating strings of symbols stored in registers or by propagating noisy signals in a
networkofneurons.
A knowledge-based agent can be built simply by TELLing it what it needs to know.
Starting with an empty knowledge base, the agent designer can TELL sentences one by one
until the agent knows how to operate in its environment. This is called the declarative ap-
DECLARATIVE
proach to system building. In contrast, the procedural approach encodes desired behaviors
directly asprogram code. Inthe1970s and1980s, advocates ofthetwoapproaches engaged
inheateddebates. Wenowunderstandthatasuccessfulagentoftencombinesbothdeclarative
andprocedural elements initsdesign, andthatdeclarative knowledge canoften becompiled
intomoreefficientprocedural code.
We can also provide a knowledge-based agent with mechanisms that allow it to learn
for itself. These mechanisms, which are discussed in Chapter 18, create general knowledge
abouttheenvironment fromaseriesofpercepts. Alearning agentcanbefullyautonomous.
7.2 THE WUMPUS WORLD
Inthissection wedescribe anenvironment inwhichknowledge-based agents canshowtheir
worth. Thewumpusworldisacaveconsistingofroomsconnectedbypassageways. Lurking
WUMPUSWORLD
somewhere in the cave isthe terrible wumpus, a beast that eats anyone who enters its room.
Thewumpuscanbeshotbyanagent,buttheagenthasonlyonearrow. Someroomscontain
Section7.2. TheWumpusWorld 237
bottomless pitsthatwilltrapanyone whowanders intothese rooms(except forthewumpus,
which is too big to fall in). The only mitigating feature of this bleak environment is the
possibility of finding a heap of gold. Although the wumpus world is rather tame by modern
computergamestandards, itillustrates someimportant pointsaboutintelligence.
A sample wumpus world is shown in Figure 7.2. The precise definition of the task
environment isgiven,assuggested inSection2.3,bythePEASdescription:
• Performance measure: +1000 for climbing out of the cave with the gold, –1000 for
falling into a pit or being eaten by the wumpus, –1 for each action taken and –10 for
usingupthearrow. Thegameendseitherwhentheagentdiesorwhentheagentclimbs
outofthecave.
• Environment: A 4×4 grid of rooms. The agent always starts in the square labeled
[1,1], facing to the right. The locations of the gold and the wumpus are chosen ran-
domly, with a uniform distribution, from the squares other than the start square. In
addition, eachsquareotherthanthestartcanbeapit,withprobability 0.2.
• Actuators: The agent can move Forward, TurnLeft by 90 ◦ , or TurnRight by 90 ◦ . The
agent dies a miserable death if it enters a square containing apit ora live wumpus. (It
issafe, albeit smelly, toenter asquare with adead wumpus.) Ifan agent tries to move
forward and bumps into awall, then the agent does notmove. Theaction Grabcan be
used to pick up the gold if it is in the same square as the agent. The action Shoot can
beusedtofireanarrowinastraight lineinthedirection theagentisfacing. Thearrow
continues untiliteitherhits(andhence kills) thewumpusorhits awall. Theagent has
only one arrow, so only the first Shoot action has any effect. Finally, the action Climb
canbeusedtoclimboutofthecave,butonlyfromsquare[1,1].
• Sensors: Theagenthasfivesensors, eachofwhichgivesasinglebitofinformation:
– Inthe square containing the wumpusand in thedirectly (not diagonally) adjacent
squares, theagentwillperceiveaStench.
– Inthesquares directlyadjacent toapit,theagentwillperceiveaBreeze.
– Inthesquarewherethegoldis,theagentwillperceivea Glitter.
– Whenanagentwalksintoawall,itwillperceiveaBump.
– When the wumpus is killed, it emits a woeful Scream that can be perceived any-
whereinthecave.
The percepts will be given to the agent program in the form of a list of five symbols;
forexample, ifthereisastenchandabreeze, butnoglitter, bump,orscream,theagent
programwillget [Stench,Breeze,None,None,None].
We can characterize the wumpus environment along the various dimensions given in Chap-
ter2. Clearly,itisdiscrete,static,andsingle-agent. (Thewumpusdoesn’tmove,fortunately.)
It is sequential, because rewards may come only after many actions are taken. It is partially
observable, because some aspects of the state are not directly perceivable: the agent’s lo-
cation, the wumpus’s state of health, and the availability of an arrow. As for the locations
of the pits and the wumpus: we could treat them as unobserved parts of the state that hap-
pentobeimmutable—in whichcase, thetransition modelfortheenvironment iscompletely
238 Chapter 7. LogicalAgents
4 Stench Breeze PIT
Breeze
3 Stench PIT Breeze
Gold
2 Stench Breeze
1 Breeze PIT Breeze
START
1 2 3 4
Figure7.2 Atypicalwumpusworld.Theagentisinthebottomleftcorner,facingright.
known;orwecouldsaythatthetransitionmodelitselfisunknownbecausetheagentdoesn’t
know which Forward actions are fatal—in which case, discovering the locations of pits and
wumpuscompletestheagent’sknowledge ofthetransition model.
Foran agent in the environment, the main challenge is its initial ignorance of the con-
figurationoftheenvironment; overcomingthisignorance seemstorequirelogicalreasoning.
Inmostinstancesofthewumpusworld,itispossiblefortheagenttoretrievethegoldsafely.
Occasionally, theagentmustchoosebetweengoinghomeempty-handedandriskingdeathto
find the gold. About 21% of the environments are utterly unfair, because the gold is in a pit
orsurrounded bypits.
Let us watch a knowledge-based wumpus agent exploring the environment shown in
Figure 7.2. We use an informal knowledge representation language consisting of writing
downsymbolsinagrid(asinFigures7.3and7.4).
The agent’s initial knowledge base contains the rules of the environment, as described
previously; inparticular, itknowsthat itisin[1,1] and that [1,1]is asafe square; wedenote
thatwithan“A”and“OK,”respectively, insquare[1,1].
Thefirstpercept is [None,None,None,None,None], from which the agent can con-
clude that its neighboring squares, [1,2] and [2,1], are free of dangers—they are OK. Fig-
ure7.3(a)showstheagent’sstateofknowledgeatthispoint.
A cautious agent will move only into a square that it knows to be OK. Let us suppose
theagentdecidestomoveforwardto[2,1]. Theagentperceivesabreeze(denotedby“B”)in
[2,1],sotheremustbeapitinaneighboringsquare. Thepitcannotbein[1,1],bytherulesof
the game, so there must be apitin [2,2] or[3,1] orboth. The notation “P?” inFigure 7.3(b)
indicates apossible pitinthosesquares. Atthispoint, thereisonlyoneknownsquare thatis
OKandthathasnotyetbeenvisited. Sotheprudentagentwillturnaround, gobackto[1,1],
andthenproceed to[1,2].
The agent perceives a stench in [1,2], resulting in the state of knowledge shown in
Figure 7.4(a). The stench in [1,2] means that there must be a wumpus nearby. But the
Section7.2. TheWumpusWorld 239
1,4 2,4 3,4 4,4 A = Agent 1,4 2,4 3,4 4,4
B = Breeze
G = Glitter, Gold
OK = Safe square
1,3 2,3 3,3 4,3 P = Pit 1,3 2,3 3,3 4,3
S = Stench
V = Visited
W = Wumpus
1,2 2,2 3,2 4,2 1,2 2,2 3,2 4,2
P?
OK OK
1,1 2,1 3,1 4,1 1,1 2,1 3,1 4,1
A P?
A
V B
OK OK OK OK
(a) (b)
Figure 7.3 The first step taken by the agent in the wumpus world. (a) The initial sit-
uation, after percept [None,None,None,None,None]. (b) After one move, with percept
[None,Breeze,None,None,None].
1,4 2,4 3,4 4,4 A = Agent 1,4 2,4 3,4 4,4
P?
B = Breeze
G = Glitter, Gold
OK = Safe square
1,3 W! 2,3 3,3 4,3 P = Pit 1,3 W! 2,3 A 3,3 P? 4,3
S = Stench
S G
V = Visited
B
W = Wumpus
1,2 2,2 3,2 4,2 1,2 2,2 3,2 4,2
A S
S V V
OK OK OK OK
1,1 2,1 3,1 4,1 1,1 2,1 3,1 4,1
B P! B P!
V V V V
OK OK OK OK
(a) (b)
Figure 7.4 Two later stages in the progress of the agent. (a) After the third move,
with percept [Stench,None,None,None,None]. (b) After the fifth move, with percept
[Stench,Breeze,Glitter,None,None].
wumpus cannot be in [1,1], by the rules of the game, and it cannot be in [2,2] (or the agent
would have detected a stench when it was in [2,1]). Therefore, the agent can infer that the
wumpusisin[1,3]. Thenotation W!indicates thisinference. Moreover, thelack ofabreeze
in[1,2]impliesthatthereisnopitin[2,2]. Yettheagenthasalready inferredthattheremust
be a pit in either [2,2] or [3,1], so this means it must be in [3,1]. This is a fairly difficult
inference, because it combines knowledge gained at different times in different places and
reliesonthelackofapercepttomakeonecrucialstep.
240 Chapter 7. LogicalAgents
Theagenthasnowprovedtoitselfthatthereisneitherapitnorawumpusin[2,2],soit
isOKtomovethere. Wedonotshowtheagent’sstateofknowledgeat[2,2];wejustassume
that the agent turns and moves to [2,3], giving us Figure 7.4(b). In [2,3], the agent detects a
glitter, soitshouldgrabthegoldandthenreturnhome.
Note that in each case for which the agent draws a conclusion from the available in-
formation, that conclusion is guaranteed tobecorrect iftheavailable information iscorrect.
This is a fundamental property of logical reasoning. In the rest of this chapter, we describe
howtobuildlogicalagentsthatcanrepresentinformationanddrawconclusionssuchasthose
described inthepreceding paragraphs.
7.3 LOGIC
This section summarizes the fundamental concepts of logical representation and reasoning.
These beautiful ideas are independent of any of logic’s particular forms. We therefore post-
pone the technical details of those forms until the next section, using instead the familiar
exampleofordinary arithmetic.
In Section 7.1, we said that knowledge bases consist of sentences. These sentences
areexpressed according tothe syntax oftherepresentation language, whichspecifies allthe
SYNTAX
sentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:
“x+y =4”isawell-formed sentence, whereas“x4y+ =”isnot.
Alogicmustalsodefinethesemanticsormeaningofsentences. Thesemanticsdefines
SEMANTICS
the truth of each sentence with respect to each possible world. Forexample, the semantics
TRUTH
for arithmetic specifies that the sentence “x +y=4” is true in a world where x is 2 and y
POSSIBLEWORLD
is 2, but false in a world where x is 1 and y is1. In standard logics, every sentence must be
eithertrueorfalseineachpossible world—there isno“inbetween.”1
When we need to be precise, we use the term model in place of “possible world.”
MODEL
Whereaspossibleworldsmightbethoughtofas(potentially)realenvironmentsthattheagent
might or might not be in, models are mathematical abstractions, each of which simply fixes
thetruthorfalsehoodofeveryrelevantsentence. Informally,wemaythinkofapossibleworld
as,forexample,havingxmenandywomensittingatatableplayingbridge,andthesentence
x+y=4 is true when there are four people in total. Formally, the possible models are just
allpossibleassignmentsofrealnumberstothevariables xandy. Eachsuchassignmentfixes
thetruthofanysentence ofarithmetic whosevariables are xandy. Ifasentence αistruein
model m, we say that m satisfies α or sometimes m is a model of α. We use the notation
SATISFACTION
M(α)tomeanthesetofallmodelsofα.
Nowthat wehave a notion of truth, we are ready to talk about logical reasoning. This
involves the relation of logical entailment between sentences—the idea that a sentence fol-
ENTAILMENT
lowslogically fromanothersentence. Inmathematicalnotation, wewrite
α |= β
1 Fuzzylogic,discussedinChapter14,allowsfordegreesoftruth.
Section7.3. Logic 241
2 2 PIT 2 2 PIT α
KB 1 1 Bree 2 ze P 3 IT α 1 1 Bre 2 eze 3 KB 1 1 Bree 2 ze P 3 IT 1 1 Bree 2 ze 3 2
1
2 PIT 2 2 PIT 2 PIT 2 2 PIT
1 1 Bree 2 ze 3 1 1 Bree 2 ze 3 1 1 Bree 2 ze P 3 IT 1 1 Bre 2 eze 3 1 1 Bree 2 ze 3 1 1 Bre 2 eze P 3 IT
2 PIT 2 PIT PIT 2 PIT 2 PIT PIT
1 Breeze PIT 2 PIT PIT 1 1 Bre 2 eze 3 1 Breeze PIT 2 PIT PIT 1 1 Bre 2 eze 3
1 2 3 1 2 3
1 Breeze PIT 1 Breeze PIT
1 2 3 1 2 3
(a) (b)
Figure7.5 Possiblemodelsforthepresenceofpitsinsquares[1,2],[2,2],and[3,1]. The
KB correspondingto theobservationsofnothingin [1,1]anda breezein[2,1]isshownby
the solid line. (a) Dotted line shows modelsof α (no pit in [1,2]). (b) Dotted line shows
1
modelsofα (nopitin[2,2]).
2
tomeanthatthesentenceαentailsthesentenceβ. Theformaldefinitionofentailmentisthis:
α |= β ifandonlyif,ineverymodelinwhichαistrue,β isalsotrue. Usingthenotationjust
introduced, wecanwrite
α |= β ifandonlyifM(α) ⊆ M(β).
(Notethedirection ofthe⊆here: ifα |= β,thenαisastronger assertionthanβ: itrulesout
more possible worlds.) The relation of entailment is familiar from arithmetic; we are happy
with the idea that the sentence x = 0 entails the sentence xy = 0. Obviously, in any model
wherexiszero,itisthecasethatxy iszero(regardlessofthevalueof y).
Wecanapplythesamekindofanalysistothewumpus-worldreasoning examplegiven
in the preceding section. Consider the situation in Figure 7.3(b): the agent has detected
nothing in[1,1]andabreezein[2,1]. Thesepercepts, combined withtheagent’s knowledge
of the rules of the wumpus world, constitute the KB. The agent is interested (among other
things) in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits. Each of the three
squaresmightormightnotcontainapit,so(forthepurposes ofthisexample)thereare23=8
possible models. TheseeightmodelsareshowninFigure7.5.2
The KB can be thought of as a set of sentences or as a single sentence that asserts all
the individual sentences. The KB is false in models that contradict what the agent knows—
for example, the KB is false in any model in which [1,2] contains a pit, because there is
no breeze in[1,1]. Thereare in fact just three models inwhich the KBis true, and these are
2 Althoughthefigureshowsthemodelsaspartialwumpusworlds,theyarereallynothingmorethanassignments
oftrue andfalsetothesentences“thereisapitin[1,2]”etc. Models,inthemathematicalsense,donotneedto
have’orrible’airywumpusesinthem.
242 Chapter 7. LogicalAgents
shownsurroundedbyasolidlineinFigure7.5. Nowletusconsidertwopossibleconclusions:
α = “Thereisnopitin[1,2].”
1
α = “Thereisnopitin[2,2].”
2
Wehavesurrounded themodelsof α andα withdotted linesinFigures7.5(a)and 7.5(b),
1 2
respectively. Byinspection, weseethefollowing:
ineverymodelinwhichKB istrue, α isalsotrue.
1
Hence,KB |= α : thereisnopitin[1,2]. Wecanalsoseethat
1
insomemodelsinwhichKB istrue, α isfalse.
2
Hence,KB (cid:7)|= α : theagentcannotconcludethatthereisnopitin[2,2]. (Norcanitconclude
2
thatthere isapitin[2,2].)3
Theprecedingexamplenotonlyillustratesentailmentbutalsoshowshowthedefinition
of entailment can be applied to derive conclusions—that is, to carry out logical inference.
LOGICALINFERENCE
The inference algorithm illustrated in Figure 7.5 is called model checking, because it enu-
MODELCHECKING
meratesallpossible modelstocheckthat αistrueinallmodelsinwhich KB istrue, thatis,
thatM(KB)⊆ M(α).
Inunderstanding entailmentandinference,itmighthelptothinkofthesetofallconse-
quences ofKB asahaystack andofαasaneedle. Entailmentisliketheneedlebeing inthe
haystack;inferenceislikefindingit. Thisdistinctionisembodiedinsomeformalnotation: if
aninferencealgorithm icanderiveαfrom KB,wewrite
KB (cid:20) α,
i
whichispronounced “αisderivedfrom KB byi”or“iderives αfromKB.”
An inference algorithm that derives only entailed sentences is called sound or truth-
SOUND
preserving. Soundness is a highly desirable property. An unsound inference procedure es-
TRUTH-PRESERVING
sentiallymakesthingsupasitgoesalong—itannouncesthediscoveryofnonexistentneedles.
Itiseasytoseethatmodelchecking, whenitisapplicable,4 isasoundprocedure.
The property of completeness is also desirable: an inference algorithm is complete if
COMPLETENESS
it can derive any sentence that is entailed. For real haystacks, which are finite in extent,
it seems obvious that a systematic examination can always decide whether the needle is in
thehaystack. Formanyknowledge bases, however, thehaystack ofconsequences isinfinite,
and completeness becomes an important issue.5 Fortunately, there are complete inference
procedures forlogicsthataresufficiently expressivetohandle manyknowledgebases.
We have described a reasoning process whose conclusions are guaranteed to be true
in any world in which the premises are true; in particular, if KB is true in the real world,
thenanysentence αderived fromKB byasoundinference procedure isalsotrueinthereal
world. So,whileaninferenceprocessoperateson“syntax”—internal physicalconfigurations
such as bits in registers or patterns of electrical blips in brains—the process corresponds
3 Theagentcancalculatetheprobabilitythatthereisapitin[2,2];Chapter13showshow.
4 Model checking works if the space of models isfinite—forexample, inwumpus worlds of fixedsize. For
arithmetic,ontheotherhand,thespaceofmodelsisinfinite: evenifwerestrictourselvestotheintegers,there
areinfinitelymanypairsofvaluesforxandyinthesentencex+y=4.
5 ComparewiththecaseofinfinitesearchspacesinChapter3,wheredepth-firstsearchisnotcomplete.
Section7.4. Propositional Logic: AVerySimpleLogic 243
Sentences Sentence
Entails
Follows
Semantics Semantics
Representation
World
Aspects of the Aspect of the
real world real world
Figure7.6 Sentencesarephysicalconfigurationsoftheagent,andreasoningisaprocess
of constructing new physical configurations from old ones. Logical reasoning should en-
surethatthenewconfigurationsrepresentaspectsoftheworldthatactuallyfollowfromthe
aspectsthattheoldconfigurationsrepresent.
to the real-world relationship whereby some aspect of the real world is the case6 by virtue
of other aspects of the real world being the case. This correspondence between world and
representation isillustrated inFigure7.6.
The final issue to consider is grounding—the connection between logical reasoning
GROUNDING
processes andthe real environment inwhich theagent exists. Inparticular, how do weknow
that KB is true in the real world? (After all, KB is just “syntax” inside the agent’s head.)
This is a philosophical question about which many, many books have been written. (See
Chapter26.) Asimpleansweristhattheagent’ssensors createtheconnection. Forexample,
our wumpus-world agent has a smell sensor. The agent program creates a suitable sentence
whenever there is a smell. Then, whenever that sentence is in the knowledge base, it is
true in the real world. Thus, the meaning and truth of percept sentences are defined by the
processesofsensingandsentenceconstruction thatproducethem. Whatabouttherestofthe
agent’s knowledge, such as its belief that wumpuses cause smells in adjacent squares? This
is not a direct representation of a single percept, but a general rule—derived, perhaps, from
perceptual experience but not identical to a statement of that experience. General rules like
this are produced by a sentence construction process called learning, which is the subject
of Part V. Learning is fallible. It could be the case that wumpuses cause smells except on
February29inleapyears,whichiswhentheytaketheirbaths. Thus,KB maynotbetruein
therealworld,butwithgoodlearning procedures, thereisreasonforoptimism.
7.4 PROPOSITIONAL LOGIC: A VERY SIMPLE LOGIC
PROPOSITIONAL Wenowpresentasimplebutpowerfullogiccalled propositional logic. Wecoverthesyntax
LOGIC
of propositional logic and its semantics—the way in which the truth of sentences is deter-
mined. Then we look at entailment—the relation between a sentence and another sentence
thatfollows from it—andseehow thisleads toasimple algorithm forlogical inference. Ev-
erything takesplace, ofcourse, inthewumpusworld.
6 AsWittgenstein(1922)putitinhisfamousTractatus:“Theworldiseverythingthatisthecase.”
244 Chapter 7. LogicalAgents
7.4.1 Syntax
The syntax of propositional logic defines the allowable sentences. The atomic sentences
ATOMICSENTENCES
PROPOSITION consist of a single proposition symbol. Each such symbol stands for a proposition that can
SYMBOL
be true or false. We use symbols that start with an uppercase letter and may contain other
letters or subscripts, for example: P, Q, R, W and North. The names are arbitrary but
1,3
are often chosen to have some mnemonic value—we use W to stand for the proposition
1,3
that the wumpus is in [1,3]. (Remember that symbols such as W are atomic, i.e., W, 1,
1,3
and3arenotmeaningfulpartsofthesymbol.) Therearetwoproposition symbolswithfixed
meanings: True is the always-true proposition and False is the always-false proposition.
COMPLEX Complex sentences are constructed from simpler sentences, using parentheses and logical
SENTENCES
LOGICAL connectives. Therearefiveconnectives incommonuse:
CONNECTIVES
¬ (not). A sentence such as ¬W is called the negation of W . A literal is either an
NEGATION 1,3 1,3
atomicsentence(apositiveliteral)oranegatedatomicsentence (anegative literal).
LITERAL
∧ (and). A sentence whose main connective is ∧, such as W ∧P , is called a con-
1,3 3,1
junction;itspartsaretheconjuncts. (The∧lookslikean“A”for“And.”)
CONJUNCTION
∨ (or). Asentenceusing∨,suchas(W ∧P )∨W ,isadisjunctionofthedisjuncts
DISJUNCTION 1,3 3,1 2,2
(W ∧P )andW . (Historically, the ∨comes fromtheLatin“vel,”which means
1,3 3,1 2,2
“or.” Formostpeople, itiseasiertoremember ∨asanupside-down ∧.)
⇒ (implies). Asentencesuchas(W ∧P ) ⇒ ¬W iscalledanimplication(orcon-
IMPLICATION 1,3 3,1 2,2
ditional). Itspremiseorantecedentis(W ∧P ),anditsconclusionorconsequent
PREMISE 1,3 3,1
is¬W . Implications arealsoknownas rulesorif–thenstatements. Theimplication
CONCLUSION 2,2
symbolissometimeswritteninotherbooksas ⊃or→.
RULES
⇔ (if and only if). The sentence W ⇔ ¬W is a biconditional. Some other books
BICONDITIONAL 1,3 2,2
writethisas≡.
Sentence → AtomicSentence | ComplexSentence
AtomicSentence → True | False | P | Q | R | ...
ComplexSentence → (Sentence )| [Sentence ]
| ¬Sentence
| Sentence ∧Sentence
| Sentence ∨Sentence
| Sentence ⇒ Sentence
| Sentence ⇔ Sentence
OPERATORPRECEDENCE : ¬,∧,∨,⇒,⇔
Figure 7.7 A BNF (Backus–Naur Form) grammar of sentences in propositional logic,
alongwithoperatorprecedences,fromhighesttolowest.
Section7.4. Propositional Logic: AVerySimpleLogic 245
Figure7.7givesaformalgrammarofpropositional logic; seepage1060 ifyouarenot
familiar with the BNF notation. The BNF grammar by itself is ambiguous; a sentence with
severaloperatorscanbeparsedbythegrammarinmultipleways. Toeliminatetheambiguity
wedefineaprecedenceforeachoperator. The“not”operator(¬)hasthehighestprecedence,
which means that in the sentence ¬A∧B the ¬binds most tightly, giving us the equivalent
of(¬A)∧Bratherthan¬(A∧B). (Thenotationforordinaryarithmeticisthesame: −2+4
is2,not–6.) Whenindoubt,useparentheses tomakesureoftherightinterpretation. Square
brackets meanthesamething asparentheses; thechoice ofsquare brackets orparentheses is
solelytomakeiteasierforahumantoreadasentence.
7.4.2 Semantics
Having specified the syntax of propositional logic, we now specify its semantics. The se-
mantics defines the rules for determining the truth of a sentence with respect to a particular
model. Inpropositional logic, amodelsimply fixesthetruthvalue—true orfalse—forev-
TRUTHVALUE
eryproposition symbol. Forexample,ifthesentencesinthe knowledgebasemakeuseofthe
proposition symbols P ,P ,andP ,thenonepossible modelis
1,2 2,2 3,1
m = {P =false, P =false, P =true}.
1 1,2 2,2 3,1
With three proposition symbols, there are 23=8 possible models—exactly those depicted
in Figure 7.5. Notice, however, that the models are purely mathematical objects with no
necessary connection to wumpus worlds. P is just asymbol; itmight mean “there is apit
1,2
in[1,2]”or“I’minParistodayandtomorrow.”
The semantics for propositional logic must specify how to compute the truth value of
any sentence, given a model. This is done recursively. All sentences are constructed from
atomicsentences andthefiveconnectives; therefore, weneed tospecify how tocompute the
truthofatomicsentences andhowtocomputethetruthofsentences formedwitheachofthe
fiveconnectives. Atomicsentences areeasy:
• True istrueineverymodeland False isfalseineverymodel.
• The truth value of every other proposition symbol must be specified directly in the
model. Forexample,inthemodelm givenearlier, P isfalse.
1 1,2
Forcomplexsentences, wehave fiverules, whichhold foranysubsentences P andQinany
modelm(here“iff”means“ifandonlyif”):
• ¬P istrueiffP isfalseinm.
• P ∧QistrueiffbothP andQaretruein m.
• P ∨Qistrueiffeither P orQistrueinm.
• P ⇒ Qistrueunless P istrueandQisfalseinm.
• P ⇔ QistrueiffP andQarebothtrueorbothfalsein m.
The rules can also be expressed with truth tables that specify the truth value of a complex
TRUTHTABLE
sentence foreach possible assignment oftruth values toits components. Truthtables forthe
fiveconnectives aregiven inFigure7.8. Fromthese tables, thetruth value ofanysentence s
canbecomputedwithrespecttoanymodelmbyasimplerecursiveevaluation. Forexample,
246 Chapter 7. LogicalAgents
P Q ¬P P ∧Q P ∨Q P ⇒ Q P ⇔ Q
false false true false false true true
false true true false true true false
true false false false true false false
true true false true true true true
Figure7.8 Truthtablesforthefivelogicalconnectives. Tousethetabletocompute,for
example,thevalueofP ∨QwhenP istrueandQisfalse,firstlookontheleftfortherow
whereP istrueandQisfalse(thethirdrow).ThenlookinthatrowundertheP∨Qcolumn
toseetheresult: true.
the sentence ¬P ∧(P ∨ P ), evaluated in m , gives true ∧(false ∨true)=true ∧
1,2 2,2 3,1 1
true=true. Exercise7.3asksyoutowritethealgorithm PL-TRUE?(s,m),whichcomputes
thetruthvalueofapropositional logicsentence sinamodelm.
Thetruth tables for“and,” “or,” and“not” areinclose accord withourintuitions about
theEnglishwords. ThemainpointofpossibleconfusionisthatP ∨QistruewhenP istrue
or Q is true or both. A different connective, called “exclusive or” (“xor” for short), yields
false when both disjuncts are true.7 There is no consensus on the symbol for exclusive or;
somechoicesare∨˙ or(cid:7)=or⊕.
Thetruth table for ⇒ maynot quite fitone’s intuitive understanding of “P implies Q”
or“ifP thenQ.” Foronething,propositionallogicdoesnotrequireanyrelationofcausation
orrelevancebetweenP andQ. Thesentence“5isoddimpliesTokyoisthecapitalofJapan”
is a true sentence of propositional logic (under the normal interpretation), even though it is
a decidedly odd sentence of English. Another point of confusion is that any implication is
true whenever its antecedent is false. Forexample, “5 is even implies Sam is smart” is true,
regardless of whether Sam is smart. This seems bizarre, but it makes sense if you think of
“P ⇒ Q”assaying,“IfP istrue,thenIamclaimingthatQistrue. OtherwiseIammaking
noclaim.” Theonlywayforthissentence tobefalseisifP istruebutQisfalse.
The biconditional, P ⇔ Q, is true whenever both P ⇒ Q and Q ⇒ P are true. In
English,thisisoftenwrittenas“P ifandonlyifQ.” Manyoftherulesofthewumpusworld
are best written using ⇔. Forexample, a square is breezy if a neighboring square has a pit,
andasquareisbreezy onlyifaneighboring squarehasapit. Soweneedabiconditional,
B ⇔ (P ∨P ),
1,1 1,2 2,1
whereB meansthatthereisabreezein[1,1].
1,1
7.4.3 A simpleknowledge base
Nowthatwehavedefinedthesemanticsforpropositionallogic,wecanconstructaknowledge
base for the wumpus world. We focus first on the immutable aspects of the wumpus world,
leaving the mutable aspects for a later section. Fornow, weneed the following symbols for
each[x,y]location:
7 Latinhasaseparateword,aut,forexclusiveor.
Section7.4. Propositional Logic: AVerySimpleLogic 247
P istrueifthereisapitin[x,y].
x,y
W istrueifthereisawumpusin [x,y],deadoralive.
x,y
B istrueiftheagentperceives abreezein [x,y].
x,y
S istrueiftheagentperceivesastenchin [x,y].
x,y
The sentences we write will suffice to derive ¬P (there is no pit in [1,2]), as was done
1,2
informally inSection7.3. Welabeleachsentence R sothatwecanrefertothem:
i
• Thereisnopitin[1,1]:
R : ¬P .
1 1,1
• A square is breezy if and only if there is a pit in a neighboring square. This has to be
statedforeachsquare; fornow,weincludejusttherelevant squares:
R : B ⇔ (P ∨P ).
2 1,1 1,2 2,1
R : B ⇔ (P ∨P ∨P ).
3 2,1 1,1 2,2 3,1
• The preceding sentences are true in all wumpus worlds. Now we include the breeze
perceptsforthefirsttwosquaresvisitedinthespecificworldtheagentisin,leadingup
tothesituation inFigure7.3(b).
R : ¬B .
4 1,1
R : B .
5 2,1
7.4.4 A simpleinference procedure
Our goal now is to decide whether KB |= α for some sentence α. For example, is ¬P
1,2
entailedbyourKB? Ourfirstalgorithm forinference isamodel-checking approachthatisa
direct implementation of the definition of entailment: enumerate the models, and check that
α is true in every model in which KB is true. Models are assignments of true or false to
every proposition symbol. Returning to our wumpus-world example, the relevant proposi-
tion symbols are B , B , P , P , P , P , and P . With seven symbols, there are
1,1 2,1 1,1 1,2 2,1 2,2 3,1
27=128 possible models; in three of these, KB is true (Figure 7.9). In those three models,
¬P istrue,hencethereisnopitin[1,2]. Ontheotherhand, P istrueintwoofthethree
1,2 2,2
modelsandfalseinone,sowecannotyettellwhetherthereisapitin[2,2].
Figure7.9reproduces inamoreprecise form thereasoning illustrated inFigure7.5. A
generalalgorithmfordecidingentailmentinpropositionallogicisshowninFigure7.10. Like
the BACKTRACKING-SEARCH algorithm on page 215, TT-ENTAILS? performs a recursive
enumeration of afinite space of assignments to symbols. Thealgorithm is soundbecause it
implements directly thedefinition ofentailment, and completebecause itworksforany KB
andαandalwaysterminates—there areonlyfinitelymanymodelsto examine.
Of course, “finitely many” is not always the same as “few.” If KB and α contain n
symbols in all, then there are 2n models. Thus, the time complexity of the algorithm is
O(2n). (ThespacecomplexityisonlyO(n)becausetheenumeration isdepth-first.) Laterin
this chapter weshow algorithms that are much more efficient in many cases. Unfortunately,
propositional entailment isco-NP-complete (i.e.,probably noeasierthanNP-complete—see
Appendix A), so every known inference algorithm for propositional logic has a worst-case
complexity thatisexponential inthesizeoftheinput.
248 Chapter 7. LogicalAgents
B 1,1 B 2,1 P 1,1 P 1,2 P 2,1 P 2,2 P 3,1 R 1 R 2 R 3 R 4 R 5 KB
false false false false false false false true true true true false false
false false false false false false true true true false true false false
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
false true false false false false false true true false true true false
false true false false false false true true true true true true true
false true false false false true false true true true true true true
false true false false false true true true true true true true true
false true false false true false false true false false true true false
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
true true true true true true true false true true false true false
Figure7.9 Atruthtableconstructedfortheknowledgebasegiveninthetext. KBistrue
ifR throughR aretrue,whichoccursinjust3ofthe128rows(theonesunderlinedinthe
1 5
right-handcolumn).Inall3rows,P 1,2 isfalse,sothereisnopitin[1,2].Ontheotherhand,
theremight(ormightnot)beapitin[2,2].
functionTT-ENTAILS?(KB,α)returnstrue orfalse
inputs:KB,theknowledgebase,asentenceinpropositionallogic
α,thequery,asentenceinpropositionallogic
symbols←alistofthepropositionsymbolsinKB andα
returnTT-CHECK-ALL(KB,α,symbols,{})
functionTT-CHECK-ALL(KB,α,symbols,model)returnstrue orfalse
ifEMPTY?(symbols)then
ifPL-TRUE?(KB,model)thenreturnPL-TRUE?(α,model)
elsereturntrue // whenKBisfalse,alwaysreturntrue
elsedo
P←FIRST(symbols)
rest←REST(symbols)
return(TT-CHECK-ALL(KB,α,rest,model ∪{P = true})
and
TT-CHECK-ALL(KB,α,rest,model ∪{P = false }))
Figure 7.10 A truth-table enumeration algorithm for deciding propositionalentailment.
(TTstandsfortruthtable.) PL-TRUE? returnstrueifasentenceholdswithinamodel. The
variablemodelrepresentsapartialmodel—anassignmenttosomeofthesymbols. Thekey-
word“and”isusedhereasalogicaloperationonitstwoarguments,returningtrue orfalse.
Section7.5. Propositional TheoremProving 249
(α∧β) ≡ (β ∧α) commutativity of∧
(α∨β) ≡ (β ∨α) commutativity of∨
((α∧β)∧γ) ≡ (α∧(β∧γ)) associativity of∧
((α∨β)∨γ) ≡ (α∨(β∨γ)) associativity of∨
¬(¬α) ≡ α double-negation elimination
(α ⇒ β) ≡ (¬β ⇒ ¬α) contraposition
(α ⇒ β) ≡ (¬α∨β) implication elimination
(α ⇔ β) ≡ ((α ⇒ β)∧(β ⇒ α)) biconditional elimination
¬(α∧β) ≡ (¬α∨¬β) DeMorgan
¬(α∨β) ≡ (¬α∧¬β) DeMorgan
(α∧(β ∨γ)) ≡ ((α∧β)∨(α∧γ)) distributivity of ∧ over∨
(α∨(β ∧γ)) ≡ ((α∨β)∧(α∨γ)) distributivity of ∨ over∧
Figure7.11 Standardlogicalequivalences. Thesymbolsα, β, andγ standforarbitrary
sentencesofpropositionallogic.
7.5 PROPOSITIONAL THEOREM PROVING
Sofar,wehaveshownhowtodetermineentailmentbymodelchecking: enumeratingmodels
and showing that the sentence must hold in all models. In this section, weshow how entail-
mentcanbedonebytheoremproving—applying rulesofinferencedirectlytothesentences
THEOREMPROVING
inourknowledgebasetoconstructaproofofthedesiredsentencewithoutconsultingmodels.
Ifthenumberofmodelsislargebutthelengthoftheproofisshort,thentheoremprovingcan
bemoreefficientthanmodelchecking.
Before we plunge into the details of theorem-proving algorithms, we will need some
LOGICAL additional concepts related toentailment. Thefirstconcept islogical equivalence: twosen-
EQUIVALENCE
tences α and β are logically equivalent if they are true in the same set of models. We write
this as α ≡ β. Forexample, wecan easily show (using truth tables) that P ∧Q and Q∧P
are logically equivalent; other equivalences are shown in Figure 7.11. These equivalences
play much the same role in logic as arithmetic identities do in ordinary mathematics. An
alternative definition of equivalence is asfollows: any two sentences α and β are equivalent
onlyifeachofthementailstheother:
α ≡ β ifandonlyif α |= β andβ |= α.
Thesecondconceptwewillneedisvalidity. Asentenceisvalidifitistrueinallmodels. For
VALIDITY
example,thesentenceP ∨¬P isvalid. Validsentencesarealsoknownastautologies—they
TAUTOLOGY
are necessarily true. Because the sentence True is true in all models, every valid sentence
is logically equivalent to True. What good are valid sentences? From our definition of
DEDUCTION entailment, wecanderivethedeductiontheorem,whichwasknowntotheancientGreeks:
THEOREM
Foranysentences αandβ,α |= β ifandonlyifthesentence(α ⇒ β)isvalid.
(Exercise7.5asksforaproof.) Hence, wecandecide if α |= β bychecking that(α ⇒ β)is
trueineverymodel—whichisessentiallywhattheinference algorithminFigure7.10does—
250 Chapter 7. LogicalAgents
orby proving that (α ⇒ β) isequivalent toTrue. Conversely, the deduction theorem states
thateveryvalidimplication sentence describes alegitimateinference.
The final concept we will need is satisfiability. A sentence is satisfiable if it is true
SATISFIABILITY
in, orsatisfied by, some model. Forexample, the knowledge base given earlier, (R ∧R ∧
1 2
R ∧ R ∧ R ), is satisfiable because there are three models in which it is true, as shown
3 4 5
in Figure 7.9. Satisfiability can be checked by enumerating the possible models until one is
found that satisfies the sentence. The problem of determining the satisfiability of sentences
inpropositional logic—the SATproblem—was thefirstproblem proved tobeNP-complete.
SAT
Many problems in computer science are really satisfiability problems. For example, all the
constraint satisfaction problems in Chapter 6 ask whether the constraints are satisfiable by
someassignment.
Validity and satisfiability are of course connected: α is valid iff ¬α is unsatisfiable;
contrapositively, αissatisfiable iff¬αisnotvalid. Wealsohavethefollowingusefulresult:
α|= β ifandonlyifthesentence (α∧¬β)isunsatisfiable.
Proving β from α by checking the unsatisfiability of (α ∧ ¬β) corresponds exactly to the
REDUCTIOAD standard mathematical proof technique of reductio ad absurdum (literally, “reduction to an
ABSURDUM
absurdthing”). Itisalsocalledproofbyrefutationorproofbycontradiction. Oneassumesa
REFUTATION
sentenceβtobefalseandshowsthatthisleadstoacontradiction withknownaxiomsα. This
CONTRADICTION
contradiction isexactlywhatismeantbysayingthatthesentence (α∧¬β)isunsatisfiable.
7.5.1 Inference andproofs
Thissectioncoversinferencerulesthatcanbeappliedtoderiveaproof—achainofconclu-
INFERENCERULES
sions that leads tothe desired goal. Thebest-known rule is called ModusPonens(Latin for
PROOF
modethataffirms)andiswritten
MODUSPONENS
α ⇒ β, α
.
β
The notation means that, whenever any sentences of the form α ⇒ β and α are given, then
thesentenceβcanbeinferred. Forexample,if(WumpusAhead∧WumpusAlive) ⇒ Shoot
and(WumpusAhead ∧WumpusAlive)aregiven,thenShoot canbeinferred.
AnotherusefulinferenceruleisAnd-Elimination,whichsaysthat,fromaconjunction,
AND-ELIMINATION
anyoftheconjuncts canbeinferred:
α∧β
.
α
Forexample,from (WumpusAhead ∧WumpusAlive),WumpusAlive canbeinferred.
By considering the possible truth values of α and β, one can show easily that Modus
Ponens and And-Elimination are sound once and for all. These rules can then be used in
any particular instances where they apply, generating sound inferences without the need for
enumerating models.
Allofthelogicalequivalences inFigure7.11canbeusedasinferencerules. Forexam-
ple,theequivalence forbiconditional elimination yields thetwoinference rules
α ⇔ β (α ⇒ β)∧(β ⇒ α)
and .
(α ⇒ β)∧(β ⇒ α) α ⇔ β
Section7.5. Propositional TheoremProving 251
Notall inference rules work inboth directions like this. Forexample, wecannot run Modus
Ponensintheoppositedirection toobtain α ⇒ β andαfromβ.
Letusseehowtheseinferencerulesandequivalencescanbeusedinthewumpusworld.
We start with the knowledge base containing R through R and show how to prove ¬P ,
1 5 1,2
thatis,thereisnopitin[1,2]. First,weapplybiconditional elimination toR toobtain
2
R : (B ⇒ (P ∨P )) ∧ ((P ∨P ) ⇒ B ).
6 1,1 1,2 2,1 1,2 2,1 1,1
ThenweapplyAnd-Elimination toR toobtain
6
R : ((P ∨P ) ⇒ B ).
7 1,2 2,1 1,1
Logicalequivalence forcontrapositives gives
R : (¬B ⇒ ¬(P ∨P )).
8 1,1 1,2 2,1
NowwecanapplyModusPonenswithR andthepercept R (i.e.,¬B ),toobtain
8 4 1,1
R : ¬(P ∨P ).
9 1,2 2,1
Finally,weapplyDeMorgan’srule,givingtheconclusion
R : ¬P ∧¬P .
10 1,2 2,1
Thatis,neither[1,2]nor[2,1]contains apit.
Wefoundthisproofbyhand,butwecanapplyanyofthesearchalgorithmsinChapter3
tofindasequence ofstepsthatconstitutes aproof. Wejustneedtodefineaproofproblemas
follows:
• INITIAL STATE: theinitialknowledgebase.
• ACTIONS: the set of actions consists of all the inference rules applied to all the sen-
tencesthatmatchthetophalfoftheinferencerule.
• RESULT: theresultofanactionistoaddthesentenceinthebottomhalfoftheinference
rule.
• GOAL: thegoalisastatethatcontainsthesentence wearetryingtoprove.
Thus, searching for proofs is an alternative to enumerating models. In many practical cases
findingaproofcanbemoreefficient becausetheproofcanignoreirrelevantpropositions, no
matter how manyof them there are. Forexample, the proof given earlier leading to ¬P ∧
1,2
¬P does not mention the propositions B , P , P , or P . They can be ignored
2,1 2,1 1,1 2,2 3,1
becausethegoalproposition, P ,appearsonlyinsentence R ;theotherpropositions inR
1,2 2 2
appearonlyinR andR ;soR ,R ,andR havenobearingontheproof. Thesamewould
4 2 1 3 5
holdevenifweaddedamillionmoresentencestotheknowledgebase;thesimpletruth-table
algorithm,ontheotherhand,wouldbeoverwhelmedbytheexponentialexplosionofmodels.
One final property of logical systems is monotonicity, which says that the set of en-
MONOTONICITY
tailed sentences can only increase as information is added to the knowledge base.8 Forany
sentences αandβ,
if KB |= α then KB ∧β |= α.
8 Nonmonotoniclogics, whichviolatethemonotonicity property, capturea commonproperty ofhuman rea-
soning:changingone’smind.TheyarediscussedinSection12.6.
252 Chapter 7. LogicalAgents
Forexample,supposetheknowledgebasecontainstheadditionalassertionβstatingthatthere
areexactlyeightpitsintheworld. Thisknowledgemighthelptheagentdrawadditionalcon-
clusions, but it cannot invalidate any conclusion α already inferred—such as the conclusion
thatthereisnopitin[1,2]. Monotonicitymeansthatinferencerulescanbeappliedwhenever
suitable premises are found in the knowledge base—the conclusion of the rule must follow
regardless ofwhatelseisintheknowledge base.
7.5.2 Proofby resolution
Wehave argued that the inference rules covered so farare sound, but wehave not discussed
the question of completeness for the inference algorithms that use them. Search algorithms
such as iterative deepening search (page 89) are complete in the sense that they will find
any reachable goal, but if the available inference rules are inadequate, then the goal is not
reachable—no proof existsthatuses onlythose inference rules. Forexample, ifweremoved
the biconditional elimination rule, the proof in the preceding section would not go through.
The current section introduces a single inference rule, resolution, that yields a complete
inference algorithm whencoupledwithanycompletesearchalgorithm.
Webeginbyusingasimpleversion oftheresolution ruleinthewumpusworld. Letus
consider the steps leading up to Figure 7.4(a): the agent returns from [2,1] to [1,1] and then
goes to [1,2], where it perceives a stench, but no breeze. We add the following facts to the
knowledgebase:
R : ¬B .
11 1,2
R : B ⇔ (P ∨P ∨P ).
12 1,2 1,1 2,2 1,3
By the same process that led to R earlier, we can now derive the absence of pits in [2,2]
10
and[1,3](rememberthat[1,1]isalreadyknowntobepitless):
R : ¬P .
13 2,2
R : ¬P .
14 1,3
We can also apply biconditional elimination to R , followed by Modus Ponens with R , to
3 5
obtainthefactthatthereisapitin[1,1],[2,2],or[3,1]:
R : P ∨P ∨P .
15 1,1 2,2 3,1
Now comes the firstapplication of the resolution rule: the literal ¬P in R resolves with
2,2 13
theliteral P inR togivetheresolvent
RESOLVENT 2,2 15
R : P ∨P .
16 1,1 3,1
InEnglish;ifthere’sapitinoneof[1,1],[2,2],and[3,1]andit’snotin[2,2],thenit’sin[1,1]
or[3,1]. Similarly,theliteral ¬P inR resolveswiththeliteral P inR togive
1,1 1 1,1 16
R : P .
17 3,1
In English: if there’s a pit in [1,1] or [3,1] and it’s not in [1,1], then it’s in [3,1]. These last
twoinference stepsareexamplesoftheunitresolution inference rule,
UNITRESOLUTION
(cid:3) ∨···∨(cid:3) , m
1 k
,
(cid:3) 1 ∨···∨(cid:3) i−1 ∨(cid:3) i+1 ∨···∨(cid:3) k
COMPLEMENTARY where each (cid:3) is a literal and (cid:3) and m are complementary literals (i.e., one is the negation
LITERALS i
Section7.5. Propositional TheoremProving 253
of the other). Thus, the unit resolution rule takes a clause—a disjunction of literals—and a
CLAUSE
literalandproduces anewclause. Notethatasingleliteral canbeviewedasadisjunction of
oneliteral, alsoknownasaunitclause.
UNITCLAUSE
Theunitresolution rulecanbegeneralized tothefull resolution rule,
RESOLUTION
(cid:3) ∨···∨(cid:3) , m ∨···∨m
1 k 1 n
,
(cid:3) 1 ∨···∨(cid:3) i−1 ∨(cid:3) i+1 ∨···∨(cid:3) k ∨m 1 ∨···∨m j−1 ∨m j+1 ∨···∨m n
where (cid:3) and m are complementary literals. Thissays that resolution takes twoclauses and
i j
produces a new clause containing all the literals of the two original clauses except the two
complementary literals. Forexample,wehave
P ∨P , ¬P ∨¬P
1,1 3,1 1,1 2,2
.
P ∨¬P
3,1 2,2
Thereisonemoretechnical aspectoftheresolution rule: theresulting clauseshould contain
onlyonecopyofeachliteral.9 Theremovalofmultiplecopies ofliterals iscalled factoring.
FACTORING
Forexample,ifweresolve (A∨B)with(A∨¬B),weobtain(A∨A),whichisreducedto
justA.
Thesoundnessoftheresolutionrulecanbeseeneasilybyconsidering theliteral(cid:3) that
i
is complementary to literal m in the other clause. If (cid:3) is true, then m is false, and hence
j i j
m 1 ∨···∨m j−1 ∨m j+1 ∨···∨m n mustbetrue, because m 1 ∨···∨m n isgiven. If(cid:3) i is
false,then(cid:3) 1 ∨···∨(cid:3) i−1 ∨(cid:3) i+1 ∨···∨(cid:3) k mustbetruebecause (cid:3) 1 ∨···∨(cid:3) k isgiven. Now
(cid:3) iseithertrueorfalse,sooneorotheroftheseconclusions holds—exactly astheresolution
i
rulestates.
Whatis moresurprising about theresolution rule is that itforms the basis forafamily
ofcomplete inference procedures. Aresolution-based theorem prover can, foranysentences
α and β in propositional logic, decide whether α |= β. The next two subsections explain
howresolution accomplishes this.
Conjunctivenormalform
Theresolution ruleapplies onlytoclauses (that is,disjunctions ofliterals), soitwouldseem
to be relevant only to knowledge bases and queries consisting of clauses. How, then, can
it lead to a complete inference procedure for all of propositional logic? The answer is that
every sentence of propositional logic is logically equivalent to a conjunction of clauses. A
CONJUNCTIVE sentence expressed as a conjunction of clauses is said to be in conjunctive normal form or
NORMALFORM
CNF (see Figure 7.14). We now describe a procedure for converting to CNF. We illustrate
the procedure by converting the sentence B ⇔ (P ∨P ) into CNF. The steps are as
1,1 1,2 2,1
follows:
1. Eliminate⇔,replacing α ⇔ β with(α ⇒ β)∧(β ⇒ α).
(B ⇒ (P ∨P ))∧((P ∨P ) ⇒ B ).
1,1 1,2 2,1 1,2 2,1 1,1
2. Eliminate⇒,replacing α ⇒ β with¬α∨β:
(¬B ∨P ∨P )∧(¬(P ∨P )∨B ).
1,1 1,2 2,1 1,2 2,1 1,1
9 Ifaclauseisviewedasasetofliterals,thenthisrestrictionisautomaticallyrespected. Usingsetnotationfor
clausesmakestheresolutionrulemuchcleaner,atthecostofintroducingadditionalnotation.
254 Chapter 7. LogicalAgents
3. CNFrequires ¬ to appear only in literals, so we “move ¬ inwards” by repeated appli-
cationofthefollowingequivalences fromFigure7.11:
¬(¬α) ≡ α (double-negation elimination)
¬(α∧β)≡ (¬α∨¬β) (DeMorgan)
¬(α∨β)≡ (¬α∧¬β) (DeMorgan)
Intheexample,werequirejustoneapplication ofthelastrule:
(¬B ∨P ∨P )∧((¬P ∧¬P )∨B ).
1,1 1,2 2,1 1,2 2,1 1,1
4. Now we have a sentence containing nested ∧ and ∨ operators applied to literals. We
applythedistributivity lawfromFigure7.11,distributing ∨over∧whereverpossible.
(¬B ∨P ∨P )∧(¬P ∨B )∧(¬P ∨B ).
1,1 1,2 2,1 1,2 1,1 2,1 1,1
The original sentence is now in CNF,as aconjunction of three clauses. It is much harder to
read,butitcanbeusedasinputtoaresolution procedure.
Aresolution algorithm
Inference procedures based on resolution work by using the principle of proof bycontradic-
tion introduced on page 250. That is, to show that KB |= α, we show that (KB ∧¬α) is
unsatisfiable. Wedothisbyproving acontradiction.
A resolution algorithm is shown in Figure 7.12. First, (KB ∧ ¬α) is converted into
CNF. Then, the resolution rule is applied to the resulting clauses. Each pair that contains
complementary literals is resolved to produce a new clause, which is added to the set if it is
notalreadypresent. Theprocesscontinues untiloneoftwothingshappens:
• therearenonewclauses thatcanbeadded, inwhichcase KB doesnotentailα;or,
• twoclausesresolvetoyieldtheemptyclause,inwhichcaseKB entailsα.
Theemptyclause—adisjunctionofnodisjuncts—isequivalenttoFalse becauseadisjunction
is true only if at least one of its disjuncts is true. Another way to see that an empty clause
represents acontradiction istoobservethatitarisesonly fromresolving twocomplementary
unitclausessuchasP and¬P.
Wecanapplytheresolutionproceduretoaverysimpleinferenceinthewumpusworld.
Whentheagentisin[1,1],thereisnobreeze, sotherecanbenopitsinneighboring squares.
Therelevantknowledgebaseis
KB = R ∧R = (B ⇔ (P ∨P ))∧¬B
2 4 1,1 1,2 2,1 1,1
and we wish to prove α which is, say, ¬P . When we convert (KB ∧¬α) into CNF, we
1,2
obtain the clauses shown at the top of Figure 7.13. The second row of the figure shows
clauses obtained by resolving pairs in the firstrow. Then, whenP isresolved with ¬P ,
1,2 1,2
we obtain the empty clause, shown as a small square. Inspection of Figure 7.13 reveals that
manyresolutionstepsarepointless. Forexample,theclauseB ∨¬B ∨P isequivalent
1,1 1,1 1,2
to True ∨P which is equivalent to True. Deducing that True is true is not very helpful.
1,2
Therefore, anyclauseinwhichtwocomplementary literalsappearcanbediscarded.
Section7.5. Propositional TheoremProving 255
functionPL-RESOLUTION(KB,α)returnstrue orfalse
inputs:KB,theknowledgebase,asentenceinpropositionallogic
α,thequery,asentenceinpropositionallogic
clauses←thesetofclausesintheCNFrepresentationofKB ∧¬α
new←{}
loopdo
foreachpairofclausesCi,Cj inclauses do
resolvents←PL-RESOLVE(Ci,Cj)
ifresolvents containstheemptyclausethenreturntrue
new←new∪ resolvents
ifnew ⊆clauses thenreturnfalse
clauses←clauses∪new
Figure 7.12 A simple resolution algorithm for propositional logic. The function
PL-RESOLVEreturnsthesetofallpossibleclausesobtainedbyresolvingitstwoinputs.
¬P ^ B ¬B ^ P ^ P ¬P ^ B ¬B P
2,1 1,1 1,1 1,2 2,1 1,2 1,1 1,1 1,2
¬B ^ P ^ B P ^ P ^ ¬P ¬B ^ P ^ B P ^ P ^ ¬P ¬P ¬P
1,1 1,2 1,1 1,2 2,1 2,1 1,1 2,1 1,1 1,2 2,1 1,2 2,1 1,2
Figure7.13 PartialapplicationofPL-RESOLUTIONtoasimpleinferenceinthewumpus
world. ¬P 1,2 isshowntofollowfromthefirstfourclausesinthetoprow.
Completenessofresolution
Toconclude ourdiscussion of resolution, wenow show why PL-RESOLUTION iscomplete.
RESOLUTION Todothis,weintroducetheresolutionclosureRC(S)ofasetofclausesS,whichistheset
CLOSURE
of all clauses derivable by repeated application of the resolution rule to clauses in S ortheir
derivatives. The resolution closure is what PL-RESOLUTION computes as the final value of
thevariableclauses. ItiseasytoseethatRC(S)mustbefinite,becausethereareonlyfinitely
manydistinctclausesthatcanbeconstructed outofthesymbolsP ,...,P thatappearinS.
1 k
(Noticethatthiswouldnotbetruewithoutthefactoring stepthatremovesmultiplecopiesof
literals.) Hence, PL-RESOLUTION alwaysterminates.
The completeness theorem for resolution in propositional logic is called the ground
GROUND
resolution theorem:
RESOLUTION
THEOREM
If a set of clauses is unsatisfiable, then the resolution closure of those clauses
containstheemptyclause.
This theorem is proved by demonstrating its contrapositive: if the closure RC(S) does not
256 Chapter 7. LogicalAgents
contain the empty clause, then S is satisfiable. In fact, we can construct a model for S with
suitable truthvaluesforP ,...,P . Theconstruction procedure isasfollows:
1 k
Forifrom1tok,
– IfaclauseinRC(S)containstheliteral¬P andallitsotherliteralsarefalseunder
i
theassignment chosenforP 1 ,...,P i−1 ,thenassignfalse toP i .
– Otherwise,assign true toP .
i
This assignment to P ,...,P is a model of S. To see this, assume the opposite—that, at
1 k
some stage i in the sequence, assigning symbol P causes some clause C to become false.
i
Forthis tohappen, itmust bethe case that all the other literals in C mustalready have been
falsifiedbyassignments toP 1 ,...,P i−1 . Thus,C mustnowlooklikeeither(false ∨false ∨
···false∨P )orlike(false∨false∨···false∨¬P ). IfjustoneofthesetwoisinRC(S),then
i i
the algorithm will assign the appropriate truth value to P to make C true, so C can only be
i
falsifiedifbothoftheseclausesareinRC(S). Now,sinceRC(S)isclosedunderresolution,
itwillcontaintheresolventofthesetwoclauses,andthatresolventwillhaveallofitsliterals
already falsified by the assignments to P 1 ,...,P i−1 . This contradicts our assumption that
thefirstfalsifiedclauseappearsatstage i. Hence,wehaveprovedthattheconstruction never
falsifies a clause in RC(S); that is, it produces a model of RC(S) and thus a model of S
itself(sinceS iscontained inRC(S)).
7.5.3 Hornclauses and definite clauses
Thecompletenessofresolutionmakesitaveryimportantinferencemethod. Inmanypractical
situations, however, the full power of resolution is not needed. Some real-world knowledge
bases satisfy certain restrictions on the form of sentences they contain, which enables them
touseamorerestricted andefficientinference algorithm.
One such restricted form is the definite clause, which is a disjunction of literals of
DEFINITECLAUSE
whichexactlyoneispositive. Forexample,theclause(¬L ∨¬Breeze∨B )isadefinite
1,1 1,1
clause, whereas(¬B ∨P ∨P )isnot.
1,1 1,2 2,1
Slightlymoregeneral isthe Hornclause, whichisadisjunction ofliterals ofwhich at
HORNCLAUSE
mostone is positive. Soall definite clauses areHorn clauses, as areclauses with no positive
literals;thesearecalledgoalclauses. Hornclausesareclosedunderresolution: ifyouresolve
GOALCLAUSES
twoHornclauses, yougetbackaHornclause.
Knowledgebasescontaining onlydefiniteclausesareinteresting forthreereasons:
1. Every definite clause can be written as an implication whose premise is a conjunction
ofpositiveliteralsandwhoseconclusionisasinglepositiveliteral. (SeeExercise7.13.)
For example, the definite clause (¬L ∨ ¬Breeze ∨ B ) can be written as the im-
1,1 1,1
plication (L ∧ Breeze) ⇒ B . In the implication form, the sentence is easier to
1,1 1,1
understand: itsaysthatiftheagentisin[1,1]andthereisabreeze,then[1,1]isbreezy.
In Horn form, the premise is called the body and the conclusion is called the head. A
BODY
sentence consisting of a single positive literal, such as L , is called a fact. It too can
HEAD 1,1
bewritteninimplication formas True ⇒ L ,butitissimplertowritejust L .
FACT 1,1 1,1
Section7.5. Propositional TheoremProving 257
CNFSentence → Clause
1
∧···∧Clausen
Clause → Literal
1
∨···∨Literalm
Literal → Symbol | ¬Symbol
Symbol → P | Q| R| ...
HornClauseForm → DefiniteClauseForm | GoalClauseForm
DefiniteClauseForm → (Symbol ∧···∧Symbol ) ⇒ Symbol
1 l
GoalClauseForm → (Symbol ∧···∧Symbol ) ⇒ False
1 l
Figure7.14 Agrammarforconjunctivenormalform,Hornclauses,anddefiniteclauses.
AclausesuchasA∧B ⇒ C isstilladefiniteclausewhenitiswrittenas¬A∨¬B∨C,
butonlytheformerisconsideredthecanonicalformfordefiniteclauses. Onemoreclassis
thek-CNFsentence,whichisaCNFsentencewhereeachclausehasatmostkliterals.
2. InferencewithHornclausescanbedonethroughtheforward-chainingandbackward-
FORWARD-CHAINING
BACKWARD- chaining algorithms, which we explain next. Both of these algorithms are natural,
CHAINING
in that the inference steps are obvious and easy for humans to follow. This type of
inferenceisthebasisforlogicprogramming, whichisdiscussed inChapter9.
3. Deciding entailment with Hornclauses can be done in timethat is linear inthe size of
theknowledge base—apleasant surprise.
7.5.4 Forwardandbackward chaining
The forward-chaining algorithm PL-FC-ENTAILS?(KB,q) determines if a single proposi-
tion symbol q—the query—is entailed by a knowledge base of definite clauses. It begins
from knownfacts (positive literals) intheknowledge base. Ifallthepremises ofanimplica-
tion are known, then its conclusion is added to the set of known facts. For example, if L
1,1
andBreeze areknownand (L ∧Breeze) ⇒ B isintheknowledge base, thenB can
1,1 1,1 1,1
beadded. Thisprocesscontinues untilthequery q isaddedoruntilnofurtherinferences can
bemade. Thedetailedalgorithm isshowninFigure7.15;themainpointtorememberisthat
itrunsinlineartime.
The best way to understand the algorithm is through an example and a picture. Fig-
ure 7.16(a) shows a simple knowledge base of Horn clauses with A and B as known facts.
Figure 7.16(b) shows the same knowledge base drawn as an AND–OR graph (see Chap-
ter 4). In AND–OR graphs, multiple links joined by an arc indicate a conjunction—every
link must be proved—while multiple links without an arc indicate a disjunction—any link
canbeproved. Itiseasytoseehowforwardchaining worksinthegraph. Theknownleaves
(here, A and B) are set, and inference propagates up the graph as far as possible. Wher-
ever a conjunction appears, the propagation waits until all the conjuncts are known before
proceeding. Thereaderisencouraged toworkthroughtheexampleindetail.
258 Chapter 7. LogicalAgents
functionPL-FC-ENTAILS?(KB,q)returnstrue orfalse
inputs:KB,theknowledgebase,asetofpropositionaldefiniteclauses
q,thequery,apropositionsymbol
count←atable,wherecount[c]isthenumberofsymbolsinc’spremise
inferred←atable,whereinferred[s]isinitiallyfalse forallsymbols
agenda←aqueueofsymbols,initiallysymbolsknowntobetrueinKB
whileagenda isnotemptydo
p←POP(agenda)
ifp =q thenreturntrue
ifinferred[p]=false then
inferred[p]←true
foreachclausec inKB wherep isinc.PREMISEdo
decrementcount[c]
ifcount[c]=0thenaddc.CONCLUSIONtoagenda
returnfalse
Figure7.15 Theforward-chainingalgorithmforpropositionallogic. Theagenda keeps
trackofsymbolsknownto betruebutnotyet“processed.” The count tablekeepstrackof
howmanypremisesofeachimplicationareasyetunknown.Wheneveranewsymbolpfrom
theagendaisprocessed,thecountisreducedbyoneforeachimplicationinwhosepremise
p appears(easily identified in constanttime with appropriateindexing.) If a countreaches
zero, all the premises of the implication are known, so its conclusion can be added to the
agenda.Finally,weneedtokeeptrackofwhichsymbolshavebeenprocessed;asymbolthat
isalreadyinthesetofinferredsymbolsneednotbeaddedtotheagendaagain. Thisavoids
redundantworkandpreventsloopscausedbyimplicationssuchasP ⇒QandQ⇒P.
Itiseasytoseethatforward chaining issound: everyinference isessentially anappli-
cationofModusPonens. Forwardchaining isalsocomplete: everyentailed atomicsentence
willbederived. Theeasiest waytoseethis istoconsider the finalstate oftheinferred table
(after the algorithm reaches a fixed point where no new inferences are possible). The table
FIXEDPOINT
contains true for each symbol inferred during the process, and false for all other symbols.
Wecanviewthetableasalogicalmodel;moreover,everydefiniteclauseintheoriginalKBis
trueinthismodel. Toseethis,assumetheopposite,namelythatsomeclausea ∧...∧a ⇒ b
1 k
is false in the model. Then a ∧...∧a must be true in the model and b must be false in
1 k
the model. But this contradicts ourassumption that the algorithm has reached a fixed point!
Wecanconclude,therefore,thatthesetofatomicsentences inferredatthefixedpointdefines
a model of the original KB. Furthermore, any atomic sentence q that is entailed by the KB
must be true in all its models and in this model in particular. Hence, every entailed atomic
sentence q mustbeinferred bythealgorithm.
Forwardchainingisanexampleofthegeneralconceptof data-drivenreasoning—that
DATA-DRIVEN
is,reasoninginwhichthefocusofattention startswiththeknowndata. Itcanbeusedwithin
an agent to derive conclusions from incoming percepts, often without a specific query in
mind. Forexample, thewumpusagent might TELL itspercepts totheknowledge baseusing
Section7.6. EffectivePropositional ModelChecking 259
Q
P ⇒ Q
L∧M ⇒ P P
B∧L ⇒ M
A∧P ⇒ L M
A∧B ⇒ L
L
A
B
A B
(a) (b)
Figure7.16 (a)AsetofHornclauses. (b)ThecorrespondingAND–ORgraph.
anincremental forward-chaining algorithm inwhichnewfactscanbeaddedtotheagendato
initiate newinferences. Inhumans, acertain amount ofdata-driven reasoning occurs asnew
information arrives. Forexample,ifIamindoorsandhearrainstartingtofall,itmightoccur
tomethatthepicnicwillbecanceled. Yetitwillprobablynotoccurtomethattheseventeenth
petalonthelargestroseinmyneighbor’sgardenwillgetwet;humanskeepforwardchaining
undercarefulcontrol, lesttheybeswampedwithirrelevant consequences.
The backward-chaining algorithm, as its name suggests, works backward from the
query. Ifthe query q is known to be true, then no work is needed. Otherwise, the algorithm
finds those implications inthe knowledge base whose conclusion is q. If all the premises of
one of those implications can be proved true (by backward chaining), then q is true. When
applied to the query Q in Figure 7.16, it works back downthe graph until it reaches a set of
knownfacts,AandB,thatformsthebasisforaproof. Thealgorithm isessentially identical
to the AND-OR-GRAPH-SEARCH algorithm in Figure 4.11. As with forward chaining, an
efficientimplementation runsinlineartime.
GOAL-DIRECTED Backward chaining is a form of goal-directed reasoning. It is useful for answering
REASONING
specificquestionssuchas“WhatshallIdonow?” and“Wherearemykeys?” Often,thecost
ofbackward chaining is muchlessthanlinearinthesizeoftheknowledge base, because the
processtouches onlyrelevantfacts.
7.6 EFFECTIVE PROPOSITIONAL MODEL CHECKING
In this section, we describe two families of efficient algorithms for general propositional
inference based on model checking: One approach based on backtracking search, and one
onlocalhill-climbing search. Thesealgorithms arepartof the“technology” ofpropositional
logic. Thissectioncanbeskimmedonafirstreadingofthechapter.
260 Chapter 7. LogicalAgents
Thealgorithmswedescribeareforcheckingsatisfiability: theSATproblem. (Asnoted
earlier, testing entailment, α |= β, can be done by testing unsatisfiability of α∧ ¬β.) We
have already noted the connection between finding a satisfying model fora logical sentence
andfindingasolutionforaconstraintsatisfactionproblem,soitisperhapsnotsurprisingthat
the two families of algorithms closely resemble the backtracking algorithms of Section 6.3
and the local search algorithms of Section 6.4. They are, however, extremely important in
theirownrightbecausesomanycombinatorialproblemsincomputersciencecanbereduced
to checking the satisfiability of a propositional sentence. Any improvement in satisfiability
algorithms hashugeconsequences forourabilitytohandlecomplexityingeneral.
7.6.1 A completebacktracking algorithm
DAVIS–PUTNAM Thefirstalgorithm weconsider isoften called the Davis–Putnamalgorithm, afterthesem-
ALGORITHM
inal paper by Martin Davis and Hilary Putnam (1960). The algorithm is in fact the version
described by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the ini-
tials of all four authors. DPLL takes as input a sentence in conjunctive normal form—a set
of clauses. Like BACKTRACKING-SEARCH and TT-ENTAILS?, it is essentially a recursive,
depth-first enumeration ofpossible models. Itembodies threeimprovements overthesimple
schemeof TT-ENTAILS?:
• Early termination: The algorithm detects whether the sentence must be true or false,
even with a partially completed model. A clause is true if any literal is true, even if
the other literals do not yet have truth values; hence, the sentence as a whole could be
judged true even before the model is complete. For example, the sentence (A∨B)∧
(A∨C)istrue if Aistrue, regardless of thevalues of B and C. Similarly, asentence
isfalseifanyclauseisfalse,whichoccurswheneachofitsliteralsisfalse. Again,this
can occurlong before the model iscomplete. Early termination avoids examination of
entiresubtrees inthesearchspace.
• Pure symbol heuristic: A puresymbol is a symbol that always appears with the same
PURESYMBOL
“sign” in all clauses. For example, in the three clauses (A ∨ ¬B), (¬B ∨ ¬C), and
(C ∨ A), the symbol A is pure because only the positive literal appears, B is pure
because only the negative literal appears, and C is impure. It is easy to see that if
a sentence has a model, then it has a model with the pure symbols assigned so as to
make their literals true, because doing so can never make a clause false. Note that, in
determining the purity of a symbol, the algorithm can ignore clauses that are already
known to be true in the model constructed so far. For example, if the model contains
B=false, then the clause (¬B ∨¬C)is already true, and in the remaining clauses C
appearsonlyasapositiveliteral;therefore C becomespure.
• Unit clause heuristic: A unit clause was defined earlier as a clause with just one lit-
eral. In the context of DPLL, it also means clauses in which all literals but one are
already assigned false by the model. For example, if the model contains B=true,
then (¬B ∨¬C) simplifies to ¬C, which is a unit clause. Obviously, for this clause
to be true, C must be set to false. The unit clause heuristic assigns all such symbols
before branching on the remainder. Oneimportant consequence ofthe heuristic is that
Section7.6. EffectivePropositional ModelChecking 261
functionDPLL-SATISFIABLE?(s)returnstrue orfalse
inputs:s,asentenceinpropositionallogic
clauses←thesetofclausesintheCNFrepresentationofs
symbols←alistofthepropositionsymbolsins
returnDPLL(clauses,symbols,{})
functionDPLL(clauses,symbols,model)returnstrue orfalse
ifeveryclauseinclauses istrueinmodel thenreturntrue
ifsomeclauseinclauses isfalseinmodel thenreturnfalse
P,value←FIND-PURE-SYMBOL(symbols,clauses,model)
ifP isnon-nullthenreturnDPLL(clauses,symbols –P,model ∪{P=value})
P,value←FIND-UNIT-CLAUSE(clauses,model)
ifP isnon-nullthenreturnDPLL(clauses,symbols –P,model ∪{P=value})
P←FIRST(symbols);rest←REST(symbols)
returnDPLL(clauses,rest,model ∪{P=true})or
DPLL(clauses,rest,model ∪{P=false}))
Figure7.17 TheDPLLalgorithmforcheckingsatisfiabilityofasentenceinpropositional
logic. The ideas behind FIND-PURE-SYMBOL and FIND-UNIT-CLAUSE are described in
the text; each returnsa symbol(or null) and the truth value to assign to that symbol. Like
TT-ENTAILS?,DPLLoperatesoverpartialmodels.
any attempt toprove (by refutation) a literal that is already in the knowledge base will
succeed immediately (Exercise 7.23). Notice also that assigning one unit clause can
create another unit clause—for example, when C is set to false, (C ∨ A) becomes a
unit clause, causing true to be assigned to A. This “cascade” of forced assignments
is called unitpropagation. It resembles the process of forward chaining with definite
UNITPROPAGATION
clauses, and indeed, if the CNF expression contains only definite clauses then DPLL
essentially replicates forwardchaining. (SeeExercise7.24.)
The DPLL algorithm is shown in Figure 7.17, which gives the the essential skeleton of the
searchprocess.
What Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to
large problems. It is interesting that most of these tricks are in fact rather general, and we
haveseenthembeforeinotherguises:
1. Componentanalysis (asseenwithTasmaniainCSPs): AsDPLL assigns truthvalues
tovariables, thesetofclauses maybecomeseparated intodisjoint subsets, calledcom-
ponents,thatsharenounassigned variables. Givenanefficientwaytodetectwhenthis
occurs,asolvercangainconsiderablespeedbyworkingoneachcomponentseparately.
2. Variable and value ordering (as seen in Section 6.3.1 for CSPs): Our simple imple-
mentation of DPLL uses an arbitrary variable ordering and always tries the value true
before false. The degree heuristic (see page 216) suggests choosing the variable that
appearsmostfrequently overallremainingclauses.
262 Chapter 7. LogicalAgents
3. Intelligent backtracking (as seen in Section 6.3 for CSPs): Many problems that can-
not be solved in hours of run time with chronological backtracking can be solved in
seconds with intelligent backtracking that backs up all the way to the relevant point of
conflict. All SAT solvers that do intelligent backtracking use some form of conflict
clause learning to record conflicts so that they won’t be repeated later in the search.
Usuallyalimited-size setofconflictsiskept,andrarelyusedonesaredropped.
4. Randomrestarts(asseenonpage124forhill-climbing): Sometimesarunappearsnot
to be making progress. In this case, we can start over from the top of the search tree,
rather than trying to continue. After restarting, different random choices (in variable
andvalueselection) aremade. Clausesthatarelearnedinthefirstrunareretainedafter
the restart and can help prune the search space. Restarting does not guarantee that a
solutionwillbefoundfaster, butitdoesreducethevariance onthetimetosolution.
5. Clever indexing (as seen in many algorithms): The speedup methods used in DPLL
itself, aswell asthe tricks used in modern solvers, require fast indexing ofsuch things
as “the set of clauses in which variable X appears as a positive literal.” This task is
i
complicated by the fact that the algorithms are interested only in the clauses that have
not yet been satisfied by previous assignments to variables, so the indexing structures
mustbeupdateddynamically asthecomputation proceeds.
Withtheseenhancements, modernsolverscanhandleproblemswithtensofmillionsofvari-
ables. They have revolutionized areas such as hardware verification and security protocol
verification, whichpreviously required laborious, hand-guided proofs.
7.6.2 Local search algorithms
Wehaveseenseverallocalsearchalgorithmssofarinthisbook,including HILL-CLIMBING
(page 122) and SIMULATED-ANNEALING (page 126). These algorithms can be applied di-
rectly to satisfiability problems, provided that we choose the right evaluation function. Be-
cause thegoalistofindanassignment that satisfieseveryclause, anevaluation function that
counts the number of unsatisfied clauses will do the job. In fact, this is exactly the measure
usedbytheMIN-CONFLICTS algorithmforCSPs(page221). Allthesealgorithmstakesteps
in the space of complete assignments, flipping the truth value of one symbol at a time. The
space usually contains many local minima, to escape from which various forms of random-
ness are required. In recent years, there has been a great deal of experimentation to find a
goodbalancebetweengreediness andrandomness.
Oneofthesimplestandmosteffectivealgorithmstoemergefromallthisworkiscalled
WALKSAT (Figure 7.18). On every iteration, the algorithm picks an unsatisfied clause and
picks a symbol in the clause to flip. It chooses randomly between two ways to pick which
symboltoflip: (1)a“min-conflicts” stepthatminimizesthenumberofunsatisfied clausesin
thenewstateand(2)a“random walk”stepthatpicksthesymbolrandomly.
When WALKSAT returns a model, the input sentence is indeed satisfiable, but when
it returns failure, there are two possible causes: either the sentence is unsatisfiable or we
need togive the algorithm more time. If weset max flips=∞ and p > 0, WALKSAT will
eventually return a model (if one exists), because the random-walk steps will eventually hit
Section7.6. EffectivePropositional ModelChecking 263
functionWALKSAT(clauses,p,max flips)returnsasatisfyingmodelorfailure
inputs:clauses,asetofclausesinpropositionallogic
p,theprobabilityofchoosingtodoa“randomwalk”move,typicallyaround0.5
max flips,numberofflipsallowedbeforegivingup
model←arandomassignmentoftrue/false tothesymbolsinclauses
fori= 1tomax flips do
ifmodel satisfiesclauses thenreturnmodel
clause←arandomlyselectedclausefromclauses thatisfalseinmodel
withprobabilitypflipthevalueinmodel ofarandomlyselectedsymbolfromclause
elseflipwhicheversymbolinclause maximizesthenumberofsatisfiedclauses
returnfailure
Figure 7.18 The WALKSAT algorithm for checking satisfiability by randomly flipping
thevaluesofvariables.Manyversionsofthealgorithmexist.
upon the solution. Alas, if max flips is infinity and the sentence is unsatisfiable, then the
algorithm neverterminates!
Forthisreason, WALKSAT ismostuseful whenweexpect asolution toexist—forex-
ample,theproblemsdiscussedinChapters3and6usuallyhavesolutions. Ontheotherhand,
WALKSAT cannot always detect unsatisfiability, which is required for deciding entailment.
For example, an agent cannot reliably use WALKSAT to prove that a square is safe in the
wumpusworld. Instead, itcansay,“Ithoughtaboutitforanhourandcouldn’tcomeupwith
apossible worldinwhichthesquare isn’tsafe.” Thismaybeagoodempirical indicator that
thesquareissafe,butit’scertainly notaproof.
7.6.3 The landscapeofrandom SATproblems
Some SAT problems are harder than others. Easy problems can be solved by any old algo-
rithm, but because weknow that SATisNP-complete, atleast someproblem instances must
requireexponential runtime. InChapter6,wesawsomesurprisingdiscoveries aboutcertain
kindsofproblems. Forexample,the n-queensproblem—thought tobequitetrickyforback-
tracking search algorithms—turned outtobetrivially easy forlocal search methods, such as
min-conflicts. This is because solutions are very densely distributed in the space of assign-
ments, and anyinitial assignment isguaranteed tohaveasolution nearby. Thus, n-queens is
easybecause itisunderconstrained.
UNDERCONSTRAINED
When we look at satisfiability problems in conjunctive normal form, an undercon-
strained problem is one with relatively few clauses constraining the variables. For example,
hereisarandomlygenerated 3-CNFsentence withfivesymbols andfiveclauses:
(¬D∨¬B∨C)∧(B∨¬A∨¬C)∧(¬C ∨¬B∨E)
∧(E ∨¬D∨B)∧(B∨E ∨¬C).
Sixteen of the 32 possible assignments are models of this sentence, so, on average, it would
take just two random guesses to find a model. This is an easy satisfiability problem, as are
264 Chapter 7. LogicalAgents
most such underconstrained problems. On the other hand, an overconstrained problem has
manyclauses relativetothenumberofvariablesandislikelytohavenosolutions.
To go beyond these basic intuitions, we must define exactly how random sentences
are generated. The notation CNF (m,n) denotes a k-CNF sentence with m clauses and n
k
symbols, where the clauses are chosen uniformly, independently, and without replacement
fromamongallclauseswithkdifferentliterals,whicharepositiveornegativeatrandom. (A
symbolmaynotappeartwiceinaclause,normayaclauseappeartwiceinasentence.)
Given a source of random sentences, we can measure the probability of satisfiability.
Figure 7.19(a) plots the probability for CNF (m,50), that is, sentences with 50 variables
3
and 3 literals per clause, as a function of the clause/symbol ratio, m/n. As we expect, for
small m/n the probability of satisfiability is close to 1, and at large m/n the probability
is close to 0. The probability drops fairly sharply around m/n=4.3. Empirically, we find
that the “cliff” stays in roughly the same place (for k=3)and gets sharper and sharper as n
SATISFIABILITY increases. Theoretically, the satisfiability threshold conjecture says that for every k ≥ 3,
THRESHOLD
CONJECTURE
thereisathreshold ratio r suchthat,asngoestoinfinity,theprobability thatCNF (n,rn)
k k
is satisfiable becomes 1 for all values of r below the threshold, and 0 for all values above.
Theconjecture remainsunproven.
1
0.8
0.6
0.4
0.2
0
0 1 2 3 4 5 6 7 8
)elbaifsitas(P
2000
1800
1600
1400
1200
1000
800
600
400
200
0
0 1 2 3 4 5 6 7 8
Clause/symbol ratio m/n
emitnuR
DPLL
WalkSAT
Clause/symbol ratio m/n
(a) (b)
Figure7.19 (a)Graphshowingtheprobabilitythatarandom3-CNFsentencewithn=50
symbolsissatisfiable,asafunctionoftheclause/symbolratiom/n.(b)Graphofthemedian
runtime(measuredinnumberofrecursivecallstoDPLL,agoodproxy)onrandom3-CNF
sentences.Themostdifficultproblemshaveaclause/symbolratioofabout4.3.
Nowthatwehaveagood ideawherethesatisfiable andunsatisfiable problems are, the
next question is, where are the hard problems? It turns out that they are also often at the
threshold value. Figure7.19(b) showsthat 50-symbol problems atthe threshold value of4.3
are about 20 times more difficult to solve than those at a ratio of 3.3. The underconstrained
problems are easiest to solve (because it is so easy to guess a solution); the overconstrained
problemsarenotaseasyastheunderconstrained, butstillaremucheasierthantheonesright
atthethreshold.
Section7.7. AgentsBasedonPropositional Logic 265
7.7 AGENTS BASED ON PROPOSITIONAL LOGIC
In this section, webring together what we have learned so far in order to construct wumpus
worldagentsthatusepropositionallogic. Thefirststepistoenabletheagenttodeduce,tothe
extent possible, thestate oftheworldgiven itspercept history. Thisrequires writing downa
completelogicalmodeloftheeffectsofactions. Wealsoshowhowtheagentcankeeptrackof
the world efficiently without going back into the percept history for each inference. Finally,
we show how the agent can use logical inference to construct plans that are guaranteed to
achieveitsgoals.
7.7.1 The current stateofthe world
As stated at the beginning of the chapter, a logical agent operates by deducing what to do
from a knowledge base of sentences about the world. The knowledge base is composed of
axioms—general knowledge about how the world works—and percept sentences obtained
fromtheagent’sexperienceinaparticularworld. Inthissection,wefocusontheproblemof
deducing thecurrentstateofthewumpusworld—whereamI,is thatsquaresafe,andsoon.
Webegancollecting axioms inSection 7.4.3. Theagent knows thatthe starting square
containsnopit(¬P )andnowumpus(¬W ). Furthermore, foreachsquare, itknowsthat
1,1 1,1
thesquareisbreezyifandonlyifaneighboringsquarehasapit;andasquareissmellyifand
onlyifaneighboring squarehasawumpus. Thus,weinclude alargecollection ofsentences
ofthefollowingform:
B ⇔ (P ∨P )
1,1 1,2 2,1
S ⇔ (W ∨W )
1,1 1,2 2,1
···
Theagentalsoknowsthatthereisexactlyonewumpus. Thisisexpressed intwoparts. First,
wehavetosaythatthereisatleastonewumpus:
W ∨W ∨···∨W ∨W .
1,1 1,2 4,3 4,4
Then, wehave tosaythat there is atmostonewumpus. Foreach pairoflocations, weadd a
sentence sayingthatatleastoneofthemmustbewumpus-free:
¬W ∨¬W
1,1 1,2
¬W ∨¬W
1,1 1,3
···
¬W ∨¬W .
4,3 4,4
So far, so good. Now let’s consider the agent’s percepts. If there is currently a stench, one
mightsuppose thataproposition Stench shouldbeaddedtotheknowledgebase. Thisisnot
quiteright,however: iftherewasnostenchattheprevioustimestep,then¬Stench wouldal-
readybeasserted,andthenewassertionwouldsimplyresult inacontradiction. Theproblem
issolvedwhenwerealizethataperceptassertssomething onlyaboutthecurrenttime. Thus,
ifthetimestep(assupplied to MAKE-PERCEPT-SENTENCE inFigure7.1)is4,thenweadd
266 Chapter 7. LogicalAgents
Stench4 totheknowledge base, ratherthan Stench—neatlyavoiding anycontradiction with
¬Stench3. Thesamegoesforthebreeze, bump,glitter, andscream percepts.
Theideaofassociating propositions withtimesteps extends toanyaspect oftheworld
thatchangesovertime. Forexample,theinitialknowledgebaseincludesL0 —theagentisin
1,1
square[1,1]attime0—aswellasFacingEast0,HaveArrow0,andWumpusAlive0. Weuse
theword fluent(from theLatin fluens, flowing) toreferanaspect of theworld thatchanges.
FLUENT
“Fluent”isasynonymfor“statevariable,”inthesensedescribedinthediscussionoffactored
representations in Section 2.4.7 on page 57. Symbols associated with permanent aspects of
ATEMPORAL theworlddonotneedatimesuperscript andaresometimescalled atemporalvariables.
VARIABLE
We can connect stench and breeze percepts directly to the properties of the squares
where they areexperienced through the location fluentas follows.10 Forany timestep t and
anysquare [x,y],weassert
Lt ⇒ (Breezet ⇔ B )
x,y x,y
Lt ⇒ (Stencht ⇔ S ).
x,y x,y
Now, of course, we need axioms that allow the agent to keep track of fluents such as Lt .
x,y
These fluents change as the result of actions taken by the agent, so, in the terminology of
Chapter 3, we need to write down the transition model of the wumpus world as a set of
logicalsentences.
First, we need proposition symbols for the occurrences of actions. As with percepts,
thesesymbolsareindexedbytime;thus,Forward0meansthattheagentexecutestheForward
action at time 0. Byconvention, the percept fora given time step happens first, followed by
theactionforthattimestep,followedbyatransition tothe nexttimestep.
To describe how the world changes, we can try writing effect axioms that specify the
EFFECTAXIOM
outcomeofanactionatthenexttimestep. Forexample,iftheagentisatlocation[1,1]facing
east at time0 and goes Forward, the result isthat the agent is in square [2,1] and no longer
isin[1,1]:
L0 ∧FacingEast0∧Forward0 ⇒ (L1 ∧¬L1 ). (7.1)
1,1 2,1 1,1
We would need one such sentence for each possible time step, for each of the 16 squares,
andeachofthefourorientations. Wewouldalsoneedsimilarsentencesfortheotheractions:
Grab,Shoot,Climb,TurnLeft,andTurnRight.
Let us suppose that the agent does decide to move Forward at time 0 and asserts this
fact into its knowledge base. Given the effect axiom in Equation (7.1), combined with the
initial assertions about the state at time 0, the agent can now deduce that it is in [2,1]. That
is, ASK(KB,L1 )=true. Sofar, so good. Unfortunately, the news elsewhere isless good:
2,1
if we ASK(KB,HaveArrow1), the answer is false, that is, the agent cannot prove it still
has the arrow; norcan it prove it doesn’t have it! Theinformation has been lost because the
effectaxiom fails tostatewhatremains unchanged astheresultofanaction. Theneed todo
this gives rise to the frame problem.11 One possible solution to the frame problem would
FRAMEPROBLEM
10 Section7.4.3convenientlyglossedoverthisrequirement.
11 Thename“frameproblem”comesfrom“frameofreference”inphysics—theassumedstationarybackground
withrespecttowhichmotionismeasured. Italsohasananalogytotheframesofamovie, inwhichnormally
mostofthebackgroundstaysconstantwhilechangesoccurintheforeground.
Section7.7. AgentsBasedonPropositional Logic 267
be to add frame axioms explicitly asserting all the propositions that remain the same. For
FRAMEAXIOM
example,foreachtimetwewouldhave
Forwardt ⇒ (HaveArrowt ⇔ HaveArrowt+1)
Forwardt ⇒ (WumpusAlivet ⇔ WumpusAlivet+1)
···
where we explicitly mention every proposition that stays unchanged from time t to time
t + 1 under the action Forward. Although the agent now knows that it still has the arrow
aftermovingforwardandthatthewumpushasn’t diedorcomebacktolife,theproliferation
of frame axioms seems remarkably inefficient. In a world with m different actions and n
fluents, the set of frame axioms will be of size O(mn). This specific manifestation of the
REPRESENTATIONAL frame problem is sometimes called the representational frame problem. Historically, the
FRAMEPROBLEM
problemwasasignificantoneforAIresearchers; weexploreitfurtherinthenotesattheend
ofthechapter.
Therepresentational frameproblemissignificantbecausetherealworldhasverymany
fluents, to put it mildly. Fortunately for us humans, each action typically changes no more
than some small number k of those fluents—the world exhibits locality. Solving the repre-
LOCALITY
sentational frameproblem requires definingthetransition modelwithasetofaxiomsofsize
INFERENTIALFRAME O(mk) rather than size O(mn). There is also an inferential frame problem: the problem
PROBLEM
ofprojecting forwardtheresultsofa tstepplanofactionintimeO(kt)ratherthan O(nt).
The solution to the problem involves changing one’s focus from writing axioms about
actions towriting axioms about fluents. Thus, foreach fluent F, wewillhavean axiom that
definesthetruthvalueofFt+1intermsoffluents(includingF itself)attimetandtheactions
thatmayhaveoccurredattimet. Now,thetruthvalueofFt+1 canbesetinoneoftwoways:
eithertheactionattimetcausesF tobetrueatt+1,orF wasalreadytrueattimetandthe
actionattimetdoesnotcauseittobefalse. Anaxiomofthisformiscalledasuccessor-state
SUCCESSOR-STATE axiomandhasthisschema:
AXIOM
Ft+1 ⇔ ActionCausesFt∨(Ft∧¬ActionCausesNotFt).
One of the simplest successor-state axioms is the one for HaveArrow. Because there is no
actionforreloading, the ActionCausesFt partgoesawayandweareleftwith
HaveArrowt+1 ⇔ (HaveArrowt∧¬Shoott). (7.2)
For the agent’s location, the successor-state axioms are more elaborate. For example, Lt+1
1,1
is true if either (a) the agent moved Forward from [1,2] when facing south, or from [2,1]
whenfacingwest;or(b)Lt wasalreadytrueandtheactiondidnotcausemovement(either
1,1
because the action wasnot Forward orbecause theaction bumped into awall). Written out
inpropositional logic,thisbecomes
Lt+1 ⇔ (Lt ∧(¬Forwardt∨Bumpt+1))
1,1 1,1
∨ (Lt ∧(Southt∧Forwardt)) (7.3)
1,2
∨ (Lt ∧(Westt∧Forwardt)).
2,1
Exercise7.26asksyoutowriteoutaxiomsfortheremaining wumpusworldfluents.
268 Chapter 7. LogicalAgents
Givenacompletesetofsuccessor-state axiomsandtheotheraxiomslistedatthebegin-
ningofthissection, theagentwillbeabletoASK andansweranyanswerablequestionabout
thecurrentstateoftheworld. Forexample,inSection7.2theinitialsequenceofperceptsand
actionsis
¬Stench0∧¬Breeze0∧¬Glitter0∧¬Bump0∧¬Scream0 ; Forward0
¬Stench1∧Breeze1∧¬Glitter1∧¬Bump1∧¬Scream1 ; TurnRight1
¬Stench2∧Breeze2∧¬Glitter2∧¬Bump2∧¬Scream2 ; TurnRight2
¬Stench3∧Breeze3∧¬Glitter3∧¬Bump3∧¬Scream3 ; Forward3
¬Stench4∧¬Breeze4∧¬Glitter4∧¬Bump4∧¬Scream4 ; TurnRight4
¬Stench5∧¬Breeze5∧¬Glitter5∧¬Bump5∧¬Scream5 ; Forward5
Stench6∧¬Breeze6∧¬Glitter6∧¬Bump6∧¬Scream6
At this point, we have ASK(KB,L6 )=true, so the agent knows where it is. Moreover,
1,2
ASK(KB,W
1,3
)=true andASK(KB,P
3,1
)=true,sotheagenthasfoundthewumpusand
oneofthepits. ThemostimportantquestionfortheagentiswhetherasquareisOKtomove
into, that is, the square contains no pit nor live wumpus. It’s convenient to add axioms for
this,havingtheform
OKt ⇔ ¬P ∧¬(W ∧WumpusAlivet).
x,y x,y x,y
Finally, ASK(KB,OK6 )=true, so the square [2,2] is OK to move into. In fact, given a
2,2
soundandcompleteinferencealgorithmsuchasDPLL,theagentcanansweranyanswerable
questionaboutwhichsquaresareOK—andcandosoinjustafewmilliseconds forsmall-to-
mediumwumpusworlds.
Solving the representational and inferential frame problems is a big step forward, but
apernicious problem remains: weneed toconfirm that allthenecessary preconditions ofan
actionholdforittohaveitsintendedeffect. WesaidthattheForward actionmovestheagent
aheadunlessthereisawallintheway,buttherearemanyotherunusualexceptionsthatcould
causetheactiontofail: theagentmighttripandfall,bestrickenwithaheartattack,becarried
QUALIFICATION awaybygiant bats, etc. Specifying allthese exceptions iscalled thequalification problem.
PROBLEM
There is no complete solution within logic; system designers have to use good judgment in
deciding how detailed they want to be in specifying their model, and what details they want
toleaveout. WewillseeinChapter13thatprobability theoryallowsustosummarizeallthe
exceptions withoutexplicitly namingthem.
7.7.2 A hybridagent
Theabilitytodeducevariousaspectsofthestateoftheworldcanbecombinedfairlystraight-
forwardly withcondition–action rulesandwithproblem-solving algorithms fromChapters3
and4toproduceahybridagentforthewumpusworld. Figure7.20showsonepossibleway
HYBRIDAGENT
to do this. The agent program maintains and updates a knowledge base as well as a current
plan. The initial knowledge base contains the atemporal axioms—those that don’t depend
on t, such as the axiom relating the breeziness of squares to the presence of pits. At each
timestep,thenewperceptsentenceisaddedalongwithalltheaxiomsthatdependont,such
Section7.7. AgentsBasedonPropositional Logic 269
asthesuccessor-state axioms. (Thenextsectionexplainswhytheagentdoesn’t needaxioms
for future time steps.) Then, the agent uses logical inference, by ASKing questions of the
knowledgebase,toworkoutwhichsquares aresafeandwhichhaveyettobevisited.
Themainbodyoftheagentprogramconstructsaplanbasedonadecreasingpriorityof
goals. First,ifthereisaglitter, theprogram constructs a plantograbthegold,followaroute
back tothe initial location, and climb out of the cave. Otherwise, ifthere isno current plan,
the program plans a route to the closest safe square that it has not visited yet, making sure
∗
the route goes through only safe squares. Route planning is done with A search, not with
ASK. Iftherearenosafesquarestoexplore,thenextstep—ifthe agentstillhasanarrow—is
totry to makea safe square byshooting at one ofthe possible wumpus locations. These are
determined by asking where ASK(KB,¬W
x,y
) is false—that is, where it is not known that
there is not awumpus. The function PLAN-SHOT (not shown) uses PLAN-ROUTE to plan a
sequence ofactions thatwillline upthis shot. Ifthis fails, the program looks forasquare to
explore that isnot provably unsafe—that is, asquare forwhich ASK(KB,¬OKt
x,y
)returns
false. Ifthereisnosuchsquare, thenthemissionisimpossibleandtheagentretreatsto[1,1]
andclimbsoutofthecave.
7.7.3 Logicalstateestimation
The agent program in Figure 7.20 works quite well, but it has one major weakness: as time
goesby,thecomputationalexpenseinvolvedinthecallstoASKgoesupandup. Thishappens
mainlybecausetherequiredinferenceshavetogobackfurtherandfurtherintimeandinvolve
more and more proposition symbols. Obviously, this is unsustainable—we cannot have an
agent whose timetoprocess each percept grows inproportion to the length ofits life! What
wereallyneedisaconstant updatetime—thatis,independent oft. Theobviousansweristo
save,orcache,theresultsofinference, sothattheinferenceprocessatthenexttimestepcan
CACHING
buildontheresultsofearlierstepsinstead ofhavingtostartagainfromscratch.
As we saw in Section 4.4, the past history of percepts and all their ramifications can
bereplaced bythe beliefstate—that is,somerepresentation ofthesetofallpossible current
statesoftheworld.12 Theprocessofupdating thebeliefstateasnewpercepts arriveiscalled
state estimation. Whereas in Section 4.4 the belief state was an explicit list of states, here
we can use a logical sentence involving the proposition symbols associated with the current
timestep,aswellastheatemporal symbols. Forexample,the logicalsentence
WumpusAlive1∧L1 ∧B ∧(P ∨P ) (7.4)
2,1 2,1 3,1 2,2
represents the set of all states at time 1 in which the wumpus is alive, the agent is at [2,1],
thatsquareisbreezy, andthereisapitin [3,1]or[2,2]orboth.
Maintaining an exact belief state as a logical formula turns out not to be easy. If there
arenfluentsymbolsfortimet,thenthereare2npossiblestates—thatis,assignmentsoftruth
valuestothosesymbols. Now,thesetofbeliefstatesisthepowerset(setofallsubsets)ofthe
set of physical states. There are 2n physical states, hence 22n belief states. Even if weused
the most compact possible encoding of logical formulas, with each belief state represented
12 Wecanthinkofthepercepthistoryitselfasarepresentationofthebeliefstate,butonethatmakesinference
increasinglyexpensiveasthehistorygetslonger.
270 Chapter 7. LogicalAgents
functionHYBRID-WUMPUS-AGENT(percept)returnsanaction
inputs:percept,alist,[stench,breeze,glitter,bump,scream]
persistent: KB,aknowledgebase,initiallytheatemporal“wumpusphysics”
t,acounter,initially0,indicatingtime
plan,anactionsequence,initiallyempty
TELL(KB,MAKE-PERCEPT-SENTENCE(percept,t))
TELLtheKB thetemporal“physics”sentencesfortimet
safe←{[x,y] : ASK(KB,OK t
x,y
) = true}
t
ifASK(KB,Glitter ) = true then
plan←[Grab]+PLAN-ROUTE(current,{[1,1]},safe)+[Climb]
ifplan isemptythen
unvisited←{[x,y] : ASK(KB,Lt
x
(cid:3)
,y
) = false forall t(cid:5) ≤ t}
plan←PLAN-ROUTE(current,unvisited∩safe,safe)
t
ifplan isemptyandASK(KB,HaveArrow ) = true then
possible wumpus←{[x,y] : ASK(KB,¬Wx,y) = false}
plan←PLAN-SHOT(current,possible wumpus,safe)
ifplan isemptythen //nochoicebuttotakearisk
not unsafe←{[x,y] : ASK(KB,¬OK t
x,y
) = false}
plan←PLAN-ROUTE(current,unvisited∩not unsafe,safe)
ifplan isemptythen
plan←PLAN-ROUTE(current,{[1,1]},safe)+[Climb]
action←POP(plan)
TELL(KB,MAKE-ACTION-SENTENCE(action,t))
t←t +1
returnaction
functionPLAN-ROUTE(current,goals,allowed)returnsanactionsequence
inputs:current,theagent’scurrentposition
goals,asetofsquares;trytoplanaroutetooneofthem
allowed,asetofsquaresthatcanformpartoftheroute
problem←ROUTE-PROBLEM(current,goals,allowed)
returnA*-GRAPH-SEARCH(problem)
Figure7.20 Ahybridagentprogramforthewumpusworld.Itusesapropositionalknowl-
edgebase to inferthestate ofthe world, anda combinationof problem-solvingsearch and
domain-specificcodetodecidewhatactionstotake.
by a unique binary number, we would need numbers with log (22n )=2n bits to label the
2
currentbeliefstate. Thatis,exactstateestimationmayrequirelogicalformulaswhosesizeis
exponential inthenumberofsymbols.
Onevery common and natural scheme for approximate state estimation is to represent
beliefstatesasconjunctionsofliterals,thatis,1-CNFformulas. Todothis,theagentprogram
simply tries to prove Xt and ¬Xt for each symbol Xt (as well as each atemporal symbol
whose truth value is not yet known), given the belief state at t − 1. The conjunction of
Section7.7. AgentsBasedonPropositional Logic 271
Figure 7.21 Depiction of a 1-CNF belief state (bold outline) as a simply representable,
conservative approximation to the exact (wiggly) belief state (shaded region with dashed
outline).Eachpossibleworldisshownasacircle;theshadedonesareconsistentwithallthe
percepts.
provableliteralsbecomesthenewbeliefstate,andthepreviousbeliefstateisdiscarded.
Itis important to understand that this scheme maylose some information as timegoes
along. Forexample, if the sentence in Equation (7.4) were the true belief state, then neither
P nor P would be provable individually and neither would appear in the 1-CNF belief
3,1 2,2
state. (Exercise 7.27 explores one possible solution to this problem.) On the other hand,
because every literal in the 1-CNF belief state is proved from the previous belief state, and
the initial belief state is a true assertion, we know that entire 1-CNF belief state must be
true. Thus, thesetofpossible statesrepresented bythe1-CNFbeliefstateincludes allstates
that are in fact possible given the full percept history. As illustrated in Figure 7.21, the 1-
CONSERVATIVE CNFbeliefstateactsasasimpleouterenvelope,orconservativeapproximation,aroundthe
APPROXIMATION
exact belief state. We see this idea of conservative approximations to complicated sets as a
recurring themeinmanyareasofAI.
7.7.4 Making plans by propositional inference
TheagentinFigure7.20useslogicalinference todetermine whichsquaresaresafe,butuses
∗
A search to make plans. In this section, we show how to make plans by logical inference.
Thebasicideaisverysimple:
1. Constructasentence thatincludes
(a) Init0,acollection ofassertions abouttheinitialstate;
(b) Transition1,...,Transitiont, the successor-state axioms for all possible actions
ateachtimeuptosomemaximumtimet;
(c) theassertion thatthegoalisachieved attimet: HaveGoldt∧ClimbedOutt.
272 Chapter 7. LogicalAgents
2. Presentthewholesentence toaSATsolver. Ifthesolverfindsasatisfying model, then
the goal is achievable; if the sentence is unsatisfiable, then the planning problem is
impossible.
3. Assuming a model is found, extract from the model those variables that represent ac-
tionsandareassigned true. Togethertheyrepresent aplantoachievethegoals.
A propositional planning procedure, SATPLAN, is shown in Figure 7.22. It implements the
basic idea just given, with one twist. Because the agent does not know how many steps it
will take to reach the goal, the algorithm tries each possible number of steps t, up to some
maximumconceivableplanlengthT . Inthisway,itisguaranteedtofindtheshortestplan
max
if one exists. Because of the way SATPLAN searches for a solution, this approach cannot
be used in a partially observable environment; SATPLAN would just set the unobservable
variables tothevaluesitneedstocreateasolution.
functionSATPLAN(init, transition, goal,T
max
)returnssolutionorfailure
inputs:init, transition, goal,constituteadescriptionoftheproblem
T ,anupperlimitforplanlength
max
fort =0toT do
max
cnf ←TRANSLATE-TO-SAT(init, transition, goal,t)
model←SAT-SOLVER(cnf)
ifmodel isnotnullthen
returnEXTRACT-SOLUTION(model)
returnfailure
Figure 7.22 The SATPLAN algorithm. The planning problem is translated into a CNF
sentenceinwhichthegoalisassertedtoholdatafixedtimesteptandaxiomsareincluded
foreachtimestepuptot. Ifthesatisfiabilityalgorithmfindsamodel,thenaplanisextracted
by looking at those proposition symbols that refer to actions and are assigned true in the
model.Ifnomodelexists,thentheprocessisrepeatedwiththegoalmovedonesteplater.
The key step in using SATPLAN is the construction of the knowledge base. It might
seem, on casual inspection, that the wumpus world axioms in Section 7.7.1 suffice forsteps
1(a)and1(b)above. Thereis,however,asignificantdifference betweentherequirements for
entailment (as tested by ASK)and those forsatisfiability. Consider, forexample, theagent’s
location, initially [1,1], and suppose the agent’s unambitious goal isto be in[2,1] attime 1.
TheinitialknowledgebasecontainsL0 andthegoalisL1 . UsingASK,wecanproveL1
1,1 2,1 2,1
if Forward0 is asserted, and, reassuringly, we cannot prove L1 if, say, Shoot0 is asserted
2,1
instead. Now, SATPLAN will find the plan [Forward0]; so far, so good. Unfortunately,
SATPLANalsofindstheplan[Shoot0]. Howcouldthisbe? Tofindout,weinspectthemodel
that SATPLAN constructs: it includes the assignment L0 , that is, the agent can be in [2,1]
2,1
attime1bybeingthereattime0andshooting. Onemightask,“Didn’twesaytheagentisin
[1,1]attime0?” Yes,wedid,butwedidn’ttelltheagentthatitcan’tbeintwoplacesatonce!
For entailment, L0 is unknown and cannot, therefore, be used in a proof; for satisfiability,
2,1
Section7.7. AgentsBasedonPropositional Logic 273
on the other hand, L0 is unknown and can, therefore, be set to whatever value helps to
2,1
makethegoaltrue. Forthisreason, SATPLAN isagooddebuggingtoolforknowledgebases
because it reveals places where knowledge is missing. Inthis particular case, wecan fixthe
knowledgebasebyassertingthat,ateachtimestep,theagentisinexactlyonelocation,using
acollection ofsentences similartothoseusedtoasserttheexistence ofexactlyonewumpus.
Alternatively,wecanassert¬L0 foralllocationsotherthan[1,1];thesuccessor-stateaxiom
x,y
for location takes care of subsequent time steps. The same fixes also work to make sure the
agenthasonlyoneorientation.
SATPLAN has more surprises in store, however. The first is that it finds models with
impossibleactions,suchasshootingwithnoarrow. Tounderstandwhy,weneedtolookmore
carefullyatwhatthesuccessor-state axioms(suchasEquation(7.3))sayaboutactionswhose
preconditionsarenotsatisfied. Theaxiomsdopredictcorrectlythatnothingwillhappenwhen
suchanaction isexecuted (seeExercise10.14), buttheydonotsaythattheactioncannot be
PRECONDITION executed! Toavoid generating plans withillegal actions, wemustadd precondition axioms
AXIOMS
statingthatanactionoccurrencerequiresthepreconditionstobesatisfied.13 Forexample,we
needtosay,foreachtimet,that
Shoott ⇒ HaveArrowt .
This ensures that if a plan selects the Shoot action at any time, it must be the case that the
agenthasanarrowatthattime.
SATPLAN’ssecondsurpriseisthecreationofplanswithmultiplesimultaneousactions.
For example, it may come up with a model in which both Forward0 and Shoot0 are true,
ACTIONEXCLUSION whichisnotallowed. Toeliminate thisproblem, weintroduce action exclusion axioms: for
AXIOM
everypairofactions At andAt weaddtheaxiom
i j
¬At∨¬At .
i j
Itmight bepointed out that walking forward and shooting at the same timeis not so hard to
do, whereas, say, shooting and grabbing atthesametimeis ratherimpractical. Byimposing
action exclusion axioms only on pairs of actions that really do interfere with each other, we
canallowforplansthatincludemultiplesimultaneousactions—andbecauseSATPLANfinds
theshortestlegalplan,wecanbesurethatitwilltakeadvantage ofthiscapability.
To summarize, SATPLAN finds models for a sentence containing the initial state, the
goal, the successor-state axioms, the precondition axioms, and the action exclusion axioms.
It can be shown that this collection of axioms is sufficient, in the sense that there are no
longer any spurious “solutions.” Any model satisfying the propositional sentence will be a
valid plan for the original problem. Modern SAT-solving technology makes the approach
quite practical. Forexample, aDPLL-stylesolverhasno difficulty in generating the11-step
solution forthewumpusworldinstance showninFigure7.2.
Thissectionhasdescribedadeclarativeapproachtoagentconstruction: theagentworks
byacombination ofasserting sentences intheknowledge baseandperforming logical infer-
ence. This approach has some weaknesses hidden in phrases such as “for each time t” and
13 Noticethattheadditionofpreconditionaxiomsmeansthatweneednotincludepreconditionsforactionsin
thesuccessor-stateaxioms.
274 Chapter 7. LogicalAgents
“for each square [x,y].” For any practical agent, these phrases have to be implemented by
codethatgenerates instances ofthegeneral sentence schemaautomatically forinsertion into
theknowledge base. Forawumpusworldofreasonable size—one comparable toasmallish
computer game—we might need a 100×100 board and 1000 time steps, leading to knowl-
edge bases withtens orhundreds ofmillions ofsentences. Notonly does thisbecome rather
impractical, but it also illustrates a deeper problem: we know something about the wum-
pus world—namely, that the “physics” works the same way across all squares and all time
steps—that we cannot express directly in the language of propositional logic. To solve this
problem, we need a more expressive language, one in which phrases like “for each time t”
and “for each square [x,y]” can be written in a natural way. First-order logic, described in
Chapter 8, is such a language; in first-order logic a wumpus world of any size and duration
canbedescribed inabouttensentences ratherthantenmillionortentrillion.
7.8 SUMMARY
We have introduced knowledge-based agents and have shown how to define a logic with
whichsuchagentscanreasonabouttheworld. Themainpoints areasfollows:
• Intelligent agentsneedknowledge abouttheworldinordertoreachgooddecisions.
• Knowledge is contained in agents in the form of sentences in a knowledge represen-
tationlanguagethatarestoredinaknowledgebase.
• A knowledge-based agent is composed of a knowledge base and an inference mecha-
nism. Itoperates bystoring sentences abouttheworldinits knowledge base, using the
inference mechanism toinfernew sentences, and using these sentences todecide what
actiontotake.
• A representation language is defined by its syntax, which specifies the structure of
sentences,anditssemantics,whichdefinesthetruthofeachsentenceineachpossible
worldormodel.
• The relationship of entailment between sentences is crucial to our understanding of
reasoning. A sentence α entails another sentence β if β is true in all worlds where
α is true. Equivalent definitions include the validity of the sentence α ⇒ β and the
unsatisfiabilityofthesentence α∧¬β.
• Inferenceistheprocessofderivingnewsentencesfromoldones. Soundinferencealgo-
rithmsderiveonlysentencesthatareentailed; completealgorithmsderiveallsentences
thatareentailed.
• Propositionallogicisasimplelanguageconsistingofpropositionsymbolsandlogical
connectives. Itcanhandlepropositionsthatareknowntrue,knownfalse,orcompletely
unknown.
• The set of possible models, given a fixed propositional vocabulary, is finite, so en-
tailment can be checked by enumerating models. Efficient model-checking inference
algorithms for propositional logic include backtracking and local search methods and
canoftensolvelargeproblemsquickly.
Bibliographical andHistorical Notes 275
• Inference rules are patterns of sound inference that can be used to find proofs. The
resolution rule yields a complete inference algorithm for knowledge bases that are
expressed in conjunctive normal form. Forward chaining and backward chaining
areverynaturalreasoning algorithmsforknowledgebasesinHornform.
• Local search methods such as WALKSAT can be used to find solutions. Such algo-
rithmsaresoundbutnotcomplete.
• Logical state estimation involves maintaining alogical sentence that describes the set
of possible states consistent with the observation history. Each update step requires
inferenceusingthetransitionmodeloftheenvironment, whichisbuiltfromsuccessor-
stateaxiomsthatspecifyhoweachfluentchanges.
• Decisionswithinalogical agentcanbemadebySATsolving: findingpossible models
specifying future action sequences that reach the goal. This approach works only for
fullyobservable orsensorless environments.
• Propositional logic does not scale to environments of unbounded size because it lacks
theexpressive powertodeal concisely withtime, space, and universal patterns ofrela-
tionships amongobjects.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
John McCarthy’s paper “Programs with Common Sense” (McCarthy, 1958, 1968) promul-
gatedthenotionofagentsthatuselogicalreasoningtomediatebetweenperceptsandactions.
Italsoraisedtheflagofdeclarativism,pointingoutthattellinganagentwhatitneedstoknow
is an elegant way to build software. Allen Newell’s (1982) article “The Knowledge Level”
makesthecasethatrationalagentscanbedescribedandanalyzedatanabstractleveldefined
bytheknowledgetheypossessratherthantheprogramstheyrun. Thedeclarativeandproce-
dural approaches to AI are analyzed in depth by Boden (1977). The debate was revived by,
amongothers, Brooks(1991) andNilsson(1991), andcontinues tothisday(Shaparau etal.,
2008). Meanwhile, thedeclarative approach has spread into otherareas ofcomputerscience
suchasnetworking (Looetal.,2006).
Logicitself haditsorigins inancient Greek philosophy and mathematics. Variouslog-
ical principles—principles connecting the syntactic structure of sentences with their truth
and falsity, with their meaning, or with the validity of arguments in which they figure—are
scattered in the works of Plato. The first known systematic study of logic was carried out
by Aristotle, whose work was assembled by his students after his death in 322 B.C. as a
treatise called the Organon. Aristotle’s syllogisms were what we would now call inference
SYLLOGISM
rules. Although thesyllogisms included elements ofbothpropositional andfirst-orderlogic,
the system as a whole lacked the compositional properties required to handle sentences of
arbitrary complexity.
The closely related Megarian and Stoic schools (originating in the fifth century B.C.
andcontinuingforseveralcenturiesthereafter)beganthe systematicstudyofthebasiclogical
connectives. The use of truth tables fordefining connectives is due to Philo of Megara. The
276 Chapter 7. LogicalAgents
Stoics took five basic inference rules as valid without proof, including the rule we now call
Modus Ponens. They derived a number of other rules from these five, using, among other
principles, the deduction theorem (page 249) and were much clearer about the notion of
proofthanwasAristotle. Agoodaccount ofthehistory ofMegarianandStoiclogic isgiven
byBensonMates(1953).
Theidea of reducing logical inference toapurely mechanical process applied to afor-
mallanguageisduetoWilhelmLeibniz(1646–1716), althoughhehadlimitedsuccessinim-
plementing theideas. GeorgeBoole(1847)introduced thefirstcomprehensive andworkable
system of formal logic in his book The Mathematical Analysis of Logic. Boole’s logic was
closely modeled on the ordinary algebra of real numbers and used substitution of logically
equivalent expressions as its primary inference method. Although Boole’s system still fell
shortoffullpropositional logic,itwascloseenoughthatothermathematicianscouldquickly
fill in the gaps. Schro¨der (1877) described conjunctive normal form, while Horn form was
introduced muchlaterbyAlfredHorn(1951). Thefirstcomprehensive exposition ofmodern
propositional logic (and first-order logic) is found in Gottlob Frege’s (1879) Begriffschrift
(“ConceptWriting”or“Conceptual Notation”).
Thefirstmechanical devicetocarryoutlogicalinferences wasconstructed bythethird
Earl of Stanhope (1753–1816). The Stanhope Demonstrator could handle syllogisms and
certain inferences in the theory of probability. William Stanley Jevons, one of those who
improved upon and extended Boole’s work, constructed his “logical piano” in 1869 to per-
form inferences in Boolean logic. An entertaining and instructive history of these and other
early mechanical devices for reasoning is given by Martin Gardner (1968). The first pub-
lished computer program for logical inference was the Logic Theorist of Newell, Shaw,
and Simon (1957). This program was intended to model human thought processes. Mar-
tinDavis(1957) hadactually designed aprogram thatcameup withaproof in1954, but the
LogicTheorist’sresultswerepublished slightlyearlier.
Truthtablesasamethodoftestingvalidityorunsatisfiabilityinpropositionallogicwere
introducedindependentlybyEmilPost(1921)andLudwigWittgenstein(1922). Inthe1930s,
a great deal of progress was made on inference methods for first-order logic. In particular,
Go¨del (1930) showed that a complete procedure for inference in first-order logic could be
obtained via areduction topropositional logic, using Herbrand’s theorem (Herbrand, 1930).
We take up this history again in Chapter 9; the important point here is that the development
of efficient propositional algorithms in the 1960s was motivated largely by the interest of
mathematicians inaneffectivetheorem proverforfirst-orderlogic. TheDavis–Putnam algo-
rithm(DavisandPutnam,1960)wasthefirsteffectivealgorithm forpropositional resolution
butwasinmostcasesmuch lessefficient thanthe DPLL backtracking algorithm introduced
twoyearslater(1962). Thefullresolution ruleandaproof ofitscompleteness appeared ina
seminal paper by J. A. Robinson (1965), which also showed how to do first-order reasoning
withoutresorttopropositional techniques.
Stephen Cook (1971) showed that deciding satisfiability ofa sentence in propositional
logic (the SAT problem) is NP-complete. Since deciding entailment is equivalent to decid-
ingunsatisfiability, itisco-NP-complete. Manysubsets ofpropositional logic areknownfor
which the satisfiability problem is polynomially solvable; Horn clauses are one such subset.
Bibliographical andHistorical Notes 277
The linear-time forward-chaining algorithm for Horn clauses is due to Dowling and Gallier
(1984), whodescribe theiralgorithm asadataflow process similartothepropagation of sig-
nalsinacircuit.
Early theoretical investigations showed that DPLL has polynomial average-case com-
plexity for certain natural distributions of problems. This potentially exciting fact became
less exciting when Franco and Paull (1983) showed that the same problems could be solved
in constant time simply by guessing random assignments. The random-generation method
describedinthechapterproducesmuchharderproblems. Motivatedbytheempiricalsuccess
oflocalsearchontheseproblems,Koutsoupias andPapadimitriou (1992)showedthatasim-
plehill-climbing algorithmcansolvealmostallsatisfiabilityprobleminstancesveryquickly,
suggesting that hard problems are rare. Moreover, Scho¨ning (1999) exhibited a randomized
hill-climbingalgorithm whoseworst-caseexpectedruntimeon3-SATproblems(thatis,sat-
isfiability of 3-CNF sentences) is O(1.333n)—still exponential, but substantially faster than
previous worst-case bounds. The current record is O(1.324n) (Iwama and Tamaki, 2004).
Achlioptas et al. (2004) and Alekhnovich et al. (2005) exhibit families of 3-SAT instances
forwhichallknownDPLL-likealgorithms requireexponential running time.
Onthepracticalside,efficiencygainsinpropositionalsolvershavebeenmarked. Given
tenminutesofcomputingtime,theoriginal DPLL algorithm in1962couldonlysolveprob-
lems with no more than 10 or 15 variables. By 1995 the SATZ solver (Li and Anbulagan,
1997) could handle 1,000 variables, thanks to optimized data structures for indexing vari-
ables. Twocrucial contributions were the watched literal indexing technique of Zhang and
Stickel (1996), which makes unit propagation very efficient, and the introduction of clause
(i.e.,constraint)learningtechniquesfromtheCSPcommunitybyBayardoandSchrag(1997).
Using these ideas, andspurred by theprospect ofsolving industrial-scale circuit verification
problems, Moskewicz et al. (2001) developed the CHAFF solver, which could handle prob-
lems with millions of variables. Beginning in 2002, SAT competitions have been held reg-
ularly; most of the winning entries have either been descendants of CHAFF orhave used the
samegeneral approach. RSAT (Pipatsrisawat andDarwiche, 2007), the2007winner, falls in
thelattercategory. AlsonoteworthyisMINISAT (EenandSo¨rensson,2003),anopen-source
implementation available at http://minisat.sethat is designed to be easily modified
andimproved. Thecurrentlandscape ofsolversissurveyedbyGomesetal.(2008).
Local search algorithms for satisfiability were tried by various authors throughout the
1980s; all ofthe algorithms werebased onthe idea ofminimizing the number ofunsatisfied
clauses (Hansen and Jaumard, 1990). A particularly effective algorithm was developed by
Gu(1989) and independently bySelman etal. (1992), whocalled it GSAT and showed that
it wascapable of solving a wide range of very hard problems very quickly. The WALKSAT
algorithm described inthechapterisduetoSelman etal.(1996).
The “phase transition” in satisfiability of random k-SAT problems was first observed
by Simon and Dubois (1989) and has given rise to a great deal of theoretical and empirical
research—due, inpart,totheobviousconnection tophasetransitionphenomenainstatistical
physics. Cheeseman etal. (1991) observed phase transitions in several CSPsand conjecture
that all NP-hard problems have a phase transition. Crawford and Auton (1993) located the
3-SAT transition at a clause/variable ratio of around 4.26, noting that this coincides with a
278 Chapter 7. LogicalAgents
sharppeakintheruntimeoftheirSATsolver. CookandMitchell(1997)provideanexcellent
summaryoftheearlyliterature ontheproblem.
The current state of theoretical understanding is summarized by Achlioptas (2009).
SATISFIABILITY
The satisfiability threshold conjecture states that, for each k, there is a sharp satisfiability
THRESHOLD
CONJECTURE threshold r ,suchthatasthenumberofvariables n → ∞,instances belowthethreshold are
k
satisfiable with probability 1, while those above the threshold are unsatisfiable with proba-
bility1. Theconjecture wasnotquiteprovedbyFriedgut(1999): asharpthresholdexistsbut
its location might depend on n even as n → ∞. Despite significant progress in asymptotic
analysis of the threshold location for large k (Achlioptas and Peres, 2004; Achlioptas et al.,
2007), all that can be proved for k=3 is that it lies in the range [3.52,4.51]. Current theory
suggests that apeak in the run time of aSATsolver is not necessarily related to the satisfia-
bility threshold, but instead to a phase transition in the solution distribution and structure of
SAT instances. Empirical results due to Coarfa et al. (2003) support this view. In fact, al-
SURVEY gorithms suchas surveypropagation (ParisiandZecchina, 2002; Manevaetal.,2007)take
PROPAGATION
advantage ofspecial properties ofrandom SATinstances nearthesatisfiability threshold and
greatlyoutperform generalSATsolversonsuchinstances.
Thebestsources forinformation onsatisfiability, boththeoretical andpractical, arethe
Handbook of Satisfiability (Biere et al., 2009) and the regular International Conferences on
TheoryandApplications ofSatisfiability Testing,knownasSAT.
The idea of building agents with propositional logic can be traced back to the seminal
paper of McCulloch and Pitts (1943), which initiated the field of neural networks. Con-
trary topopular supposition, thepaperwasconcerned withthe implementation ofaBoolean
circuit-based agentdesign inthebrain. Circuit-based agents, whichperform computation by
propagating signals in hardware circuits rather than running algorithms in general-purpose
computers, have received little attention in AI, however. The most notable exception is the
work of Stan Rosenschein (Rosenschein, 1985; Kaelbling andRosenschein, 1990), who de-
veloped ways to compile circuit-based agents from declarative descriptions of the task envi-
ronment. (Rosenschein’s approach is described at some length in the second edition of this
book.) TheworkofRodBrooks(1986,1989)demonstratestheeffectivenessofcircuit-based
designs for controlling robots—a topic we take up in Chapter 25. Brooks (1991) argues
that circuit-based designs are all that is needed for AI—that representation and reasoning
are cumbersome, expensive, and unnecessary. In our view, neither approach is sufficient by
itself. Williams et al. (2003) show how a hybrid agent design not too different from our
wumpusagenthasbeenusedtocontrolNASAspacecraft, planningsequencesofactionsand
diagnosing andrecoveringfromfaults.
Thegeneral problem ofkeeping track of apartially observable environment wasintro-
duced for state-based representations in Chapter 4. Its instantiation for propositional repre-
sentations was studied by Amir and Russell (2003), who identified several classes of envi-
ronments that admit efficient state-estimation algorithms and showed that for several other
TEMPORAL- classes theproblem isintractable. The temporal-projection problem, whichinvolves deter-
PROJECTION
mining what propositions hold true after an action sequence is executed, can be seen as a
specialcaseofstateestimationwithemptypercepts. Manyauthorshavestudiedthisproblem
because of its importance in planning; some important hardness results were established by
Exercises 279
Liberatore (1997). The idea of representing a belief state with propositions can be traced to
Wittgenstein (1922).
Logical state estimation, of course, requires a logical representation of the effects of
actions—a key problem in AI since the late 1950s. The dominant proposal has been the sit-
uation calculusformalism (McCarthy, 1963), whichiscouched within first-order logic. We
discusssituationcalculus,andvariousextensionsandalternatives,inChapters10and12. The
approach taken in this chapter—using temporal indices on propositional variables—is more
restrictivebuthasthebenefitofsimplicity. ThegeneralapproachembodiedintheSATPLAN
algorithm wasproposed by Kautzand Selman (1992). Latergenerations of SATPLAN were
able to take advantage of the advances in SAT solvers, described earlier, and remain among
themosteffectivewaysofsolving difficultproblems(Kautz,2006).
The frame problem was first recognized by McCarthy and Hayes (1969). Many re-
searchers considered the problem unsolvable within first-order logic, and it spurred a great
deal of research into nonmonotonic logics. Philosophers from Dreyfus (1972) to Crockett
(1994) have cited the frame problem as one symptom of the inevitable failure of the entire
AI enterprise. The solution of the frame problem with successor-state axioms is due to Ray
Reiter (1991). Thielscher (1999) identifies the inferential frame problem as a separate idea
and provides a solution. In retrospect, one can see that Rosenschein’s (1985) agents were
using circuits that implemented successor-state axioms, but Rosenschein did not notice that
the frame problem was thereby largely solved. Foo (2001) explains why the discrete-event
control theory models typically used by engineers do not have to explicitly deal with the
frame problem: because they are dealing with prediction and control, not with explanation
andreasoning aboutcounterfactual situations.
Modernpropositionalsolvershavewideapplicabilityinindustrialapplications. Theap-
plication ofpropositional inference inthe synthesis ofcomputer hardware isnow astandard
technique having manylarge-scale deployments (Nowick etal.,1993). The SATMC satisfi-
abilitycheckerwasusedtodetectapreviously unknownvulnerability inaWebbrowseruser
sign-onprotocol (Armando etal.,2008).
Thewumpus world wasinvented byGregory Yob(1975). Ironically, Yob developed it
because he was bored with games played on a rectangular grid: the topology of his original
wumpus world was a dodecahedron, and we put it back in the boring old grid. Michael
Genesereth wasthefirsttosuggest thatthewumpusworldbeusedasanagenttestbed.
EXERCISES
7.1 SupposetheagenthasprogressedtothepointshowninFigure 7.4(a),page239,having
perceivednothingin[1,1],abreezein[2,1],andastenchin[1,2],andisnowconcernedwith
the contents of [1,3], [2,2], and [3,1]. Each of these can contain a pit, and at most one can
containawumpus. FollowingtheexampleofFigure7.5,constructthesetofpossibleworlds.
(You should find 32 of them.) Mark the worlds in which the KB is true and those in which
280 Chapter 7. LogicalAgents
eachofthefollowingsentences istrue:
α = “Thereisnopitin[2,2].”
2
α = “Thereisawumpusin[1,3].”
3
HenceshowthatKB |= α andKB |= α .
2 3
7.2 (Adapted from Barwise and Etchemendy (1993).) Given the following, can you prove
thattheunicornismythical? Howaboutmagical? Horned?
Iftheunicorn ismythical, thenitisimmortal, but ifitisnotmythical, thenitisa
mortalmammal. Iftheunicorn iseitherimmortaloramammal, thenitishorned.
Theunicornismagicalifitishorned.
7.3 Consider the problem of deciding whether a propositional logic sentence is true in a
givenmodel.
a. Write a recursive algorithm PL-TRUE?(s,m) that returns true if and only if the sen-
tence s is true in the model m (where m assigns a truth value for every symbol in s).
Thealgorithm shouldrunintimelinearinthesizeofthesentence. (Alternatively, usea
versionofthisfunction fromtheonlinecoderepository.)
b. Givethree examplesofsentences thatcanbedetermined tobetrueorfalseina partial
modelthatdoesnotspecifyatruthvalueforsomeofthesymbols.
c. Showthatthetruthvalue(ifany)ofasentenceinapartialmodelcannotbedetermined
efficientlyingeneral.
d. Modify your PL-TRUE? algorithm so that it can sometimes judge truth from partial
models,whileretainingitsrecursivestructure andlinear runtime. Givethreeexamples
ofsentences whosetruthinapartialmodelisnotdetected byyouralgorithm.
e. Investigate whetherthemodifiedalgorithm makes TT-ENTAILS? moreefficient.
7.4 Whichofthefollowingarecorrect?
a. False |= True.
b. True |= False.
c. (A∧B)|= (A ⇔ B).
d. A ⇔ B |= A∨B.
e. A ⇔ B |= ¬A∨B.
f. (A∧B) ⇒ C |= (A ⇒ C)∨(B ⇒ C).
g. (C ∨(¬A∧¬B))≡ ((A ⇒ C)∧(B ⇒ C)).
h. (A∨B)∧(¬C ∨¬D∨E) |= (A∨B).
i. (A∨B)∧(¬C ∨¬D∨E) |= (A∨B)∧(¬D∨E).
j. (A∨B)∧¬(A ⇒ B)issatisfiable.
k. (A ⇔ B)∧(¬A∨B)issatisfiable.
l. (A ⇔ B) ⇔ C has the same number of models as (A ⇔ B)forany fixed set of
proposition symbolsthatincludes A,B,C.
Exercises 281
7.5 Proveeachofthefollowingassertions:
a. αisvalidifandonlyifTrue |= α.
b. Foranyα,False |= α.
c. α|= β ifandonlyifthesentence (α ⇒ β)isvalid.
d. α≡ β ifandonlyifthesentence (α ⇔ β)isvalid.
e. α|= β ifandonlyifthesentence (α∧¬β)isunsatisfiable.
7.6 Prove,orfindacounterexample to,eachofthefollowingassertions:
a. Ifα |= γ orβ |= γ (orboth)then(α∧β) |= γ
b. Ifα |= (β∧γ)thenα|= β andα |= γ.
c. Ifα |= (β∨γ)thenα|= β orα |= γ (orboth).
7.7 Consideravocabularywithonlyfourpropositions, A,B,C,andD. Howmanymodels
arethereforthefollowingsentences?
a. B∨C.
b. ¬A∨¬B∨¬C ∨¬D.
c. (A ⇒ B)∧A∧¬B∧C ∧D.
7.8 Wehavedefinedfourbinarylogicalconnectives.
a. Arethereanyothersthatmightbeuseful?
b. Howmanybinaryconnectives cantherebe?
c. Whyaresomeofthemnotveryuseful?
7.9 Usingamethodofyourchoice,verifyeachoftheequivalencesinFigure7.11(page249).
7.10 Decidewhethereachofthefollowingsentencesisvalid,unsatisfiable, orneither. Ver-
ifyyourdecisions usingtruthtablesortheequivalence rulesofFigure7.11(page249).
a. Smoke ⇒ Smoke
b. Smoke ⇒ Fire
c. (Smoke ⇒ Fire) ⇒ (¬Smoke ⇒ ¬Fire)
d. Smoke ∨Fire ∨¬Fire
e. ((Smoke ∧Heat) ⇒ Fire) ⇔ ((Smoke ⇒ Fire)∨(Heat ⇒ Fire))
f. (Smoke ⇒ Fire) ⇒ ((Smoke ∧Heat) ⇒ Fire)
g. Big ∨Dumb ∨(Big ⇒ Dumb)
7.11 Anypropositional logicsentenceislogically equivalent totheassertion thateachpos-
sible world in which it would be false is not the case. From this observation, prove that any
sentence canbewritteninCNF.
7.12 Useresolutiontoprovethesentence¬A∧¬BfromtheclausesinExercise7.20.
7.13 Thisexerciselooksintotherelationship betweenclausesandimplication sentences.
282 Chapter 7. LogicalAgents
a. Showthattheclause (¬P ∨···∨¬P ∨Q)islogically equivalent totheimplication
1 m
sentence(P ∧···∧P ) ⇒ Q.
1 m
b. Show that every clause (regardless of the number of positive literals) can bewritten in
the form (P ∧···∧P ) ⇒ (Q ∨···∨Q ), where the Ps and Qs are proposition
1 m 1 n
symbols. A knowledge base consisting of such sentences is in implicative normal
IMPLICATIVE formorKowalskiform(Kowalski,1979).
NORMALFORM
c. Writedownthefullresolution ruleforsentences inimplicativenormalform.
7.14 According to some political pundits, a person who is radical (R) is electable (E) if
he/sheisconservative (C),butotherwiseisnotelectable.
a. Whichofthefollowingarecorrectrepresentations ofthis assertion?
(i) (R∧E) ⇐⇒ C
(ii) R ⇒ (E ⇐⇒ C)
(iii) R ⇒ ((C ⇒ E)∨¬E)
b. Whichofthesentences in(a)canbeexpressedinHornform?
7.15 Thisquestion considers representing satisfiability (SAT)problemsasCSPs.
a. Drawtheconstraint graphcorresponding totheSATproblem
(¬X 1 ∨X 2 )∧(¬X 2 ∨X 3 )∧...∧(¬X n−1 ∨X n )
fortheparticularcase n=5.
b. Howmanysolutions arethereforthisgeneralSATproblem as afunction ofn?
c. Supposeweapply BACKTRACKING-SEARCH (page215)tofindallsolutions toaSAT
CSP of the type given in (a). (To find all solutions to a CSP, we simply modify the
basic algorithm so it continues searching after each solution is found.) Assume that
variables are ordered X ,...,X and false is ordered before true. How much time
1 n
willthealgorithm taketoterminate? (Writean O(·)expression asafunction ofn.)
d. We know that SAT problems in Horn form can be solved in linear time by forward
chaining (unit propagation). We also know that every tree-structured binary CSP with
discrete, finite domains can be solved in time linear in the number of variables (Sec-
tion6.5). Arethesetwofactsconnected? Discuss.
7.16 Explain why every nonempty propositional clause, by itself, is satisfiable. Prove rig-
orouslythateverysetoffive3-SATclausesissatisfiable, provided thateachclausementions
exactly three distinct variables. Whatisthesmallest setofsuch clauses thatisunsatisfiable?
Constructsuchaset.
7.17 Apropositional 2-CNFexpressionisaconjunction ofclauses,eachcontaining exactly
2literals, e.g.,
(A∨B)∧(¬A∨C)∧(¬B∨D)∧(¬C ∨G)∧(¬D∨G).
a. Proveusingresolution thattheabovesentenceentails G.
Exercises 283
b. Two clauses are semantically distinct if they are not logically equivalent. How many
semantically distinct2-CNFclausescanbeconstructed fromnproposition symbols?
c. Usingyouranswerto(b),provethatpropositional resolution alwaysterminatesintime
polynomialinngivena2-CNFsentencecontaining nomorethanndistinctsymbols.
d. Explainwhyyourargument in(c)doesnotapplyto3-CNF.
7.18 Considerthefollowingsentence:
[(Food ⇒ Party)∨(Drinks ⇒ Party)] ⇒ [(Food ∧Drinks) ⇒ Party].
a. Determine,usingenumeration,whetherthissentenceisvalid,satisfiable(butnotvalid),
orunsatisfiable.
b. Convert the left-hand and right-hand sides of the main implication into CNF, showing
eachstep,andexplainhowtheresultsconfirmyouranswerto(a).
c. Proveyouranswerto(a)usingresolution.
DISJUNCTIVE 7.19 Asentenceisindisjunctivenormalform(DNF)ifitisthedisjunctionofconjunctions
NORMALFORM
ofliterals. Forexample, thesentence (A∧B∧¬C)∨(¬A∧C)∨(B ∧¬C)isinDNF.
a. Anypropositional logic sentence islogically equivalent totheassertion that somepos-
sible world in which it would be true is in fact the case. From this observation, prove
thatanysentence canbewritteninDNF.
b. Construct an algorithm that converts any sentence in propositional logic into DNF.
(Hint: The algorithm is similar to the algorithm for conversion to CNF given in Sec-
tion7.5.2.)
c. Construct asimplealgorithm thattakes asinput asentence inDNFandreturns asatis-
fyingassignment ifoneexists,orreportsthatnosatisfying assignment exists.
d. Applythealgorithms in(b)and(c)tothefollowingsetofsentences:
A ⇒ B
B ⇒ C
C ⇒ ¬A.
e. Since the algorithm in (b) is very similar to the algorithm for conversion to CNF, and
since the algorithm in (c) is much simpler than any algorithm for solving a set of sen-
tencesinCNF,whyisthistechnique notusedinautomated reasoning?
7.20 Convertthefollowingsetofsentences toclausalform.
S1: A ⇔ (B ∨E).
S2: E ⇒ D.
S3: C ∧F ⇒ ¬B.
S4: E ⇒ B.
S5: B ⇒ F.
S6: B ⇒ C
GiveatraceoftheexecutionofDPLLontheconjunction oftheseclauses.
284 Chapter 7. LogicalAgents
7.21 Is arandomly generated 4-CNF sentence with n symbols and m clauses more orless
likely to be solvable than a randomly generated 3-CNF sentence with n symbols and m
clauses? Explain.
7.22 Minesweeper,thewell-knowncomputergame,iscloselyrelatedtothewumpusworld.
A minesweeper world is a rectangular grid of N squares with M invisible mines scattered
among them. Any square may be probed by the agent; instant death follows if a mine is
probed. Minesweeper indicates the presence of mines by revealing, in each probed square,
the number of mines that are directly or diagonally adjacent. The goal is to probe every
unminedsquare.
a. LetX be true iffsquare [i,j] contains amine. Write down the assertion that exactly
i,j
two mines are adjacent to [1,1] as a sentence involving some logical combination of
X propositions.
i,j
b. Generalize your assertion from (a) by explaining how to construct a CNF sentence
assertingthat kofnneighbors contain mines.
c. Explain precisely how an agent can use DPLL to prove that a given square does (or
doesnot)containamine,ignoring theglobal constraint thatthereareexactly M mines
inall.
d. Supposethattheglobalconstraint isconstructed fromyourmethodfrompart(b). How
does the number of clauses depend on M and N? Suggest a way to modify DPLL so
thattheglobalconstraint doesnotneedtoberepresented explicitly.
e. Are any conclusions derived by the method in part (c) invalidated when the global
constraint istakenintoaccount?
f. Give examples of configurations of probe values that induce long-range dependencies
such that the contents of a given unprobed square would give information about the
contentsofafar-distant square. (Hint: consideranN×1board.)
7.23 How long does it take to prove KB |= α using DPLL when α is a literal already
contained inKB? Explain.
7.24 Trace the behavior of DPLL on the knowledge base in Figure 7.16 when trying to
proveQ,andcomparethisbehaviorwiththatoftheforward-chaining algorithm.
7.25 Write a successor-state axiom for the Locked predicate, which applies to doors, as-
sumingtheonlyactionsavailable areLock andUnlock.
7.26 Section 7.7.1 provides some of the successor-state axioms required for the wumpus
world. Writedownaxiomsforallremainingfluentsymbols.
7.27 Modify the HYBRID-WUMPUS-AGENT to use the 1-CNF logical state estimation
method described on page 271. We noted on that page that such an agent will not be able
toacquire, maintain, andusemorecomplexbeliefs suchasthedisjunction P ∨P . Sug-
3,1 2,2
gest a method for overcoming this problem by defining additional proposition symbols, and
tryitoutinthewumpusworld. Doesitimprovetheperformance oftheagent?
8
FIRST-ORDER LOGIC
Inwhichwenoticethattheworldisblessedwithmanyobjects,someofwhichare
relatedtootherobjects, andinwhichweendeavor toreasonaboutthem.
InChapter7,weshowedhowaknowledge-based agentcouldrepresenttheworldinwhichit
operates and deduce what actions totake. Weused propositional logic as ourrepresentation
language because it sufficed to illustrate the basic concepts of logic and knowledge-based
agents. Unfortunately, propositional logic is too puny a language to represent knowledge
of complex environments in a concise way. In this chapter, we examine first-order logic,1
FIRST-ORDERLOGIC
which is sufficiently expressive to represent a good deal of our commonsense knowledge.
It also either subsumes or forms the foundation of many other representation languages and
hasbeenstudied intensively formanydecades. WebegininSection8.1withadiscussion of
representationlanguagesingeneral;Section8.2coversthesyntaxandsemanticsoffirst-order
logic;Sections8.3and8.4illustrate theuseoffirst-order logicforsimplerepresentations.
8.1 REPRESENTATION REVISITED
In this section, we discuss the nature of representation languages. Ourdiscussion motivates
thedevelopmentoffirst-orderlogic,amuchmoreexpressive languagethanthepropositional
logicintroduced inChapter7. Welookatpropositional logicandatotherkindsoflanguages
to understand what works and what fails. Our discussion will be cursory, compressing cen-
turiesofthought, trial,anderrorintoafewparagraphs.
Programming languages (such as C++ or Java or Lisp) are by far the largest class of
formal languages in common use. Programs themselves represent, in a direct sense, only
computational processes. Data structures within programs can represent facts; for example,
aprogram could usea 4×4array torepresent the contents ofthe wumpusworld. Thus, the
programminglanguagestatementWorld[2,2]←Pit isafairlynaturalwaytoassertthatthere
is apit in square [2,2]. (Such representations might be considered ad hoc; database systems
were developed precisely to provide a more general, domain-independent way to store and
1 Alsocalledfirst-orderpredicatecalculus,sometimesabbreviatedasFOLorFOPC.
285
286 Chapter 8. First-OrderLogic
retrieve facts.) What programming languages lack is any general mechanism for deriving
factsfromotherfacts;eachupdatetoadatastructureisdonebyadomain-specificprocedure
whosedetails arederived bytheprogrammer fromhisorherownknowledge ofthedomain.
Thisproceduralapproachcanbecontrastedwiththedeclarativenatureofpropositionallogic,
inwhichknowledgeandinferenceareseparate,andinferenceisentirelydomainindependent.
A second drawback of data structures in programs (and of databases, for that matter)
is the lack of any easy way to say, for example, “There is a pit in [2,2] or [3,1]” or “If the
wumpusisin[1,1]thenheisnotin[2,2].” Programscanstoreasinglevalueforeachvariable,
andsomesystemsallowthevaluetobe“unknown,”buttheylacktheexpressivenessrequired
tohandle partialinformation.
Propositional logic is a declarative language because its semantics is based on a truth
relation between sentences and possible worlds. It also has sufficient expressive power to
deal withpartial information, using disjunction and negation. Propositional logic hasathird
property that is desirable in representation languages, namely, compositionality. In a com-
COMPOSITIONALITY
positional language, the meaning of asentence is a function of the meaning of its parts. For
example, the meaning of “S ∧ S ” is related to the meanings of “S ” and “S .” It
1,4 1,2 1,4 1,2
wouldbeverystrange if“S ”meantthatthere isastench insquare [1,4]and“S ”meant
1,4 1,2
thatthereisastenchinsquare[1,2],but“S ∧S ”meantthatFranceandPolanddrew1–1
1,4 1,2
in last week’s ice hockey qualifying match. Clearly, noncompositionality makes life much
moredifficultforthereasoning system.
As we saw in Chapter 7, however, propositional logic lacks the expressive power to
concisely describe anenvironment withmanyobjects. Forexample, wewereforced towrite
aseparate ruleaboutbreezes andpitsforeachsquare, suchas
B ⇔ (P ∨P ).
1,1 1,2 2,1
InEnglish,ontheotherhand,itseemseasyenoughtosay,onceandforall,“Squaresadjacent
topitsarebreezy.” ThesyntaxandsemanticsofEnglishsomehowmakeitpossibletodescribe
theenvironment concisely.
8.1.1 The languageofthought
Natural languages (such as English or Spanish) are very expressive indeed. We managed to
writealmostthiswholebookinnatural language, withonlyoccasional lapsesintootherlan-
guages (including logic, mathematics, and the language of diagrams). There is a long tradi-
tioninlinguisticsandthephilosophyoflanguagethatviewsnaturallanguageasadeclarative
knowledge representation language. If we could uncover the rules for natural language, we
could use it in representation and reasoning systems and gain the benefit of the billions of
pagesthathavebeenwritteninnaturallanguage.
Themodernviewofnaturallanguageisthatitservesaasamediumforcommunication
ratherthanpurerepresentation. Whenaspeakerpoints andsays, “Look!” thelistenercomes
to know that, say, Superman has finally appeared over the rooftops. Yet we would not want
to say that the sentence “Look!” represents that fact. Rather, the meaning of the sentence
depends both on the sentence itself and on the context in which the sentence was spoken.
Clearly, one could not store a sentence such as “Look!” in a knowledge base and expect to
Section8.1. Representation Revisited 287
recover its meaning without also storing a representation of the context—which raises the
question of how the context itself can be represented. Natural languages also suffer from
ambiguity,aproblemforarepresentation language. AsPinker(1995)putsit: “Whenpeople
AMBIGUITY
thinkaboutspring,surelytheyarenotconfusedastowhethertheyarethinkingaboutaseason
or something that goes boing—and if one word can correspond to two thoughts, thoughts
can’tbewords.”
The famous Sapir–Whorf hypothesis claims that our understanding of the world is
strongly influenced bythelanguage wespeak. Whorf(1956)wrote“Wecutnature up,orga-
nizeitintoconcepts, andascribe significances aswedo, largely because weareparties toan
agreement to organize it this way—an agreement that holds throughout our speech commu-
nity and is codified in the patterns of our language.” It is certainly true that different speech
communities divide uptheworlddifferently. TheFrenchhavetwowords“chaise” and“fau-
teuil,” for a concept that English speakers cover with one: “chair.” But English speakers
caneasilyrecognizethecategoryfauteuilandgiveitaname—roughly“open-armchair”—so
does language really make a difference? Whorf relied mainly on intuition and speculation,
but in the intervening years we actually have real data from anthropological, psychological
andneurological studies.
Forexample,canyourememberwhichofthefollowingtwophrasesformedtheopening
ofSection8.1?
“Inthissection, wediscussthenatureofrepresentation languages ...”
“Thissectioncoversthetopicofknowledge representation languages ...”
Wanner (1974) did a similar experiment and found that subjects made the right choice at
chance level—about 50% of the time—but remembered the content of what they read with
betterthan90% accuracy. Thissuggests thatpeople process thewordstoform somekindof
nonverbal representation.
More interesting is the case in which a concept is completely absent in a language.
Speakers of the Australian aboriginal language Guugu Yimithirr have no words for relative
directions, such as front, back, right, or left. Instead they use absolute directions, saying,
for example, the equivalent of “I have a pain in my north arm.” This difference in language
makes a difference in behavior: Guugu Yimithirr speakers are better at navigating in open
terrain, whileEnglishspeakers arebetteratplacingtheforktotherightoftheplate.
Language also seems to influence thought through seemingly arbitrary grammatical
features such as the gender of nouns. For example, “bridge” is masculine in Spanish and
feminine in German. Boroditsky (2003) asked subjects to choose English adjectives to de-
scribe a photograph of a particular bridge. Spanish speakers chose big, dangerous, strong,
andtowering,whereasGermanspeakerschosebeautiful,elegant,fragile,andslender. Words
can serve as anchor points that affect how weperceive the world. Loftus and Palmer(1974)
showed experimental subjects a movie of an auto accident. Subjects who were asked “How
fast were the cars going when they contacted each other?” reported an average of 32 mph,
whilesubjects whowereaskedthequestion withtheword“smashed” insteadof“contacted”
reported 41mphforthesamecarsinthesamemovie.
288 Chapter 8. First-OrderLogic
Inafirst-orderlogicreasoningsystemthatusesCNF,wecanseethatthelinguisticform
“¬(A∨B)” and “¬A∧¬B” are the same because we can look inside the system and see
that the two sentences are stored as the same canonical CNFform. Can wedo that with the
human brain? Until recently the answer was “no,” but now it is “maybe.” Mitchell et al.
(2008) put subjects in an fMRI (functional magnetic resonance imaging) machine, showed
themwordssuchas“celery,”andimagedtheirbrains. Theresearcherswerethenabletotrain
acomputerprogramtopredict,fromabrainimage,whatwordthesubjecthadbeenpresented
with. Given two choices (e.g., “celery” or “airplane”), the system predicts correctly 77% of
the time. The system can even predict at above-chance levels for words it has never seen
an fMRI image of before (by considering the images of related words) and for people it has
never seen before (proving that fMRI reveals some level of common representation across
people). This type of work is still in its infancy, but fMRI (and other imaging technology
such as intracranial electrophysiology (Sahin et al., 2009)) promises to give us much more
concrete ideasofwhathumanknowledge representations are like.
From the viewpoint of formal logic, representing the same knowledge in two different
ways makes absolutely no difference; the same facts will be derivable from either represen-
tation. Inpractice, however, onerepresentation mightrequire fewerstepstoderiveaconclu-
sion, meaning that a reasoner with limited resources could get to the conclusion using one
representation but not the other. For nondeductive tasks such as learning from experience,
outcomes are necessarily dependent on the form of the representations used. We show in
Chapter 18 that when a learning program considers two possible theories of the world, both
ofwhichareconsistentwithallthedata,themostcommonwayofbreakingthetieistochoose
themostsuccincttheory—andthatdependsonthelanguageusedtorepresenttheories. Thus,
theinfluenceoflanguage onthoughtisunavoidable foranyagentthatdoeslearning.
8.1.2 Combining the bestofformaland natural languages
Wecan adopt the foundation of propositional logic—a declarative, compositional semantics
that is context-independent and unambiguous—and build a more expressive logic on that
foundation, borrowing representational ideas fromnatural language whileavoiding itsdraw-
backs. Whenwelookatthesyntaxofnaturallanguage,themostobviouselementsarenouns
and noun phrases that referto objects (squares, pits, wumpuses) and verbs and verbphrases
OBJECT
that refer to relations among objects (is breezy, is adjacent to, shoots). Some of these rela-
RELATION
tions are functions—relations in which there is only one “value” for a given “input.” It is
FUNCTION
easytostartlistingexamplesofobjects, relations, andfunctions:
• Objects: people,houses,numbers,theories,RonaldMcDonald,colors,baseballgames,
wars,centuries ...
• Relations: thesecanbeunaryrelations or propertiessuchasred,round, bogus, prime,
PROPERTY
multistoried ...,ormoregeneral n-aryrelations suchasbrotherof,biggerthan,inside,
partof,hascolor,occurred after,owns,comesbetween, ...
• Functions: fatherof,bestfriend,thirdinningof,onemore than,beginning of...
Indeed, almost any assertion can be thought of as referring toobjects and properties orrela-
tions. Someexamplesfollow:
Section8.1. Representation Revisited 289
• “Oneplustwoequalsthree.”
Objects: one, two, three, one plus two; Relation: equals; Function: plus. (“One plus
two” is a name for the object that is obtained by applying the function “plus” to the
objects“one”and“two.” “Three”isanothernameforthisobject.)
• “Squaresneighboring thewumpusaresmelly.”
Objects: wumpus,squares; Property: smelly;Relation: neighboring.
• “EvilKingJohnruledEnglandin1200.”
Objects: John,England,1200;Relation: ruled;Properties: evil,king.
Thelanguageoffirst-orderlogic,whosesyntaxandsemanticswedefineinthenextsection,
isbuiltaroundobjectsandrelations. Ithasbeensoimportanttomathematics,philosophy,and
artificial intelligence precisely because those fields—and indeed, much of everyday human
existence—can beusefully thought ofasdealing withobjects andtherelations amongthem.
First-order logic can also express facts about some orall of theobjects inthe universe. This
enables one to represent general laws or rules, such as the statement “Squares neighboring
thewumpusaresmelly.”
Theprimarydifference betweenpropositional andfirst-orderlogicliesinthe ontologi-
ONTOLOGICAL calcommitmentmadebyeachlanguage—thatis,whatitassumesaboutthenatureofreality.
COMMITMENT
Mathematically, this commitmentisexpressed through thenature oftheformal modelswith
respect to which the truth of sentences is defined. Forexample, propositional logic assumes
that there are facts that either hold or do not hold in the world. Each fact can be in one
of two states: true or false, and each model assigns true or false to each proposition sym-
bol (see Section 7.4.2).2 First-order logic assumes more; namely, that the world consists of
objects with certain relations among them that do or do not hold. The formal models are
correspondingly morecomplicatedthanthoseforpropositional logic. Special-purpose logics
make still further ontological commitments; forexample, temporal logic assumes that facts
TEMPORALLOGIC
hold at particular times and that those times (which may be points or intervals) are ordered.
Thus, special-purpose logics givecertain kinds ofobjects (andtheaxioms about them)“first
class” status within the logic, rather than simply defining them within the knowledge base.
HIGHER-ORDER Higher-order logic views the relations and functions referred to by first-order logic as ob-
LOGIC
jectsinthemselves. Thisallowsonetomakeassertionsaboutallrelations—forexample,one
couldwishtodefinewhatitmeansforarelationtobetransitive. Unlikemostspecial-purpose
logics, higher-order logic is strictly more expressive than first-order logic, in the sense that
somesentences ofhigher-order logiccannot beexpressed by anyfinitenumberoffirst-order
logicsentences.
EPISTEMOLOGICAL A logic can also be characterized by its epistemological commitments—the possible
COMMITMENT
states of knowledge that it allows with respect to each fact. In both propositional and first-
orderlogic, asentence represents afactand theagent eitherbelieves thesentence tobetrue,
believes it to be false, or has no opinion. These logics therefore have three possible states
ofknowledge regarding anysentence. Systemsusing probability theory,ontheotherhand,
2 Incontrast,factsinfuzzylogichaveadegreeoftruthbetween0and1.Forexample,thesentence“Viennais
alargecity”mightbetrueinourworldonlytodegree0.6infuzzylogic.
290 Chapter 8. First-OrderLogic
can have any degree of belief, ranging from 0 (total disbelief) to 1 (total belief).3 For ex-
ample, a probabilistic wumpus-world agent might believe that the wumpus is in [1,3] with
probability 0.75. The ontological and epistemological commitments of five different logics
aresummarizedinFigure8.1.
Language OntologicalCommitment EpistemologicalCommitment
(Whatexistsintheworld) (Whatanagentbelievesaboutfacts)
Propositionallogic facts true/false/unknown
First-orderlogic facts,objects,relations true/false/unknown
Temporallogic facts,objects,relations,times true/false/unknown
Probabilitytheory facts degreeofbelief∈[0,1]
Fuzzylogic factswithdegreeoftruth∈[0,1] knownintervalvalue
Figure8.1 Formallanguagesandtheirontologicalandepistemologicalcommitments.
Inthenextsection,wewilllaunchintothedetailsoffirst-orderlogic. Justasastudentof
physicsrequiressomefamiliaritywithmathematics,astudentofAImustdevelopatalentfor
workingwithlogicalnotation. Ontheotherhand,itisalsoimportantnottogettooconcerned
with the specifics of logical notation—after all, there are dozens of different versions. The
mainthingstokeepholdofarehowthelanguage facilitates concise representations andhow
itssemanticsleadstosoundreasoning procedures.
8.2 SYNTAX AND SEMANTICS OF FIRST-ORDER LOGIC
We begin this section by specifying more precisely the way in which the possible worlds
of first-order logic reflect the ontological commitment to objects and relations. Then we
introduce thevariouselementsofthelanguage, explaining theirsemantics aswegoalong.
8.2.1 Models forfirst-order logic
Recall from Chapter 7 that the models of a logical language are the formal structures that
constitute the possible worlds under consideration. Each model links the vocabulary of the
logical sentences to elements of the possible world, so that the truth of any sentence can
be determined. Thus, models for propositional logic link proposition symbols to predefined
truth values. Models forfirst-order logic are much moreinteresting. First, they have objects
inthem! Thedomainofamodelisthesetofobjectsordomainelementsitcontains. Thedo-
DOMAIN
mainisrequiredtobenonempty—everypossibleworldmustcontainatleastoneobject. (See
DOMAINELEMENTS
Exercise 8.7 for a discussion of empty worlds.) Mathematically speaking, it doesn’t matter
whattheseobjects are—all thatmatters is howmanythereareineachparticularmodel—but
for pedagogical purposes we’ll use a concrete example. Figure 8.2 shows a model with five
3 Itisimportantnottoconfusethedegreeofbeliefinprobabilitytheorywiththedegreeoftruthinfuzzylogic.
Indeed,somefuzzysystemsallowuncertainty(degreeofbelief)aboutdegreesoftruth.
Section8.2. SyntaxandSemanticsofFirst-OrderLogic 291
objects: RichardtheLionheart,KingofEnglandfrom1189to1199;hisyoungerbrother, the
evilKingJohn,whoruledfrom1199to1215;theleftlegsofRichardandJohn;andacrown.
The objects in the model may be related in various ways. In the figure, Richard and
John are brothers. Formally speaking, a relation is just the set of tuples of objects that are
TUPLE
related. (Atuple isacollection ofobjects arranged inafixedorderandiswrittenwithangle
brackets surrounding theobjects.) Thus,thebrotherhood relationinthismodelistheset
{(cid:16)Richard theLionheart, KingJohn(cid:17), (cid:16)KingJohn, RichardtheLionheart(cid:17)}. (8.1)
(HerewehavenamedtheobjectsinEnglish,butyoumay,ifyouwish,mentallysubstitutethe
picturesforthenames.) ThecrownisonKingJohn’shead,sothe“onhead”relationcontains
just one tuple, (cid:16)thecrown, KingJohn(cid:17). The “brother” and “on head” relations are binary
relations—that is, they relate pairs of objects. The model also contains unary relations, or
properties: the“person”propertyistrueofbothRichardandJohn;the“king”propertyistrue
onlyofJohn(presumably becauseRichardisdeadatthispoint);andthe“crown”propertyis
trueonlyofthecrown.
Certain kinds of relationships are best considered as functions, in that a given object
must berelated to exactly one object in this way. Forexample, each person has one left leg,
sothemodelhasaunary“leftleg”function thatincludes thefollowingmappings:
(cid:16)RichardtheLionheart(cid:17) → Richard’sleftleg
(8.2)
(cid:16)KingJohn(cid:17)→ John’sleftleg.
Strictly speaking, models in first-order logic require total functions, that is, there must be a
TOTALFUNCTIONS
valueforeveryinputtuple. Thus, thecrownmusthavealeftlegandsomusteachoftheleft
legs. Thereisatechnicalsolutiontothisawkwardprobleminvolvinganadditional“invisible”
crown
on head
brother
person person
king
brother
R J
$
left leg left leg
Figure 8.2 A model containing five objects, two binary relations, three unary relations
(indicatedbylabelsontheobjects),andoneunaryfunction,left-leg.
292 Chapter 8. First-OrderLogic
object that is the left leg of everything that has no left leg, including itself. Fortunately, as
long as one makes no assertions about the left legs of things that have no left legs, these
technicalities areofnoimport.
So far, we have described the elements that populate models for first-order logic. The
other essential part of a model is the link between those elements and the vocabulary of the
logicalsentences, whichweexplainnext.
8.2.2 Symbols andinterpretations
We turn now to the syntax of first-order logic. The impatient reader can obtain a complete
description fromtheformalgrammarinFigure8.3.
Thebasic syntactic elements of first-order logic are the symbols that stand forobjects,
relations, and functions. The symbols, therefore, come in three kinds: constant symbols,
CONSTANTSYMBOL
which stand for objects; predicate symbols, which stand for relations; and function sym-
PREDICATESYMBOL
bols, whichstand forfunctions. Weadopt theconvention thatthese symbols willbeginwith
FUNCTIONSYMBOL
uppercase letters. For example, we might use the constant symbols Richard and John; the
predicate symbols Brother, OnHead, Person, King, and Crown; and the function symbol
LeftLeg. As with proposition symbols, the choice of names is entirely up to the user. Each
predicate andfunction symbolcomeswithanaritythatfixesthenumberofarguments.
ARITY
Asin propositional logic, every model must provide the information required to deter-
mine if any given sentence is true or false. Thus, in addition to its objects, relations, and
functions, each model includes an interpretation that specifies exactly which objects, rela-
INTERPRETATION
tions and functions are referred to by the constant, predicate, and function symbols. One
possibleinterpretation forourexample—whichalogicianwouldcalltheintendedinterpre-
INTENDED tation—isasfollows:
INTERPRETATION
• Richard referstoRichardtheLionheartand John referstotheevilKingJohn.
• Brother refers to the brotherhood relation, that is, the set of tuples of objects given in
Equation (8.1); OnHead refers to the“on head” relation that holds between the crown
andKingJohn;Person,King,andCrown refertothesetsofobjects thatarepersons,
kings,andcrowns.
• LeftLeg referstothe“leftleg”function, thatis,themappinggiven inEquation(8.2).
There are many other possible interpretations, of course. For example, one interpretation
maps Richard to the crown and John to King John’s left leg. There are five objects in
the model, so there are 25 possible interpretations just for the constant symbols Richard
and John. Notice that not all the objects need have a name—for example, the intended
interpretation does not name the crown or the legs. It is also possible for an object to have
several names; there is an interpretation under which both Richard and John refer to the
crown.4 If you find this possibility confusing, remember that, in propositional logic, it is
perfectly possible tohaveamodelinwhich Cloudy andSunny arebothtrue; itisthejobof
theknowledgebasetoruleoutmodelsthatareinconsistent withourknowledge.
4 Later,inSection8.2.8,weexamineasemanticsinwhicheveryobjecthasexactlyonename.
Section8.2. SyntaxandSemanticsofFirst-OrderLogic 293
Sentence → AtomicSentence | ComplexSentence
AtomicSentence → Predicate | Predicate(Term,...)| Term =Term
ComplexSentence → (Sentence )| [Sentence ]
| ¬Sentence
| Sentence ∧Sentence
| Sentence ∨Sentence
| Sentence ⇒ Sentence
| Sentence ⇔ Sentence
| Quantifier Variable,... Sentence
Term → Function(Term,...)
| Constant
| Variable
Quantifier → ∀| ∃
Constant → A| X | John | ···
1
Variable → a| x| s| ···
Predicate → True | False | After | Loves | Raining | ···
Function → Mother | LeftLeg | ···
OPERATORPRECEDENCE : ¬,=,∧,∨,⇒,⇔
Figure8.3 The syntax of first-orderlogic with equality, specified in Backus–Naurform
(seepage1060ifyouarenotfamiliarwiththisnotation).Operatorprecedencesarespecified,
from highest to lowest. The precedence of quantifiers is such that a quantifier holds over
everythingtotherightofit.
R J R J R J R J R J R J
. . . . . . . . .
Figure8.4 Somemembersofthesetofallmodelsforalanguagewithtwoconstantsym-
bols,RandJ,andonebinaryrelationsymbol.Theinterpretationofeachconstantsymbolis
shownbyagrayarrow.Withineachmodel,therelatedobjectsareconnectedbyarrows.
294 Chapter 8. First-OrderLogic
Insummary,amodelinfirst-orderlogicconsistsofasetofobjectsandaninterpretation
that maps constant symbols to objects, predicate symbols to relations on those objects, and
function symbols to functions on those objects. Just as with propositional logic, entailment,
validity, and so on are defined in terms of all possible models. To get an idea of what the
set ofallpossible models looks like, see Figure 8.4. Itshows that models vary inhow many
objects they contain—from one up to infinity—and in the way the constant symbols map
to objects. If there are two constant symbols and one object, then both symbols must refer
to the same object; but this can still happen even with more objects. When there are more
objects thanconstant symbols, someoftheobjects willhavenonames. Becausethenumber
of possible models is unbounded, checking entailment by the enumeration of all possible
modelsisnotfeasibleforfirst-orderlogic(unlikepropositional logic). Evenifthenumberof
objects is restricted, the number of combinations can be very large. (See Exercise 8.5.) For
theexampleinFigure8.4,thereare137,506,194,466 models withsixorfewerobjects.
8.2.3 Terms
Atermisalogicalexpression thatreferstoanobject. Constantsymbolsarethereforeterms,
TERM
but itisnotalways convenient tohave adistinct symbol tonameevery object. Forexample,
in English we might use the expression “King John’s left leg” rather than giving a name
to his leg. This is what function symbols are for: instead of using a constant symbol, we
use LeftLeg(John). In the general case, a complex term is formed by a function symbol
followedbyaparenthesized listoftermsasargumentstothe functionsymbol. Itisimportant
to remember that acomplex term is just a complicated kind of name. It is not a “subroutine
call” that “returns a value.” There is no LeftLeg subroutine that takes a person as input and
returnsaleg. Wecanreasonaboutleftlegs(e.g.,statingthegeneralrulethateveryonehasone
and then deducing that John must have one) without ever providing a definition of LeftLeg.
Thisissomethingthatcannotbedonewithsubroutines inprogramming languages.5
The formal semantics of terms is straightforward. Consider a term f(t ,...,t ). The
1 n
function symbol f refers to somefunction inthe model(call it F); theargument termsrefer
to objects in the domain (call them d ,...,d ); and the term as a whole refers to the object
1 n
that is the value of the function F applied to d ,...,d . Forexample, suppose the LeftLeg
1 n
functionsymbolreferstothefunctionshowninEquation(8.2)andJohn referstoKingJohn,
then LeftLeg(John) refers to King John’s left leg. In this way, the interpretation fixes the
referentofeveryterm.
8.2.4 Atomicsentences
Now that we have both terms for referring to objects and predicate symbols for referring to
relations, we can put them together to make atomic sentences that state facts. An atomic
5 λ-expressions provide a useful notation in which new function symbols are constructed “on the fly.” For
example, thefunctionthatsquaresitsargumentcanbewrittenas(λx x×x)andcanbeappliedtoarguments
justlikeanyotherfunctionsymbol. Aλ-expressioncanalsobedefinedandusedasapredicatesymbol. (See
Chapter22.)ThelambdaoperatorinLispplaysexactlythesamerole.Noticethattheuseofλinthiswaydoes
notincreasetheformalexpressivepoweroffirst-orderlogic,becauseanysentencethatincludesaλ-expression
canberewrittenby“pluggingin”itsargumentstoyieldanequivalentsentence.
Section8.2. SyntaxandSemanticsofFirst-OrderLogic 295
sentence (or atom for short) is formed from a predicate symbol optionally followed by a
ATOMICSENTENCE
parenthesized listofterms,suchas
ATOM
Brother(Richard,John).
This states, under the intended interpretation given earlier, that Richard the Lionheart is the
brotherofKingJohn.6 Atomicsentences canhavecomplextermsasarguments. Thus,
Married(Father(Richard),Mother(John))
states that Richard the Lionheart’s father is married to King John’s mother (again, under a
suitable interpretation).
Anatomic sentence is true in agiven model if the relation referred to by the predicate
symbolholdsamongtheobjectsreferredtobythearguments.
8.2.5 Complex sentences
We can use logical connectives to construct more complex sentences, with the same syntax
andsemanticsasinpropositional calculus. Herearefoursentences thataretrueinthemodel
ofFigure8.2underourintendedinterpretation:
¬Brother(LeftLeg(Richard),John)
Brother(Richard,John)∧Brother(John,Richard)
King(Richard)∨King(John)
¬King(Richard) ⇒ King(John).
8.2.6 Quantifiers
Once we have a logic that allows objects, it is only natural to want to express properties of
entire collections of objects, instead of enumerating the objects by name. Quantifierslet us
QUANTIFIER
dothis. First-orderlogiccontains twostandard quantifiers, calleduniversal andexistential.
Universalquantification(∀)
Recall the difficulty we had in Chapter 7 with the expression of general rules in proposi-
tional logic. Rules such as “Squares neighboring the wumpus are smelly” and “All kings
are persons” are the bread and butter of first-order logic. We deal with the first of these in
Section8.3. Thesecondrule,“Allkingsarepersons,”iswritteninfirst-orderlogicas
∀x King(x) ⇒ Person(x).
∀ is usually pronounced “Forall ...”. (Remember that the upside-down A stands for “all.”)
Thus,thesentence says, “Forallx,ifxisaking, thenxisaperson.” Thesymbolxiscalled
a variable. By convention, variables are lowercase letters. A variable is a term all by itself,
VARIABLE
and as such can also serve as the argument of a function—for example, LeftLeg(x). Aterm
withnovariables iscalledagroundterm.
GROUNDTERM
Intuitively, the sentence ∀x P, where P is any logical expression, says that P is true
forevery object x. More precisely, ∀x P istrue in agiven model if P istrue inall possible
EXTENDED extendedinterpretationsconstructedfromtheinterpretationgiveninthemodel,whereeach
INTERPRETATION
6 Weusuallyfollowtheargument-orderingconventionthatP(x,y)isreadas“xisaP ofy.”
296 Chapter 8. First-OrderLogic
extended interpretation specifiesadomainelementtowhich xrefers.
Thissoundscomplicated,butitisreallyjustacarefulwayofstatingtheintuitivemean-
ing of universal quantification. Consider the model shown in Figure 8.2 and the intended
interpretation thatgoeswithit. Wecanextendtheinterpretation infiveways:
x→ RichardtheLionheart,
x→ KingJohn,
x→ Richard’sleftleg,
x→ John’sleftleg,
x→ thecrown.
Theuniversallyquantifiedsentence ∀x King(x) ⇒ Person(x)istrueintheoriginalmodel
if the sentence King(x) ⇒ Person(x) is true under each of the five extended interpreta-
tions. Thatis,theuniversally quantifiedsentence isequivalent toasserting thefollowingfive
sentences:
RichardtheLionheartisaking ⇒ RichardtheLionheartisaperson.
KingJohnisaking ⇒ KingJohnisaperson.
Richard’sleftlegisaking ⇒ Richard’sleftlegisaperson.
John’sleftlegisaking ⇒ John’sleftlegisaperson.
Thecrownisaking ⇒ thecrownisaperson.
Let us look carefully at this set of assertions. Since, in our model, King John is the only
king, the second sentence asserts that he is a person, as we would hope. But what about
the other four sentences, which appear to make claims about legs and crowns? Is that part
of the meaning of “All kings are persons”? In fact, the other four assertions are true in the
model, but make no claim whatsoever about the personhood qualifications of legs, crowns,
orindeed Richard. Thisisbecause noneofthese objects isaking. Lookingatthetruthtable
for ⇒ (Figure 7.8 on page 246), we see that the implication is true whenever its premise is
false—regardless ofthetruthoftheconclusion. Thus,byasserting theuniversally quantified
sentence, which is equivalent to asserting a whole list of individual implications, we end
up asserting the conclusion of the rule just for those objects for whom the premise is true
and saying nothing at all about those individuals for whom the premise is false. Thus, the
truth-table definition of ⇒ turns out to be perfect for writing general rules with universal
quantifiers.
Acommonmistake,madefrequently evenbydiligent readers whohavereadthispara-
graphseveraltimes,istouseconjunction instead ofimplication. Thesentence
∀x King(x)∧Person(x)
wouldbeequivalent toasserting
RichardtheLionheartisaking∧RichardtheLionheartisaperson,
KingJohnisaking∧KingJohnisaperson,
Richard’sleftlegisaking∧Richard’sleftlegisaperson,
andsoon. Obviously, thisdoesnotcapturewhatwewant.
Section8.2. SyntaxandSemanticsofFirst-OrderLogic 297
Existentialquantification(∃)
Universalquantificationmakesstatementsabouteveryobject. Similarly,wecanmakeastate-
ment about some object in the universe without naming it, by using anexistential quantifier.
Tosay,forexample,thatKingJohnhasacrownonhishead,wewrite
∃x Crown(x)∧OnHead(x,John).
∃xispronounced “Thereexistsan xsuchthat...”or“Forsome x...”.
Intuitively, the sentence ∃x P says that P is true for at least one object x. More
precisely, ∃x P is true in a given model if P is true in at least one extended interpretation
thatassignsxtoadomainelement. Thatis,atleastoneofthefollowingistrue:
RichardtheLionheartisacrown∧RichardtheLionheartisonJohn’shead;
KingJohnisacrown∧KingJohnisonJohn’shead;
Richard’sleftlegisacrown∧Richard’sleftlegisonJohn’shead;
John’sleftlegisacrown∧John’sleftlegisonJohn’shead;
Thecrownisacrown∧thecrownisonJohn’shead.
The fifth assertion is true in the model, so the original existentially quantified sentence is
true in the model. Notice that, by ourdefinition, the sentence would also be true in a model
in which King John was wearing two crowns. This is entirely consistent with the original
sentence “KingJohnhasacrownonhishead.” 7
Justas⇒appearstobethenaturalconnectivetousewith∀,∧isthenaturalconnective
to use with ∃. Using ∧ as the main connective with ∀ led to an overly strong statement in
theexample inthe previous section; using ⇒with∃usually leads toavery weakstatement,
indeed. Considerthefollowingsentence:
∃x Crown(x) ⇒ OnHead(x,John).
On the surface, this might look like a reasonable rendition of our sentence. Applying the
semantics, weseethatthesentence saysthatatleastoneofthefollowingassertions istrue:
RichardtheLionheartisacrown ⇒ RichardtheLionheart isonJohn’shead;
KingJohnisacrown ⇒ KingJohnisonJohn’s head;
Richard’sleftlegisacrown ⇒ Richard’sleftlegisonJohn’shead;
andsoon. Nowanimplicationistrueifbothpremiseandconclusionaretrue,orifitspremise
is false. So if Richard the Lionheart is not a crown, then the first assertion is true and the
existential is satisfied. So, an existentially quantified implication sentence is true whenever
anyobjectfailstosatisfythepremise;hencesuchsentences reallydonotsaymuchatall.
Nestedquantifiers
We will often want to express more complex sentences using multiple quantifiers. The sim-
plestcaseiswherethequantifiers areofthesametype. Forexample, “Brothersaresiblings”
canbewrittenas
∀x ∀y Brother(x,y) ⇒ Sibling(x,y).
7 Thereisavariantoftheexistentialquantifier,usuallywritten∃1or∃!,thatmeans“Thereexistsexactlyone.”
Thesamemeaningcanbeexpressedusingequalitystatements.
298 Chapter 8. First-OrderLogic
Consecutive quantifiers of the same type can be written as one quantifier with several vari-
ables. Forexample,tosaythatsiblinghood isasymmetricrelationship, wecanwrite
∀x,y Sibling(x,y) ⇔ Sibling(y,x).
In other cases we will have mixtures. “Everybody loves somebody” means that for every
person, thereissomeonethatpersonloves:
∀x ∃y Loves(x,y).
Ontheotherhand,tosay“Thereissomeonewhoislovedbyeveryone,”wewrite
∃y ∀x Loves(x,y).
Theorderofquantification isthereforeveryimportant. Itbecomesclearerifweinsertparen-
theses. ∀x(∃y Loves(x,y)) says that everyone has a particular property, namely, the prop-
erty that they love someone. On the other hand, ∃y (∀x Loves(x,y)) says that someone in
theworldhasaparticularproperty, namelythepropertyofbeinglovedbyeverybody.
Someconfusion can arise when two quantifiers are used with the same variable name.
Considerthesentence
∀x (Crown(x)∨(∃x Brother(Richard,x))).
Here the x in Brother(Richard,x) is existentially quantified. The rule is that the variable
belongs to the innermost quantifier that mentions it; then it will not be subject to any other
quantification. Another way to think of it is this: ∃x Brother(Richard,x) is a sentence
aboutRichard(thathehasabrother), notabout x;soputtinga∀xoutsideithasnoeffect. It
couldequallywellhavebeenwritten∃z Brother(Richard,z). Becausethiscanbeasource
ofconfusion, wewillalwaysusedifferentvariablenameswithnestedquantifiers.
Connectionsbetween∀and∃
Thetwoquantifiers areactually intimately connected witheachother, through negation. As-
serting that everyone dislikes parsnips is the same as asserting there does not exist someone
wholikesthem,andviceversa:
∀x ¬Likes(x,Parsnips) isequivalent to ¬∃x Likes(x,Parsnips).
Wecangoonestepfurther: “Everyonelikesicecream”meansthatthereisnoonewhodoes
notlikeicecream:
∀x Likes(x,IceCream) isequivalent to ¬∃x ¬Likes(x,IceCream).
Because∀isreallyaconjunctionovertheuniverseofobjectsand ∃isadisjunction, itshould
notbesurprising that theyobey DeMorgan’s rules. TheDeMorgan rules forquantified and
unquantified sentences areasfollows:
∀x ¬P ≡ ¬∃x P ¬(P ∨Q) ≡ ¬P ∧¬Q
¬∀x P ≡ ∃x ¬P ¬(P ∧Q) ≡ ¬P ∨¬Q
∀x P ≡ ¬∃x ¬P P ∧Q ≡ ¬(¬P ∨¬Q)
∃x P ≡ ¬∀x ¬P P ∨Q ≡ ¬(¬P ∧¬Q).
Thus, we do not really need both ∀and ∃, just as wedo not really need both ∧ and ∨. Still,
readability ismoreimportant thanparsimony, sowewillkeepbothofthequantifiers.
Section8.2. SyntaxandSemanticsofFirst-OrderLogic 299
8.2.7 Equality
First-order logic includes onemorewaytomakeatomic sentences, otherthan using apredi-
cateandtermsasdescribedearlier. Wecanusethe equalitysymboltosignifythattwoterms
EQUALITYSYMBOL
refertothesameobject. Forexample,
Father(John)=Henry
saysthattheobjectreferred toby Father(John)andtheobjectreferred toby Henry arethe
same. Because an interpretation fixes the referent of any term, determining the truth of an
equalitysentenceissimplyamatterofseeingthatthereferentsofthetwotermsarethesame
object.
Theequalitysymbolcanbeusedtostatefactsaboutagivenfunction,aswejustdidfor
theFather symbol. Itcanalsobeusedwithnegationtoinsistthattwotermsarenotthesame
object. TosaythatRichardhasatleasttwobrothers, wewouldwrite
∃x,y Brother(x,Richard)∧Brother(y,Richard)∧¬(x=y).
Thesentence
∃x,y Brother(x,Richard)∧Brother(y,Richard)
doesnothavetheintendedmeaning. Inparticular, itistrue inthemodelofFigure8.2,where
Richardhasonlyonebrother. Toseethis,considertheextended interpretation inwhichboth
x and y are assigned to King John. The addition of ¬(x=y) rules out such models. The
notation x (cid:7)= y issometimesusedasanabbreviation for¬(x=y).
8.2.8 Analternativesemantics?
Continuing the example from the previous section, suppose that webelieve that Richard has
twobrothers, JohnandGeoffrey.8 Canwecapturethisstateofaffairsbyasserting
Brother(John,Richard)∧Brother(Geoffrey,Richard)? (8.3)
Not quite. First, this assertion is true in a model where Richard has only one brother—
we need to add John (cid:7)= Geoffrey. Second, the sentence doesn’t rule out models in which
Richard has manymore brothers besides John and Geoffrey. Thus, thecorrect translation of
“Richard’s brothersareJohnandGeoffrey”isasfollows:
Brother(John,Richard)∧Brother(Geoffrey,Richard)∧John (cid:7)= Geoffrey
∧∀x Brother(x,Richard) ⇒ (x=John ∨x=Geoffrey).
For many purposes, this seems much more cumbersome than the corresponding natural-
language expression. As a consequence, humans may make mistakes in translating their
knowledge into first-order logic, resulting in unintuitive behaviors from logical reasoning
systems that use the knowledge. Can we devise a semantics that allows a more straightfor-
wardlogicalexpression?
Oneproposalthatisverypopularindatabasesystemsworksasfollows. First,weinsist
that every constant symbol refer to a distinct object—the so-called unique-names assump-
UNIQUE-NAMES tion. Second, we assume that atomic sentences not known to be true are in fact false—the
ASSUMPTION
CLOSED-WORLD closed-world assumption. Finally, we invoke domain closure, meaning that each model
ASSUMPTION
DOMAINCLOSURE 8 Actuallyhehadfour,theothersbeingWilliamandHenry.
300 Chapter 8. First-OrderLogic
R J R J R J R J R J
R R R R . . . R
J J J J J
Figure8.5 Somemembersofthesetofallmodelsforalanguagewithtwoconstantsym-
bols,RandJ,andonebinaryrelationsymbol,underdatabasesemantics. Theinterpretation
oftheconstantsymbolsisfixed,andthereisadistinctobjectforeachconstantsymbol.
contains no more domain elements than those named by the constant symbols. Under the
DATABASE resulting semantics, which we call database semantics to distinguish it from the standard
SEMANTICS
semantics of first-order logic, the sentence Equation (8.3) does indeed state that Richard’s
two brothers are John and Geoffrey. Database semantics is also used in logic programming
systems,asexplained inSection9.4.5.
It is instructive to consider the set of all possible models under database semantics for
the same case as shown in Figure 8.4. Figure 8.5 shows some of the models, ranging from
the model with no tuples satisfying the relation to the model with all tuples satisfying the
relation. With two objects, there are four possible two-element tuples, so there are 24=16
different subsets oftuples that cansatisfy therelation. Thus, there are16possible modelsin
all—alotfewerthantheinfinitelymanymodelsforthestandardfirst-ordersemantics. Onthe
otherhand,thedatabasesemantics requiresdefiniteknowledgeofwhattheworldcontains.
This example brings up an important point: there is no one “correct” semantics for
logic. The usefulness of any proposed semantics depends on how concise and intuitive it
makes the expression of the kinds of knowledge we want to write down, and on how easy
andnatural itistodevelop thecorresponding rules ofinference. Database semantics ismost
useful when we are certain about the identity of all the objects described in the knowledge
base and whenwehave allthe facts athand; inother cases, itisquite awkward. Forthe rest
ofthischapter,weassumethestandardsemanticswhilenotinginstancesinwhichthischoice
leadstocumbersomeexpressions.
8.3 USING FIRST-ORDER LOGIC
Nowthatwehavedefinedanexpressivelogicallanguage,itistimetolearnhowtouseit. The
bestwaytodothisisthroughexamples. Wehaveseensomesimplesentencesillustrating the
variousaspectsoflogicalsyntax;inthissection, weprovidemoresystematicrepresentations
of some simple domains. In knowledge representation, a domain is just some part of the
DOMAIN
worldaboutwhichwewishtoexpresssomeknowledge.
Webegin withabriefdescription ofthe TELL/ASK interface forfirst-orderknowledge
bases. Then we look at the domains of family relationships, numbers, sets, and lists, and at
Section8.3. UsingFirst-OrderLogic 301
thewumpusworld. Thenextsectioncontainsamoresubstantialexample(electroniccircuits)
andChapter12coverseverything intheuniverse.
8.3.1 Assertions andqueries infirst-order logic
SentencesareaddedtoaknowledgebaseusingTELL,exactlyasinpropositional logic. Such
sentences are called assertions. Forexample, wecan assert that John is aking, Richard is a
ASSERTION
person, andallkingsarepersons:
TELL(KB, King(John)).
TELL(KB, Person(Richard)).
TELL(KB, ∀x King(x) ⇒ Person(x)).
Wecanaskquestions oftheknowledgebaseusing ASK. Forexample,
ASK(KB, King(John))
QUERY
returnstrue. QuestionsaskedwithASKarecalledqueriesorgoals. Generallyspeaking,any
query thatislogically entailed bytheknowledge baseshould beanswered affirmatively. For
GOAL
example,giventhetwopreceding assertions, thequery
ASK(KB, Person(John))
shouldalsoreturn true. Wecanaskquantifiedqueries, suchas
ASK(KB, ∃x Person(x)).
The answer is true, but this is perhaps not as helpful as we would like. It is rather like
answering “Can you tell me the time?” with “Yes.” If we want to know what value of x
makesthesentence true,wewillneedadifferentfunction, ASKVARS,whichwecallwith
ASKVARS(KB,Person(x))
andwhichyieldsastream ofanswers. Inthiscasetherewillbetwoanswers: {x/John}and
SUBSTITUTION
{x/Richard}. Suchanansweriscalledasubstitutionorbindinglist. ASKVARS isusually
reserved forknowledge bases consisting solely of Horn clauses, because in such knowledge
BINDINGLIST
bases every way of making the query true will bind the variables to specific values. That is
notthecasewithfirst-orderlogic; if KB hasbeentoldKing(John)∨King(Richard),then
thereisnobinding toxforthequery ∃x King(x),eventhoughthequeryistrue.
8.3.2 The kinshipdomain
Thefirstexampleweconsideristhedomainoffamilyrelationships, orkinship. Thisdomain
includes facts such as “Elizabeth is the mother of Charles” and “Charles is the father of
William”andrulessuchas“One’sgrandmotheristhemotherofone’sparent.”
Clearly,theobjectsinourdomainarepeople. Wehavetwounarypredicates, Male and
Female. Kinship relations—parenthood, brotherhood, marriage, andsoon—arerepresented
by binary predicates: Parent, Sibling, Brother, Sister, Child, Daughter, Son, Spouse,
Wife, Husband, Grandparent, Grandchild, Cousin, Aunt, and Uncle. We use functions
for Mother and Father, because every person has exactly one of each of these (at least
according tonature’sdesign).
302 Chapter 8. First-OrderLogic
We can go through each function and predicate, writing down what we know in terms
oftheothersymbols. Forexample, one’smotherisone’sfemaleparent:
∀m,c Mother(c)=m ⇔ Female(m)∧Parent(m,c).
One’shusband isone’smalespouse:
∀w,h Husband(h,w) ⇔ Male(h)∧Spouse(h,w).
Maleandfemalearedisjoint categories:
∀x Male(x) ⇔ ¬Female(x).
Parentandchildareinverserelations:
∀p,c Parent(p,c) ⇔ Child(c,p).
Agrandparent isaparentofone’sparent:
∀g,c Grandparent(g,c) ⇔ ∃p Parent(g,p)∧Parent(p,c).
Asibling isanotherchildofone’sparents:
∀x,y Sibling(x,y) ⇔ x (cid:7)= y∧∃p Parent(p,x)∧Parent(p,y).
Wecouldgoonforseveralmorepageslikethis,andExercise8.14asksyoutodojustthat.
Eachofthesesentencescanbeviewedasanaxiomofthekinshipdomain,asexplained
in Section 7.1. Axioms are commonly associated with purely mathematical domains—we
willseesomeaxiomsfornumbersshortly—buttheyareneeded inalldomains. Theyprovide
the basic factual information from which useful conclusions can be derived. Our kinship
axioms are also definitions; they have theform ∀x,y P(x,y) ⇔ .... Theaxioms define
DEFINITION
theMother functionandtheHusband,Male,Parent,Grandparent,andSibling predicates
intermsofotherpredicates. Ourdefinitions “bottom out”at abasicsetofpredicates (Child,
Spouse, and Female) in terms of which the others are ultimately defined. This is a natural
way in which to build up the representation of a domain, and it is analogous to the way in
whichsoftwarepackages arebuiltupbysuccessive definitions ofsubroutines fromprimitive
library functions. Notice that there is not necessarily a unique set of primitive predicates;
wecould equally wellhave usedParent,Spouse,andMale. Insomedomains, asweshow,
thereisnoclearly identifiablebasicset.
Notalllogicalsentencesaboutadomainareaxioms. Somearetheorems—thatis,they
THEOREM
areentailedbytheaxioms. Forexample,considertheassertionthatsiblinghoodissymmetric:
∀x,y Sibling(x,y) ⇔ Sibling(y,x).
Is this an axiom ora theorem? In fact, it is a theorem that follows logically from the axiom
thatdefinessiblinghood. IfweASK theknowledge basethissentence, itshouldreturn true.
From a purely logical point of view, a knowledge base need contain only axioms and
no theorems, because the theorems do not increase the set of conclusions that follow from
the knowledge base. From a practical point of view, theorems are essential to reduce the
computational costofderiving newsentences. Without them,areasoning system hastostart
fromfirstprincipleseverytime,ratherlikeaphysicisthavingtorederivetherulesofcalculus
foreverynewproblem.
Section8.3. UsingFirst-OrderLogic 303
Not all axioms are definitions. Some provide more general information about certain
predicates without constituting a definition. Indeed, some predicates have no complete defi-
nition because we do not know enough to characterize them fully. For example, there is no
obviousdefinitivewaytocompletethesentence
∀x Person(x) ⇔ ...
Fortunately, first-order logic allows us to make use of the Person predicate without com-
pletelydefiningit. Instead, wecanwritepartialspecifications ofproperties thateveryperson
hasandproperties thatmakesomething aperson:
∀x Person(x) ⇒ ...
∀x ... ⇒ Person(x).
Axioms can also be “just plain facts,” such as Male(Jim) and Spouse(Jim,Laura).
Such facts form the descriptions of specific problem instances, enabling specific questions
to be answered. The answers to these questions will then be theorems that follow from
the axioms. Often, one finds that the expected answers are not forthcoming—for example,
fromSpouse(Jim,Laura)oneexpects(underthelawsofmanycountries)tobeabletoinfer
¬Spouse(George,Laura);butthisdoesnotfollowfromtheaxiomsgivenearlier—evenafter
weaddJim (cid:7)= George assuggested inSection8.2.8. Thisisasignthatanaxiomismissing.
Exercise8.8asksthereadertosupplyit.
8.3.3 Numbers, sets,and lists
Numbers are perhaps the most vivid example of how a large theory can be built up from
a tiny kernel of axioms. We describe here the theory of natural numbers or non-negative
NATURALNUMBERS
integers. We need a predicate NatNum that will be true of natural numbers; we need one
constant symbol, 0; and we need one function symbol, S (successor). The Peano axioms
PEANOAXIOMS
definenaturalnumbersandaddition.9 Naturalnumbersaredefinedrecursively:
NatNum(0).
∀n NatNum(n) ⇒ NatNum(S(n)).
That is, 0 is a natural number, and for every object n, if n is a natural number, then S(n) is
a natural number. So the natural numbers are 0, S(0), S(S(0)), and so on. (After reading
Section 8.2.8, you will notice that these axioms allow for other natural numbers besides the
usualones;seeExercise8.12.) Wealsoneedaxiomstoconstrain thesuccessorfunction:
∀n 0 (cid:7)= S(n).
∀m,n m (cid:7)= n ⇒ S(m)(cid:7)= S(n).
Nowwecandefineadditionintermsofthesuccessorfunction:
∀m NatNum(m) ⇒ +(0,m) = m.
∀m,n NatNum(m)∧NatNum(n) ⇒ +(S(m),n) = S(+(m,n)).
The first of these axioms says that adding 0 to any natural number m gives m itself. Notice
theuseofthebinaryfunctionsymbol“+”intheterm+(m,0);inordinary mathematics, the
termwouldbewritten m+0usinginfixnotation. (Thenotationwehaveusedforfirst-order
INFIX
9 ThePeano axiomsalso include theprinciple of induction, whichisa sentence of second-order logic rather
thanoffirst-orderlogic.TheimportanceofthisdistinctionisexplainedinChapter9.
304 Chapter 8. First-OrderLogic
logiciscalledprefix.) Tomakeoursentencesaboutnumberseasiertoread,weallowtheuse
PREFIX
ofinfixnotation. WecanalsowriteS(n)asn+1,sothesecond axiombecomes
∀m,n NatNum(m)∧NatNum(n) ⇒ (m+1)+n = (m+n)+1.
Thisaxiomreduces additiontorepeated application ofthesuccessorfunction.
The use of infix notation is an example of syntactic sugar, that is, an extension to or
SYNTACTICSUGAR
abbreviation of the standard syntax that does not change the semantics. Any sentence that
usessugarcanbe“desugared”toproduceanequivalentsentenceinordinaryfirst-orderlogic.
Once we have addition, it is straightforward to define multiplication as repeated addi-
tion, exponentiation asrepeated multiplication, integer division and remainders, primenum-
bers, and so on. Thus, the whole of numbertheory (including cryptography) can be built up
fromoneconstant, onefunction, onepredicate andfouraxioms.
The domain of sets is also fundamental to mathematics as well as to commonsense
SET
reasoning. (Infact, itispossible todefinenumbertheory in termsofsettheory.) Wewantto
beable torepresent individual sets, including the empty set. Weneed awaytobuild up sets
by adding an element to a set or taking the union or intersection of two sets. We will want
to know whether an element is a member of a set and we will want to distinguish sets from
objectsthatarenotsets.
Wewilluse the normal vocabulary of set theory as syntactic sugar. The empty set is a
constant written as {}. There is one unary predicate, Set, which is true of sets. The binary
predicates are x∈s (x is a memberof set s)and s ⊆ s (set s is a subset, not necessarily
1 2 1
proper, of set s ). The binary functions are s ∩s (the intersection of two sets), s ∪s
2 1 2 1 2
(the union of two sets), and {x|s} (the set resulting from adjoining element x to set s). One
possible setofaxiomsisasfollows:
1. Theonlysetsaretheemptysetandthosemadebyadjoining something toaset:
∀s Set(s) ⇔ (s={})∨(∃x,s Set(s )∧s={x|s }).
2 2 2
2. The empty set has no elements adjoined into it. In other words, there is no way to
decompose{}intoasmallersetandanelement:
¬∃x,s {x|s}={}.
3. Adjoininganelementalreadyinthesethasnoeffect:
∀x,s x∈s ⇔ s={x|s}.
4. The only members of a set are the elements that were adjoined into it. We express
this recursively, saying that x is a member of s if and only if s is equal to some set s
2
adjoinedwithsomeelementy,whereeither y isthesameasxorxisamemberofs :
2
∀x,s x∈s ⇔ ∃y,s (s={y|s }∧(x=y∨x∈s )).
2 2 2
5. Asetisasubset ofanothersetifandonly ifallofthefirstset’s membersaremembers
ofthesecondset:
∀s ,s s ⊆ s ⇔ (∀x x∈s ⇒ x∈s ).
1 2 1 2 1 2
6. Twosetsareequalifandonlyifeachisasubsetoftheother:
∀s ,s (s =s ) ⇔ (s ⊆ s ∧s ⊆ s ).
1 2 1 2 1 2 2 1
Section8.3. UsingFirst-OrderLogic 305
7. Anobjectisintheintersection oftwosetsifandonlyifitisamemberofbothsets:
∀x,s ,s x∈(s ∩s ) ⇔ (x∈s ∧x∈s ).
1 2 1 2 1 2
8. Anobjectisintheunionoftwosetsifandonlyifitisamemberofeitherset:
∀x,s ,s x∈(s ∪s ) ⇔ (x∈s ∨x∈s ).
1 2 1 2 1 2
Lists are similar to sets. The differences are that lists are ordered and the same element can
LIST
appearmorethanonceinalist. WecanusethevocabularyofLispforlists: Nil istheconstant
list with no elements; Cons, Append, First, and Rest are functions; and Find is the pred-
icate that does for lists what Member does for sets. List? is a predicate that is true only of
lists. Aswithsets,itiscommontousesyntacticsugarinlogicalsentencesinvolvinglists. The
empty list is[]. Theterm Cons(x,y), where y isanonempty list, iswritten [x|y]. Theterm
Cons(x,Nil) (i.e., the list containing the element x) is written as [x]. A list of several ele-
ments,suchas[A,B,C],correspondstothenestedtermCons(A,Cons(B,Cons(C,Nil))).
Exercise8.16asksyoutowriteouttheaxiomsforlists.
8.3.4 The wumpus world
Some propositional logic axioms for the wumpus world were given in Chapter 7. The first-
orderaxioms inthis section aremuch moreconcise, capturing inanatural wayexactly what
wewanttosay.
Recall that the wumpus agent receives a percept vector with fiveelements. The corre-
spondingfirst-ordersentencestoredintheknowledgebasemustincludeboththeperceptand
thetimeatwhichitoccurred; otherwise,theagentwillgetconfused aboutwhenitsawwhat.
Weuseintegersfortimesteps. Atypicalperceptsentence wouldbe
Percept([Stench,Breeze,Glitter,None,None],5).
Here,Percept isabinary predicate, and Stench andsoonareconstants placedinalist. The
actionsinthewumpusworldcanberepresented bylogicalterms:
Turn(Right), Turn(Left), Forward, Shoot, Grab, Climb .
Todetermine whichisbest,theagentprogram executesthequery
ASKVARS(∃a BestAction(a,5)),
which returns a binding list such as {a/Grab}. The agent program can then return Grab as
the action to take. The raw percept data implies certain facts about the current state. For
example:
∀t,s,g,m,c Percept([s,Breeze,g,m,c],t) ⇒ Breeze(t),
∀t,s,b,m,c Percept([s,b,Glitter,m,c],t) ⇒ Glitter(t),
andsoon. Theserulesexhibitatrivialformofthereasoning processcalledperception,which
westudyindepthinChapter24. Noticethequantificationovertimet. Inpropositionallogic,
wewouldneedcopiesofeachsentence foreachtimestep.
Simple“reflex”behaviorcanalsobeimplemented byquantifiedimplication sentences.
Forexample,wehave
∀t Glitter(t) ⇒ BestAction(Grab,t).
306 Chapter 8. First-OrderLogic
Giventheperceptandrulesfromthepreceding paragraphs, thiswouldyieldthedesiredcon-
clusionBestAction(Grab,5)—that is,Grab istherightthingtodo.
We have represented the agent’s inputs and outputs; now it is time to represent the
environment itself. Let us begin with objects. Obvious candidates are squares, pits, and the
wumpus. Wecouldnameeachsquare—Square andsoon—butthenthefactthatSquare
1,2 1,2
and Square are adjacent would have to be an “extra” fact, and we would need one such
1,3
factforeach pairofsquares. Itisbettertouseacomplex term inwhich therowandcolumn
appearasintegers; forexample,wecansimplyusethelistterm [1,2]. Adjacencyofanytwo
squarescanbedefinedas
∀x,y,a,b Adjacent([x,y],[a,b]) ⇔
(x = a∧(y = b−1∨y = b+1))∨(y = b∧(x = a−1∨x = a+1)).
We could name each pit, but this would be inappropriate for a different reason: there is no
reason to distinguish among pits.10 It is simpler to use a unary predicate Pit that is true of
squares containing pits. Finally, since there is exactly one wumpus, a constant Wumpus is
justasgoodasaunarypredicate(andperhapsmoredignifiedfromthewumpus’sviewpoint).
The agent’s location changes over time, so we write At(Agent,s,t) to mean that the
agentisatsquaresattimet. Wecanfixthewumpus’slocationwith∀tAt(Wumpus,[2,2],t).
Wecanthensaythatobjectscanonlybeatonelocation atatime:
∀x,s ,s ,t At(x,s ,t)∧At(x,s ,t) ⇒ s =s .
1 2 1 2 1 2
Given its current location, the agent can infer properties of the square from properties of its
current percept. For example, if the agent is at a square and perceives a breeze, then that
squareisbreezy:
∀s,t At(Agent,s,t)∧Breeze(t) ⇒ Breezy(s).
Itisusefultoknowthatasquareisbreezybecauseweknowthatthepitscannotmoveabout.
NoticethatBreezy hasnotimeargument.
Havingdiscovered whichplacesarebreezy(orsmelly)and,veryimportant, notbreezy
(ornotsmelly),theagentcandeducewherethepitsare(andwherethewumpusis). Whereas
propositionallogicnecessitatesaseparateaxiomforeachsquare(seeR andR onpage247)
2 3
andwouldneedadifferentsetofaxiomsforeachgeographicallayoutoftheworld,first-order
logicjustneedsoneaxiom:
∀s Breezy(s) ⇔ ∃r Adjacent(r,s)∧Pit(r). (8.4)
Similarly, in first-order logic wecan quantify overtime, so weneed just one successor-state
axiom for each predicate, rather than a different copy for each time step. For example, the
axiomforthearrow(Equation(7.2)onpage267)becomes
∀t HaveArrow(t+1) ⇔ (HaveArrow(t)∧¬Action(Shoot,t)).
From these two example sentences, we can see that the first-order logic formulation is no
less concise than the original English-language description given in Chapter 7. The reader
10 Similarly,mostofusdonotnameeachbirdthatfliesoverheadasitmigratestowarmerregionsinwinter. An
ornithologistwishingtostudymigrationpatterns,survivalrates,andsoondoesnameeachbird,bymeansofa
ringonitsleg,becauseindividualbirdsmustbetracked.
Section8.4. KnowledgeEngineering inFirst-OrderLogic 307
is invited to construct analogous axioms for the agent’s location and orientation; in these
cases, the axioms quantify over both space and time. As in the case of propositional state
estimation,anagentcanuselogicalinferencewithaxiomsofthiskindtokeeptrackofaspects
oftheworldthatarenotdirectlyobserved. Chapter10goesintomoredepthonthesubjectof
first-ordersuccessor-state axiomsandtheirusesforconstructing plans.
8.4 KNOWLEDGE ENGINEERING IN FIRST-ORDER LOGIC
The preceding section illustrated the use of first-order logic to represent knowledge in three
simpledomains. Thissectiondescribesthegeneralprocessofknowledge-baseconstruction—
KNOWLEDGE aprocesscalledknowledgeengineering. Aknowledgeengineerissomeonewhoinvestigates
ENGINEERING
aparticular domain, learns whatconcepts areimportant inthatdomain, andcreates aformal
representation of the objects and relations in the domain. We illustrate the knowledge engi-
neering process in anelectronic circuit domain that should already befairly familiar, so that
wecanconcentrate ontherepresentational issues involved. Theapproach wetakeissuitable
for developing special-purpose knowledge bases whose domain is carefully circumscribed
and whose range of queries is known in advance. General-purpose knowledge bases, which
cover a broad range of human knowledge and are intended to support tasks such as natural
language understanding, arediscussed inChapter12.
8.4.1 The knowledge-engineering process
Knowledge engineering projects vary widely in content, scope, and difficulty, but all such
projects includethefollowingsteps:
1. Identify the task. The knowledge engineer must delineate the range of questions that
the knowledge base will support and the kinds of facts that will be available for each
specific problem instance. Forexample, does the wumpus knowledge base need to be
able to choose actions or is it required to answer questions only about the contents
of the environment? Will the sensor facts include the current location? The task will
determinewhatknowledgemustberepresentedinordertoconnectprobleminstancesto
answers. ThisstepisanalogoustothePEASprocessfordesigning agentsinChapter2.
2. Assembletherelevant knowledge. Theknowledge engineer might already beanexpert
in the domain, or might need to work with real experts to extract what they know—a
KNOWLEDGE process called knowledgeacquisition. Atthisstage, theknowledge isnotrepresented
ACQUISITION
formally. Theideaistounderstand thescope oftheknowledge base, asdetermined by
thetask,andtounderstand howthedomainactuallyworks.
For the wumpus world, which is defined by an artificial set of rules, the relevant
knowledge is easy to identify. (Notice, however, that the definition of adjacency was
not supplied explicitly in the wumpus-world rules.) For real domains, the issue of
relevance can be quite difficult—for example, a system for simulating VLSI designs
mightormightnotneedtotakeintoaccountstraycapacitances andskineffects.
308 Chapter 8. First-OrderLogic
3. Decide on a vocabulary of predicates, functions, and constants. That is, translate the
important domain-level concepts intologic-level names. Thisinvolves manyquestions
of knowledge-engineering style. Like programming style, this can have a significant
impact on the eventual success of the project. Forexample, should pits be represented
by objects or by a unary predicate on squares? Should the agent’s orientation be a
function or a predicate? Should the wumpus’s location depend on time? Once the
choices have been made, the result is a vocabulary that is known as the ontology of
ONTOLOGY
the domain. The word ontology means a particular theory of the nature of being or
existence. Theontology determines what kinds of things exist, but does not determine
theirspecificproperties andinterrelationships.
4. Encode general knowledge about the domain. The knowledge engineer writes down
the axioms for all the vocabulary terms. This pins down (to the extent possible) the
meaningoftheterms, enabling theexpert tocheck thecontent. Often,thisstepreveals
misconceptions orgaps in the vocabulary that mustbe fixedby returning tostep 3 and
iteratingthrough theprocess.
5. Encode a description of the specific problem instance. If the ontology is well thought
out, this step will be easy. It will involve writing simple atomic sentences about in-
stances of concepts that are already part of the ontology. Fora logical agent, problem
instancesaresuppliedbythesensors,whereasa“disembodied” knowledgebaseissup-
plied with additional sentences in the same way that traditional programs are supplied
withinputdata.
6. Pose queries to the inference procedure and get answers. This is where the reward is:
wecanlettheinference procedure operate ontheaxiomsandproblem-specific factsto
derive the facts we are interested in knowing. Thus, we avoid the need for writing an
application-specific solution algorithm.
7. Debug the knowledge base. Alas, the answers to queries will seldom be correct on
the first try. More precisely, the answers will be correct for the knowledge base as
written, assuming that the inference procedure is sound, but they will not be the ones
thattheuserisexpecting. Forexample,ifanaxiomismissing,somequerieswillnotbe
answerable from the knowledge base. A considerable debugging process could ensue.
Missingaxiomsoraxiomsthataretooweakcanbeeasily identifiedbynoticing places
where the chain of reasoning stops unexpectedly. For example, if the knowledge base
includesadiagnostic rule(seeExercise8.13)forfindingthewumpus,
∀s Smelly(s) ⇒ Adjacent(Home(Wumpus),s),
instead of the biconditional, then the agent will never be able to prove the absence of
wumpuses. Incorrect axioms can be identified because they are false statements about
theworld. Forexample,thesentence
∀x NumOfLegs(x,4) ⇒ Mammal(x)
is false for reptiles, amphibians, and, more importantly, tables. The falsehood of this
sentencecanbedeterminedindependentlyoftherestoftheknowledgebase. Incontrast,
Section8.4. KnowledgeEngineering inFirst-OrderLogic 309
atypicalerrorinaprogramlookslikethis:
offset = position + 1.
Itisimpossibletotellwhetherthisstatementiscorrectwithoutlookingattherestofthe
program to see whether, for example, offsetis used to refer to the current position,
or to one beyond the current position, or whether the value of positionis changed
byanotherstatementandsooffsetshouldalsobechanged again.
To understand this seven-step process better, we now apply it to an extended example—the
domainofelectronic circuits.
8.4.2 The electronic circuits domain
Wewilldevelopanontologyandknowledgebasethatallowustoreasonaboutdigitalcircuits
ofthekindshowninFigure8.6. Wefollowtheseven-stepprocessforknowledgeengineering.
Identifythetask
There are many reasoning tasks associated with digital circuits. At the highest level, one
analyzes the circuit’s functionality. For example, does the circuit in Figure 8.6 actually add
properly? If all the inputs are high, what is the output of gate A2? Questions about the
circuit’s structure are also interesting. For example, what are all the gates connected to the
firstinput terminal? Doesthe circuit contain feedback loops? Thesewillbeourtasks inthis
section. There are more detailed levels of analysis, including those related to timing delays,
circuit area, power consumption, production cost, and so on. Each of these levels would
requireadditional knowledge.
Assembletherelevantknowledge
What do we know about digital circuits? Forour purposes, they are composed of wires and
gates. Signals flow along wires to the input terminals of gates, and each gate produces a
C
1
1
2 X 1 X 2 1
3 A
2
A O 1 2
1
Figure8.6 AdigitalcircuitC1,purportingtobeaone-bitfulladder. Thefirsttwoinputs
arethetwobitstobeadded,andthethirdinputisacarrybit. Thefirstoutputisthesum,and
thesecondoutputisacarrybitforthenextadder. ThecircuitcontainstwoXORgates,two
ANDgates,andoneORgate.
310 Chapter 8. First-OrderLogic
signal on theoutput terminal thatflowsalong another wire. Todetermine whatthese signals
will be, we need to know how the gates transform their input signals. There are four types
ofgates: AND,OR,and XORgateshave twoinput terminals, and NOTgates have one. All
gateshaveoneoutputterminal. Circuits, likegates,haveinputandoutput terminals.
Toreason about functionality and connectivity, we do not need to talk about the wires
themselves, the paths they take, or the junctions where they come together. All that matters
is the connections between terminals—we can say that one output terminal is connected to
anotherinputterminalwithouthavingtosaywhatactuallyconnectsthem. Otherfactorssuch
asthesize,shape, color,orcostofthevariouscomponents areirrelevant toouranalysis.
Ifourpurpose weresomething otherthan verifying designs atthegatelevel, theontol-
ogywouldbedifferent. Forexample,ifwewereinterested in debugging faultycircuits, then
it would probably be a good idea to include the wires in the ontology, because a faulty wire
cancorruptthesignalflowingalongit. Forresolvingtiming faults,wewouldneedtoinclude
gate delays. If we were interested in designing a product that would be profitable, then the
costofthecircuitanditsspeedrelativetootherproductsonthemarketwouldbeimportant.
Decideonavocabulary
Wenowknowthatwewanttotalkaboutcircuits,terminals,signals,andgates. Thenextstep
istochoose functions, predicates, and constants torepresent them. First, weneed tobeable
to distinguish gates from each other and from other objects. Each gate is represented as an
objectnamedbyaconstant, aboutwhichweassertthatitisagatewith,say,Gate(X ). The
1
behavior of each gate is determined by its type: one of the constants AND,OR, XOR, or
NOT. Because a gate has exactly one type, a function is appropriate: Type(X )=XOR.
1
Circuits,likegates,areidentifiedbyapredicate: Circuit(C ).
1
Nextweconsiderterminals,whichareidentifiedbythepredicateTerminal(x). Agate
orcircuitcanhaveoneormoreinputterminalsandoneormore outputterminals. Weusethe
function In(1,X ) to denote the first input terminal for gate X . A similar function Out is
1 1
usedforoutputterminals. Thefunction Arity(c,i,j)saysthatcircuitchasiinputandj out-
putterminals. Theconnectivitybetweengatescanberepresentedbyapredicate, Connected,
whichtakestwoterminalsasarguments, asin Connected(Out(1,X ),In(1,X )).
1 2
Finally,weneedtoknowwhetherasignalisonoroff. Onepossibility istouseaunary
predicate, On(t), which is true when the signal at a terminal is on. This makes it a little
difficult, however, to pose questions such as “What are all the possible values of the signals
attheoutputterminalsofcircuit C ?” Wethereforeintroduce asobjectstwosignalvalues, 1
1
and0,andafunction Signal(t)thatdenotes thesignalvaluefortheterminal t.
Encodegeneralknowledgeofthedomain
Onesignthatwehaveagoodontology isthatwerequireonlyafewgeneralrules,whichcan
bestatedclearlyandconcisely. Thesearealltheaxiomswewillneed:
1. Iftwoterminalsareconnected, thentheyhavethesamesignal:
∀t ,t Terminal(t )∧Terminal(t )∧Connected(t ,t ) ⇒
1 2 1 2 1 2
Signal(t )=Signal(t ).
1 2
Section8.4. KnowledgeEngineering inFirst-OrderLogic 311
2. Thesignalateveryterminaliseither1or0:
∀t Terminal(t) ⇒ Signal(t)=1∨Signal(t)=0 .
3. Connectediscommutative:
∀t ,t Connected(t ,t ) ⇔ Connected(t ,t ).
1 2 1 2 2 1
4. Therearefourtypesofgates:
∀g Gate(g)∧k = Type(g) ⇒ k = AND ∨k = OR∨k = XOR∨k = NOT .
5. AnANDgate’soutputis0ifandonlyifanyofitsinputsis0:
∀g Gate(g)∧Type(g)=AND ⇒
Signal(Out(1,g))=0 ⇔ ∃n Signal(In(n,g))=0.
6. AnORgate’soutputis1ifandonlyifanyofitsinputsis1:
∀g Gate(g)∧Type(g)=OR ⇒
Signal(Out(1,g))=1 ⇔ ∃n Signal(In(n,g))=1.
7. AnXORgate’soutputis1ifandonlyifitsinputsaredifferent:
∀g Gate(g)∧Type(g)=XOR ⇒
Signal(Out(1,g))=1 ⇔ Signal(In(1,g)) (cid:7)= Signal(In(2,g)).
8. ANOTgate’soutputisdifferentfromitsinput:
∀g Gate(g)∧(Type(g)=NOT) ⇒
Signal(Out(1,g)) (cid:7)= Signal(In(1,g)) .
9. Thegates(exceptforNOT)havetwoinputsandoneoutput.
∀g Gate(g)∧Type(g) = NOT ⇒ Arity(g,1,1) .
∀g Gate(g)∧k = Type(g)∧(k = AND ∨k = OR∨k = XOR) ⇒
Arity(g,2,1)
10. Acircuithasterminals, uptoitsinputandoutputarity, andnothing beyonditsarity:
∀c,i,j Circuit(c)∧Arity(c,i,j) ⇒
∀n (n ≤ i ⇒ Terminal(In(c,n)))∧(n > i ⇒ In(c,n) = Nothing)∧
∀n (n ≤ j ⇒ Terminal(Out(c,n)))∧(n > j ⇒ Out(c,n) = Nothing)
11. Gates,terminals, signals, gatetypes,andNothing arealldistinct.
∀g,t Gate(g)∧Terminal(t) ⇒
g (cid:7)= t (cid:7)= 1 (cid:7)= 0 (cid:7)=OR (cid:7)= AND (cid:7)= XOR (cid:7)= NOT (cid:7)= Nothing .
12. Gatesarecircuits.
∀g Gate(g) ⇒ Circuit(g)
Encodethespecificprobleminstance
ThecircuitshowninFigure8.6isencodedascircuit C withthefollowingdescription. First,
1
wecategorize thecircuitanditscomponent gates:
Circuit(C )∧Arity(C ,3,2)
1 1
Gate(X )∧Type(X )=XOR
1 1
Gate(X )∧Type(X )=XOR
2 2
Gate(A )∧Type(A )=AND
1 1
Gate(A )∧Type(A )=AND
2 2
Gate(O )∧Type(O )=OR .
1 1
312 Chapter 8. First-OrderLogic
Then,weshowtheconnections betweenthem:
Connected(Out(1,X ),In(1,X )) Connected(In(1,C ),In(1,X ))
1 2 1 1
Connected(Out(1,X ),In(2,A )) Connected(In(1,C ),In(1,A ))
1 2 1 1
Connected(Out(1,A ),In(1,O )) Connected(In(2,C ),In(2,X ))
2 1 1 1
Connected(Out(1,A ),In(2,O )) Connected(In(2,C ),In(2,A ))
1 1 1 1
Connected(Out(1,X ),Out(1,C )) Connected(In(3,C ),In(2,X ))
2 1 1 2
Connected(Out(1,O ),Out(2,C )) Connected(In(3,C ),In(1,A )).
1 1 1 2
Posequeriestotheinferenceprocedure
Whatcombinations ofinputs wouldcause thefirstoutput of C (thesumbit)tobe0andthe
1
secondoutputofC (thecarrybit)tobe1?
1
∃i ,i ,i Signal(In(1,C ))=i ∧Signal(In(2,C ))=i ∧Signal(In(3,C ))=i
1 2 3 1 1 1 2 1 3
∧Signal(Out(1,C ))=0∧Signal(Out(2,C ))=1.
1 1
The answers are substitutions forthe variables i , i , and i such that the resulting sentence
1 2 3
isentailed bytheknowledge base. ASKVARS willgiveusthreesuchsubstitutions:
{i /1, i /1, i /0} {i /1, i /0, i /1} {i /0, i /1, i /1}.
1 2 3 1 2 3 1 2 3
Whatarethepossible setsofvaluesofalltheterminalsfortheaddercircuit?
∃i ,i ,i ,o ,o Signal(In(1,C ))=i ∧Signal(In(2,C ))=i
1 2 3 1 2 1 1 1 2
∧Signal(In(3,C ))=i ∧Signal(Out(1,C ))=o ∧Signal(Out(2,C ))=o .
1 3 1 1 1 2
This final query will return a complete input–output table for the device, which can be used
to check that it does in fact add its inputs correctly. This is a simple example of circuit
CIRCUIT verification. We can also use the definition of the circuit to build larger digital systems, for
VERIFICATION
whichthesamekindofverification procedure canbecarriedout. (SeeExercise8.26.) Many
domainsareamenabletothesamekindofstructuredknowledge-base development,inwhich
morecomplexconcepts aredefinedontopofsimplerconcepts.
Debugtheknowledgebase
Wecanperturbtheknowledgebaseinvariouswaystoseewhatkindsoferroneousbehaviors
emerge. For example, suppose we fail to read Section 8.2.8 and hence forget to assert that
1 (cid:7)= 0. Suddenly, the system will be unable to prove any outputs for the circuit, except for
the input cases 000 and 110. Wecan pinpoint the problem byasking forthe outputs ofeach
gate. Forexample,wecanask
∃i ,i ,o Signal(In(1,C ))=i ∧Signal(In(2,C ))=i ∧Signal(Out(1,X )),
1 2 1 1 1 2 1
which reveals that nooutputs are knownat X forthe input cases 10 and01. Then, welook
1
attheaxiomforXORgates,asappliedtoX :
1
Signal(Out(1,X ))=1 ⇔ Signal(In(1,X )) (cid:7)= Signal(In(2,X )).
1 1 1
Iftheinputsareknowntobe,say,1and0,thenthisreducesto
Signal(Out(1,X ))=1 ⇔ 1 (cid:7)= 0.
1
Nowthe problem is apparent: the system isunable to infer that Signal(Out(1,X ))=1, so
1
weneedtotellitthat1 (cid:7)=0.
Section8.5. Summary 313
8.5 SUMMARY
Thischapterhasintroduced first-orderlogic,arepresentationlanguagethatisfarmorepow-
erfulthanpropositional logic. Theimportantpointsareas follows:
• Knowledge representation languages should be declarative, compositional, expressive,
contextindependent, andunambiguous.
• Logics differ in their ontological commitments and epistemological commitments.
Whilepropositional logiccommitsonlytotheexistence offacts, first-orderlogiccom-
mitstotheexistence ofobjects andrelationsandthereby gainsexpressivepower.
• The syntax of first-order logic builds on that of propositional logic. It adds terms to
represent objects, and has universal and existential quantifiers to construct assertions
aboutallorsomeofthepossible valuesofthequantifiedvariables.
• Apossibleworld,ormodel,forfirst-orderlogicincludes asetofobjectsandan inter-
pretationthatmapsconstantsymbolstoobjects, predicate symbolstorelationsamong
objects, andfunctionsymbolstofunctions onobjects.
• Anatomicsentenceistruejustwhentherelationnamedbythepredicateholdsbetween
theobjectsnamedbytheterms. Extendedinterpretations, whichmapquantifiervari-
ablestoobjectsinthemodel,definethetruthofquantified sentences.
• Developingaknowledgebaseinfirst-orderlogicrequiresacarefulprocessofanalyzing
the domain, choosing a vocabulary, and encoding the axioms required to support the
desiredinferences.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Although Aristotle’s logic deals with generalizations over objects, it fell far short of the ex-
pressivepoweroffirst-orderlogic. Amajorbarriertoitsfurtherdevelopmentwasitsconcen-
trationonone-place predicates totheexclusionofmany-placerelational predicates. Thefirst
systematic treatment of relations was given by Augustus De Morgan (1864), who cited the
followingexampletoshowthesortsofinferencesthatAristotle’slogiccouldnothandle: “All
horses are animals; therefore, the head of a horse is the head of an animal.” This inference
is inaccessible to Aristotle because any valid rule that can support this inference must first
analyzethesentenceusingthetwo-placepredicate“xistheheadofy.” Thelogicofrelations
wasstudied indepthbyCharlesSandersPeirce(1870, 2004).
Truefirst-orderlogicdatesfromtheintroductionofquantifiersinGottlobFrege’s(1879)
Begriffschrift (“Concept Writing” or “Conceptual Notation”). Peirce (1883) also developed
first-order logic independently of Frege, although slightly later. Frege’s ability to nest quan-
tifiers was a big step forward, but he used an awkward notation. The present notation for
first-order logic isduesubstantially toGiuseppe Peano (1889), but thesemantics isvirtually
identicaltoFrege’s. Oddlyenough,Peano’saxiomsweredueinlargemeasuretoGrassmann
(1861)andDedekind(1888).
314 Chapter 8. First-OrderLogic
LeopoldLo¨wenheim(1915)gaveasystematictreatmentofmodeltheoryforfirst-order
logic, including the firstproper treatment ofthe equality symbol. Lo¨wenheim’s results were
further extended by Thoralf Skolem (1920). Alfred Tarski (1935, 1956) gave an explicit
definitionoftruthandmodel-theoretic satisfaction infirst-orderlogic,usingsettheory.
McCarthy(1958)wasprimarilyresponsible fortheintroductionoffirst-orderlogicasa
toolforbuildingAIsystems. Theprospectsforlogic-based AIwereadvancedsignificantlyby
Robinson’s (1965) development ofresolution, acomplete procedure forfirst-order inference
describedinChapter9. ThelogicistapproachtookrootatStanfordUniversity. CordellGreen
(1969a,1969b)developedafirst-orderreasoningsystem,QA3,leadingtothefirstattemptsto
buildalogicalrobotatSRI(FikesandNilsson,1971). First-orderlogicwasappliedbyZohar
Manna and Richard Waldinger (1971) for reasoning about programs and later by Michael
Genesereth (1984) for reasoning about circuits. In Europe, logic programming (a restricted
formoffirst-orderreasoning) wasdevelopedforlinguistic analysis(Colmerauer etal.,1973)
and for general declarative systems (Kowalski, 1974). Computational logic was also well
entrenchedatEdinburghthroughtheLCF(LogicforComputableFunctions)project(Gordon
etal.,1979). Thesedevelopments arechronicled furtherinChapters9and12.
Practical applications built with first-order logic include a system for evaluating the
manufacturing requirementsforelectronicproducts(Mannion,2002),asystemforreasoning
about policies for fileaccess and digital rights management (Halpern and Weissman, 2008),
andasystemfortheautomatedcomposition ofWebservices (McIlraithandZeng,2001).
Reactions to the Whorf hypothesis (Whorf, 1956) and the problem of language and
thoughtingeneral,appearinseveralrecentbooks(Gumperz andLevinson,1996;Bowerman
andLevinson,2001;Pinker,2003;GentnerandGoldin-Meadow,2003). The“theory”theory
(Gopnik and Glymour, 2002; Tenenbaum et al., 2007) views children’s learning about the
world as analogous to the construction of scientific theories. Just as the predictions of a
machine learning algorithm depend strongly on the vocabulary supplied to it, so will the
child’sformulationoftheoriesdependonthelinguisticenvironmentinwhichlearningoccurs.
There are a number of good introductory texts on first-order logic, including some by
leading figures in the history of logic: Alfred Tarski (1941), Alonzo Church (1956), and
W.V.Quine(1982) (whichisoneofthemostreadable). Enderton (1972)givesamoremath-
ematically oriented perspective. A highly formal treatment of first-order logic, along with
many more advanced topics in logic, is provided by Bell and Machover (1977). Manna and
Waldinger (1985) give a readable introduction to logic from a computer science perspec-
tive, as do Huth and Ryan (2004), who concentrate on program verification. Barwise and
Etchemendy(2002)takeanapproachsimilartotheoneusedhere. Smullyan(1995)presents
results concisely, using the tableau format. Gallier (1986) provides an extremely rigorous
mathematical exposition of first-order logic, along witha great deal ofmaterial on its use in
automatedreasoning. LogicalFoundationsofArtificialIntelligence (GeneserethandNilsson,
1987)isbothasolid introduction tologic andthefirstsystematic treatment oflogical agents
with percepts and actions, and there are two good handbooks: van Bentham and terMeulen
(1997) andRobinson andVoronkov (2001). Thejournal ofrecord forthefieldofpuremath-
ematical logic is the Journal of Symbolic Logic, whereas the Journal of Applied Logic deals
withconcerns closertothoseofartificialintelligence.
Exercises 315
EXERCISES
8.1 Alogicalknowledgebaserepresents theworldusingasetofsentenceswithnoexplicit
structure. An analogical representation, ontheotherhand, hasphysical structure that corre-
spondsdirectlytothestructureofthethingrepresented. Consideraroadmapofyourcountry
asananalogicalrepresentation offactsaboutthecountry—itrepresentsfactswithamaplan-
guage. Thetwo-dimensionalstructureofthemapcorrespondstothetwo-dimensionalsurface
ofthearea.
a. Givefiveexamplesofsymbolsinthemaplanguage.
b. An explicit sentence is a sentence that the creator of the representation actually writes
down. An implicit sentence is a sentence that results from explicit sentences because
ofpropertiesoftheanalogical representation. Givethree exampleseachofimplicitand
explicitsentences inthemaplanguage.
c. Givethreeexamplesoffactsaboutthephysicalstructureofyourcountrythatcannotbe
represented inthemaplanguage.
d. Givetwoexamplesoffactsthataremucheasiertoexpressin themaplanguage thanin
first-orderlogic.
e. Givetwootherexamplesofusefulanalogical representations. Whataretheadvantages
anddisadvantages ofeachoftheselanguages?
8.2 Consider a knowledge base containing just two sentences: P(a) and P(b). Does this
knowledgebaseentail∀xP(x)? Explainyouranswerintermsofmodels.
8.3 Isthesentence∃x,y x=y valid? Explain.
8.4 Writedownalogicalsentencesuchthateveryworldinwhichitistruecontainsexactly
oneobject.
8.5 Consider a symbol vocabulary that contains c constant symbols, p predicate symbols
k
ofeach arity k, and f function symbols ofeach arity k, where 1 ≤ k ≤ A. Letthe domain
k
sizebefixedatD. Foranygivenmodel,eachpredicateorfunction symbolismappedontoa
relationorfunction, respectively, ofthesamearity. Youmayassumethatthefunctions inthe
modelallowsomeinputtuplestohavenovalueforthefunction(i.e.,thevalueistheinvisible
object). Derivea formula forthe number of possible models foradomain with D elements.
Don’tworryabouteliminating redundant combinations.
8.6 Whichofthefollowingarevalid(necessarily true)sentences?
a. (∃xx=x) ⇒ (∀y ∃z y=z).
b. ∀x P(x)∨¬P(x).
c. ∀x Smart(x)∨(x=x).
8.7 Consider a version of the semantics for first-order logic in which models with empty
domains areallowed. Giveatleasttwoexamples ofsentences thatarevalidaccording tothe
316 Chapter 8. First-OrderLogic
standard semantics but not according to the new semantics. Discuss which outcome makes
moreintuitivesenseforyourexamples.
8.8 Does the fact ¬Spouse(George,Laura) follow from the facts Jim (cid:7)= George and
Spouse(Jim,Laura)? If so, give aproof; if not, supply additional axioms as needed. What
happens ifweuseSpouse asaunaryfunctionsymbolinsteadofabinarypredicate?
8.9 Thisexerciseusesthefunction MapColor andpredicates In(x,y),Borders(x,y),and
Country(x), whose arguments are geographical regions, along with constant symbols for
various regions. In each of the following wegive an English sentence and a number ofcan-
didate logical expressions. Foreach of the logical expressions, state whether it (1) correctly
expresses the English sentence; (2) issyntactically invalid and therefore meaningless; or(3)
issyntactically validbutdoesnotexpressthemeaningoftheEnglishsentence.
a. ParisandMarseillesarebothinFrance.
(i) In(Paris ∧Marseilles,France).
(ii) In(Paris,France)∧In(Marseilles,France).
(iii) In(Paris,France)∨In(Marseilles,France).
b. Thereisacountrythatborders bothIraqandPakistan.
(i) ∃c Country(c)∧Border(c,Iraq)∧Border(c,Pakistan).
(ii) ∃c Country(c) ⇒ [Border(c,Iraq)∧Border(c,Pakistan)].
(iii) [∃c Country(c)] ⇒ [Border(c,Iraq)∧Border(c,Pakistan)].
(iv) ∃c Border(Country(c),Iraq ∧Pakistan).
c. Allcountries thatborderEcuadorareinSouthAmerica.
(i) ∀c Country(c)∧Border(c,Ecuador) ⇒ In(c,SouthAmerica).
(ii) ∀c Country(c) ⇒ [Border(c,Ecuador) ⇒ In(c,SouthAmerica)].
(iii) ∀c [Country(c) ⇒ Border(c,Ecuador)] ⇒ In(c,SouthAmerica).
(iv) ∀c Country(c)∧Border(c,Ecuador)∧In(c,SouthAmerica).
d. NoregioninSouthAmericabordersanyregioninEurope.
(i) ¬[∃c,d In(c,SouthAmerica)∧In(d,Europe)∧Borders(c,d)].
(ii) ∀c,d [In(c,SouthAmerica)∧In(d,Europe)] ⇒ ¬Borders(c,d)].
(iii) ¬∀c In(c,SouthAmerica) ⇒ ∃d In(d,Europe)∧¬Borders(c,d).
(iv) ∀c In(c,SouthAmerica) ⇒ ∀d In(d,Europe) ⇒ ¬Borders(c,d).
e. Notwoadjacentcountries havethesamemapcolor.
(i) ∀x,y ¬Country(x)∨¬Country(y)∨¬Borders(x,y)∨
¬(MapColor(x) = MapColor(y)).
(ii) ∀x,y (Country(x)∧Country(y)∧Borders(x,y)∧¬(x= y)) ⇒
¬(MapColor(x) = MapColor(y)).
(iii) ∀x,y Country(x)∧Country(y)∧Borders(x,y)∧
¬(MapColor(x) = MapColor(y)).
(iv) ∀x,y (Country(x)∧Country(y)∧Borders(x,y)) ⇒ MapColor(x(cid:7)= y).
Exercises 317
8.10 Consideravocabulary withthefollowingsymbols:
Occupation(p,o): Predicate. Person phasoccupation o.
Customer(p1,p2): Predicate. Personp1isacustomerofperson p2.
Boss(p1,p2): Predicate. Personp1isabossofperson p2.
Doctor,Surgeon,Lawyer,Actor: Constants denoting occupations.
Emily,Joe: Constantsdenoting people.
Usethesesymbolstowritethefollowingassertions infirst-orderlogic:
a. Emilyiseitherasurgeon oralawyer.
b. Joeisanactor, buthealsoholdsanotherjob.
c. Allsurgeons aredoctors.
d. Joedoesnothavealawyer(i.e.,isnotacustomerofanylawyer).
e. Emilyhasabosswhoisalawyer.
f. Thereexistsalawyerallofwhosecustomersaredoctors.
g. Everysurgeon hasalawyer.
8.11 Completethefollowingexercisesaboutlogicalsenntences:
a. Translateinto good,naturalEnglish(noxsorys!):
∀x,y,l SpeaksLanguage(x,l)∧SpeaksLanguage(y,l)
⇒ Understands(x,y)∧Understands(y,x).
b. Explainwhythissentence isentailed bythesentence
∀x,y,l SpeaksLanguage(x,l)∧SpeaksLanguage(y,l)
⇒ Understands(x,y).
c. Translateintofirst-orderlogicthefollowingsentences:
(i) Understanding leadstofriendship.
(ii) Friendship istransitive.
Remembertodefineallpredicates, functions, andconstants youuse.
8.12 Rewrite the first two Peano axioms in Section 8.3.3 as a single axiom that defines
NatNum(x)soastoexcludethepossibilityofnaturalnumbersexceptforthosegeneratedby
thesuccessorfunction.
8.13 Equation(8.4)onpage306definestheconditionsunderwhichasquareisbreezy. Here
weconsidertwootherwaystodescribe thisaspectofthewumpusworld.
a. Wecanwritediagnosticrulesleadingfromobservedeffectstohiddencauses. Forfind-
DIAGNOSTICRULE
ingpits,theobviousdiagnosticrulessaythatifasquareisbreezy,someadjacentsquare
mustcontain apit; andifasquareisnotbreezy, then noadjacent square contains apit.
Write these two rules in first-order logic and show that their conjunction is logically
equivalenttoEquation(8.4).
b. Wecanwritecausalrulesleadingfromcausetoeffect. Oneobviouscausalruleisthat
CAUSALRULE
apitcausesalladjacentsquarestobebreezy. Writethisruleinfirst-orderlogic,explain
whyitisincomplete comparedtoEquation(8.4),andsupplythemissingaxiom.
318 Chapter 8. First-OrderLogic
George Mum
Spencer Kydd Elizabeth Philip Margaret
Diana Charles Anne Mark Andrew Sarah Edward Sophie
William Harry Peter Zara Beatrice Eugenie Louise James
Figure8.7 Atypicalfamilytree. Thesymbol“(cid:10)(cid:11)”connectsspousesandarrowspointto
children.
8.14 Write axioms describing the predicates Grandchild, Greatgrandparent, Ancestor,
Brother, Sister, Daughter, Son, FirstCousin, BrotherInLaw, SisterInLaw, Aunt, and
Uncle. Find out the proper definition of mth cousin n times removed, and write the def-
inition in first-order logic. Now write down the basic facts depicted in the family tree in
Figure 8.7. Using a suitable logical reasoning system, TELL it all the sentences you have
writtendown,and ASK itwhoareElizabeth’s grandchildren, Diana’sbrothers-in-law, Zara’s
great-grandparents, andEugenie’sancestors.
8.15 Explain what is wrong with the following proposed definition of the set membership
predicate ∈:
∀x,s x∈{x|s}
∀x,s x∈s ⇒ ∀y x∈{y|s}.
8.16 Using the set axioms as examples, write axioms for the list domain, including all the
constants, functions, andpredicates mentionedinthechapter.
8.17 Explain what is wrong with the following proposed definition of adjacent squares in
thewumpusworld:
∀x,y Adjacent([x,y],[x+1,y])∧Adjacent([x,y],[x,y +1]).
8.18 Write out the axioms required for reasoning about the wumpus’s location, using a
constant symbol Wumpus and a binary predicate At(Wumpus,Location). Remember that
thereisonlyonewumpus.
8.19 Assuming predicates Parent(p,q) and Female(p) and constants Joan and Kevin,
withtheobviousmeanings, expresseachofthefollowingsentences infirst-orderlogic. (You
mayusetheabbreviation ∃1 tomean“thereexistsexactlyone.”)
a. Joanhasadaughter(possibly morethanone,andpossibly sonsaswell).
b. Joanhasexactlyonedaughter(butmayhavesonsaswell).
c. Joanhasexactlyonechild,adaughter.
d. JoanandKevinhaveexactlyonechildtogether.
e. JoanhasatleastonechildwithKevin,andnochildren withanyoneelse.
Exercises 319
8.20 Arithmetic assertions can be written in first-order logic with the predicate symbol <,
the function symbols +and×, and theconstant symbols 0and 1. Additional predicates can
alsobedefinedwithbiconditionals.
a. Representtheproperty“xisanevennumber.”
b. Representtheproperty“xisprime.”
c. Goldbach’s conjecture is the conjecture (unproven as yet) that every even number is
equaltothesumoftwoprimes. Representthisconjecture asa logicalsentence.
8.21 InChapter6,weusedequalitytoindicatetherelationbetweenavariableanditsvalue.
For instance, we wrote WA=red to mean that Western Australia is colored red. Repre-
senting this in first-order logic, we must write more verbosely ColorOf(WA)=red. What
incorrectinferencecouldbedrawnifwewrotesentencessuchasWA=red directlyaslogical
assertions?
8.22 Write in first-order logic the assertion that every key and at least one of every pair of
sockswilleventuallybelostforever,usingonlythefollowingvocabulary: Key(x),xisakey;
Sock(x), x is a sock; Pair(x,y), x and y are a pair; Now, the current time; Before(t ,t ),
1 2
timet comesbeforetimet ;Lost(x,t),objectxislostattimet.
1 2
8.23 Foreachofthefollowingsentences inEnglish, decideiftheaccompanying first-order
logic sentence is a good translation. If not, explain whynot and correct it. (Some sentences
mayhavemorethanoneerror!)
a. Notwopeoplehavethesamesocialsecurity number.
¬∃x,y,n Person(x)∧Person(y) ⇒ [HasSS#(x,n)∧HasSS#(y,n)].
b. John’ssocialsecurity numberisthesameasMary’s.
∃n HasSS#(John,n)∧HasSS#(Mary,n).
c. Everyone’ssocialsecuritynumberhasninedigits.
∀x,n Person(x) ⇒ [HasSS#(x,n)∧Digits(n,9)].
d. Rewriteeachoftheabove(uncorrected)sentencesusingafunctionsymbolSS#instead
ofthepredicate HasSS#.
8.24 Represent the following sentences in first-order logic, using a consistent vocabulary
(whichyoumustdefine):
a. Somestudents tookFrenchinspring2001.
b. EverystudentwhotakesFrenchpassesit.
c. OnlyonestudenttookGreekinspring2001.
d. ThebestscoreinGreekisalwayshigherthanthebestscoreinFrench.
e. Everypersonwhobuysapolicyissmart.
f. Nopersonbuysanexpensivepolicy.
g. Thereisanagentwhosellspolicies onlytopeoplewhoarenotinsured.
320 Chapter 8. First-OrderLogic
X Y 0 0 Ad 0 Z 0
X Y 1 1 Ad 1 Z 1 X 3 X 2 X 1 X 0
+ Y 3 Y 2 Y 1 Y 0
X Y 2 2 Ad 2 Z 2 Z 4 Z 3 Z 2 Z 1 Z 0
X Z
Y 3 3 Ad 3 Z 3 4
Figure8.8 Afour-bitadder.EachAdi isaone-bitadder,asinFigure8.6onpage309.
h. Thereisabarberwhoshavesallmenintownwhodonotshavethemselves.
i. Aperson born in the UK, each of whose parents is aUK citizen oraUK resident, is a
UKcitizenbybirth.
j. A person born outside the UK, one of whose parents is a UK citizen by birth, is a UK
citizenbydescent.
k. Politicianscanfoolsomeofthepeopleallofthetime,andtheycanfoolallofthepeople
someofthetime,buttheycan’tfoolallofthepeople allofthetime.
l. All Greeks speak the same language. (Use Speaks(x,l) to mean that person x speaks
languagel.)
8.25 Write a general set of facts and axioms to represent the assertion “Wellington heard
about Napoleon’s death” and to correctly answer the question “Did Napoleon hear about
Wellington’s death?”
8.26 Extend the vocabulary from Section 8.4 to define addition for n-bit binary numbers.
Thenencode the description ofthefour-bit adderin Figure8.8, and pose thequeries needed
toverifythatitisinfactcorrect.
8.27 Obtain a passport application for your country, identify the rules determining eligi-
bility for a passport, and translate them into first-order logic, following the steps outlined in
Section8.4.
8.28 Considerafirst-orderlogicalknowledgebasethatdescribesworldscontainingpeople,
songs,albums(e.g.,“MeettheBeatles”)anddisks(i.e.,particularphysicalinstancesofCDs).
Thevocabulary contains thefollowingsymbols:
CopyOf(d,a): Predicate. Diskdisacopyofalbuma.
Owns(p,d): Predicate. Personpownsdiskd.
Sings(p,s,a): Albumaincludesarecording ofsong ssungbyperson p.
Wrote(p,s): Personpwrotesongs.
McCartney,Gershwin,BHoliday,Joe,EleanorRigby,TheManILove,Revolver:
Constantswiththeobviousmeanings.
Exercises 321
Expressthefollowingstatements infirst-orderlogic:
a. Gershwinwrote“TheManILove.”
b. Gershwindidnotwrite“EleanorRigby.”
c. EitherGershwinorMcCartneywrote“TheManILove.”
d. Joehaswrittenatleastonesong.
e. JoeownsacopyofRevolver.
f. EverysongthatMcCartneysingsonRevolverwaswrittenbyMcCartney.
g. GershwindidnotwriteanyofthesongsonRevolver.
h. EverysongthatGershwinwrotehasbeenrecorded onsomealbum. (Possiblydifferent
songsarerecorded ondifferent albums.)
i. Thereisasinglealbumthatcontains everysongthatJoehas written.
j. Joeownsacopyofanalbum thathasBillieHolidaysinging “TheManILove.”
k. Joe owns a copy of every album that has a song sung by McCartney. (Of course, each
differentalbumisinstantiated inadifferentphysical CD.)
l. JoeownsacopyofeveryalbumonwhichallthesongsaresungbyBillieHoliday.
9
INFERENCE IN
FIRST-ORDER LOGIC
In which we define effective procedures for answering questions posed in first-
orderlogic.
Chapter7showedhowsoundandcompleteinferencecanbeachievedforpropositionallogic.
In this chapter, we extend those results to obtain algorithms that can answer any answer-
ablequestion statedinfirst-orderlogic. Section9.1introduces inference rulesforquantifiers
andshowshowtoreducefirst-orderinference topropositional inference, albeitatpotentially
great expense. Section 9.2 describes the idea of unification, showing how it can be used
to construct inference rules that work directly with first-order sentences. We then discuss
three major families of first-order inference algorithms. Forward chaining and its applica-
tionstodeductivedatabasesandproductionsystemsarecoveredinSection9.3;backward
chainingandlogic programmingsystems aredeveloped inSection 9.4. Forwardandback-
ward chaining can be very efficient, but are applicable only to knowledge bases that can
be expressed as sets of Horn clauses. General first-order sentences require resolution-based
theoremproving,whichisdescribed inSection9.5.
9.1 PROPOSITIONAL VS. FIRST-ORDER INFERENCE
This section and the next introduce the ideas underlying modern logical inference systems.
Webegin with some simple inference rules that can be applied to sentences with quantifiers
toobtain sentences without quantifiers. These rules lead naturally tothe idea that first-order
inference can be done by converting the knowledge base to propositional logic and using
propositional inference, which we already know how to do. The next section points out an
obviousshortcut, leadingtoinference methodsthatmanipulate first-ordersentences directly.
9.1.1 Inference rules forquantifiers
Let us begin with universal quantifiers. Suppose our knowledge base contains the standard
folkloric axiomstatingthatallgreedykingsareevil:
∀x King(x)∧Greedy(x) ⇒ Evil(x).
322
Section9.1. Propositional vs.First-OrderInference 323
Thenitseemsquitepermissible toinferanyofthefollowing sentences:
King(John)∧Greedy(John) ⇒ Evil(John)
King(Richard)∧Greedy(Richard) ⇒ Evil(Richard)
King(Father(John))∧Greedy(Father(John)) ⇒ Evil(Father(John)).
.
.
.
UNIVERSAL The rule of Universal Instantiation (UI for short) says that we can infer any sentence ob-
INSTANTIATION
tained by substituting a ground term (a term without variables) for the variable.1 To write
GROUNDTERM
outtheinference ruleformally, weusethenotion of substitutionsintroduced inSection8.3.
Let SUBST(θ,α)denote theresult ofapplying thesubstitution θ tothesentence α. Thenthe
ruleiswritten
∀v α
SUBST({v/g},α)
for any variable v and ground term g. For example, the three sentences given earlier are
obtained withthesubstitutions {x/John},{x/Richard},and{x/Father(John)}.
EXISTENTIAL In the rule for Existential Instantiation, the variable is replaced by a single new con-
INSTANTIATION
stantsymbol. Theformalstatementisasfollows: foranysentence α,variablev,andconstant
symbolk thatdoesnotappearelsewhereintheknowledgebase,
∃v α
.
SUBST({v/k},α)
Forexample,fromthesentence
∃x Crown(x)∧OnHead(x,John)
wecaninferthesentence
Crown(C )∧OnHead(C ,John)
1 1
as long as C does not appear elsewhere in the knowledge base. Basically, the existential
1
sentence saysthereissomeobjectsatisfying acondition, andapplying theexistential instan-
tiation rule just gives a name to that object. Of course, that name must not already belong
toanotherobject. Mathematics provides anice example: suppose wediscoverthatthere isa
numberthatisalittlebiggerthan2.71828andthatsatisfiestheequationd(xy)/dy=xy forx.
Wecan give this number a name, such as e, but it would be a mistake to give itthe name of
an existing object, such as π. In logic, the new name is called a Skolem constant. Existen-
SKOLEMCONSTANT
tialInstantiation isaspecial caseofamoregeneral process called skolemization, which we
coverinSection9.5.
Whereas Universal Instantiation can be applied many times to produce many different
consequences, Existential Instantiation can be applied once, and then the existentially quan-
tified sentence can be discarded. Forexample, weno longer need ∃x Kill(x,Victim)once
wehaveaddedthesentence Kill(Murderer,Victim). Strictlyspeaking, thenewknowledge
INFERENTIAL baseisnotlogically equivalent totheold,butitcanbeshowntobeinferentially equivalent
EQUIVALENCE
inthesensethatitissatisfiableexactlywhentheoriginal knowledgebaseissatisfiable.
1 Donotconfusethesesubstitutionswiththeextendedinterpretationsusedtodefinethesemanticsofquantifiers.
The substitution replaces a variable with a term (a piece of syntax) to produce a new sentence, whereas an
interpretationmapsavariabletoanobjectinthedomain.
324 Chapter 9. Inference inFirst-OrderLogic
9.1.2 Reduction to propositional inference
Once we have rules for inferring nonquantified sentences from quantified sentences, it be-
comes possible to reduce first-order inference to propositional inference. In this section we
givethemainideas;thedetailsaregiveninSection9.5.
The first idea is that, just as an existentially quantified sentence can be replaced by
one instantiation, a universally quantified sentence can be replaced by the set of all possible
instantiations. Forexample, supposeourknowledgebasecontainsjustthesentences
∀x King(x)∧Greedy(x) ⇒ Evil(x)
King(John)
(9.1)
Greedy(John)
Brother(Richard,John).
Then weapply UI to the first sentence using all possible ground-term substitutions from the
vocabulary oftheknowledge base—inthiscase, {x/John}and{x/Richard}. Weobtain
King(John)∧Greedy(John) ⇒ Evil(John)
King(Richard)∧Greedy(Richard) ⇒ Evil(Richard),
and we discard the universally quantified sentence. Now, the knowledge base is essentially
propositional if we view the ground atomic sentences—King(John), Greedy(John), and
so on—as proposition symbols. Therefore, we can apply any of the complete propositional
algorithms inChapter7toobtainconclusions suchas Evil(John).
This technique of propositionalization can be made completely general, as we show
in Section 9.5; that is, every first-order knowledge base and query can be propositionalized
in such a way that entailment is preserved. Thus, we have a complete decision procedure
for entailment ... or perhaps not. There is a problem: when the knowledge base includes
a function symbol, the set of possible ground-term substitutions is infinite! For example, if
the knowledge base mentions the Father symbol, then infinitely many nested terms such as
Father(Father(Father(John)))canbeconstructed. Ourpropositional algorithms willhave
difficultywithaninfinitelylargesetofsentences.
Fortunately, there is a famous theorem due to Jacques Herbrand (1930) to the effect
thatifasentence isentailed bytheoriginal, first-order knowledge base, thenthere isaproof
involving justafinite subset ofthepropositionalized knowledge base. Sinceanysuch subset
has a maximum depth of nesting among its ground terms, we can find the subset by first
generatingalltheinstantiations withconstantsymbols(Richard andJohn),thenalltermsof
depth1(Father(Richard)andFather(John)),thenalltermsofdepth2,andsoon,untilwe
areabletoconstruct apropositional proofoftheentailedsentence.
We have sketched an approach to first-order inference via propositionalization that is
complete—that is, any entailed sentence can be proved. This is a major achievement, given
that the space of possible models is infinite. On the other hand, we do not know until the
proof isdone that thesentence isentailed! Whathappens whenthesentence isnot entailed?
Canwetell? Well, forfirst-order logic, itturns outthat wecannot. Ourproof procedure can
goonandon, generating moreandmoredeeply nested terms, butwewillnotknowwhether
itisstuckinahopeless looporwhethertheproofisjustabout topopout. Thisisverymuch
Section9.2. UnificationandLifting 325
likethehaltingproblemforTuringmachines. AlanTuring(1936)andAlonzoChurch(1936)
bothproved, inratherdifferent ways, theinevitability of thisstateofaffairs. Thequestion of
entailmentforfirst-orderlogicissemidecidable—thatis,algorithmsexistthatsayyestoevery
entailed sentence, butnoalgorithm existsthatalsosaysnotoeverynonentailed sentence.
9.2 UNIFICATION AND LIFTING
The preceding section described the understanding of first-order inference that existed up
to the early 1960s. The sharp-eyed reader (and certainly the computational logicians of the
early1960s)willhavenoticedthatthepropositionalization approachisratherinefficient. For
example, given the query Evil(x) and the knowledge base in Equation (9.1), it seems per-
versetogenerate sentences suchasKing(Richard)∧Greedy(Richard) ⇒ Evil(Richard).
Indeed, theinference ofEvil(John)fromthesentences
∀x King(x)∧Greedy(x) ⇒ Evil(x)
King(John)
Greedy(John)
seems completely obvious to a human being. We now show how to make it completely
obvioustoacomputer.
9.2.1 A first-order inference rule
TheinferencethatJohnisevil—thatis,that{x/John}solvesthequeryEvil(x)—workslike
this: to use the rule that greedy kings are evil, find some x such that x is a king and x is
greedy, and then infer that this x is evil. More generally, if there is some substitution θ that
makes each of the conjuncts of the premise of the implication identical to sentences already
intheknowledgebase,thenwecanasserttheconclusion oftheimplication, afterapplying θ.
Inthiscase,thesubstitution θ={x/John}achievesthataim.
We can actually make the inference step do even more work. Suppose that instead of
knowingGreedy(John),weknowthateveryoneisgreedy:
∀y Greedy(y). (9.2)
Then we would still like to be able to conclude that Evil(John), because we know that
John is a king (given) and John is greedy (because everyone is greedy). What we need for
this to work is to find a substitution both for the variables in the implication sentence and
for the variables in the sentences that are in the knowledge base. In this case, applying the
substitution{x/John,y/John}totheimplicationpremisesKing(x)andGreedy(x)andthe
knowledge-base sentences King(John) and Greedy(y) will make them identical. Thus, we
caninfertheconclusion oftheimplication.
This inference process can be captured as a single inference rule that we call Gener-
GENERALIZED alized Modus Ponens:2 For atomic sentences p , p (cid:2) , and q, where there is a substitution θ
MODUSPONENS i i
326 Chapter 9. Inference inFirst-OrderLogic
(cid:2)
suchthat SUBST(θ,p
i
)=SUBST(θ,p
i
),foralli,
p (cid:2) , p (cid:2) , ..., p (cid:2) , (p ∧p ∧...∧p ⇒ q)
1 2 n 1 2 n
.
SUBST(θ,q)
(cid:2)
Therearen+1premisestothisrule: thenatomicsentencesp andtheoneimplication. The
i
conclusion istheresultofapplying thesubstitution θ totheconsequent q. Forourexample:
(cid:2)
p isKing(John) p isKing(x)
1 1
(cid:2)
p isGreedy(y) p isGreedy(x)
2 2
θ is{x/John,y/John} q isEvil(x)
SUBST(θ,q)isEvil(John).
ItiseasytoshowthatGeneralizedModusPonensisasoundinferencerule. First,weobserve
that, for any sentence p (whose variables are assumed to be universally quantified) and for
anysubstitution θ,
p |= SUBST(θ,p)
holds by Universal Instantiation. It holds in particular for a θ that satisfies the conditions of
(cid:2) (cid:2)
theGeneralized ModusPonensrule. Thus,from p ,...,p wecaninfer
1 n
SUBST(θ,p
1
(cid:2) )∧...∧SUBST(θ,p
n
(cid:2) )
andfromtheimplication p ∧...∧p ⇒ q wecaninfer
1 n
SUBST(θ,p
1
)∧...∧SUBST(θ,p
n
) ⇒ SUBST(θ,q).
(cid:2)
Now, θ in Generalized Modus Ponens is defined so that SUBST(θ,p
i
)=SUBST(θ,p
i
), for
all i; therefore the first of these two sentences matches the premise of the second exactly.
Hence, SUBST(θ,q)followsbyModusPonens.
Generalized Modus Ponens is a lifted version of Modus Ponens—it raises Modus Po-
LIFTING
nens from ground (variable-free) propositional logic to first-order logic. We will see in the
rest of this chapter that we can develop lifted versions of the forward chaining, backward
chaining, and resolution algorithms introduced in Chapter 7. The key advantage of lifted
inference rules over propositionalization is that they make only those substitutions that are
required toallowparticularinferences toproceed.
9.2.2 Unification
Lifted inference rules require finding substitutions that make different logical expressions
look identical. This process is called unification and is a key component of all first-order
UNIFICATION
UNIFIER
inference algorithms. The UNIFY algorithm takes two sentences and returns a unifier for
themifoneexists:
UNIFY(p,q)=θ where SUBST(θ,p)=SUBST(θ,q).
Let us look at some examples of how UNIFY should behave. Suppose we have a query
AskVars(Knows(John,x)): whom does John know? Answers to this query can be found
2 GeneralizedModusPonensismoregeneralthanModusPonens(page249)inthesensethattheknownfacts
andthepremiseoftheimplicationneedmatchonlyuptoasubstitution,ratherthanexactly. Ontheotherhand,
ModusPonensallowsanysentenceαasthepremise,ratherthanjustaconjunctionofatomicsentences.
Section9.2. UnificationandLifting 327
byfindingallsentencesintheknowledgebasethatunifywithKnows(John,x). Herearethe
resultsofunificationwithfourdifferentsentences thatmightbeintheknowledgebase:
UNIFY(Knows(John,x), Knows(John,Jane)) = {x/Jane}
UNIFY(Knows(John,x), Knows(y,Bill)) = {x/Bill,y/John}
UNIFY(Knows(John,x), Knows(y,Mother(y))) = {y/John,x/Mother(John)}
UNIFY(Knows(John,x), Knows(x,Elizabeth)) = fail .
The last unification fails because x cannot take on the values John and Elizabeth at the
same time. Now, remember that Knows(x,Elizabeth) means “Everyone knows Elizabeth,”
so we should be able to infer that John knows Elizabeth. The problem arises only because
the two sentences happen to use the same variable name, x. The problem can be avoided
STANDARDIZING by standardizing apart one of the two sentences being unified, which means renaming its
APART
variables to avoid name clashes. Forexample, we can rename xin Knows(x,Elizabeth) to
x (anewvariablename)withoutchanging itsmeaning. Nowtheunification willwork:
17
UNIFY(Knows(John,x), Knows(x
17
,Elizabeth)) = {x/Elizabeth,x
17
/John}.
Exercise9.12delvesfurtherintotheneedforstandardizing apart.
There is one more complication: we said that UNIFY should return a substitution
that makes the two arguments look the same. But there could be more than one such uni-
fier. For example, UNIFY(Knows(John,x),Knows(y,z)) could return {y/John,x/z} or
{y/John,x/John,z/John}. The first unifier gives Knows(John,z) as the result of unifi-
cation, whereas thesecond gives Knows(John,John). Thesecond result could beobtained
from the first by an additional substitution {z/John}; we say that the first unifier is more
generalthanthesecond, because itplaces fewerrestrictions onthe valuesofthevariables. It
MOSTGENERAL turnsoutthat,foreveryunifiablepairofexpressions,thereisasinglemostgeneralunifier(or
UNIFIER
MGU)thatisunique uptorenaming andsubstitution ofvariables. (Forexample, {x/John}
and{y/John}areconsideredequivalent,asare{x/John,y/John}and{x/John,y/x}.) In
thiscaseitis{y/John,x/z}.
Analgorithm forcomputing mostgeneral unifiers isshownin Figure 9.1. Theprocess
issimple: recursivelyexplorethetwoexpressions simultaneously “sidebyside,”building up
aunifieralongtheway,butfailingiftwocorresponding pointsinthestructuresdonotmatch.
There is one expensive step: when matching a variable against a complex term, one must
checkwhetherthevariableitselfoccursinsidetheterm;if itdoes,thematchfailsbecauseno
consistent unifiercanbeconstructed. Forexample, S(x)can’t unify withS(S(x)). Thisso-
called occur checkmakes thecomplexity oftheentire algorithm quadratic inthesize ofthe
OCCURCHECK
expressions being unified. Some systems, including all logic programming systems, simply
omittheoccurcheck andsometimes makeunsound inferences asaresult; othersystems use
morecomplexalgorithms withlinear-time complexity.
9.2.3 Storageand retrieval
Underlying the TELL and ASK functions used to inform and interrogate a knowledge base
are the more primitive STORE and FETCH functions. STORE(s) stores a sentence s into the
knowledge base and FETCH(q) returns all unifiers such that the query q unifies with some
328 Chapter 9. Inference inFirst-OrderLogic
functionUNIFY(x,y,θ)returnsasubstitutiontomakex andy identical
inputs:x,avariable,constant,list,orcompoundexpression
y,avariable,constant,list,orcompoundexpression
θ,thesubstitutionbuiltupsofar(optional,defaultstoempty)
ifθ=failurethenreturnfailure
elseifx =y thenreturnθ
elseifVARIABLE?(x)thenreturnUNIFY-VAR(x,y,θ)
elseifVARIABLE?(y)thenreturnUNIFY-VAR(y,x,θ)
elseifCOMPOUND?(x)andCOMPOUND?(y)then
returnUNIFY(x.ARGS,y.ARGS,UNIFY(x.OP,y.OP,θ))
elseifLIST?(x)andLIST?(y)then
returnUNIFY(x.REST,y.REST,UNIFY(x.FIRST,y.FIRST,θ))
elsereturnfailure
functionUNIFY-VAR(var,x,θ)returnsasubstitution
if{var/val} ∈ θthenreturnUNIFY(val,x,θ)
elseif{x/val} ∈ θthenreturnUNIFY(var,val,θ)
elseifOCCUR-CHECK?(var,x)thenreturnfailure
elsereturnadd{var/x}toθ
Figure9.1 Theunificationalgorithm. Thealgorithmworksbycomparing thestructures
oftheinputs,elementbyelement. Thesubstitutionθ thatistheargumenttoUNIFY isbuilt
upalongthewayandisusedtomakesurethatlatercomparisonsareconsistentwithbindings
thatwereestablishedearlier. InacompoundexpressionsuchasF(A,B),theOPfieldpicks
outthefunctionsymbolF andtheARGSfieldpicksouttheargumentlist(A,B).
sentence in the knowledge base. The problem we used to illustrate unification—finding all
factsthatunifywithKnows(John,x)—isaninstanceof FETCHing.
The simplest way to implement STORE and FETCH is to keep all the facts in one long
list and unify each query against every element of the list. Such a process is inefficient, but
it works, and it’s all you need to understand the rest of the chapter. The remainder of this
sectionoutlines waystomakeretrievalmoreefficient;itcanbeskippedonfirstreading.
We can make FETCH more efficient by ensuring that unifications are attempted only
with sentences that have some chance of unifying. For example, there is no point in trying
tounifyKnows(John,x)withBrother(Richard,John). Wecanavoidsuchunificationsby
indexing the facts in the knowledge base. A simple scheme called predicate indexing puts
INDEXING
PREDICATE all the Knows facts in one bucket and all the Brother facts in another. The buckets can be
INDEXING
storedinahashtableforefficientaccess.
Predicate indexing is useful when there are many predicate symbols but only a few
clauses for each symbol. Sometimes, however, a predicate has many clauses. For example,
suppose that the tax authorities want to keep track of who employs whom, using a predi-
cate Employs(x,y). This would be a very large bucket with perhaps millions of employers
Section9.2. UnificationandLifting 329
Employs(x,y) Employs(x,y)
Employs(x,Richard) Employs(IBM,y) Employs(x,John) Employs(x,x) Employs(John,y)
Employs(IBM,Richard) Employs(John,John)
(a) (b)
Figure9.2 (a)ThesubsumptionlatticewhoselowestnodeisEmploys(IBM,Richard).
(b)ThesubsumptionlatticeforthesentenceEmploys(John,John).
and tens of millions of employees. Answering a query such as Employs(x,Richard) with
predicate indexingwouldrequirescanning theentirebucket.
Forthis particular query, it would help if facts were indexed both by predicate and by
secondargument, perhaps usingacombined hashtablekey. Thenwecouldsimplyconstruct
the key from the query and retrieve exactly those facts that unify with the query. For other
queries, such as Employs(IBM,y), we would need to have indexed the facts by combining
the predicate with the first argument. Therefore, facts can be stored under multiple index
keys,rendering theminstantly accessible tovariousqueriesthattheymightunifywith.
Givenasentence tobestored, itispossible toconstruct indices forallpossible queries
thatunifywithit. ForthefactEmploys(IBM,Richard),thequeriesare
Employs(IBM,Richard) DoesIBMemployRichard?
Employs(x,Richard) WhoemploysRichard?
Employs(IBM,y) WhomdoesIBMemploy?
Employs(x,y) Whoemployswhom?
SUBSUMPTION These queries form a subsumption lattice, as shown in Figure 9.2(a). The lattice has some
LATTICE
interesting properties. For example, the child of any node in the lattice is obtained from its
parent by a single substitution; and the “highest” common descendant of any two nodes is
the result of applying their mostgeneral unifier. Theportion ofthe lattice above anyground
factcanbeconstructedsystematically (Exercise9.5). Asentencewithrepeatedconstantshas
a slightly different lattice, as shown in Figure 9.2(b). Function symbols and variables in the
sentences tobestoredintroduce stillmoreinteresting latticestructures.
The scheme we have described works very well whenever the lattice contains a small
number of nodes. For a predicate with n arguments, however, the lattice contains O(2n)
nodes. If function symbols are allowed, the number of nodes is also exponential in the size
ofthetermsinthesentencetobestored. Thiscanleadtoahugenumberofindices. Atsome
point, the benefits of indexing are outweighed by the costs of storing and maintaining all
the indices. Wecan respond by adopting afixedpolicy, such as maintaining indices only on
keyscomposedofapredicatepluseachargument,orbyusinganadaptivepolicythatcreates
indices to meet the demands of the kinds of queries being asked. For most AI systems, the
number of facts to be stored is small enough that efficient indexing is considered a solved
problem. For commercial databases, where facts number in the billions, the problem has
beenthesubjectofintensivestudyandtechnology development..
330 Chapter 9. Inference inFirst-OrderLogic
9.3 FORWARD CHAINING
A forward-chaining algorithm for propositional definite clauses was given in Section 7.5.
The idea is simple: start with the atomic sentences in the knowledge base and apply Modus
Ponens in the forward direction, adding new atomic sentences, until no further inferences
can be made. Here, we explain how the algorithm is applied to first-order definite clauses.
DefiniteclausessuchasSituation ⇒ Response areespeciallyusefulforsystemsthatmake
inferences in response to newlyarrived information. Manysystems can bedefined this way,
andforwardchainingcanbeimplementedveryefficiently.
9.3.1 First-order definite clauses
First-order definite clauses closely resemble propositional definite clauses (page 256): they
aredisjunctions ofliterals ofwhich exactly oneispositive. Adefiniteclause eitherisatomic
or is an implication whose antecedent is a conjunction of positive literals and whose conse-
quentisasinglepositiveliteral. Thefollowingarefirst-orderdefiniteclauses:
King(x)∧Greedy(x) ⇒ Evil(x).
King(John).
Greedy(y).
Unlike propositional literals, first-order literals can include variables, in which case those
variables are assumed to be universally quantified. (Typically, we omit universal quantifiers
when writing definite clauses.) Not every knowledge base can be converted into a set of
definite clauses because of the single-positive-literal restriction, but many can. Consider the
followingproblem:
The law says thatit is a crime foran American to sell weaponsto hostile nations. The
countryNono,anenemyofAmerica,hassomemissiles,andallofitsmissilesweresold
toitbyColonelWest,whoisAmerican.
WewillprovethatWestisacriminal. First,wewillrepresentthesefactsasfirst-orderdefinite
clauses. Thenextsection showshowtheforward-chaining algorithm solvestheproblem.
“...itisacrimeforanAmericantosellweaponstohostilenations”:
American(x)∧Weapon(y)∧Sells(x,y,z)∧Hostile(z) ⇒ Criminal(x). (9.3)
“Nono...hassomemissiles.” Thesentence∃xOwns(Nono,x)∧Missile(x)istransformed
intotwodefiniteclausesbyExistentialInstantiation, introducing anewconstant M :
1
Owns(Nono,M ) (9.4)
1
M. issile(M ) (9.5)
1
“AllofitsmissilesweresoldtoitbyColonelWest”:
Missile(x)∧Owns(Nono,x) ⇒ Sells(West,x,Nono). (9.6)
Wewillalsoneedtoknowthatmissilesareweapons:
Missile(x) ⇒ Weapon(x) (9.7)
Section9.3. ForwardChaining 331
andwemustknowthatanenemyofAmericacountsas“hostile”:
Enemy(x,America) ⇒ Hostile(x). (9.8)
“West,whoisAmerican...”:
American(West). (9.9)
“Thecountry Nono,anenemyofAmerica ...”:
Enemy(Nono,America). (9.10)
This knowledge base contains no function symbols and is therefore an instance of the class
of Datalog knowledge bases. Datalog is a language that is restricted to first-order definite
DATALOG
clauses withnofunction symbols. Datalog getsitsnamebecause itcanrepresent thetypeof
statements typically made in relational databases. We will see that the absence of function
symbolsmakesinference mucheasier.
9.3.2 A simpleforward-chaining algorithm
Thefirstforward-chaining algorithmweconsiderisasimple one,showninFigure9.3. Start-
ing from the known facts, it triggers all the rules whose premises are satisfied, adding their
conclusions to the known facts. The process repeats until the query is answered (assuming
that just one answer is required) or no new facts are added. Notice that a fact is not “new”
if it is just a renaming of a known fact. One sentence is a renaming of another if they
RENAMING
are identical except for the names of the variables. For example, Likes(x,IceCream) and
Likes(y,IceCream) arerenamings ofeach otherbecause theydiffer only in thechoice of x
ory;theirmeaningsareidentical: everyonelikesicecream.
We use our crime problem to illustrate how FOL-FC-ASK works. The implication
sentences are(9.3),(9.6),(9.7),and(9.8). Twoiterations arerequired:
• Onthefirstiteration, rule(9.3)hasunsatisfiedpremises.
Rule(9.6)issatisfiedwith{x/M },andSells(West,M ,Nono)isadded.
1 1
Rule(9.7)issatisfiedwith{x/M },andWeapon(M )isadded.
1 1
Rule(9.8)issatisfiedwith{x/Nono},andHostile(Nono)isadded.
• On the second iteration, rule (9.3) is satisfied with {x/West,y/M ,z/Nono}, and
1
Criminal(West)isadded.
Figure 9.4shows theproof treethat isgenerated. Notice that nonewinferences arepossible
at this point because every sentence that could be concluded by forward chaining is already
containedexplicitlyintheKB.Suchaknowledgebaseiscalledafixedpointoftheinference
process. Fixedpointsreachedbyforwardchainingwithfirst-orderdefiniteclausesaresimilar
tothoseforpropositional forwardchaining (page258);the principal difference isthatafirst-
orderfixedpointcaninclude universally quantifiedatomicsentences.
FOL-FC-ASK iseasy to analyze. First, itis sound,because every inference is just an
applicationofGeneralizedModusPonens,whichissound. Second,itiscompletefordefinite
clause knowledge bases; that is, it answers every query whose answers are entailed by any
knowledgebaseofdefiniteclauses. ForDatalogknowledgebases,whichcontainnofunction
symbols, the proof of completeness is fairly easy. We begin by counting the number of
332 Chapter 9. Inference inFirst-OrderLogic
functionFOL-FC-ASK(KB,α)returnsasubstitutionorfalse
inputs:KB,theknowledgebase,asetoffirst-orderdefiniteclauses
α,thequery,anatomicsentence
localvariables: new,thenewsentencesinferredoneachiteration
repeatuntilnew isempty
new←{}
foreachrule inKB do
(p
1
∧...∧ pn ⇒ q)←STANDARDIZE-VARIABLES(rule)
foreachθsuchthatSUBST(θ,p
1
∧ ... ∧ pn)=SUBST(θ,p
1
(cid:5) ∧ ... ∧ p
n
(cid:5))
forsomep(cid:5),...,p(cid:5) inKB
1 n
q(cid:5)←SUBST(θ,q)
ifq(cid:5) doesnotunifywithsomesentencealreadyinKB ornew then
addq(cid:5)tonew
φ←UNIFY(q(cid:5),α)
ifφisnotfail thenreturnφ
addnew toKB
returnfalse
Figure 9.3 A conceptually straightforward, but very inefficient, forward-chainingalgo-
rithm. Oneachiteration,itaddsto KB alltheatomicsentencesthatcanbeinferredinone
step fromthe implicationsentencesandtheatomicsentencesalreadyin KB. Thefunction
STANDARDIZE-VARIABLES replacesallvariablesinitsargumentswithnewonesthathave
notbeenusedbefore.
Criminal(West)
Weapon(M) Sells(West,M,Nono) Hostile(Nono)
1 1
American(West) Missile(M) Owns(Nono,M) Enemy(Nono,America)
1 1
Figure9.4 Theprooftreegeneratedbyforwardchainingonthecrimeexample.Theinitial
factsappearatthe bottomlevel, factsinferredon the firstiterationin the middlelevel, and
factsinferredontheseconditerationatthetoplevel.
possible factsthatcanbeadded, whichdetermines themaximumnumberofiterations. Letk
bethemaximumarity(numberofarguments)ofanypredicate, pbethenumberofpredicates,
and n be the number of constant symbols. Clearly, there can be no more than pnk distinct
groundfacts,soafterthismanyiterationsthealgorithmmusthavereachedafixedpoint. Then
wecanmakeanargumentverysimilartotheproofofcompletenessforpropositionalforward
Section9.3. ForwardChaining 333
chaining. (See page 258.) The details of how to make the transition from propositional to
first-ordercompleteness aregivenfortheresolution algorithm inSection9.5.
For general definite clauses with function symbols, FOL-FC-ASK can generate in-
finitely many new facts, so we need to be more careful. Forthe case in which an answer to
the query sentence q is entailed, we must appeal to Herbrand’s theorem to establish that the
algorithm will find a proof. (See Section 9.5 for the resolution case.) If the query has no
answer, the algorithm could fail to terminate in some cases. For example, if the knowledge
baseincludes thePeanoaxioms
NatNum(0)
∀n NatNum(n) ⇒ NatNum(S(n)),
then forward chaining adds NatNum(S(0)), NatNum(S(S(0))), NatNum(S(S(S(0)))),
and so on. This problem is unavoidable in general. As with general first-order logic, entail-
mentwithdefiniteclauses issemidecidable.
9.3.3 Efficient forwardchaining
The forward-chaining algorithm in Figure 9.3 is designed for ease of understanding rather
than for efficiency of operation. There are three possible sources of inefficiency. First, the
“inner loop” of the algorithm involves finding all possible unifiers such that the premise of
arule unifies with asuitable set offacts in the knowledge base. This is often called pattern
matching and can be very expensive. Second, the algorithm rechecks every rule on every
PATTERNMATCHING
iteration to see whether its premises are satisfied, even ifvery few additions are made to the
knowledge base on each iteration. Finally, the algorithm might generate many facts that are
irrelevant tothegoal. Weaddresseachoftheseissuesinturn.
Matchingrulesagainstknownfacts
Theproblemofmatchingthepremiseofaruleagainstthefactsintheknowledgebasemight
seemsimpleenough. Forexample,suppose wewanttoapplytherule
Missile(x) ⇒ Weapon(x).
ThenweneedtofindallthefactsthatunifywithMissile(x);inasuitablyindexedknowledge
base,thiscanbedoneinconstant timeperfact. Nowconsider arulesuchas
Missile(x)∧Owns(Nono,x) ⇒ Sells(West,x,Nono).
Again,wecanfindalltheobjects ownedbyNonoinconstant timeperobject; then, foreach
object, wecould check whether it isa missile. If the knowledge base contains manyobjects
ownedbyNonoandveryfewmissiles,however,itwouldbebettertofindallthemissilesfirst
CONJUNCT and then check whether they are owned by Nono. This is the conjunct ordering problem:
ORDERING
findanorderingtosolvetheconjuncts oftherulepremisesothatthetotalcostisminimized.
It turns out that finding the optimal ordering is NP-hard, but good heuristics are available.
Forexample, the minimum-remaining-values (MRV)heuristic used forCSPsin Chapter6
would suggest ordering the conjuncts to look formissiles first if fewer missiles than objects
areownedbyNono.
334 Chapter 9. Inference inFirst-OrderLogic
NT Diff(wa,nt)∧Diff(wa,sa)∧
Q
Diff(nt,q)∧Diff(nt,sa)∧
WA
Diff(q,nsw)∧Diff(q,sa)∧
Diff(nsw,v)∧Diff(nsw,sa)∧
SA NSW
Diff(v,sa) ⇒ Colorable()
V Diff(Red,Blue) Diff(Red,Green)
Diff(Green,Red)Diff(Green,Blue)
Diff(Blue,Red) Diff(Blue,Green)
T
(a) (b)
Figure9.5 (a)ConstraintgraphforcoloringthemapofAustralia. (b)Themap-coloring
CSPexpressedasasingledefiniteclause.Eachmapregionisrepresentedasavariablewhose
valuecanbeoneoftheconstantsRed,Green orBlue.
The connection between pattern matching and constraint satisfaction is actually very
close. We can view each conjunct as a constraint on the variables that it contains—for ex-
ample, Missile(x) is a unary constraint on x. Extending this idea, we can express every
finite-domain CSP as a single definite clause together with some associated ground facts.
Considerthemap-coloringproblemfromFigure6.1,shownagaininFigure9.5(a). Anequiv-
alentformulation asasingledefiniteclauseisgiveninFigure9.5(b). Clearly, theconclusion
Colorable()canbeinferred onlyiftheCSPhasasolution. BecauseCSPsingeneralinclude
3-SAT problems as special cases, wecan conclude that matching a definite clause against a
setoffactsisNP-hard.
ItmightseemratherdepressingthatforwardchaininghasanNP-hardmatchingproblem
initsinnerloop. Therearethreewaystocheerourselvesup:
• We can remind ourselves that most rules in real-world knowledge bases are small and
simple (like the rules in our crime example) rather than large and complex (like the
CSP formulation in Figure 9.5). It is common in the database world to assume that
both the sizes of rules and the arities of predicates are bounded by a constant and to
worry only about data complexity—that is, the complexity of inference as a function
DATACOMPLEXITY
of the number of ground facts in the knowledge base. It is easy to show that the data
complexityofforwardchaining ispolynomial.
• We can consider subclasses of rules for which matching is efficient. Essentially every
Datalog clause can be viewed as defining a CSP, so matching will be tractable just
whenthecorresponding CSPistractable. Chapter6describesseveraltractablefamilies
of CSPs. For example, if the constraint graph (the graph whose nodes are variables
and whose links are constraints) forms a tree, then the CSP can be solved in linear
time. Exactlythesameresultholdsforrulematching. Forinstance,ifweremoveSouth
Section9.3. ForwardChaining 335
AustraliafromthemapinFigure9.5,theresultingclauseis
Diff(wa,nt)∧Diff(nt,q)∧Diff(q,nsw)∧Diff(nsw,v) ⇒ Colorable()
whichcorresponds tothe reduced CSPshown inFigure 6.12 onpage 224. Algorithms
forsolvingtree-structuredCSPscanbeapplieddirectlytotheproblemofrulematching.
• We can try to to eliminate redundant rule-matching attempts in the forward-chaining
algorithm, asdescribed next.
Incrementalforwardchaining
Whenweshowedhow forward chaining worksonthecrimeexample, wecheated; inpartic-
ular, we omitted some of the rule matching done by the algorithm shown in Figure 9.3. For
example,ontheseconditeration, therule
Missile(x) ⇒ Weapon(x)
matchesagainstMissile(M )(again), andofcourse theconclusion Weapon(M )isalready
1 1
known so nothing happens. Such redundant rule matching can be avoided if we make the
following observation: Every new fact inferred on iteration t must be derived from at least
one new fact inferred on iteration t − 1. This is true because any inference that does not
requireanewfactfromiteration t−1couldhavebeendoneatiteration t−1already.
This observation leads naturally to an incremental forward-chaining algorithm where,
atiterationt,wecheckaruleonlyifitspremiseincludesaconjunct p thatunifieswithafact
i
p (cid:2) newlyinferredatiteration t−1. Therule-matching stepthenfixesp tomatchwithp (cid:2) ,but
i i i
allows the other conjuncts of the rule to match with facts from any previous iteration. This
algorithmgeneratesexactlythesamefactsateachiterationasthealgorithminFigure9.3,but
ismuchmoreefficient.
With suitable indexing, it is easy to identify all the rules that can be triggered by any
givenfact,andindeedmanyrealsystemsoperateinan“update”modewhereinforwardchain-
ing occurs in response to each new fact that is TELLed to the system. Inferences cascade
through thesetofrulesuntilthefixedpointisreached, andthentheprocess beginsagainfor
thenextnewfact.
Typically,onlyasmallfractionoftherulesintheknowledgebaseareactuallytriggered
by the addition of a given fact. This means that a great deal of redundant work is done in
repeatedly constructing partial matches that have some unsatisfied premises. Our crime ex-
ampleisrathertoosmalltoshowthiseffectively,butnoticethatapartialmatchisconstructed
onthefirstiterationbetweentherule
American(x)∧Weapon(y)∧Sells(x,y,z)∧Hostile(z) ⇒ Criminal(x)
andthefactAmerican(West). Thispartialmatchisthendiscardedandrebuiltonthesecond
iteration (when the rule succeeds). It would be better to retain and gradually complete the
partialmatchesasnewfactsarrive,ratherthandiscarding them.
The rete algorithm3 was the first to address this problem. The algorithm preprocesses
RETE
the set of rules in the knowledge base to construct a sort of dataflow network in which each
3 ReteisLatinfornet.TheEnglishpronunciationrhymeswithtreaty.
336 Chapter 9. Inference inFirst-OrderLogic
node is a literal from a rule premise. Variable bindings flow through the network and are
filtered out when they fail to match a literal. If two literals in a rule share a variable—for
example, Sells(x,y,z) ∧ Hostile(z) in the crime example—then the bindings from each
literal are filtered through an equality node. A variable binding reaching a node for an n-
ary literal such as Sells(x,y,z) might have towait forbindings forthe other variables tobe
established before the process can continue. At any given point, the state of a rete network
captures allthepartialmatchesoftherules,avoiding agreatdealofrecomputation.
Rete networks, and various improvements thereon, have been a key component of so-
PRODUCTION called production systems, which were among the earliest forward-chaining systems in
SYSTEM
widespread use.4 The XCON system (originally called R1; McDermott, 1982) was built
withaproduction-system architecture. XCONcontainedseveralthousand rulesfordesigning
configurationsofcomputercomponentsforcustomersoftheDigitalEquipmentCorporation.
It was one of the first clear commercial successes in the emerging field of expert systems.
Manyothersimilarsystemshavebeenbuiltwiththesameunderlying technology, whichhas
beenimplemented inthegeneral-purpose language OPS-5.
COGNITIVE Productionsystemsarealsopopularincognitivearchitectures—that is,modelsofhu-
ARCHITECTURES
manreasoning—such asACT (Anderson, 1983)and SOAR (Lairdetal.,1987). Insuchsys-
tems, the “working memory” of the system models human short-term memory, and the pro-
ductionsarepartoflong-termmemory. Oneachcycleofoperation, productions arematched
againsttheworkingmemoryoffacts. Aproduction whoseconditions aresatisfiedcanaddor
delete facts in working memory. In contrast to the typical situation in databases, production
systems often have many rules and relatively few facts. With suitably optimized matching
technology, somemodernsystemscanoperateinrealtimewithtensofmillionsofrules.
Irrelevant facts
The final source of inefficiency in forward chaining appears to be intrinsic to the approach
andalsoarisesinthepropositional context. Forwardchainingmakesallallowableinferences
basedontheknownfacts,eveniftheyareirrelevanttothegoalathand. Inourcrimeexample,
therewerenorulescapableofdrawingirrelevantconclusions, sothelackofdirectednesswas
notaproblem. Inothercases(e.g.,ifmanyrulesdescribetheeatinghabitsofAmericansand
thepricesofmissiles), FOL-FC-ASK willgenerate manyirrelevant conclusions.
One way to avoid drawing irrelevant conclusions is to use backward chaining, as de-
scribedinSection9.4. Anothersolutionistorestrictforwardchainingtoaselectedsubsetof
rules, as in PL-FC-ENTAILS? (page 258). A third approach has emerged in the field of de-
DEDUCTIVE ductive databases, which are large-scale databases, like relational databases, but which use
DATABASES
forwardchainingasthestandardinferencetoolratherthan SQLqueries. Theideaistorewrite
the rule set, using information from the goal, so that only relevant variable bindings—those
belongingtoaso-calledmagicset—areconsideredduringforwardinference. Forexample,if
MAGICSET
thegoalisCriminal(West),therulethatconcludesCriminal(x)willberewrittentoinclude
anextraconjunctthatconstrains thevalueof x:
Magic(x)∧American(x)∧Weapon(y)∧Sells(x,y,z)∧Hostile(z) ⇒ Criminal(x).
4 Thewordproductioninproductionsystemsdenotesacondition–actionrule.
Section9.4. BackwardChaining 337
The fact Magic(West) is also added to the KB. In this way, even if the knowledge base
contains factsaboutmillionsofAmericans, onlyColonelWestwillbeconsidered during the
forward inference process. The complete process for defining magic sets and rewriting the
knowledge base is too complex to go into here, but the basic idea is to perform a sort of
“generic” backward inference from the goal in order to work out which variable bindings
need to be constrained. The magic sets approach can therefore be thought of as a kind of
hybridbetweenforwardinference andbackwardpreprocessing.
9.4 BACKWARD CHAINING
The second major family of logical inference algorithms uses the backward chaining ap-
proach introduced inSection7.5fordefiniteclauses. These algorithms workbackward from
the goal, chaining through rules to find known facts that support the proof. We describe
thebasicalgorithm, andthenwedescribehowitisusedinlogicprogramming,whichisthe
mostwidelyusedformofautomatedreasoning. Wealsoseethatbackwardchaininghassome
disadvantages compared withforward chaining, and welook atwaysto overcome them. Fi-
nally,welookatthecloseconnectionbetweenlogicprogrammingandconstraintsatisfaction
problems.
9.4.1 A backward-chaining algorithm
Figure 9.6 shows a backward-chaining algorithm for definite clauses. FOL-BC-ASK(KB,
goal)willbe proved iftheknowledge base contains aclause of the form lhs ⇒ goal, where
lhs (left-handside)isalistofconjuncts. AnatomicfactlikeAmerican(West)isconsidered
asaclause whoselhs istheemptylist. Nowaquery thatcontains variables mightbeproved
in multiple ways. For example, the query Person(x) could be proved with the substitution
GENERATOR
{x/John}aswellaswith{x/Richard}. SoweimplementFOL-BC-ASK asagenerator—
afunction thatreturnsmultipletimes,eachtimegivingone possible result.
Backward chaining is a kind of AND/OR search—the OR part because the goal query
canbeprovedbyanyruleintheknowledgebase,andthe ANDpartbecausealltheconjuncts
inthelhs ofaclausemustbeproved. FOL-BC-OR worksbyfetchingallclauses thatmight
unify with the goal, standardizing the variables in the clause to be brand-new variables, and
then, if the rhs of the clause does indeed unify with the goal, proving every conjunct in the
lhs, using FOL-BC-AND. That function in turn works by proving each of the conjuncts in
turn, keeping track oftheaccumulated substitution aswego. Figure 9.7isthe proof tree for
deriving Criminal(West)fromsentences (9.3)through(9.10).
Backward chaining, as we have written it, is clearly a depth-first search algorithm.
This means that its space requirements are linear in the size of the proof (neglecting, for
now, the space required to accumulate the solutions). It also means that backward chaining
(unlikeforwardchaining)suffersfromproblemswithrepeatedstatesandincompleteness. We
will discuss these problems and some potential solutions, but first we show how backward
chaining isusedinlogicprogrammingsystems.
338 Chapter 9. Inference inFirst-OrderLogic
functionFOL-BC-ASK(KB,query)returnsageneratorofsubstitutions
returnFOL-BC-OR(KB,query,{})
generatorFOL-BC-OR(KB,goal,θ)yieldsasubstitution
foreachrule(lhs ⇒ rhs)inFETCH-RULES-FOR-GOAL(KB,goal)do
(lhs,rhs)←STANDARDIZE-VARIABLES((lhs,rhs))
foreachθ(cid:5)inFOL-BC-AND(KB,lhs,UNIFY(rhs,goal,θ))do
yieldθ(cid:5)
generatorFOL-BC-AND(KB,goals,θ)yieldsasubstitution
ifθ = failure thenreturn
elseifLENGTH(goals)=0thenyieldθ
elsedo
first,rest←FIRST(goals),REST(goals)
foreachθ(cid:5)inFOL-BC-OR(KB, SUBST(θ,first),θ)do
foreachθ(cid:5)(cid:5)inFOL-BC-AND(KB,rest,θ(cid:5))do
yieldθ(cid:5)(cid:5)
Figure9.6 Asimplebackward-chainingalgorithmforfirst-orderknowledgebases.
Criminal(West)
American(West) Weapon(y) Sells(West,M ,z) Hostile(Nono)
1
{ } {z/Nono}
Missile(y) Missile(M ) Owns(Nono,M ) Enemy(Nono,America)
1 1
{y/M 1 } { } { } { }
Figure9.7 ProoftreeconstructedbybackwardchainingtoprovethatWestisacriminal.
Thetreeshouldbereaddepthfirst,lefttoright.ToproveCriminal(West),wehavetoprove
the four conjuncts below it. Some of these are in the knowledge base, and others require
furtherbackward chaining. Bindings for each successful unification are shown next to the
correspondingsubgoal.Notethatonceonesubgoalinaconjunctionsucceeds,itssubstitution
isappliedtosubsequentsubgoals.Thus,bythetimeFOL-BC-ASKgetstothelastconjunct,
originallyHostile(z),zisalreadyboundtoNono.
Section9.4. BackwardChaining 339
9.4.2 Logicprogramming
Logic programming is a technology that comes fairly close to embodying the declarative
idealdescribedinChapter7: thatsystemsshouldbeconstructed byexpressingknowledgein
aformallanguageandthatproblemsshouldbesolvedbyrunninginferenceprocessesonthat
knowledge. TheidealissummedupinRobertKowalski’sequation,
Algorithm = Logic +Control .
Prolog isthemostwidelyused logicprogramming language. Itisused primarily asarapid-
PROLOG
prototypinglanguageandforsymbol-manipulationtaskssuchaswritingcompilers(VanRoy,
1990) and parsing natural language (Pereira and Warren, 1980). Many expert systems have
beenwritteninPrologforlegal,medical, financial,andotherdomains.
Prolog programs are sets of definite clauses written in a notation somewhat different
from standard first-order logic. Prologuses uppercase letters forvariables and lowercase for
constants—the opposite ofourconvention forlogic. Commasseparate conjuncts inaclause,
and theclause iswritten“backwards” from whatweareused to; instead ofA∧B ⇒ C in
PrologwehaveC :- A, B.Hereisatypicalexample:
criminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).
The notation [E|L] denotes a list whose first element is E and whose rest is L. Here is a
Prolog program for append(X,Y,Z),which succeeds if list Z is the result of appending
listsXandY:
append([],Y,Y).
append([A|X],Y,[A|Z]) :- append(X,Y,Z).
In English, we can read these clauses as (1) appending an empty list with a list Y produces
the same list Y and (2) [A|Z]is the result of appending [A|X] onto Y, provided that Z is
the result ofappending Xonto Y.Inmost high-level languages wecan writeasimilar recur-
sive function that describes how to append two lists. The Prolog definition is actually much
more powerful, however, because it describes a relation that holds among three arguments,
rather than a function computed from two arguments. For example, we can ask the query
append(X,Y,[1,2]): whattwo lists can be appended to give [1,2]? Weget back the
solutions
X=[] Y=[1,2];
X=[1] Y=[2];
X=[1,2] Y=[]
The execution of Prolog programs is done through depth-first backward chaining, where
clauses are tried inthe order inwhich theyare written in the knowledge base. Someaspects
ofPrologfalloutsidestandard logicalinference:
• Prolog uses the database semantics of Section 8.2.8 rather than first-order semantics,
andthisisapparent initstreatmentofequality andnegation(seeSection9.4.5).
• Thereisasetofbuilt-in functions forarithmetic. Literalsusingthesefunction symbols
are “proved” by executing code rather than doing further inference. For example, the
340 Chapter 9. Inference inFirst-OrderLogic
goal“Xis4+3”succeedswithXboundto7. Ontheotherhand,thegoal“5isX+Y”
fails,becausethebuilt-in functions donotdoarbitrary equationsolving.5
• Therearebuilt-inpredicatesthathavesideeffectswhenexecuted. Theseincludeinput–
output predicates and the assert/retractpredicates formodifying theknowledge
base. Suchpredicateshavenocounterpartinlogicandcanproduceconfusingresults—
forexample,iffactsareassertedinabranchoftheprooftreethateventually fails.
• TheoccurcheckisomittedfromProlog’sunificationalgorithm. Thismeansthatsome
unsoundinferences canbemade;thesearealmostneveraprobleminpractice.
• Prologusesdepth-first backward-chaining search withnochecks forinfiniterecursion.
Thismakes itvery fast when given theright setof axioms, but incomplete when given
thewrongones.
Prolog’sdesignrepresentsacompromisebetweendeclarativenessandexecutionefficiency—
inasmuchasefficiencywasunderstood atthetimePrologwasdesigned.
9.4.3 Efficient implementationoflogicprograms
The execution of a Prolog program can happen in two modes: interpreted and compiled.
Interpretation essentially amounts to running the FOL-BC-ASK algorithm from Figure 9.6,
with the program as the knowledge base. We say “essentially” because Prolog interpreters
containavarietyofimprovementsdesigned tomaximizespeed. Hereweconsideronlytwo.
First, our implementation had to explicitly manage the iteration over possible results
generated by each of the subfunctions. Prolog interpreters have a global data structure,
a stack of choice points, to keep track of the multiple possibilities that we considered in
CHOICEPOINT
FOL-BC-OR. This global stack is more efficient, and it makes debugging easier, because
thedebuggercanmoveupanddownthestack.
Second,oursimpleimplementationofFOL-BC-ASKspendsagooddealoftimegener-
atingsubstitutions. Insteadofexplicitlyconstructingsubstitutions, Prologhaslogicvariables
that remember their current binding. At any point in time, every variable in the program ei-
ther is unbound or is bound to some value. Together, these variables and values implicitly
define the substitution for the current branch of the proof. Extending the path can only add
new variable bindings, because an attempt to add a different binding for an already bound
variable results in a failure of unification. When a path in the search fails, Prolog will back
uptoaprevious choice point, andthen itmight have tounbind somevariables. Thisisdone
bykeepingtrackofallthevariables thathavebeenboundinastackcalledthetrail. Aseach
TRAIL
newvariableisboundbyUNIFY-VAR,thevariableispushedontothetrail. Whenagoalfails
and it is time to back up to a previous choice point, each of the variables is unbound as it is
removedfromthetrail.
Even the most efficient Prolog interpreters require several thousand machine instruc-
tions per inference step because of the cost of index lookup, unification, and building the
recursive call stack. In effect, the interpreter always behaves as if it has never seen the pro-
gram before; for example, it has to find clauses that match the goal. A compiled Prolog
5 NotethatifthePeanoaxiomsareprovided,suchgoalscanbesolvedbyinferencewithinaPrologprogram.
Section9.4. BackwardChaining 341
procedureAPPEND(ax,y,az,continuation)
trail←GLOBAL-TRAIL-POINTER()
ifax =[]andUNIFY(y,az)thenCALL(continuation)
RESET-TRAIL(trail)
a,x,z←NEW-VARIABLE(),NEW-VARIABLE(),NEW-VARIABLE()
ifUNIFY(ax,[a |x])andUNIFY(az,[a |z])thenAPPEND(x,y,z,continuation)
Figure9.8 Pseudocoderepresentingtheresultofcompilingthe Appendpredicate. The
functionNEW-VARIABLEreturnsanewvariable,distinctfromallothervariablesusedsofar.
TheprocedureCALL(continuation)continuesexecutionwiththespecifiedcontinuation.
program,ontheotherhand,isaninferenceprocedureforaspecificsetofclauses,soitknows
whatclauses matchthegoal. Prologbasically generates aminiature theorem proverforeach
differentpredicate, therebyeliminatingmuchoftheoverheadofinterpretation. Itisalsopos-
sible to open-code the unification routine for each different call, thereby avoiding explicit
OPEN-CODE
analysis oftermstructure. (Fordetailsofopen-coded unification, seeWarren etal.(1977).)
The instruction sets of today’s computers give a poor match with Prolog’s semantics,
somostPrologcompilerscompileintoanintermediatelanguageratherthandirectlyintoma-
chine language. The most popular intermediate language is the Warren Abstract Machine,
orWAM,named afterDavidH.D.Warren, oneoftheimplementers ofthefirstPrologcom-
piler. The WAM is an abstract instruction set that is suitable for Prolog and can be either
interpretedortranslatedintomachinelanguage. OthercompilerstranslatePrologintoahigh-
levellanguagesuchasLisporCandthenusethatlanguage’scompilertotranslatetomachine
language. Forexample,thedefinitionoftheAppendpredicatecanbecompiledintothecode
showninFigure9.8. Severalpointsareworthmentioning:
• Rather than having to search the knowledge base for Appendclauses, the clauses be-
comeaprocedure andtheinferences arecarriedoutsimplyby callingtheprocedure.
• Asdescribedearlier,thecurrentvariablebindingsarekeptonatrail. Thefirststepofthe
proceduresavesthecurrentstateofthetrail,sothatitcan berestoredbyRESET-TRAIL
ifthefirstclausefails. ThiswillundoanybindingsgeneratedbythefirstcalltoUNIFY.
• Thetrickiestpartistheuseofcontinuationstoimplementchoicepoints. Youcanthink
CONTINUATION
of a continuation as packaging up a procedure and a list of arguments that together
define what should be done next whenever the current goal succeeds. It would not
do just to return from a procedure like APPEND when the goal succeeds, because it
could succeed in several ways, and each of them has to be explored. Thecontinuation
argument solves this problem because it can be called each time the goal succeeds. In
the APPEND code, ifthe firstargument is empty and the second argument unifies with
the third, then the APPEND predicate has succeeded. We then CALL the continuation,
with the appropriate bindings on the trail, to do whatever should be done next. For
example, if the call to APPEND were at the top level, the continuation would print the
bindingsofthevariables.
342 Chapter 9. Inference inFirst-OrderLogic
Before Warren’s work on the compilation of inference in Prolog, logic programming was
too slow for general use. Compilers by Warren and others allowed Prolog code to achieve
speeds that are competitive with C on a variety of standard benchmarks (Van Roy, 1990).
Of course, the fact that one can write a planner or natural language parser in a few dozen
linesofPrologmakesitsomewhatmoredesirablethanCforprototypingmostsmall-scaleAI
research projects.
Parallelizationcanalsoprovidesubstantialspeedup. Therearetwoprincipalsourcesof
parallelism. The first, called OR-parallelism, comes from the possibility of agoal unifying
OR-PARALLELISM
withmanydifferentclausesintheknowledgebase. Eachgivesrisetoanindependent branch
in the search space that can lead to a potential solution, and all such branches can be solved
in parallel. The second, called AND-parallelism, comes from the possibility of solving
AND-PARALLELISM
each conjunct inthebody ofanimplication inparallel. AND-parallelism ismoredifficult to
achieve, because solutions for the whole conjunction require consistent bindings for all the
variables. Each conjunctive branch must communicate with the other branches to ensure a
globalsolution.
9.4.4 Redundant inference andinfinite loops
We now turn to the Achilles heel of Prolog: the mismatch between depth-first search and
search trees thatinclude repeated states andinfinite paths. Considerthefollowing logicpro-
gramthatdecidesifapathexistsbetweentwopointsonadirectedgraph:
path(X,Z) :- link(X,Z).
path(X,Z) :- path(X,Y), link(Y,Z).
Asimple three-node graph, described bythefacts link(a,b)and link(b,c),isshown
in Figure 9.9(a). With this program, the query path(a,c)generates the proof tree shown
inFigure9.10(a). Ontheotherhand,ifweputthetwoclauses intheorder
path(X,Z) :- path(X,Y), link(Y,Z).
path(X,Z) :- link(X,Z).
thenPrologfollowstheinfinitepathshowninFigure9.10(b). Prologistherefore incomplete
asatheoremproverfordefiniteclauses—evenforDatalogprograms,asthisexampleshows—
because, for some knowledge bases, it fails to prove sentences that are entailed. Notice that
forward chaining does not suffer from this problem: once path(a,b),path(b,c),and
path(a,c)areinferred, forwardchaining halts.
Depth-first backward chaining also has problems with redundant computations. For
example,whenfindingapathfromA toJ inFigure9.9(b),Prologperforms877inferences,
1 4
mostofwhichinvolvefindingallpossiblepathstonodesfromwhichthegoalisunreachable.
This is similar to the repeated-state problem discussed in Chapter 3. The total amount of
inference can be exponential in the number of ground facts that are generated. If we apply
forward chaining instead, at most n2 path(X,Y)facts can be generated linking n nodes.
Fortheproblem inFigure9.9(b),only62inferences areneeded.
DYNAMIC Forwardchainingongraphsearchproblemsisanexampleof dynamicprogramming,
PROGRAMMING
in which the solutions to subproblems are constructed incrementally from those of smaller
Section9.4. BackwardChaining 343
A
1
A B C
J
4
(a) (b)
Figure 9.9 (a) Finding a path from A to C can lead Prolog into an infinite loop. (b) A
graphinwhicheachnodeisconnectedtotworandomsuccessorsinthenextlayer.Findinga
pathfromA toJ requires877inferences.
1 4
path(a,c)
path(a,c)
path(a,Y) link(Y,c)
link(a,c) path(a,Y) link(b,c)
fail { }
path(a,Y’) link(Y’,Y)
link(a,Y)
{Y /b}
(a) (b)
Figure 9.10 (a) Proof that a path exists from A to C. (b) Infinite proof tree generated
whentheclausesareinthe“wrong”order.
subproblems and are cached to avoid recomputation. We can obtain a similar effect in a
backward chaining system using memoization—that is, caching solutions to subgoals as
they are found and then reusing those solutions when the subgoal recurs, rather than repeat-
TABLEDLOGIC ingthepreviouscomputation. Thisistheapproachtakenbytabledlogicprogrammingsys-
PROGRAMMING
tems, which use efficient storage and retrieval mechanisms to perform memoization. Tabled
logicprogramming combines thegoal-directedness ofbackward chaining withthedynamic-
programming efficiency of forward chaining. It is also complete for Datalog knowledge
bases, whichmeansthattheprogrammerneedworrylessabout infiniteloops. (Itisstillpos-
sible to get an infinite loop with predicates like father(X,Y) that refer to a potentially
unbounded numberofobjects.)
9.4.5 Databasesemantics ofProlog
Prologusesdatabasesemantics,asdiscussedinSection8.2.8. Theuniquenamesassumption
says that every Prolog constant and every ground term refers to a distinct object, and the
closed worldassumption says that the only sentences thatare true arethose that areentailed
344 Chapter 9. Inference inFirst-OrderLogic
bytheknowledgebase. ThereisnowaytoassertthatasentenceisfalseinProlog. Thismakes
Prologlessexpressivethanfirst-orderlogic,butitispart ofwhatmakesPrologmoreefficient
andmoreconcise. ConsiderthefollowingPrologassertions aboutsomecourseofferings:
Course(CS,101), Course(CS,102), Course(CS,106), Course(EE,101). (9.11)
Under the unique names assumption, CS and EE are different (as are 101, 102, and 106),
so this means that there are four distinct courses. Under the closed-world assumption there
are no other courses, so there are exactly four courses. But if these were assertions in FOL
rather than in Prolog, then all we could say is that there are somewhere between one and
infinitycourses. That’sbecause theassertions (inFOL)donotdenythepossibility thatother
unmentionedcoursesarealsooffered,nordotheysaythatthecoursesmentionedaredifferent
fromeachother. Ifwewantedtotranslate Equation(9.11)intoFOL,wewouldgetthis:
Course(d,n) ⇔ (d=CS ∧n = 101)∨(d=CS ∧n = 102)
∨(d=CS ∧n = 106)∨(d=EE ∧n = 101). (9.12)
Thisis called the completion ofEquation (9.11). Itexpresses inFOLthe idea that there are
COMPLETION
atmostfourcourses. Toexpress inFOLtheideathatthereare atleastfourcourses, weneed
towritethecompletion oftheequality predicate:
x= y ⇔ (x = CS ∧y = CS)∨(x= EE ∧y = EE)∨(x = 101∧y = 101)
∨(x= 102∧y = 102)∨(x = 106∧y = 106).
Thecompletion isusefulforunderstanding database semantics, butforpractical purposes, if
your problem can be described with database semantics, it is more efficient to reason with
Prolog or some other database semantics system, rather than translating into FOL and rea-
soningwithafullFOLtheorem prover.
9.4.6 Constraintlogicprogramming
In our discussion of forward chaining (Section 9.3), we showed how constraint satisfaction
problems (CSPs) can be encoded as definite clauses. Standard Prolog solves such problems
inexactlythesamewayasthebacktracking algorithm giveninFigure6.5.
Becausebacktracking enumeratesthedomainsofthevariables,itworksonlyforfinite-
domain CSPs. In Prolog terms, there must be a finite number of solutions for any goal
withunbound variables. (Forexample, thegoal diff(Q,SA),whichsaysthat Queensland
and South Australia must be different colors, has six solutions if three colors are allowed.)
Infinite-domainCSPs—forexample,withintegerorreal-valuedvariables—require quitedif-
ferentalgorithms, suchasboundspropagation orlinearprogramming.
Consider the following example. We define triangle(X,Y,Z)as a predicate that
holdsifthethreearguments arenumbersthatsatisfythetriangleinequality:
triangle(X,Y,Z) :-
X>0, Y>0, Z>0, X+Y>=Z, Y+Z>=X, X+Z>=Y.
If we ask Prolog the query triangle(3,4,5), it succeeds. On the other hand, if we
ask triangle(3,4,Z),no solution will be found, because the subgoal Z>=0 cannot be
handled byProlog;wecan’tcompareanunbound valueto0.
Section9.5. Resolution 345
CONSTRAINTLOGIC Constraint logic programming (CLP) allows variables to be constrained rather than
PROGRAMMING
bound. ACLPsolution isthemostspecific setofconstraints onthe query variables that can
bederivedfromtheknowledge base. Forexample,thesolution tothetriangle(3,4,Z)
query is the constraint 7 >= Z >= 1. Standard logic programs are just a special case of
CLPinwhichthesolutionconstraints mustbeequality constraints—that is,bindings.
CLP systems incorporate various constraint-solving algorithms for the constraints al-
lowed in the language. For example, a system that allows linear inequalities on real-valued
variables might include a linear programming algorithm for solving those constraints. CLP
systems also adopt a much more flexible approach to solving standard logic programming
queries. Forexample, instead ofdepth-first, left-to-right backtracking, theymightuseanyof
the more efficient algorithms discussed in Chapter 6, including heuristic conjunct ordering,
backjumping, cutset conditioning, and so on. CLP systems therefore combine elements of
constraint satisfaction algorithms, logicprogramming, anddeductive databases.
Several systems that allow the programmer more control over the search order for in-
ferencehavebeendefined. TheMRS language(Genesereth andSmith,1981;Russell,1985)
allows the programmer to write metarules to determine which conjuncts are tried first. The
METARULE
user could write a rule saying that the goal with the fewest variables should be tried first or
couldwritedomain-specific rulesforparticularpredicates.
9.5 RESOLUTION
Thelastofourthreefamiliesoflogicalsystemsisbasedonresolution. Wesawonpage250
that propositional resolution using refutation is a complete inference procedure for proposi-
tionallogic. Inthissection, wedescribehowtoextendresolution tofirst-orderlogic.
9.5.1 Conjunctive normal form forfirst-order logic
As in the propositional case, first-order resolution requires that sentences be in conjunctive
normalform(CNF)—thatis,aconjunction ofclauses, whereeachclauseisadisjunction of
literals.6 Literals can contain variables, which are assumed tobe universally quantified. For
example,thesentence
∀x American(x)∧Weapon(y)∧Sells(x,y,z)∧Hostile(z) ⇒ Criminal(x)
becomes, inCNF,
¬American(x)∨¬Weapon(y)∨¬Sells(x,y,z)∨¬Hostile(z)∨Criminal(x).
Every sentence of first-order logic can be converted into an inferentially equivalent CNF
sentence. Inparticular,theCNFsentencewillbeunsatisfiablejustwhentheoriginalsentence
isunsatisfiable, sowehaveabasisfordoingproofsbycontradiction ontheCNFsentences.
6 Aclausecanalsoberepresentedasanimplicationwithaconjunctionofatomsinthepremiseandadisjunction
ofatomsintheconclusion(Exercise7.13).ThisiscalledimplicativenormalformorKowalskiform(especially
whenwrittenwitharight-to-leftimplicationsymbol(Kowalski,1979))andisoftenmucheasiertoread.
346 Chapter 9. Inference inFirst-OrderLogic
TheprocedureforconversiontoCNFissimilartothepropositionalcase,whichwesaw
onpage253. Theprincipaldifferencearisesfromtheneedtoeliminateexistentialquantifiers.
We illustrate the procedure by translating the sentence “Everyone who loves all animals is
lovedbysomeone,”or
∀x [∀y Animal(y) ⇒ Loves(x,y)] ⇒ [∃y Loves(y,x)].
Thestepsareasfollows:
• Eliminateimplications:
∀x [¬∀y ¬Animal(y)∨Loves(x,y)]∨[∃y Loves(y,x)].
• Move¬inwards: Inaddition totheusual rulesfornegatedconnectives, weneedrules
fornegatedquantifiers. Thus,wehave
¬∀x p becomes ∃x ¬p
¬∃x p becomes ∀x ¬p.
Oursentencegoesthroughthefollowingtransformations:
∀x [∃y ¬(¬Animal(y)∨Loves(x,y))]∨[∃y Loves(y,x)].
∀x [∃y ¬¬Animal(y)∧¬Loves(x,y)]∨[∃y Loves(y,x)].
∀x [∃y Animal(y)∧¬Loves(x,y)]∨[∃y Loves(y,x)].
Notice how a universal quantifier (∀y) in the premise of the implication has become
an existential quantifier. The sentence now reads “Either there is some animal that x
doesn’t love, or (if this is not the case) someone loves x.” Clearly, the meaning of the
originalsentence hasbeenpreserved.
• Standardizevariables: Forsentenceslike(∃xP(x))∨(∃xQ(x))whichusethesame
variable name twice, change the name of one of the variables. This avoids confusion
laterwhenwedropthequantifiers. Thus,wehave
∀x [∃y Animal(y)∧¬Loves(x,y)]∨[∃z Loves(z,x)].
• Skolemize: Skolemization is the process of removing existential quantifiers by elimi-
SKOLEMIZATION
nation. Inthesimplecase,itisjustliketheExistentialInstantiation ruleofSection9.1:
translate∃x P(x)intoP(A),whereAisanewconstant. However,wecan’tapplyEx-
istential Instantiation tooursentence abovebecause itdoesn’t matchthepattern ∃v α;
only parts of the sentence match the pattern. If we blindly apply the rule to the two
matchingpartsweget
∀x [Animal(A)∧¬Loves(x,A)]∨Loves(B,x),
which has the wrong meaning entirely: it says that everyone either fails to love a par-
ticular animal Aoris loved by some particular entity B. In fact, our original sentence
allowseachpersontofailtoloveadifferentanimalortobelovedbyadifferentperson.
Thus,wewanttheSkolementitiestodepend onxandz:
∀x [Animal(F(x))∧¬Loves(x,F(x))]∨Loves(G(z),x).
Here F and G are Skolem functions. The general rule is that the arguments of the
SKOLEMFUNCTION
Skolem function are all the universally quantified variables in whose scope the exis-
tentialquantifierappears. AswithExistential Instantiation, theSkolemized sentence is
satisfiableexactlywhentheoriginal sentence issatisfiable.
Section9.5. Resolution 347
• Dropuniversal quantifiers: Atthis point, all remaining variables mustbe universally
quantified. Moreover, thesentence isequivalent tooneinwhichalltheuniversal quan-
tifiershavebeenmovedtotheleft. Wecantherefore droptheuniversal quantifiers:
[Animal(F(x))∧¬Loves(x,F(x))]∨Loves(G(z),x).
• Distribute∨over∧:
[Animal(F(x))∨Loves(G(z),x)]∧[¬Loves(x,F(x))∨Loves(G(z),x)] .
Thisstepmayalsorequireflatteningoutnestedconjunctions anddisjunctions.
The sentence is now in CNF and consists of two clauses. It is quite unreadable. (It may
help to explain that theSkolem function F(x)refers tothe animal potentially unloved by x,
whereas G(z) refers to someone who might love x.) Fortunately, humans seldom need look
atCNFsentences—the translation processiseasilyautomated.
9.5.2 The resolutioninference rule
Theresolution rule forfirst-order clauses issimply aliftedversion ofthepropositional reso-
lution rule given on page 253. Two clauses, which are assumed to be standardized apart so
that they share no variables, can be resolved if they contain complementary literals. Propo-
sitional literals are complementary if one is the negation of the other; first-order literals are
complementary ifoneunifieswiththenegationoftheother. Thus,wehave
(cid:3) ∨···∨(cid:3) , m ∨···∨m
1 k 1 n
SUBST(θ,(cid:3) 1 ∨···∨(cid:3) i−1 ∨(cid:3) i+1 ∨···∨(cid:3) k ∨m 1 ∨···∨m j−1 ∨m j+1 ∨···∨m n )
where UNIFY((cid:3)
i
,¬m
j
)=θ. Forexample, wecanresolvethetwoclauses
[Animal(F(x))∨Loves(G(x),x)] and [¬Loves(u,v)∨¬Kills(u,v)]
by eliminating the complementary literals Loves(G(x),x) and ¬Loves(u,v), with unifier
θ={u/G(x),v/x}, toproducetheresolvent clause
[Animal(F(x))∨¬Kills(G(x),x)].
This rule is called the binary resolution rule because it resolves exactly two literals. The
BINARYRESOLUTION
binary resolution rulebyitself does notyield acomplete inference procedure. Thefullreso-
lutionruleresolvessubsetsofliteralsineachclausethatareunifiable. Analternativeapproach
is to extend factoring—the removal of redundant literals—to the first-order case. Proposi-
tional factoring reduces two literals to one if they are identical; first-order factoring reduces
twoliterals toone iftheyare unifiable. Theunifiermustbeapplied tothe entire clause. The
combination ofbinaryresolution andfactoring iscomplete.
9.5.3 Exampleproofs
Resolution proves that KB |= α by proving KB ∧¬α unsatisfiable, that is, by deriving the
empty clause. The algorithmic approach is identical to the propositional case, described in
348 Chapter 9. Inference inFirst-OrderLogic
¬American(x) ^ ¬Weapon(y) ^ ¬Sells(x,y,z) ^ ¬Hostile(z) ^ Criminal(x) ¬Criminal(West)
American(West) ¬American(West) ^ ¬Weapon(y) ^ ¬Sells(West,y,z) ^ ¬Hostile(z)
¬Missile(x) ^ Weapon(x) ¬Weapon(y) ^ ¬Sells(West,y,z) ^ ¬Hostile(z)
Missile(M1) ¬Missile(y) ^ ¬Sells(West,y,z) ^ ¬Hostile(z)
¬Missile(x) ^ ¬Owns(Nono,x) ^ Sells(West,x,Nono) ¬Sells(West,M1,z) ^ ¬Hostile(z)
Missile(M1) ¬Missile(M1) ^ ¬Owns(Nono,M1) ^ ¬Hostile(Nono)
Owns(Nono,M1) ¬Owns(Nono,M1) ^ ¬Hostile(Nono)
¬Enemy(x,America) ^ Hostile(x) ¬Hostile(Nono)
Enemy(Nono,America) ¬Enemy(Nono,America)
Figure9.11 AresolutionproofthatWestisacriminal.Ateachstep,theliteralsthatunify
areinbold.
Figure 7.12, soweneed notrepeat ithere. Instead, wegive twoexample proofs. Thefirstis
thecrimeexamplefromSection9.3. Thesentences inCNFare
¬American(x)∨¬Weapon(y)∨¬Sells(x,y,z)∨¬Hostile(z)∨Criminal(x)
¬Missile(x)∨¬Owns(Nono,x)∨Sells(West,x,Nono)
¬Enemy(x,America)∨Hostile(x)
¬Missile(x)∨Weapon(x)
Owns(Nono,M ) Missile(M )
1 1
American(West) Enemy(Nono,America).
Wealso include the negated goal ¬Criminal(West). Theresolution proof is shown in Fig-
ure9.11. Noticethestructure: single“spine”beginningwiththegoalclause,resolvingagainst
clauses from the knowledge base until the empty clause is generated. This is characteristic
of resolution on Horn clause knowledge bases. In fact, the clauses along the main spine
correspond exactly to the consecutive values of the goals variable in the backward-chaining
algorithm of Figure 9.6. This is because we always choose to resolve with a clause whose
positive literal unified with the leftmost literal of the “current” clause on the spine; this is
exactly what happens in backward chaining. Thus, backward chaining is just a special case
ofresolution withaparticularcontrol strategytodecidewhichresolution toperform next.
Oursecond examplemakesuseofSkolemization andinvolves clauses thatarenotdef-
inite clauses. This results in a somewhat more complex proof structure. In English, the
problem isasfollows:
Everyonewholovesallanimalsislovedbysomeone.
Anyonewhokillsananimalislovedbynoone.
Jacklovesallanimals.
EitherJackorCuriositykilledthecat,whoisnamedTuna.
DidCuriositykillthecat?
Section9.5. Resolution 349
First, we express the original sentences, some background knowledge, and the negated goal
Ginfirst-orderlogic:
A. ∀x [∀y Animal(y) ⇒ Loves(x,y)] ⇒ [∃y Loves(y,x)]
B. ∀x [∃z Animal(z)∧Kills(x,z)] ⇒ [∀y ¬Loves(y,x)]
C. ∀x Animal(x) ⇒ Loves(Jack,x)
D. Kills(Jack,Tuna)∨Kills(Curiosity,Tuna)
E. Cat(Tuna)
F. ∀x Cat(x) ⇒ Animal(x)
¬G. ¬Kills(Curiosity,Tuna)
Nowweapplytheconversion procedure toconverteachsentence toCNF:
A1. Animal(F(x))∨Loves(G(x),x)
A2. ¬Loves(x,F(x))∨Loves(G(x),x)
B. ¬Loves(y,x)∨¬Animal(z)∨¬Kills(x,z)
C. ¬Animal(x)∨Loves(Jack,x)
D. Kills(Jack,Tuna)∨Kills(Curiosity,Tuna)
E. Cat(Tuna)
F. ¬Cat(x)∨Animal(x)
¬G. ¬Kills(Curiosity,Tuna)
TheresolutionproofthatCuriositykilledthecatisgiveninFigure9.12. InEnglish,theproof
couldbeparaphrased asfollows:
Suppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus
Jackmusthave. Now,Tunaisacatandcatsareanimals,soTunaisananimal. Because
anyonewhokillsananimalislovedbynoone,weknowthatnoonelovesJack. Onthe
other hand, Jack loves all animals, so someone loves him; so we have a contradiction.
Therefore,Curiositykilledthecat.
Cat(Tuna) ¬Cat(x) ^ Animal(x) Kills(Jack, Tuna) ^ Kills(Curiosity, Tuna) ¬Kills(Curiosity, Tuna)
Animal(Tuna) ¬Loves(y, x) ^ ¬Animal(z) ^ ¬Kills(x, z) Kills(Jack, Tuna) ¬Loves(x,F(x)) ^ Loves(G(x), x) ¬Animal(x) ^ Loves(Jack, x)
^ ^ ^
¬Loves(y, x) ¬Kills(x, Tuna) ¬Animal(F(Jack)) Loves(G(Jack), Jack) Animal(F(x)) Loves(G(x), x)
¬Loves(y, Jack) Loves(G(Jack),Jack)
Figure9.12 A resolutionproofthat Curiositykilled the cat. Notice the use of factoring
in the derivation of the clause Loves(G(Jack),Jack). Notice also in the upper right, the
unificationofLoves(x,F(x))andLoves(Jack,x)canonlysucceedafterthevariableshave
beenstandardizedapart.
350 Chapter 9. Inference inFirst-OrderLogic
Theproofanswersthequestion “DidCuriosity killthecat?” butoftenwewanttoposemore
general questions, such as “Who killed the cat?” Resolution can do this, but it takes a little
more work to obtain the answer. The goal is ∃w Kills(w,Tuna), which, when negated,
becomes¬Kills(w,Tuna)inCNF.RepeatingtheproofinFigure9.12withthenewnegated
goal, we obtain a similar proof tree, but with the substitution {w/Curiosity} in one of the
steps. So, in this case, finding out who killed the cat is just a matter of keeping track of the
bindings forthequeryvariables intheproof.
NONCONSTRUCTIVE Unfortunately, resolution can produce nonconstructive proofs for existential goals.
PROOF
Forexample, ¬Kills(w,Tuna)resolves withKills(Jack,Tuna)∨Kills(Curiosity,Tuna)
to give Kills(Jack,Tuna), which resolves again with ¬Kills(w,Tuna) to yield the empty
clause. Notice that w has two different bindings in this proof; resolution is telling us that,
yes, someone killed Tuna—either Jack or Curiosity. This is no great surprise! One so-
lution is to restrict the allowed resolution steps so that the query variables can be bound
only once in a given proof; then we need to be able to backtrack over the possible bind-
ings. Another solution is to add a special answer literal to the negated goal, which be-
ANSWERLITERAL
comes ¬Kills(w,Tuna) ∨ Answer(w). Now, the resolution process generates an answer
whenever a clause is generated containing just a single answer literal. Forthe proof in Fig-
ure 9.12, this is Answer(Curiosity). The nonconstructive proof would generate the clause
Answer(Curiosity)∨Answer(Jack),whichdoesnotconstitute ananswer.
9.5.4 Completeness ofresolution
Thissection givesacompleteness proof ofresolution. Itcanbesafely skipped bythosewho
arewillingtotakeitonfaith.
REFUTATION Weshowthatresolution isrefutation-complete, whichmeansthatifasetofsentences
COMPLETENESS
is unsatisfiable, then resolution will always be able to derive a contradiction. Resolution
cannot be used to generate all logical consequences of a set of sentences, but it can be used
toestablish that agivensentence isentailed bythesetofsentences. Hence, itcanbeusedto
findallanswerstoagivenquestion, Q(x),byprovingthatKB ∧¬Q(x)isunsatisfiable.
Wetakeitasgiventhatanysentenceinfirst-orderlogic(withoutequality)canberewrit-
ten as a set of clauses in CNF.This can be proved by induction on the form of the sentence,
using atomic sentences as the base case (Davis and Putnam, 1960). Our goal therefore is to
prove the following: if S is an unsatisfiable set of clauses, then the application of a finite
numberofresolution stepstoS willyieldacontradiction.
Our proof sketch follows Robinson’s original proof with some simplifications from
Genesereth andNilsson(1987). Thebasicstructure oftheproof(Figure9.13)isasfollows:
1. First, we observe that if S is unsatisfiable, then there exists a particular set of ground
instancesoftheclausesofSsuchthatthissetisalsounsatisfiable(Herbrand’stheorem).
2. WethenappealtothegroundresolutiontheoremgiveninChapter7,whichstatesthat
propositional resolution iscompleteforgroundsentences.
3. Wethen usealiftinglemmatoshowthat, foranypropositional resolution proof using
the set of ground sentences, there is a corresponding first-order resolution proof using
thefirst-ordersentences fromwhichthegroundsentences wereobtained.
Section9.5. Resolution 351
Any set of sentences S is representable in clausal form
Assume S is unsatisfiable, and in clausal form
Herbrand’s theorem
Some set S' of ground instances is unsatisfiable
Ground resolution
theorem
Resolution can find a contradiction in S'
Lifting lemma
There is a resolution proof for the contradiction in S'
Figure9.13 Structureofacompletenessproofforresolution.
Tocarryoutthefirststep,weneedthreenewconcepts:
HERBRAND • Herbrand universe: If S is a set of clauses, then H , the Herbrand universe of S, is
UNIVERSE S
thesetofallgroundtermsconstructable fromthefollowing:
a. Thefunction symbolsinS,ifany.
b. Theconstant symbolsinS,ifany;ifnone,thentheconstant symbolA.
Forexample,ifS containsjusttheclause¬P(x,F(x,A))∨¬Q(x,A)∨R(x,B),then
H isthefollowinginfinitesetofground terms:
S
{A,B,F(A,A),F(A,B),F(B,A),F(B,B),F(A,F(A,A)),...}.
• Saturation: If S is a set of clauses and P is a set of ground terms, then P(S), the
SATURATION
saturation of S with respect to P, isthe set of allground clauses obtained by applying
allpossible consistent substitutions ofgroundtermsin P withvariablesinS.
• Herbrand base: The saturation of a set S of clauses with respect to its Herbrand uni-
HERBRANDBASE
verse is called the Herbrand base of S, written as H (S). For example, if S contains
S
solelytheclausejustgiven,thenH (S)istheinfinitesetofclauses
S
{¬P(A,F(A,A))∨¬Q(A,A)∨R(A,B),
¬P(B,F(B,A))∨¬Q(B,A)∨R(B,B),
¬P(F(A,A),F(F(A,A),A)) ∨¬Q(F(A,A),A)∨R(F(A,A),B),
¬P(F(A,B),F(F(A,B),A))∨¬Q(F(A,B),A)∨R(F(A,B),B),...}
HERBRAND’S ThesedefinitionsallowustostateaformofHerbrand’stheorem(Herbrand, 1930):
THEOREM
IfasetS ofclausesisunsatisfiable, thenthereexistsafinitesubsetofH (S)that
S
isalsounsatisfiable.
(cid:2)
LetS bethisfinitesubsetofgroundsentences. Now,wecanappealtothegroundresolution
(cid:2)
theorem (page 255) to show that the resolution closure RC(S ) contains the empty clause.
(cid:2)
Thatis,running propositional resolution tocompletion on S willderiveacontradiction.
Now that we have established that there is always a resolution proof involving some
finite subset of the Herbrand base of S, the next step is to show that there is a resolution
352 Chapter 9. Inference inFirst-OrderLogic
GO¨DEL’S INCOMPLETENESS THEOREM
Byslightly extending thelanguage offirst-orderlogic toallow forthe mathemat-
ical induction schema in arithmetic, Kurt Go¨del was able to show, in his incom-
pletenesstheorem,thattherearetruearithmetic sentences thatcannotbeproved.
The proof of the incompleteness theorem is somewhat beyond the scope of
thisbook, occupying, asitdoes, atleast30pages, butwecangiveahinthere. We
beginwiththelogical theory ofnumbers. Inthis theory, there isasingle constant,
0, and a single function, S (the successor function). In the intended model, S(0)
denotes 1, S(S(0)) denotes 2, and soon; the language therefore has names forall
thenaturalnumbers. Thevocabulary alsoincludesthefunctionsymbols+,×,and
Expt (exponentiation) andtheusualsetoflogicalconnectivesandquantifiers. The
firststepistonoticethatthesetofsentencesthatwecanwriteinthislanguagecan
be enumerated. (Imagine defining an alphabetical order on the symbols and then
arranging, in alphabetical order, each of the sets of sentences of length 1, 2, and
so on.) We can then number each sentence α with a unique natural number #α
(the Go¨del number). This is crucial: number theory contains a name for each of
its own sentences. Similarly, we can number each possible proof P with a Go¨del
numberG(P),becauseaproofissimplyafinitesequence ofsentences.
Now suppose we have a recursively enumerable set A of sentences that are
true statements about the natural numbers. Recalling that A can be named by a
givensetofintegers,wecanimaginewritinginourlanguage asentenceα(j,A)of
thefollowingsort:
∀i i is not the Go¨del number of a proof of the sentence whose Go¨del
numberisj,wheretheproofusesonlypremisesin A.
Thenletσ bethesentenceα(#σ,A),thatis,asentencethatstatesitsownunprov-
abilityfromA. (Thatthissentence alwaysexistsistruebutnotentirely obvious.)
Now we make the following ingenious argument: Suppose that σ is provable
from A; then σ is false (because σ says it cannot be proved). But then we have a
falsesentencethatisprovablefromA,soAcannotconsistofonlytruesentences—
aviolationofourpremise. Therefore, σ isnotprovablefrom A. Butthisisexactly
whatσ itselfclaims;henceσ isatruesentence.
So, we have shown (barring 291 pages) that for any set of true sentences of
2
number theory, and in particular any set of basic axioms, there are other true sen-
tences that cannot be proved from those axioms. This establishes, among other
things, that we can never prove all the theorems of mathematics within any given
system of axioms. Clearly, this was an important discovery for mathematics. Its
significanceforAIhasbeenwidelydebated,beginningwithspeculationsbyGo¨del
himself. WetakeupthedebateinChapter26.
Section9.5. Resolution 353
proof using the clauses of S itself, which are not necessarily ground clauses. We start by
considering asingleapplication oftheresolution rule. Robinsonstatedthislemma:
(cid:2) (cid:2)
Let C and C be two clauses with no shared variables, and let C and C be
1 2 1 2
(cid:2) (cid:2) (cid:2)
groundinstancesofC andC . IfC isaresolventofC andC ,thenthereexists
1 2 1 2
(cid:2)
a clause C such that (1) C is a resolvent of C and C and (2) C is a ground
1 2
instanceofC.
Thisiscalledaliftinglemma,becauseitliftsaproofstepfromgroundclausesuptogeneral
LIFTINGLEMMA
first-order clauses. In order to prove his basic lifting lemma, Robinson had to invent unifi-
cation and derive all of the properties of most general unifiers. Rather than repeat the proof
here,wesimplyillustrate thelemma:
C = ¬P(x,F(x,A))∨¬Q(x,A)∨R(x,B)
1
C = ¬N(G(y),z)∨P(H(y),z)
2
C (cid:2) = ¬P(H(B),F(H(B),A))∨¬Q(H(B),A)∨R(H(B),B)
1
C (cid:2) = ¬N(G(B),F(H(B),A))∨P(H(B),F(H(B),A))
2
C (cid:2) = ¬N(G(B),F(H(B),A))∨¬Q(H(B),A)∨R(H(B),B)
C = ¬N(G(y),F(H(y),A))∨¬Q(H(y),A)∨R(H(y),B).
(cid:2) (cid:2) (cid:2)
We see that indeed C is a ground instance of C. In general, for C and C to have any
1 2
resolvents, they must be constructed by first applying to C and C the most general unifier
1 2
ofapairofcomplementary literals in C andC . Fromtheliftinglemma,itiseasytoderive
1 2
asimilarstatement aboutanysequence ofapplications oftheresolution rule:
(cid:2) (cid:2)
Foranyclause C intheresolution closureof S thereisaclause C intheresolu-
(cid:2)
tionclosureofS suchthatC isagroundinstanceofC andthederivationofC is
(cid:2)
thesamelengthasthederivation ofC .
(cid:2)
From this fact, it follows that if the empty clause appears in the resolution closure of S , it
mustalsoappearintheresolution closureof S. Thisisbecausetheemptyclausecannotbea
ground instance ofanyotherclause. Torecap: wehaveshownthatifS isunsatisfiable, then
thereisafinitederivationoftheemptyclauseusingtheresolution rule.
Theliftingoftheoremprovingfromgroundclausestofirst-orderclausesprovidesavast
increaseinpower. Thisincreasecomesfromthefactthatthefirst-orderproofneedinstantiate
variables only as far as necessary for the proof, whereas the ground-clause methods were
required toexamineahugenumberofarbitrary instantiations.
9.5.5 Equality
Noneoftheinferencemethodsdescribedsofarinthischapterhandleanassertionoftheform
x = y. Threedistinctapproachescanbetaken. Thefirstapproachistoaxiomatizeequality—
towritedownsentencesabouttheequalityrelationintheknowledgebase. Weneedtosaythat
equalityisreflexive,symmetric,andtransitive,andwealsohavetosaythatwecansubstitute
equals for equals in any predicate orfunction. So weneed three basic axioms, and then one
354 Chapter 9. Inference inFirst-OrderLogic
foreachpredicate andfunction:
∀x x=x
∀x,y x=y ⇒ y=x
∀x,y,z x=y∧y=z ⇒ x=z
∀x,y x=y ⇒ (P (x) ⇔ P (y))
1 1
∀x,y x=y ⇒ (P (x) ⇔ P (y))
2 2
.
.
.
∀w,x,y,z w=y∧x=z ⇒ (F (w,x)=F (y,z))
1 1
∀w,x,y,z w=y∧x=z ⇒ (F (w,x)=F (y,z))
2 2
.
.
.
Given these sentences, a standard inference procedure such as resolution can perform tasks
requiringequalityreasoning,suchassolvingmathematicalequations. However,theseaxioms
will generate a lot of conclusions, most of them not helpful to a proof. So there has been a
search formoreefficientwaysofhandling equality. Onealternative istoadd inference rules
rather than axioms. The simplest rule, demodulation, takes a unit clause x=y and some
clause α that contains the term x, and yields a new clause formed by substituting y for x
within α. It works if the term within α unifies with x; it need not be exactly equal to x.
Notethat demodulation isdirectional; given x = y,thexalways getsreplaced withy,never
vice versa. That means that demodulation can be used for simplifying expressions using
demodulators suchasx+0=xorx1=x. Asanotherexample, given
Father(Father(x)) = PaternalGrandfather(x)
Birthdate(Father(Father(Bella)),1926)
wecanconclude bydemodulation
Birthdate(PaternalGrandfather(Bella),1926).
Moreformally,wehave
• Demodulation: For any terms x, y, and z, where z appears somewhere in literal m
DEMODULATION i
andwhereUNIFY(x,z) = θ,
x=y, m ∨···∨m
1 n
.
SUB(SUBST(θ,x),SUBST(θ,y),m
1
∨···∨m
n
)
where SUBST is the usual substitution of a binding list, and SUB(x,y,m) means to
replace xwithy everywherethat xoccurswithinm.
Therulecanalsobeextendedtohandlenon-unitclausesinwhichanequalityliteralappears:
• Paramodulation: Foranyterms x,y,andz,wherez appearssomewhereinliteral m ,
PARAMODULATION i
andwhereUNIFY(x,z) = θ,
(cid:3) ∨···∨(cid:3) ∨x=y, m ∨···∨m
1 k 1 n
.
SUB(SUBST(θ,x),SUBST(θ,y),SUBST(θ,(cid:3)
1
∨···∨(cid:3)
k
∨m
1
∨···∨m
n
)
Forexample,from
P(F(x,B),x)∨Q(x) and F(A,y)=y∨R(y)
Section9.5. Resolution 355
we have θ=UNIFY(F(A,y),F(x,B))={x/A,y/B}, and we can conclude by paramodu-
lationthesentence
P(B,A)∨Q(A)∨R(B).
Paramodulation yieldsacompleteinference procedure forfirst-orderlogicwithequality.
A third approach handles equality reasoning entirely within an extended unification
algorithm. That is, terms are unifiable if they are provably equal under some substitution,
where “provably” allows for equality reasoning. For example, the terms 1 + 2 and 2 + 1
normally are not unifiable, but a unification algorithm that knows that x+y=y +x could
EQUATIONAL unifythemwiththeemptysubstitution. Equationalunificationofthiskindcanbedonewith
UNIFICATION
efficientalgorithmsdesignedfortheparticularaxiomsused(commutativity,associativity,and
so on) rather than through explicit inference with those axioms. Theorem provers using this
technique arecloselyrelatedtotheCLPsystemsdescribed inSection9.4.
9.5.6 Resolutionstrategies
We know that repeated applications of the resolution inference rule will eventually find a
proofifoneexists. Inthissubsection, weexaminestrategies thathelpfindproofs efficiently.
Unitpreference: Thisstrategypreferstodoresolutionswhereoneofthesentencesisasingle
UNITPREFERENCE
literal (also known as a unit clause). The idea behind the strategy is that we are trying to
produce anemptyclause, soitmightbeagoodideatopreferinferences thatproduceshorter
clauses. Resolvingaunitsentence(suchasP)withanyothersentence(suchas¬P∨¬Q∨R)
always yields a clause (in this case, ¬Q ∨ R) that is shorter than the other clause. When
the unit preference strategy was first tried for propositional inference in 1964, it led to a
dramaticspeedup,makingitfeasibletoprovetheoremsthat couldnotbehandledwithoutthe
preference. Unitresolution is arestricted form of resolution in which every resolution step
must involve a unit clause. Unit resolution is incomplete in general, but complete for Horn
clauses. Unitresolution proofsonHornclauses resembleforwardchaining.
TheOTTER theoremprover(OrganizedTechniquesforTheorem-proving andEffective
Research, McCune, 1992), uses a form of best-first search. Its heuristic function measures
the“weight”ofeachclause,wherelighterclausesarepreferred. Theexactchoiceofheuristic
is up to the user, but generally, the weight of a clause should be correlated with its size or
difficulty. Unitclauses aretreated aslight; thesearch can thus beseenasageneralization of
theunitpreference strategy.
Set of support: Preferences that try certain resolutions first are helpful, but in general it is
SETOFSUPPORT
moreeffectivetotrytoeliminatesomepotential resolutions altogether. Forexample,wecan
insist that every resolution step involve at least one element of a special set of clauses—the
set of support. The resolvent is then added into the set of support. If the set of support is
smallrelativetothewholeknowledgebase,thesearchspace willbereduced dramatically.
We have to be careful with this approach because a bad choice for the set of support
will make the algorithm incomplete. However, if wechoose the set of support S so that the
remainder of the sentences are jointly satisfiable, then set-of-support resolution is complete.
Forexample, onecan usethenegated query asthe setofsupport, ontheassumption that the
356 Chapter 9. Inference inFirst-OrderLogic
original knowledge base is consistent. (After all, if it is not consistent, then the fact that the
queryfollowsfromitisvacuous.) Theset-of-support strategyhastheadditionaladvantageof
generating goal-directed prooftreesthatareofteneasyforhumanstounderstand.
Inputresolution: Inthisstrategy,everyresolutioncombinesoneoftheinputsentences(from
INPUTRESOLUTION
the KB or the query) with some other sentence. The proof in Figure 9.11 on page 348 uses
only input resolutions and has the characteristic shape of a single “spine” with single sen-
tences combining onto the spine. Clearly, the space of proof trees of this shape is smaller
than the space of all proof graphs. In Horn knowledge bases, Modus Ponens is a kind of
inputresolutionstrategy,becauseitcombinesanimplicationfromtheoriginalKBwithsome
othersentences. Thus,itisnosurprise thatinputresolution iscomplete forknowledge bases
thatareinHornform,butincomplete inthegeneral case. The linearresolution strategyisa
LINEARRESOLUTION
slightgeneralization thatallowsP andQtoberesolvedtogethereitherif P isintheoriginal
KB orifP isanancestorofQintheprooftree. Linearresolution iscomplete.
Subsumption: Thesubsumption methodeliminates allsentences thataresubsumed by(that
SUBSUMPTION
is,morespecificthan)anexistingsentenceintheKB.Forexample,ifP(x)isintheKB,then
thereisnosenseinadding P(A)andevenlesssenseinaddingP(A)∨Q(B). Subsumption
helpskeeptheKBsmallandthushelpskeepthesearchspacesmall.
Practicalusesofresolution theoremprovers
Theorem provers can be applied to the problems involved in the synthesis and verification
SYNTHESIS
ofbothhardwareandsoftware. Thus,theorem-proving research iscarriedoutinthefieldsof
VERIFICATION
hardwaredesign, programminglanguages, andsoftwareengineering—not justinAI.
In the case of hardware, the axioms describe the interactions between signals and cir-
cuit elements. (See Section 8.4.2 on page 309 for an example.) Logical reasoners designed
specially for verification have been able to verify entire CPUs, including their timing prop-
erties (Srivas and Bickford, 1990). The AURA theorem prover has been applied to design
circuitsthataremorecompactthananyprevious design(Wojciechowski andWojcik,1983).
In the case of software, reasoning about programs is quite similar to reasoning about
actions, as in Chapter 7: axioms describe the preconditions and effects of each statement.
The formal synthesis of algorithms was one of the first uses of theorem provers, as outlined
by Cordell Green (1969a), who built on earlier ideas by Herbert Simon (1963). The idea
is to constructively prove a theorem to the effect that “there exists a program p satisfying a
DEDUCTIVE certain specification.” Although fully automated deductivesynthesis, asitiscalled, has not
SYNTHESIS
yet become feasible forgeneral-purpose programming, hand-guided deductive synthesis has
beensuccessfulindesigningseveralnovelandsophisticatedalgorithms. Synthesisofspecial-
purpose programs,suchasscientificcomputing code,isalso anactiveareaofresearch.
Similartechniquesarenowbeingappliedtosoftwareverificationbysystemssuchasthe
SPIN model checker (Holzmann, 1997). Forexample, the Remote Agent spacecraft control
program was verified before and after flight (Havelund et al., 2000). The RSA public key
encryptionalgorithmandtheBoyer–Moorestring-matching algorithmhavebeenverifiedthis
way(BoyerandMoore,1984).
Section9.6. Summary 357
9.6 SUMMARY
Wehave presented ananalysis oflogical inference infirst-orderlogic and anumberofalgo-
rithmsfordoingit.
• A first approach uses inference rules (universal instantiation and existential instan-
tiation) to propositionalize the inference problem. Typically, this approach is slow,
unlessthedomainissmall.
• Theuseofunificationtoidentify appropriate substitutions forvariables eliminates the
instantiationstepinfirst-orderproofs,makingtheprocessmoreefficientinmanycases.
• A lifted version of Modus Ponens uses unification to provide a natural and powerful
inference rule, generalized Modus Ponens. The forward-chaining and backward-
chainingalgorithmsapplythisruletosetsofdefiniteclauses.
• Generalized Modus Ponens is complete for definite clauses, although the entailment
problem is semidecidable. For Datalog knowledge bases consisting of function-free
definiteclauses, entailment isdecidable.
• Forward chaining is used in deductive databases, where it can be combined with re-
lational database operations. It is also used in production systems, which perform
efficient updates with very large rule sets. Forward chaining is complete for Datalog
andrunsinpolynomial time.
• Backward chaining is used in logic programming systems, which employ sophisti-
cated compiler technology to provide very fast inference. Backward chaining suffers
fromredundant inferences andinfiniteloops;thesecanbealleviated bymemoization.
• Prolog, unlike first-order logic, uses aclosed worldwiththe unique names assumption
and negation as failure. These make Prolog a more practical programming language,
butbringitfurtherfrompurelogic.
• The generalized resolution inference rule provides a complete proof system for first-
orderlogic,usingknowledgebasesinconjunctive normalform.
• Several strategies exist for reducing the search space of a resolution system without
compromisingcompleteness. Oneofthemostimportantissuesisdealingwithequality;
weshowedhowdemodulationandparamodulationcanbeused.
• Efficient resolution-based theorem provers have been used to prove interesting mathe-
maticaltheoremsandtoverifyandsynthesize softwareandhardware.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Gottlob Frege, who developed full first-order logic in 1879, based his system of inference
on a collection of valid schemas plus a single inference rule, Modus Ponens. Whitehead
and Russell (1910) expounded the so-called rules of passage (the actual term is from Her-
brand (1930)) that are used to move quantifiers to the front of formulas. Skolem constants
358 Chapter 9. Inference inFirst-OrderLogic
and Skolem functions were introduced, appropriately enough, by Thoralf Skolem (1920).
Oddlyenough, itwasSkolemwhointroduced theHerbranduniverse(Skolem,1928).
Herbrand’s theorem (Herbrand, 1930) has played a vital role in the development of
automated reasoning. Herbrand is also the inventor of unification. Go¨del (1930) built on
the ideas of Skolem and Herbrand to show that first-order logic has a complete proof pro-
cedure. Alan Turing (1936) and Alonzo Church (1936) simultaneously showed, using very
different proofs, that validity in first-order logic was not decidable. The excellent text by
Enderton(1972)explainsalloftheseresultsinarigorousyetunderstandable fashion.
Abraham Robinson proposed that an automated reasoner could be built using proposi-
tionalizationandHerbrand’stheorem,andPaulGilmore(1960)wrotethefirstprogram. Davis
andPutnam(1960)introducedthepropositionalizationmethodofSection9.1. Prawitz(1960)
developed the key idea of letting the quest for propositional inconsistency drive the search,
and generating terms from the Herbrand universe only when they were necessary to estab-
lishpropositional inconsistency. Afterfurtherdevelopmentbyotherresearchers,thisidealed
J.A.Robinson(norelation) todevelopresolution (Robinson, 1965).
In AI, resolution was adopted for question-answering systems by Cordell Green and
Bertram Raphael(1968). EarlyAIimplementations putagood dealofeffortinto datastruc-
tures that would allow efficient retrieval of facts; this work is covered in AI programming
texts (Charniak etal., 1987; Norvig, 1992; Forbus and de Kleer, 1993). By the early 1970s,
forward chaining was well established in AI as an easily understandable alternative to res-
olution. AI applications typically involved large numbers of rules, so it was important to
develop efficient rule-matching technology, particularly for incremental updates. The tech-
nology forproductionsystems wasdeveloped tosupport suchapplications. Theproduction
system language OPS-5 (Forgy, 1981; Brownston et al., 1985), incorporating the efficient
retematchprocess(Forgy,1982),wasusedforapplicationssuch astheR1expertsystemfor
RETE
minicomputerconfiguration (McDermott,1982).
TheSOARcognitivearchitecture (Laird etal.,1987;Laird,2008)wasdesignedtohan-
dle very large rule sets—up to a million rules (Doorenbos, 1994). Example applications of
SOAR include controlling simulated fighter aircraft (Jones et al., 1998), airspace manage-
ment (Taylor etal., 2007), AI characters forcomputer games(Wintermute etal., 2007), and
training toolsforsoldiers(WrayandJones, 2005).
The field of deductive databases began with a workshop in Toulouse in 1977 that
brought together experts in logical inference and database systems (Gallaire and Minker,
1978). Influential workbyChandraandHarel(1980) andUllman(1985) ledtotheadoption
ofDatalogasastandardlanguagefordeductivedatabases. Thedevelopmentofthemagicsets
technique for rule rewriting by Bancilhon et al. (1986) allowed forward chaining to borrow
the advantage of goal-directedness from backward chaining. Current work includes theidea
ofintegrating multipledatabases intoaconsistent dataspace (Halevy,2007).
Backward chaining for logical inference appeared first in Hewitt’s PLANNER lan-
guage (1969). Meanwhile, in1972, AlainColmerauerhaddeveloped andimplemented Pro-
log for the purpose of parsing natural language—Prolog’s clauses were intended initially
as context-free grammar rules (Roussel, 1975; Colmerauer et al., 1973). Much of the the-
oretical background for logic programming was developed by Robert Kowalski, working
Bibliographical andHistorical Notes 359
with Colmerauer; see Kowalski (1988) and Colmerauer and Roussel (1993) for a historical
overview. Efficient Prolog compilers are generally based on the Warren Abstract Machine
(WAM) model of computation developed by David H. D. Warren (1983). Van Roy (1990)
showedthatPrologprograms canbecompetitivewithCprogramsintermsofspeed.
Methodsforavoiding unnecessary loopinginrecursivelogicprogramsweredeveloped
independently by Smith et al. (1986) and Tamaki and Sato (1986). The latter paper also
included memoization for logic programs, a method developed extensively as tabled logic
programmingbyDavidS.Warren. SwiftandWarren(1994) showhowtoextend theWAM
to handle tabling, enabling Datalog programs to execute an order of magnitude faster than
forward-chaining deductivedatabase systems.
Early work on constraint logic programming was done by Jaffar and Lassez (1987).
Jaffaretal.(1992)developedtheCLP(R)systemforhandlingreal-valuedconstraints. There
arenowcommercialproductsforsolvinglarge-scaleconfigurationandoptimizationproblems
with constraint programming; one of the best known is ILOG (Junker, 2003). Answer set
programming (Gelfond,2008)extendsProlog,allowingdisjunction andnegation.
Texts on logic programming and Prolog, including Shoham (1994), Bratko (2001),
Clocksin (2003), and Clocksin and Mellish (2003). Priorto 2000, the Journal ofLogic Pro-
gramming was the journal of record; it has now been replaced by Theory and Practice of
Logic Programming. Logic programming conferences include the International Conference
onLogicProgramming(ICLP)andtheInternationalLogicProgrammingSymposium(ILPS).
Research into mathematical theorem proving began even before the first complete
first-order systemsweredeveloped. Herbert Gelernter’s GeometryTheorem Prover(Gelern-
ter, 1959) used heuristic search methods combined withdiagrams forpruning false subgoals
and was able to prove some quite intricate results in Euclidean geometry. The demodula-
tion and paramodulation rules for equality reasoning were introduced by Wos et al. (1967)
and Wosand Robinson (1968), respectively. Theserules were also developed independently
in the context of term-rewriting systems (Knuth and Bendix, 1970). The incorporation of
equalityreasoningintotheunificationalgorithmisduetoGordonPlotkin(1972). Jouannaud
and Kirchner (1991) survey equational unification from a term-rewriting perspective. An
overviewofunification isgivenbyBaaderandSnyder(2001).
A number of control strategies have been proposed for resolution, beginning with the
unitpreferencestrategy(Wosetal.,1964). Theset-of-support strategywasproposedbyWos
et al. (1965) to provide a degree of goal-directedness in resolution. Linear resolution first
appeared inLoveland (1970). Genesereth and Nilsson (1987, Chapter5) provide ashort but
thorough analysis ofawidevarietyofcontrolstrategies.
A Computational Logic (Boyer and Moore, 1979) is the basic reference on the Boyer-
Mooretheoremprover. Stickel(1992)coversthePrologTechnologyTheoremProver(PTTP),
whichcombinestheadvantagesofPrologcompilationwiththecompletenessofmodelelimi-
nation. SETHEO(Letzetal.,1992)isanotherwidelyusedtheoremproverbasedonthisap-
proach. LEANTAP (Beckert andPosegga, 1995) isanefficient theorem proverimplemented
inonly25linesofProlog. Weidenbach (2001) describes SPASS,oneofthestrongest current
theoremprovers. Themostsuccessfultheoremproverinrecentannualcompetitionshasbeen
VAMPIRE (Riazanov and Voronkov, 2002). The COQ system (Bertot et al., 2004) and the E
360 Chapter 9. Inference inFirst-OrderLogic
equational solver (Schulz, 2004) have also proven to be valuable tools for proving correct-
ness. Theorem provers have been used to automatically synthesize and verify software for
controlling spacecraft (Denney et al., 2006), including NASA’s new Orion capsule (Lowry,
2008). Thedesign ofthe FM9001 32-bit microprocessor wasproved correct bythe NQTHM
system (Hunt and Brock, 1992). TheConference onAutomated Deduction (CADE)runs an
annualcontestforautomatedtheoremprovers. From2002through2008,themostsuccessful
system has been VAMPIRE (Riazanov and Voronkov, 2002). Wiedijk (2003) compares the
strength of 15 mathematical provers. TPTP (Thousands of Problems for Theorem Provers)
is a library of theorem-proving problems, useful for comparing the performance of systems
(SutcliffeandSuttner,1998;Sutcliffe etal.,2006).
Theorem provers have come up with novel mathematical results that eluded human
mathematicians for decades, as detailed in the book Automated Reasoning and the Discov-
ery of Missing Elegant Proofs (Wos and Pieper, 2003). The SAM (Semi-Automated Math-
ematics) program was the first, proving a lemma in lattice theory (Guard et al., 1969). The
AURA program has also answered open questions in several areas of mathematics (Wos and
Winker, 1983). The Boyer–Moore theorem prover (Boyer and Moore, 1979) was used by
Natarajan Shankar to give the first fully rigorous formal proof of Go¨del’s Incompleteness
Theorem (Shankar, 1986). The NUPRL system proved Girard’s paradox (Howe, 1987) and
Higman’s Lemma(Murthyand Russell, 1990). In1933, Herbert Robbins proposed asimple
setofaxioms—theRobbinsalgebra—that appearedtodefineBooleanalgebra, butnoproof
ROBBINSALGEBRA
could be found (despite serious work by Alfred Tarski and others). On October 10, 1996,
aftereightdaysofcomputation, EQP (aversionof OTTER)foundaproof(McCune,1997).
Many early papers in mathematical logic are to be found in From Frege to Go¨del:
A Source Book in Mathematical Logic (van Heijenoort, 1967). Textbooks geared toward
automated deduction include the classic Symbolic Logic and Mechanical Theorem Prov-
ing(ChangandLee,1973),aswellasmorerecentworksbyDuffy(1991),Wosetal.(1992),
Bibel (1993), and Kaufmann et al. (2000). The principal journal for theorem proving is the
Journal of Automated Reasoning; the main conferences are the annual Conference on Auto-
mated Deduction (CADE) and the International Joint Conference on Automated Reasoning
(IJCAR). The Handbook of Automated Reasoning (Robinson and Voronkov, 2001) collects
papersinthefield. MacKenzie’sMechanizingProof(2004)coversthehistoryandtechnology
oftheorem provingforthepopularaudience.
EXERCISES
9.1 Prove that Universal Instantiation is sound and that Existential Instantiation produces
aninferentially equivalent knowledgebase.
9.2 From Likes(Jerry,IceCream) it seems reasonable to infer ∃x Likes(x,IceCream).
EXISTENTIAL Writedownageneralinference rule, ExistentialIntroduction,thatsanctions thisinference.
INTRODUCTION
Statecarefullytheconditionsthatmustbesatisfiedbythevariablesandtermsinvolved.
Exercises 361
9.3 Suppose a knowledge base contains just one sentence, ∃x AsHighAs(x,Everest).
Whichofthefollowingarelegitimateresultsofapplying ExistentialInstantiation?
a. AsHighAs(Everest,Everest).
b. AsHighAs(Kilimanjaro,Everest).
c. AsHighAs(Kilimanjaro,Everest)∧AsHighAs(BenNevis,Everest)
(aftertwoapplications).
9.4 Foreachpairofatomicsentences, givethemostgeneralunifierifitexists:
a. P(A,B,B),P(x,y,z).
b. Q(y,G(A,B)),Q(G(x,x),y).
c. Older(Father(y),y),Older(Father(x),John).
d. Knows(Father(y),y),Knows(x,x).
9.5 Considerthesubsumption latticesshowninFigure9.2(page 329).
a. Constructthelatticeforthesentence Employs(Mother(John),Father(Richard)).
b. Construct thelattice forthesentence Employs(IBM,y)(“Everyone worksforIBM”).
Remembertoincludeeverykindofquerythatunifieswiththesentence.
c. AssumethatSTORE indexeseachsentenceundereverynodeinitssubsumption lattice.
Explain how FETCH should workwhensomeofthesesentences contain variables; use
asexamplesthesentences in(a)and(b)andthequery Employs(x,Father(x)).
9.6 Write down logical representations for the following sentences, suitable for use with
Generalized ModusPonens:
a. Horses,cows,andpigsaremammals.
b. Anoffspring ofahorseisahorse.
c. Bluebeardisahorse.
d. BluebeardisCharlie’sparent.
e. Offspringandparentareinverserelations.
f. Everymammalhasaparent.
9.7 Thesequestions concernconcernissueswithsubstitution andSkolemization.
a. Giventhe premise ∀x ∃y P(x,y), it is not valid to conclude that ∃q P(q,q). Give
anexampleofapredicate P wherethefirstistruebutthesecondisfalse.
b. Suppose that an inference engine is incorrectly written with the occurs check omitted,
so that it allows a literal like P(x,F(x)) to be unified with P(q,q). (As mentioned,
most standard implementations of Prolog actually do allow this.) Show that such an
inference enginewillallowtheconclusion ∃y P(q,q)tobeinferred fromthepremise
∀x ∃y P(x,y).
362 Chapter 9. Inference inFirst-OrderLogic
c. Suppose that a procedure that converts first-order logic to clausal form incorrectly
Skolemizes ∀x ∃y P(x,y) to P(x,Sk0)—that is, it replaces y by a Skolem con-
stant rather than by a Skolem function of x. Show that an inference engine that uses
such a procedure will likewise allow ∃q P(q,q) to be inferred from the premise
∀x ∃y P(x,y).
d. A common error among students is to suppose that, in unification, one is allowed to
substituteatermforaSkolemconstantinsteadofforavariable. Forinstance, theywill
saythattheformulasP(Sk1)andP(A)canbeunifiedunderthesubstitution{Sk1/A}.
Giveanexamplewherethisleadstoaninvalidinference.
9.8 Explainhowtowriteanygiven3-SATproblemofarbitrarysizeusingasinglefirst-order
definiteclauseandnomorethan30ground facts.
9.9 Supposeyouaregiventhefollowingaxioms:
1. 0 ≤ 3.
2. 7 ≤ 9.
3. ∀x x≤ x.
4. ∀x x≤ x+0.
5. ∀x x+0≤ x.
6. ∀x,y x+y ≤ y+x.
7. ∀w,x,y,z w ≤ y ∧x≤ z ⇒ w+x ≤ y+z.
8. ∀x,y,z x ≤ y∧y ≤ z ⇒ x≤ z
a. Giveabackward-chaining proof ofthesentence 7 ≤ 3+9. (Besure, ofcourse, touse
only the axioms given here, not anything else you may know about arithmetic.) Show
onlythestepsthatleadstosuccess, nottheirrelevant steps.
b. Givea forward-chaining proof of the sentence 7 ≤ 3+9. Again, show only the steps
thatleadtosuccess.
9.10 A popular children’s riddle is “Brothers and sisters have I none, but that man’s father
is myfather’s son.” Use the rules ofthe family domain (Section 8.3.2on page 301) toshow
whothatmanis. Youmayapplyanyoftheinferencemethodsdescribedinthischapter. Why
doyouthinkthatthisriddleisdifficult?
9.11 Suppose weput into alogical knowledge base a segment of the U.S.census data list-
ing the age, city of residence, date of birth, and mother of every person, using social se-
curity numbers as identifying constants for each person. Thus, George’s age is given by
Age(443-65-1282,56). Which ofthe following indexing schemes S1–S5enable anefficient
solution forwhichofthequeriesQ1–Q4(assuming normalbackwardchaining)?
• S1: anindexforeachatomineachposition.
• S2: anindexforeachfirstargument.
• S3: anindexforeachpredicate atom.
• S4: anindexforeachcombination ofpredicate andfirstargument.
Exercises 363
• S5: an index for each combination of predicate and second argument and an index for
eachfirstargument.
• Q1: Age(443-44-4321,x)
• Q2: ResidesIn(x,Houston)
• Q3: Mother(x,y)
• Q4: Age(x,34)∧ResidesIn(x,TinyTownUSA)
9.12 One might suppose that we can avoid the problem of variable conflict in unification
during backward chaining by standardizing apart all of the sentences in the knowledge base
onceandforall. Showthat, forsomesentences, thisapproach cannot work. (Hint: Consider
asentence inwhichonepartunifieswithanother.)
9.13 In this exercise, use the sentences you wrote in Exercise 9.6 to answer a question by
usingabackward-chaining algorithm.
a. Draw the proof tree generated by an exhaustive backward-chaining algorithm for the
query∃h Horse(h),whereclauses arematchedintheordergiven.
b. Whatdoyounoticeaboutthisdomain?
c. Howmanysolutions forhactually followfromyoursentences?
d. Canyouthinkofawaytofindallofthem? (Hint: SeeSmithetal.(1986).)
9.14 Tracetheexecutionofthebackward-chainingalgorithminFigure9.6(page338)when
itisappliedtosolvethecrimeproblem(page330). Showthesequenceofvaluestakenonby
thegoals variable, andarrangethemintoatree.
9.15 ThefollowingPrologcodedefinesapredicate P.(Rememberthatuppercasetermsare
variables, notconstants, inProlog.)
P(X,[X|Y]).
P(X,[Y|Z]) :- P(X,Z).
a. ShowprooftreesandsolutionsforthequeriesP(A,[2,1,3])andP(2,[1,A,3]).
b. Whatstandard listoperation does Prepresent?
9.16 Thisexerciselooksatsorting inProlog.
a. WritePrologclausesthatdefinethepredicate sorted(L),whichistrueifandonlyif
listLissortedinascending order.
b. Writea Prolog definition forthe predicate perm(L,M),which is true ifand only if L
isapermutation ofM.
c. Definesort(L,M)(Misasortedversionof L)usingpermandsorted.
d. Runsortonlongerandlongerlistsuntilyoulosepatience. Whatisthetimecomplex-
ityofyourprogram?
e. Writeafastersortingalgorithm, suchasinsertion sortor quicksort, inProlog.
364 Chapter 9. Inference inFirst-OrderLogic
9.17 This exercise looks at the recursive application of rewrite rules, using logic program-
ming. Arewriterule(ordemodulatorinOTTER terminology)isanequationwithaspecified
direction. For example, the rewrite rule x+0 → x suggests replacing any expression that
matchesx+0withtheexpressionx. Rewriterulesareakeycomponentofequationalreason-
ingsystems. Usethepredicate rewrite(X,Y)torepresentrewriterules. Forexample,the
earlier rewrite rule is written as rewrite(X+0,X).Some terms are primitive and cannot
befurthersimplified;thus,wewrite primitive(0)tosaythat0isaprimitiveterm.
a. Writeadefinitionofapredicate simplify(X,Y),thatistruewhen Yisasimplified
versionofX—thatis,whennofurtherrewriterulesapplytoanysubexpression ofY.
b. Write a collection of rules for the simplification of expressions involving arithmetic
operators, andapplyyoursimplification algorithm tosomesampleexpressions.
c. Writeacollectionofrewriterulesforsymbolicdifferentiation, andusethemalongwith
your simplification rules to differentiate and simplify expressions involving arithmetic
expressions, including exponentiation.
9.18 This exercise considers the implementation of search algorithms in Prolog. Suppose
that successor(X,Y)is true when state Y is a successor of state X; and that goal(X)
istrue when Xisagoal state. Writeadefinition for solve(X,P),which means thatPis a
path (list of states) beginning with X, ending in a goal state, and consisting of a sequence of
legalstepsasdefinedbysuccessor.Youwillfindthatdepth-firstsearchistheeasiestway
todothis. Howeasywoulditbetoaddheuristic searchcontrol?
9.19 Supposeaknowledgebasecontainsjustthefollowingfirst-orderHornclauses:
Ancestor(Mother(x),x)
Ancestor(x,y)∧Ancestor(y,z) ⇒ Ancestor(x,z)
Consideraforwardchainingalgorithmthat,onthejthiteration,terminatesiftheKBcontains
asentence that unifies withthe query, else adds tothe KBevery atomic sentence that canbe
inferredfromthesentences alreadyintheKBafteriterationj −1.
a. Foreachofthefollowingqueries, saywhetherthealgorithm will(1)giveananswer(if
so,writedownthatanswer);or(2)terminatewithnoanswer; or(3)neverterminate.
(i) Ancestor(Mother(y),John)
(ii) Ancestor(Mother(Mother(y)),John)
(iii) Ancestor(Mother(Mother(Mother(y))),Mother(y))
(iv) Ancestor(Mother(John),Mother(Mother(John)))
b. Canaresolutionalgorithmprovethesentence ¬Ancestor(John,John)fromtheorig-
inalknowledgebase? Explainhow,orwhynot.
c. Suppose we add the assertion that ¬(Mother(x)=x) and augment the resolution al-
gorithmwithinferencerulesforequality. Nowwhatistheanswerto(b)?
9.20 LetLbethefirst-orderlanguagewithasinglepredicateS(p,q),meaning“pshaves q.”
Assumeadomainofpeople.
Exercises 365
a. Consider the sentence “There exists a person P who shaves every one who does not
shavethemselves, andonlypeoplethatdonotshavethemselves.” ExpressthisinL.
b. Convertthesentence in(a)toclausalform.
c. Construct aresolution proof to show that the clauses in(b) are inherently inconsistent.
(Note: youdonotneedanyadditional axioms.)
9.21 Howcanresolution beusedtoshowthatasentence isvalid? Unsatisfiable?
9.22 Construct an example of two clauses that can be resolved together in two different
waysgivingtwodifferentoutcomes.
9.23 From “Horses are animals,” it follows that “The head of a horse is the head of an
animal.” Demonstrate thatthisinference isvalidbycarrying outthefollowingsteps:
a. Translatethepremiseandtheconclusionintothelanguageoffirst-orderlogic. Usethree
predicates: HeadOf(h,x)(meaning“histheheadofx”),Horse(x),andAnimal(x).
b. Negate the conclusion, and convert the premise and the negated conclusion into con-
junctivenormalform.
c. Useresolution toshowthattheconclusion followsfromthe premise.
9.24 Herearetwosentences inthelanguage offirst-orderlogic:
(A)∀x ∃y (x≥ y)
(B)∃y ∀x (x ≥ y)
a. Assumethatthevariablesrangeoverallthenaturalnumbers0,1,2,...,∞andthatthe
“≥” predicate means “is greater than or equal to.” Under this interpretation, translate
(A)and(B)intoEnglish.
b. Is(A)trueunderthisinterpretation?
c. Is(B)trueunderthisinterpretation?
d. Does(A)logically entail(B)?
e. Does(B)logically entail(A)?
f. Using resolution, tryto prove that (A)follows from (B). Do this even ifyou think that
(B)does not logically entail (A);continue until the proof breaks downand you cannot
proceed(ifitdoesbreakdown). Showtheunifyingsubstitutionforeachresolutionstep.
Iftheprooffails,explainexactlywhere,how,andwhyitbreaksdown.
g. Nowtrytoprovethat(B)followsfrom(A).
9.25 Resolution can produce nonconstructive proofs for queries with variables, so we had
tointroduce special mechanismstoextract definiteanswers. Explainwhythisissuedoesnot
arisewithknowledgebasescontaining onlydefiniteclauses.
9.26 We said in this chapter that resolution cannot be used to generate all logical conse-
quences ofasetofsentences. Cananyalgorithm dothis?
10
CLASSICAL PLANNING
Inwhichweseehowanagentcantakeadvantageofthestructureofaproblemto
construct complexplansofaction.
We have defined AI as the study of rational action, which means that planning—devising a
plan of action to achieve one’s goals—is a critical part of AI. We have seen two examples
of planning agents so far: the search-based problem-solving agent of Chapter 3 and the hy-
brid logical agent of Chapter 7. In this chapter we introduce a representation for planning
problemsthatscalesuptoproblemsthatcouldnotbehandled bythoseearlierapproaches.
Section10.1developsanexpressiveyetcarefullyconstrainedlanguageforrepresenting
planning problems. Section 10.2 shows how forward and backward search algorithms can
takeadvantageofthisrepresentation,primarilythroughaccurateheuristicsthatcanbederived
automaticallyfromthestructureoftherepresentation. (Thisisanalogoustothewayinwhich
effectivedomain-independentheuristicswereconstructedforconstraintsatisfactionproblems
inChapter6.) Section10.3showshowadatastructurecalledtheplanninggraphcanmakethe
searchforaplanmoreefficient. Wethendescribe afewoftheotherapproaches toplanning,
andconclude bycomparingthevariousapproaches.
This chapter covers fully observable, deterministic, static environments with single
agents. Chapters 11 and 17 cover partially observable, stochastic, dynamic environments
withmultipleagents.
10.1 DEFINITION OF CLASSICAL PLANNING
The problem-solving agent of Chapter 3 can find sequences of actions that result in a goal
state. Butitdealswithatomicrepresentations ofstates andthus needsgood domain-specific
heuristicstoperformwell. Thehybridpropositional logicalagentofChapter7canfindplans
without domain-specific heuristics because it uses domain-independent heuristics based on
the logical structure of the problem. But it relies on ground (variable-free) propositional
inference, whichmeansthatitmaybeswampedwhentherearemanyactionsandstates. For
example,inthewumpusworld,thesimpleactionofmovingastepforwardhadtoberepeated
forallfouragentorientations, T timesteps,andn2 currentlocations.
366
Section10.1. DefinitionofClassicalPlanning 367
In response to this, planning researchers have settled on a factored representation—
oneinwhichastateoftheworldisrepresentedbyacollectionofvariables. Weusealanguage
called PDDL,thePlanning DomainDefinition Language, thatallowsustoexpress all 4Tn2
PDDL
actions with one action schema. There have been several versions of PDDL; we select a
simple version andalter itssyntax tobeconsistent with the rest ofthe book.1 Wenowshow
howPDDLdescribesthefourthingsweneedtodefineasearchproblem: theinitialstate,the
actionsthatareavailable inastate, theresultofapplying anaction, andthegoaltest.
Eachstateisrepresentedasaconjunctionoffluentsthatareground,functionlessatoms.
For example, Poor ∧ Unknown might represent the state of a hapless agent, and a state
in a package delivery problem might be At(Truck ,Melbourne) ∧ At(Truck ,Sydney).
1 2
Databasesemanticsisused: theclosed-worldassumptionmeansthatanyfluentsthatarenot
mentioned are false, and the unique names assumption means that Truck and Truck are
1 2
distinct. Thefollowingfluentsarenotallowedinastate: At(x,y)(becauseitisnon-ground),
¬Poor (becauseitisanegation),andAt(Father(Fred),Sydney)(becauseitusesafunction
symbol). The representation of states is carefully designed so that a state can be treated
either asaconjunction of fluents, which can be manipulated by logical inference, oras a set
of fluents, which can be manipulated with set operations. The set semantics is sometimes
SETSEMANTICS
easiertodealwith.
ActionsaredescribedbyasetofactionschemasthatimplicitlydefinetheACTIONS(s)
andRESULT(s,a)functionsneededtodoaproblem-solvingsearch. WesawinChapter7that
anysystemforactiondescriptionneedstosolvetheframeproblem—tosaywhatchangesand
what stays the same as the result ofthe action. Classical planning concentrates on problems
where mostactions leave mostthings unchanged. Think ofaworld consisting ofabunch of
objects onaflatsurface. Theaction ofnudging anobject causes that object tochange itslo-
cationbyavectorΔ. AconcisedescriptionoftheactionshouldmentiononlyΔ;itshouldn’t
havetomentionalltheobjectsthatstayinplace. PDDL doesthatbyspecifying theresultof
anactionintermsofwhatchanges;everything thatstaysthe sameisleftunmentioned.
A set of ground (variable-free) actions can be represented by a single action schema.
ACTIONSCHEMA
Theschemaisaliftedrepresentation—it liftsthelevelofreasoning frompropositional logic
to a restricted subset of first-order logic. For example, here is an action schema for flying a
planefromonelocation toanother:
Action(Fly(p,from,to),
PRECOND:At(p,from)∧Plane(p)∧Airport(from)∧Airport(to)
EFFECT:¬At(p,from)∧At(p,to))
The schema consists of the action name, a list of all the variables used in the schema, a
precondition and an effect. Although we haven’t said yet how the action schema converts
PRECONDITION
into logical sentences, think of the variables as being universally quantified. We are free to
EFFECT
choosewhatevervalueswewanttoinstantiatethevariables. Forexample,hereisoneground
1 PDDLwasderivedfromtheoriginal STRIPSplanninglanguage(FikesandNilsson,1971). whichisslightly
morerestrictedthanPDDL:STRIPSpreconditionsandgoalscannotcontainnegativeliterals.
368 Chapter 10. ClassicalPlanning
actionthatresultsfromsubstituting valuesforallthevariables:
Action(Fly(P ,SFO,JFK),
1
PRECOND:At(P
1
,SFO)∧Plane(P
1
)∧Airport(SFO)∧Airport(JFK)
EFFECT:¬At(P
1
,SFO)∧At(P
1
,JFK))
Theprecondition andeffectofanactionareeachconjunctions ofliterals(positiveornegated
atomic sentences). The precondition defines the states in which the action can be executed,
andtheeffect definestheresult ofexecuting theaction. Anaction acan beexecuted instate
sifsentails theprecondition of a. Entailment can also beexpressed withthe setsemantics:
s |= q iff every positive literal in q is in s and every negated literal in q is not. In formal
notation wesay
(a ∈ ACTIONS(s)) ⇔ s |= PRECOND(a),
whereanyvariables in aareuniversally quantified. Forexample,
∀p,from,to (Fly(p,from,to)∈ ACTIONS(s)) ⇔
s |= (At(p,from)∧Plane(p)∧Airport(from)∧Airport(to))
We say that action a is applicable in state s if the preconditions are satisfied by s. When
APPLICABLE
an action schema a contains variables, it may have multiple applicable instantiations. For
example, with the initial state defined in Figure 10.1, the Fly action can be instantiated as
Fly(P ,SFO,JFK) or as Fly(P ,JFK,SFO), both of which are applicable in the initial
1 2
state. Ifanactionahasvvariables,then,inadomainwithkuniquenamesofobjects,ittakes
O(vk)timeintheworstcasetofindtheapplicable groundactions.
SometimeswewanttopropositionalizeaPDDLproblem—replaceeachactionschema
PROPOSITIONALIZE
with a set of ground actions and then use apropositional solver such as SATPLAN to find a
solution. However,thisisimpractical whenv andk arelarge.
(cid:2)
The result of executing action a in state s is defined as a state s which is represented
by the set of fluents formed by starting with s, removing the fluents that appear as negative
DELETELIST
literalsintheaction’s effects(whatwecallthedeletelistorDEL(a)),andaddingthefluents
ADDLIST
thatarepositiveliteralsintheaction’s effects(whatwecalltheaddlistorADD(a)):
RESULT(s,a) = (s−DEL(a))∪ADD(a). (10.1)
Forexample, withtheaction Fly(P ,SFO,JFK),wewouldremove At(P ,SFO)andadd
1 1
At(P ,JFK). Itisarequirement ofaction schemas that anyvariable intheeffect mustalso
1
appear in the precondition. That way, when the precondition is matched against the state s,
allthe variables willbebound, and RESULT(s,a)willtherefore have only ground atoms. In
otherwords,ground statesareclosedunderthe RESULT operation.
Alsonotethatthefluentsdonotexplicitlyrefertotime,astheydidinChapter7. There
weneededsuperscripts fortime,andsuccessor-state axiomsoftheform
Ft+1 ⇔ ActionCausesFt∨(Ft∧¬ActionCausesNotFt).
In PDDL the times and states are implicit in the action schemas: the precondition always
referstotimetandtheeffecttotimet+1.
Asetofactionschemasservesasadefinitionofaplanningdomain. Aspecificproblem
within the domain is defined with the addition of an initial state and a goal. The initial
Section10.1. DefinitionofClassicalPlanning 369
Init(At(C , SFO) ∧ At(C , JFK) ∧ At(P , SFO) ∧ At(P , JFK)
1 2 1 2
∧Cargo(C ) ∧ Cargo(C ) ∧ Plane(P ) ∧ Plane(P )
1 2 1 2
∧Airport(JFK) ∧ Airport(SFO))
Goal(At(C , JFK) ∧ At(C , SFO))
1 2
Action(Load(c, p, a),
PRECOND:At(c, a) ∧ At(p, a) ∧ Cargo(c) ∧ Plane(p) ∧ Airport(a)
EFFECT:¬At(c, a) ∧ In(c, p))
Action(Unload(c, p, a),
PRECOND:In(c, p) ∧ At(p, a) ∧ Cargo(c) ∧ Plane(p) ∧ Airport(a)
EFFECT:At(c, a) ∧ ¬In(c, p))
Action(Fly(p, from, to),
PRECOND:At(p, from) ∧ Plane(p) ∧ Airport(from) ∧ Airport(to)
EFFECT:¬At(p, from) ∧ At(p, to))
Figure10.1 APDDLdescriptionofanaircargotransportationplanningproblem.
state is a conjunction of ground atoms. (As with all states, the closed-world assumption is
INITIALSTATE
used, which means that any atoms that are not mentioned are false.) The goal is just like a
GOAL
precondition: aconjunction ofliterals (positive ornegative) thatmaycontain variables, such
asAt(p,SFO)∧Plane(p). Anyvariables aretreatedasexistentially quantified, sothisgoal
is to have any plane at SFO. The problem is solved when wecan find a sequence of actions
thatendinastatesthatentailsthegoal. Forexample,thestateRich∧Famous∧Miserable
entails the goal Rich ∧Famous, and the state Plane(Plane )∧At(Plane ,SFO) entails
1 1
thegoalAt(p,SFO)∧Plane(p).
Nowwehavedefinedplanningasasearchproblem: wehaveaninitialstate,anACTIONS
function, a RESULT function, and a goal test. We’ll look at some example problems before
investigating efficientsearchalgorithms.
10.1.1 Example: Aircargo transport
Figure10.1showsanaircargotransport problem involving loading andunloading cargoand
flyingitfrom placetoplace. Theproblem canbedefined withthree actions: Load,Unload,
andFly. Theactions affecttwopredicates: In(c,p)meansthatcargo cisinsideplanep,and
At(x,a)meansthatobjectx(eitherplaneorcargo)isatairport a. Notethatsomecaremust
be taken to make sure the At predicates are maintained properly. When a plane flies from
oneairporttoanother, allthecargo insidetheplane goeswithit. Infirst-orderlogicitwould
beeasytoquantify overallobjects thatareinside theplane. Butbasic PDDL doesnothave
a universal quantifier, so we need a different solution. The approach we use is to say that a
pieceofcargoceasestobeAtanywherewhenitisInaplane;thecargoonlybecomesAtthe
new airport when it is unloaded. So At really means “available for use at a given location.”
Thefollowingplanisasolution totheproblem:
[Load(C ,P ,SFO),Fly(P ,SFO,JFK),Unload(C ,P ,JFK),
1 1 1 1 1
Load(C ,P ,JFK),Fly(P ,JFK,SFO),Unload(C ,P ,SFO)].
2 2 2 2 2
370 Chapter 10. ClassicalPlanning
Finally, there is the problem ofspurious actions such as Fly(P ,JFK,JFK), which should
1
beano-op, butwhich hascontradictory effects (according tothedefinition, theeffect would
include At(P ,JFK) ∧ ¬At(P ,JFK)). It is common to ignore such problems, because
1 1
they seldom cause incorrect plans to beproduced. Thecorrect approach isto add inequality
preconditions saying thatthe from andto airports mustbedifferent; seeanother exampleof
thisinFigure10.3.
10.1.2 Example: The spare tireproblem
Consider the problem of changing a flat tire (Figure 10.2). The goal is to have a good spare
tireproperlymountedontothecar’saxle,wheretheinitial statehasaflattireontheaxleand
a good spare tire in the trunk. To keep it simple, our version of the problem is an abstract
one,withnostickylugnutsorothercomplications. Therearejustfouractions: removingthe
spare from the trunk, removing the flat tire from the axle, putting the spare on the axle, and
leaving the car unattended overnight. We assume that the car is parked in a particularly bad
neighborhood, sothat the effect ofleaving itovernight isthat the tires disappear. Asolution
totheproblem is[Remove(Flat,Axle),Remove(Spare,Trunk),PutOn(Spare,Axle)].
Init(Tire(Flat) ∧ Tire(Spare) ∧ At(Flat,Axle) ∧ At(Spare,Trunk))
Goal(At(Spare,Axle))
Action(Remove(obj,loc),
PRECOND:At(obj,loc)
EFFECT:¬At(obj,loc) ∧ At(obj,Ground))
Action(PutOn(t, Axle),
PRECOND:Tire(t) ∧ At(t,Ground) ∧ ¬At(Flat,Axle)
EFFECT:¬At(t,Ground) ∧ At(t,Axle))
Action(LeaveOvernight,
PRECOND:
EFFECT:¬At(Spare,Ground) ∧ ¬At(Spare,Axle) ∧ ¬At(Spare,Trunk)
∧¬At(Flat,Ground) ∧ ¬At(Flat,Axle) ∧ ¬At(Flat, Trunk))
Figure10.2 Thesimplesparetireproblem.
10.1.3 Example: The blocks world
One of the most famous planning domains is known as the blocks world. This domain
BLOCKSWORLD
consists of a set of cube-shaped blocks sitting on a table.2 The blocks can be stacked, but
only one block can fitdirectly ontop ofanother. Arobot arm canpick up ablock and move
it to another position, either on the table or on top of another block. The arm can pick up
onlyoneblockatatime,soitcannotpickupablockthathasanotheroneonit. Thegoalwill
alwaysbetobuildoneormorestacksofblocks, specified intermsofwhatblocksareontop
2 TheblocksworldusedinplanningresearchismuchsimplerthanSHRDLU’sversion,shownonpage20.
Section10.1. DefinitionofClassicalPlanning 371
Init(On(A,Table) ∧ On(B,Table) ∧ On(C,A)
∧ Block(A) ∧ Block(B) ∧ Block(C) ∧ Clear(B) ∧ Clear(C))
Goal(On(A,B) ∧ On(B,C))
Action(Move(b,x,y),
PRECOND:On(b,x) ∧ Clear(b) ∧ Clear(y) ∧ Block(b) ∧ Block(y) ∧
(b(cid:7)=x) ∧ (b(cid:7)=y) ∧ (x(cid:7)=y),
EFFECT:On(b,y) ∧ Clear(x) ∧ ¬On(b,x) ∧ ¬Clear(y))
Action(MoveToTable(b,x),
PRECOND:On(b,x) ∧ Clear(b) ∧ Block(b) ∧ (b(cid:7)=x),
EFFECT:On(b,Table) ∧ Clear(x) ∧ ¬On(b,x))
Figure10.3 Aplanningproblemintheblocksworld: buildingathree-blocktower. One
solutionisthesequence[MoveToTable(C,A),Move(B,Table,C),Move(A,Table,B)].
A
C B
B A C
Start State Goal State
Figure10.4 Diagramoftheblocks-worldprobleminFigure10.3.
of what other blocks. Forexample, a goal might be to get block A on B and block B on C
(seeFigure10.4).
WeuseOn(b,x)toindicate thatblock bisonx,where xiseitheranotherblock orthe
table. Theaction formoving block bfrom thetopof xtothe topofy willbeMove(b,x,y).
Now,oneofthepreconditions onmovingbisthatnootherblockbeonit. Infirst-orderlogic,
this would be ¬∃x On(x,b) or, alternatively, ∀x ¬On(x,b). Basic PDDL does not allow
quantifiers, so instead we introduce a predicate Clear(x) that is true when nothing is on x.
(Thecompleteproblem description isinFigure10.3.)
TheactionMove movesablockbfrom xtoy ifbothbandy areclear. Afterthemove
ismade,bisstillclearbuty isnot. AfirstattemptattheMove schemais
Action(Move(b,x,y),
PRECOND:On(b,x)∧Clear(b)∧Clear(y),
EFFECT:On(b,y)∧Clear(x)∧¬On(b,x)∧¬Clear(y)).
Unfortunately, thisdoesnotmaintain Clear properly when xory isthetable. Whenxisthe
Table, this action has the effect Clear(Table), but the table should not become clear; and
wheny=Table,ithastheprecondition Clear(Table),butthetabledoesnothavetobeclear
372 Chapter 10. ClassicalPlanning
for us to move a block onto it. To fix this, we do two things. First, we introduce another
actiontomoveablockbfromxtothetable:
Action(MoveToTable(b,x),
PRECOND:On(b,x)∧Clear(b),
EFFECT:On(b,Table)∧Clear(x)∧¬On(b,x)).
Second, we take the interpretation of Clear(x) to be “there is a clear space on x to hold a
block.” Underthisinterpretation, Clear(Table)willalwaysbetrue. Theonlyproblemisthat
nothing prevents the planner from using Move(b,x,Table) instead of MoveToTable(b,x).
Wecouldlivewiththisproblem—itwillleadtoalarger-than-necessary searchspace,butwill
notleadtoincorrectanswers—orwecouldintroducethepredicateBlock andaddBlock(b)∧
Block(y)totheprecondition ofMove.
10.1.4 The complexityofclassicalplanning
In this subsection we consider the theoretical complexity of planning and distinguish two
decision problems. PlanSAT is the question of whether there exists any plan that solves a
PLANSAT
planning problem. Bounded PlanSAT asks whether there is a solution of length k or less;
BOUNDEDPLANSAT
thiscanbeusedtofindanoptimalplan.
Thefirstresult isthat bothdecision problems aredecidable forclassical planning. The
prooffollowsfromthefactthatthenumberofstatesisfinite. Butifweaddfunctionsymbols
to the language, then the number of states becomes infinite, and PlanSAT becomes only
semidecidable: analgorithmexiststhatwillterminatewiththecorrectanswerforanysolvable
problem, but may not terminate on unsolvable problems. The Bounded PlanSAT problem
remains decidable even in the presence of function symbols. For proofs of the assertions in
thissection, seeGhallabetal.(2004).
BothPlanSATandBounded PlanSATareinthecomplexity classPSPACE,aclassthat
is larger (and hence more difficult) than NP and refers to problems that can be solved by a
deterministic Turing machine with a polynomial amount of space. Even if we make some
rather severe restrictions, the problems remain quite difficult. For example, if we disallow
negative effects, both problems are still NP-hard. However, if we also disallow negative
preconditions, PlanSATreduces totheclassP.
These worst-case results may seem discouraging. We can take solace in the fact that
agents are usually not asked to find plans for arbitrary worst-case problem instances, but
ratherareaskedforplansinspecificdomains(suchasblocks-worldproblemswithnblocks),
which can be much easier than the theoretical worst case. For many domains (including the
blocks world and the air cargo world), Bounded PlanSAT is NP-complete while PlanSAT is
inP;inotherwords,optimalplanningisusuallyhard,butsub-optimalplanningissometimes
easy. To do well on easier-than-worst-case problems, we will need good search heuristics.
That’s the true advantage of the classical planning formalism: it has facilitated the develop-
ment of very accurate domain-independent heuristics, whereas systems based on successor-
stateaxiomsinfirst-orderlogichavehadlesssuccess incomingupwithgoodheuristics.
Section10.2. AlgorithmsforPlanningasState-SpaceSearch 373
10.2 ALGORITHMS FOR PLANNING AS STATE-SPACE SEARCH
Nowweturnourattentiontoplanningalgorithms. Wesawhowthedescription ofaplanning
problem defines a search problem: we can search from the initial state through the space
of states, looking for a goal. One of the nice advantages of the declarative representation of
actionschemasisthatwecanalsosearchbackwardfromthegoal,lookingfortheinitialstate.
Figure10.5comparesforwardandbackwardsearches.
10.2.1 Forward(progression) state-space search
Nowthatwehaveshownhowaplanning problem mapsintoasearch problem, wecansolve
planning problems with any of the heuristic search algorithms from Chapter 3 or a local
search algorithm from Chapter 4 (provided we keep track of the actions used to reach the
goal). From the earliest days of planning research (around 1961) until around 1998 it was
assumed that forward state-space search was too inefficient to be practical. It is not hard to
comeupwithreasonswhy.
First, forward search is prone to exploring irrelevant actions. Consider the noble task
of buying acopy of AI:A Modern Approach from an online bookseller. Suppose there is an
At(P , B)
1
At(P , A)
Fly(P 1 , A, B) 2
At(P , A)
1
(a)
At(P , A)
2
Fly(P, A, B) At(P , A)
2 1
At(P , B)
2
At(P , A)
1
At(P , B) Fly(P, A, B)
2 1
At(P , B)
(b) 1
At(P , B)
2
At(P , B) Fly(P, A, B)
1 2
At(P , A)
2
Figure 10.5 Two approaches to searching for a plan. (a) Forward (progression) search
through the space of states, starting in the initial state and using the problem’s actions to
search forward for a member of the set of goal states. (b) Backward (regression) search
throughsetsofrelevantstates,startingatthesetofstatesrepresentingthegoalandusingthe
inverseoftheactionstosearchbackwardfortheinitialstate.
374 Chapter 10. ClassicalPlanning
actionschemaBuy(isbn)witheffectOwn(isbn). ISBNsare10digits,sothisactionschema
represents 10 billion ground actions. An uninformed forward-search algorithm would have
tostartenumerating these10billionactions tofindonethat leadstothegoal.
Second,planningproblemsoftenhavelargestatespaces. Consideranaircargoproblem
with10airports, whereeachairporthas5planesand20piecesofcargo. Thegoalistomove
allthecargo atairport Atoairport B. Thereisasimplesolution totheproblem: loadthe20
piecesofcargo intooneoftheplanes atA,flytheplanetoB,andunload thecargo. Finding
the solution can be difficult because the average branching factor is huge: each of the 50
planes can fly to 9 other airports, and each of the 200 packages can be either unloaded (if
it is loaded) or loaded into any plane at its airport (if it is unloaded). So in any state there
is a minimum of 450 actions (when all the packages are at airports with no planes) and a
maximumof10,450(whenallpackagesandplanesareatthesameairport). Onaverage,let’s
saythereareabout2000possible actionsperstate,sothesearchgraphuptothedepthofthe
obvioussolution hasabout200041 nodes.
Clearly, even this relatively small problem instance is hopeless without an accurate
heuristic. Althoughmanyreal-worldapplications ofplanninghavereliedondomain-specific
heuristics,itturnsout(asweseeinSection10.2.3)thatstrongdomain-independent heuristics
canbederivedautomatically; thatiswhatmakesforwardsearchfeasible.
10.2.2 Backward (regression) relevant-states search
In regression search we start at the goal and apply the actions backward until we find a
sequence ofstepsthatreaches theinitialstate. Itiscalled relevant-states search because we
RELEVANT-STATES
only consider actions that arerelevant tothegoal (orcurrent state). Asinbelief-state search
(Section4.4),thereisasetofrelevantstatestoconsiderateachstep,notjustasingle state.
Westartwiththegoal,whichisaconjunctionofliteralsformingadescriptionofasetof
states—forexample,thegoal¬Poor∧Famous describesthosestatesinwhichPoor isfalse,
Famous is true, and any other fluent can have any value. If there are n ground fluents in a
domain,thenthereare 2n groundstates(eachfluentcanbetrueorfalse), but 3n descriptions
ofsetsofgoalstates(eachfluentcanbepositive, negative,ornotmentioned).
In general, backward search works only when we know how to regress from a state
description to the predecessor state description. Forexample, it is hard to search backwards
forasolutiontothen-queensproblembecausethereisnoeasywaytodescribethestatesthat
areonemoveawayfromthe goal. Happily, the PDDL representation wasdesigned tomake
iteasytoregressactions—ifadomaincanbeexpressedinPDDL,thenwecandoregression
search onit. Givenaground goal description g and aground action a,theregression from g
(cid:2)
overagivesusastatedescription g definedby
g (cid:2) = (g−ADD(a))∪Precond(a).
That is, the effects that were added by the action need not have been true before, and also
the preconditions must have held before, or else the action could not have been executed.
Note that DEL(a) does not appear in the formula; that’s because while we know the fluents
in DEL(a) are no longer true after the action, we don’t know whether or not they were true
before, sothere’snothing tobesaidaboutthem.
Section10.2. AlgorithmsforPlanningasState-SpaceSearch 375
Togetthefulladvantage ofbackwardsearch,weneedtodealwithpartiallyuninstanti-
atedactionsandstates,notjustgroundones. Forexample,supposethegoalistodeliveraspe-
(cid:2)
cificpieceofcargotoSFO:At(C ,SFO). ThatsuggeststheactionUnload(C ,p,SFO):
2 2
(cid:2)
Action(Unload(C ,p,SFO),
2
PRECOND:In(C
2
,p (cid:2) )∧At(p (cid:2) ,SFO)∧Cargo(C
2
)∧Plane(p (cid:2) )∧Airport(SFO)
EFFECT:At(C
2
,SFO)∧¬In(C
2
,p (cid:2) ).
(cid:2)
(Notethatwehave standardized variable names(changing ptop inthiscase) sothatthere
will be no confusion between variable names if we happen to use the same action schema
twice in a plan. The same approach was used in Chapter 9 for first-order logical inference.)
This represents unloading the package from an unspecified plane at SFO;any plane will do,
but we need not say which one now. We can take advantage of the power of first-order
representations: asingledescription summarizesthepossibility ofusinganyoftheplanesby
(cid:2)
implicitlyquantifying overp. Theregressedstatedescription is
g (cid:2) = In(C ,p (cid:2) )∧At(p (cid:2) ,SFO)∧Cargo(C )∧Plane(p (cid:2) )∧Airport(SFO).
2 2
Thefinalissueisdecidingwhichactionsarecandidates toregressover. Intheforwarddirec-
tion wechose actions that were applicable—those actions that could be the next step in the
plan. Inbackward search wewantactions that are relevant—those actions thatcould bethe
RELEVANCE
laststepinaplanleadinguptothecurrent goalstate.
Foran action to be relevant to a goal it obviously must contribute to the goal: at least
oneoftheaction’seffects(eitherpositiveornegative)mustunifywithanelementofthegoal.
What is less obvious is that the action must not have any effect (positive or negative) that
negates an element of the goal. Now, if the goal is A∧B ∧C and an action has the effect
A∧B∧¬Cthenthereisacolloquialsenseinwhichthatactionisveryrelevanttothegoal—it
gets ustwo-thirds ofthe waythere. Butitisnot relevant inthe technical sense defined here,
because this action could not be the final step of a solution—we would always need at least
onemoresteptoachieve C.
Given the goal At(C ,SFO), several instantiations of Unload are relevant: we could
2
chose any specific plane to unload from, or we could leave the plane unspecified by using
(cid:2)
theaction Unload(C ,p,SFO). Wecan reduce thebranching factorwithout ruling outany
2
solutions byalways using theaction formed bysubstituting the mostgeneral unifierinto the
(standardized) actionschema.
As another example, consider the goal Own(0136042597), given an initial state with
10billionISBNs,andthesingleactionschema
A = Action(Buy(i),PRECOND:ISBN(i),EFFECT:Own(i)).
As we mentioned before, forward search without a heuristic would have to start enumer-
ating the 10 billion ground Buy actions. But with backward search, we would unify the
(cid:2)
goal Own(0136042597) with the (standardized) effect Own(i), yielding the substitution
θ = {i (cid:2) /0136042597}. Then we would regress over the action Subst(θ,A (cid:2) ) to yield the
predecessor state description ISBN(0136042597). This is part of, and thus entailed by, the
initialstate,sowearedone.
376 Chapter 10. ClassicalPlanning
We can make this more formal. Assume a goal description g which contains a goal
(cid:2) (cid:2)
literalg andanactionschemaAthatisstandardized toproduce A. IfA hasaneffectliteral
i
(cid:2) (cid:2) (cid:2) (cid:2)
e
j
where Unify(g
i
,e
j
)=θ and wherewedefine a = SUBST(θ,A)andif there isnoeffect
(cid:2) (cid:2)
ina thatisthenegationofaliteraling,thena isarelevantactiontowards g.
Backwardsearch keepsthebranching factorlowerthan forwardsearch, formostprob-
lem domains. However, the fact that backward search uses state sets rather than individual
states makes it harder to come up with good heuristics. That is the main reason why the
majorityofcurrentsystemsfavorforwardsearch.
10.2.3 Heuristicsforplanning
Neither forward nor backward search is efficient without a good heuristic function. Recall
from Chapter 3 that a heuristic function h(s) estimates the distance from a state s to the
goal and that if we can derive an admissible heuristic for this distance—one that does not
∗
overestimate—then we can use A search to find optimal solutions. An admissible heuristic
can be derived by defining a relaxed problem that is easier to solve. The exact cost of a
solution tothiseasierproblem thenbecomestheheuristicfortheoriginal problem.
By definition, there is no way to analyze an atomic state, and thus it it requires some
ingenuity by a human analyst to define good domain-specific heuristics for search problems
with atomic states. Planning uses a factored representation for states and action schemas.
That makes it possible to define good domain-independent heuristics and for programs to
automatically applyagooddomain-independent heuristic foragivenproblem.
Think of a search problem as a graph where the nodes are states and the edges are
actions. The problem is to find a path connecting the initial state to a goal state. There are
two ways we can relax this problem to make it easier: by adding more edges to the graph,
making it strictly easier to find a path, or by grouping multiple nodes together, forming an
abstraction ofthestatespacethathasfewerstates, andthusiseasiertosearch.
We look first at heuristics that add edges to the graph. For example, the ignore pre-
IGNORE
conditions heuristicdrops allpreconditions from actions. Everyaction becomes applicable
PRECONDITIONS
HEURISTIC
in every state, and any single goal fluent can be achieved in one step (if there is an applica-
ble action—if not, the problem is impossible). This almost implies that the number of steps
required to solve the relaxed problem is the number of unsatisfied goals—almost but not
quite, because (1) some action may achieve multiple goals and (2) some actions may undo
theeffects ofothers. Formanyproblems anaccurate heuristic isobtained byconsidering (1)
and ignoring (2). First, we relax the actions by removing all preconditions and all effects
except those that are literals in the goal. Then, we count the minimum number of actions
required such that the union of those actions’ effects satisfies the goal. This is an instance
SET-COVER of the set-cover problem. There is one minor irritation: the set-cover problem is NP-hard.
PROBLEM
Fortunately a simple greedy algorithm is guaranteed to return a set covering whose size is
within a factor of logn of the true minimum covering, where n is the number of literals in
thegoal. Unfortunately, thegreedyalgorithm losestheguarantee ofadmissibility.
Itisalsopossibletoignoreonlyselectedpreconditionsofactions. Considerthesliding-
block puzzle (8-puzzle or 15-puzzle) from Section 3.2. We could encode this as a planning
Section10.2. AlgorithmsforPlanningasState-SpaceSearch 377
problem involving tileswithasingleschema Slide:
Action(Slide(t,s ,s ),
1 2
PRECOND:On(t,s
1
)∧Tile(t)∧Blank(s
2
)∧Adjacent(s
1
,s
2
)
EFFECT:On(t,s
2
)∧Blank(s
1
)∧¬On(t,s
1
)∧¬Blank(s
2
))
As we saw in Section 3.6, if we remove the preconditions Blank(s ) ∧ Adjacent(s ,s )
2 1 2
then any tile can move in one action to any space and we get the number-of-misplaced-tiles
heuristic. IfweremoveBlank(s )thenwegettheManhattan-distance heuristic. Itiseasyto
2
see how these heuristics could be derived automatically from the action schema description.
Theeaseofmanipulatingtheschemasisthegreatadvantageofthefactoredrepresentation of
planning problems, ascomparedwiththeatomicrepresentation ofsearchproblems.
IGNOREDELETE Another possibility is the ignore delete lists heuristic. Assume for a moment that all
LISTS
goalsandpreconditions contain onlypositiveliterals3 Wewanttocreatearelaxedversionof
theoriginalproblemthatwillbeeasiertosolve,andwherethelengthofthesolutionwillserve
asagoodheuristic. Wecandothatbyremovingthedeletelistsfromallactions(i.e.,removing
allnegativeliteralsfromeffects). Thatmakesitpossibletomakemonotonicprogresstowards
thegoal—noactionwilleverundoprogressmadebyanotheraction. ItturnsoutitisstillNP-
hardtofindtheoptimalsolution tothisrelaxed problem, but anapproximate solution canbe
found in polynomial time by hill-climbing. Figure 10.6 diagrams part of the state space for
two planning problems using the ignore-delete-lists heuristic. The dots represent states and
the edges actions, and theheight ofeach dot abovethe bottom plane represents the heuristic
value. States onthe bottom plane are solutions. Inboth these problems, there isawidepath
tothegoal. Therearenodeadends,sononeedforbacktracking; asimplehillclimbingsearch
willeasilyfindasolution totheseproblems(although itmaynotbeanoptimalsolution).
The relaxed problems leave us with a simplified—but still expensive—planning prob-
lemjusttocalculate thevalueoftheheuristic function. Manyplanning problems have 10100
states ormore, and relaxing the actions does nothing to reduce the number of states. There-
fore, we now look at relaxations that decrease the number of states by forming a state ab-
straction—a many-to-one mapping from states in the ground representation of the problem
STATEABSTRACTION
totheabstract representation.
The easiest form of state abstraction is to ignore some fluents. For example, consider
an air cargo problem with 10 airports, 50 planes, and 200 pieces of cargo. Each plane can
be at one of 10 airports and each package can be either in one of the planes or unloaded at
one of theairports. Sothere are 5010 ×20050+10 ≈ 10155 states. Now consider a particular
problem inthat domaininwhichithappens thatallthepackages areatjust 5oftheairports,
andallpackagesatagivenairporthavethesamedestination. Thenausefulabstractionofthe
problemistodropalltheAtfluentsexceptfortheonesinvolving oneplaneandonepackage
at each of the 5 airports. Now there are only 510 ×55+10 ≈ 1017 states. A solution in this
abstract state space will be shorter than a solution in the original space (and thus will be an
admissible heuristic), and the abstract solution is easy to extend to a solution to the original
problem (byaddingadditional Load andUnload actions).
3 Manyproblemsarewrittenwiththisconvention. Forproblemsthataren’t,replaceeverynegativeliteral¬P
inagoalorpreconditionwithanewpositiveliteral,P(cid:3).
378 Chapter 10. ClassicalPlanning
Figure10.6 Twostatespacesfromplanningproblemswiththeignore-delete-listsheuris-
tic. Theheightabovethebottomplaneistheheuristicscoreofastate;statesonthebottom
planearegoals. Therearenolocalminima,sosearchforthegoalisstraightforward. From
Hoffmann(2005).
Akeyideaindefiningheuristicsisdecomposition: dividingaproblemintoparts,solv-
DECOMPOSITION
SUBGOAL ing each part independently, and then combining the parts. The subgoal independence as-
INDEPENDENCE
sumption is that the cost of solving a conjunction of subgoals is approximated by the sum
of the costs of solving each subgoal independently. The subgoal independence assumption
canbeoptimisticorpessimistic. Itisoptimisticwhentherearenegativeinteractions between
the subplans for each subgoal—for example, when an action in one subplan deletes a goal
achieved by another subplan. It is pessimistic, and therefore inadmissible, when subplans
containredundantactions—forinstance,twoactionsthatcouldbereplacedbyasingleaction
inthemergedplan.
SupposethegoalisasetoffluentsG,whichwedivideintodisjointsubsetsG ,...,G .
1 n
Wethen findplans P ,...,P that solve therespective subgoals. Whatisanestimate ofthe
1 n
costoftheplanforachievingallofG? WecanthinkofeachCost(P )asaheuristicestimate,
i
andweknowthatifwecombineestimatesbytakingtheirmaximumvalue,wealwaysgetan
admissible heuristic. So max
i
COST(P
i
) is admissible, and sometimes it is exactly correct:
it could be that P serendipitously achieves all the G . But in most cases, in practice the
1 i
estimateistoolow. Couldwesumthecostsinstead? Formanyproblemsthatisareasonable
estimate,butitisnotadmissible. ThebestcaseiswhenwecandeterminethatG andG are
i j
independent. IftheeffectsofP leaveallthepreconditions andgoalsofP unchanged, then
i j
theestimate COST(P
i
)+COST(P
j
)isadmissible, andmoreaccuratethanthemaxestimate.
WeshowinSection10.3.1thatplanning graphscanhelpprovidebetterheuristic estimates.
Itisclearthat thereisgreat potential forcutting downthe search space byforming ab-
stractions. The trick is choosing the right abstractions and using them in a way that makes
thetotalcost—defininganabstraction, doinganabstractsearch,andmappingtheabstraction
back to the original problem—less than the cost of solving the original problem. The tech-
Section10.3. PlanningGraphs 379
niques of pattern databases from Section 3.6.3 can be useful, because the cost of creating
thepatterndatabase canbeamortizedovermultipleproblem instances.
AnexampleofasystemthatmakesuseofeffectiveheuristicsisFF,orFASTFORWARD
(Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic,
estimating the heuristic with the help of a planning graph (see Section 10.3). FF then uses
hill-climbing search (modifiedtokeeptrack oftheplan)withtheheuristic tofindasolution.
Whenithitsaplateauorlocalmaximum—whennoactionleadstoastatewithbetterheuristic
score—then FF uses iterative deepening search until it finds a state that is better, or it gives
upandrestartshill-climbing.
10.3 PLANNING GRAPHS
All of the heuristics we have suggested can suffer from inaccuracies. This section shows
how a special data structure called a planning graph can be used to give better heuristic
PLANNINGGRAPH
estimates. These heuristics can be applied to any of the search techniques we have seen so
far. Alternatively, wecansearch forasolution overthespace formedbytheplanning graph,
usinganalgorithm called GRAPHPLAN.
Aplanningproblemasksifwecanreachagoalstatefromtheinitialstate. Supposewe
aregivenatreeofallpossible actions fromtheinitial statetosuccessorstates, andtheirsuc-
cessors, andsoon. Ifweindexedthistreeappropriately, we couldanswertheplanning ques-
tion “can wereach state G from state S ” immediately, just by looking it up. Ofcourse, the
0
tree is of exponential size, so this approach is impractical. A planning graph is polynomial-
size approximation to this tree that can be constructed quickly. The planning graph can’t
answer definitively whether G is reachable from S , but it can estimate how many steps it
0
takestoreachG. Theestimateisalwayscorrectwhenitreportsthegoalisnotreachable, and
itneveroverestimates thenumberofsteps,soitisanadmissible heuristic.
Aplanninggraphisadirectedgraphorganizedinto levels: firstalevelS fortheinitial
LEVEL 0
state,consisting ofnodesrepresenting eachfluentthatholdsinS ;thenalevelA consisting
0 0
of nodes for each ground action that might be applicable in S ; then alternating levels S
0 i
followedbyA ;untilwereachatermination condition (tobediscussed later).
i
Roughly speaking, S contains all the literals that could hold at time i, depending on
i
theactions executed atpreceding timesteps. Ifitispossible thateither P or¬P could hold,
then both will be represented in S . Also roughly speaking, A contains all the actions that
i i
could have their preconditions satisfied at time i. We say “roughly speaking” because the
planning graph records only a restricted subset of the possible negative interactions among
actions; therefore, aliteral mightshow upatlevel S whenactually itcould notbetrueuntil
j
a later level, if at all. (A literal will never show up too late.) Despite the possible error, the
level j at which a literal first appears is a good estimate of how difficult it is to achieve the
literalfromtheinitialstate.
Planning graphs work only for propositional planning problems—ones with no vari-
ables. As we mentioned on page 368, it is straightforward to propositionalize a set of ac-
380 Chapter 10. ClassicalPlanning
Init(Have(Cake))
Goal(Have(Cake) ∧ Eaten(Cake))
Action(Eat(Cake)
PRECOND:Have(Cake)
EFFECT:¬Have(Cake) ∧ Eaten(Cake))
Action(Bake(Cake)
PRECOND:¬Have(Cake)
EFFECT:Have(Cake))
Figure10.7 The“havecakeandeatcaketoo”problem.
S A S A S
0 0 1 1 2
Bake(Cake)
Have(Cake) Have(Cake) Have(Cake)
¬Have(Cake) ¬Have(Cake)
Eat(Cake) Eat(Cake)
Eaten(Cake) Eaten(Cake)
¬Eaten(Cake) ¬Eaten(Cake) ¬Eaten(Cake)
Figure10.8 Theplanninggraphforthe“havecakeandeatcaketoo”problemuptolevel
S . Rectangles indicate actions (small squares indicate persistence actions), and straight
2
linesindicatepreconditionsandeffects. Mutexlinksareshownascurvedgraylines. Notall
mutexlinksareshown,becausethegraphwouldbetoocluttered. Ingeneral,iftwoliterals
are mutex at Si, then the persistence actions for those literals will be mutex at Ai and we
neednotdrawthatmutexlink.
tion schemas. Despite the resulting increase inthe size ofthe problem description, planning
graphshaveprovedtobeeffectivetoolsforsolvinghardplanningproblems.
Figure 10.7 shows a simple planning problem, and Figure 10.8 shows its planning
graph. Each action at level A is connected to its preconditions at S and its effects at S .
i i i+1
So a literal appears because an action caused it, but we also want to say that a literal can
PERSISTENCE persist ifnoaction negates it. Thisisrepresented bya persistence action (sometimes called
ACTION
ano-op). Foreveryliteral C,weaddtotheproblemapersistence actionwithprecondition C
and effect C. Level A in Figure 10.8 shows one “real” action, Eat(Cake), along with two
0
persistence actionsdrawnassmallsquareboxes.
Level A contains all the actions that could occur in state S , but just as important it
0 0
recordsconflictsbetweenactionsthatwouldpreventthemfromoccurringtogether. Thegray
linesinFigure10.8indicate mutualexclusion(ormutex)links. Forexample, Eat(Cake)is
MUTUALEXCLUSION
mutually exclusive with the persistence of either Have(Cake) or ¬Eaten(Cake). We shall
MUTEX
seeshortlyhowmutexlinksarecomputed.
LevelS containsalltheliteralsthatcouldresultfrompickinganysubsetoftheactions
1
in A , as well as mutex links (gray lines) indicating literals that could not appear together,
0
regardless ofthechoiceofactions. Forexample, Have(Cake)andEaten(Cake)aremutex:
Section10.3. PlanningGraphs 381
depending on the choice of actions in A , either, but not both, could be the result. In other
0
words, S represents a belief state: a set of possible states. The members of this set are all
1
subsetsoftheliteralssuchthatthereisnomutexlinkbetweenanymembersofthesubset.
Wecontinue inthisway,alternating betweenstatelevelS andaction levelA untilwe
i i
reach apoint wheretwoconsecutive levels areidentical. At thispoint, wesaythatthegraph
hasleveledoff. ThegraphinFigure10.8levelsoffatS .
LEVELEDOFF 2
Whatweendupwithisastructurewhereevery A levelcontainsalltheactionsthatare
i
applicableinS ,alongwithconstraintssayingthattwoactionscannotbothbeexecutedatthe
i
samelevel. Every S level contains allthe literals that could result from anypossible choice
i
of actions in A i−1 , along with constraints saying which pairs of literals are not possible.
It is important to note that the process of constructing the planning graph does not require
choosingamongactions,whichwouldentailcombinatorialsearch. Instead,itjustrecordsthe
impossibility ofcertainchoices usingmutexlinks.
Wenowdefinemutexlinksforbothactionsandliterals. Amutexrelationholdsbetween
twoactions atagivenlevelifanyofthefollowingthreeconditions holds:
• Inconsistent effects: oneactionnegatesaneffectoftheother. Forexample, Eat(Cake)
and the persistence of Have(Cake) have inconsistent effects because they disagree on
theeffectHave(Cake).
• Interference: one of the effects of one action is the negation of a precondition of the
other. ForexampleEat(Cake)interfereswiththepersistenceofHave(Cake)bynegat-
ingitsprecondition.
• Competing needs: one of the preconditions of one action is mutually exclusive with a
preconditionoftheother. Forexample,Bake(Cake)andEat(Cake)aremutexbecause
theycompeteonthevalueoftheHave(Cake)precondition.
Amutexrelationholdsbetweentwoliteralsatthesamelevelifoneisthenegationoftheother
or if each possible pair of actions that could achieve the two literals is mutually exclusive.
This condition is called inconsistent support. Forexample, Have(Cake) and Eaten(Cake)
are mutex in S because the only way of achieving Have(Cake), the persistence action, is
1
mutex with the only way of achieving Eaten(Cake), namely Eat(Cake). In S the two
2
literals are not mutex, because there are new ways of achieving them, such as Bake(Cake)
andthepersistence ofEaten(Cake),thatarenotmutex.
A planning graph is polynomial in the size of the planning problem. For a planning
problem with l literals and a actions, each S has no more than l nodes and l2 mutex links,
i
and each A has nomore than a+l nodes (including theno-ops), (a+l)2 mutex links, and
i
2(al + l) precondition and effect links. Thus, an entire graph with n levels has a size of
O(n(a+l)2). Thetimetobuildthegraphhasthesamecomplexity.
10.3.1 Planning graphs forheuristicestimation
Aplanning graph,onceconstructed, isarichsource ofinformation abouttheproblem. First,
ifanygoalliteralfailstoappearinthefinallevelofthegraph,thentheproblemisunsolvable.
Second, we can estimate the cost of achieving any goal literal g from state s as the level at
i
which g first appears in the planning graph constructed from initial state s. Wecall this the
i
382 Chapter 10. ClassicalPlanning
levelcostofg . InFigure10.8,Have(Cake)haslevelcost0andEaten(Cake)haslevelcost
LEVELCOST i
1. It is easy to show (Exercise 10.10) that these estimates are admissible for the individual
goals. The estimate might not always be accurate, however, because planning graphs allow
several actions at each level, whereas the heuristic counts just the level and not the number
SERIALPLANNING of actions. For this reason, it is common to use a serial planning graph for computing
GRAPH
heuristics. A serial graph insists that only one action can actually occur at any given time
step; thisisdonebyadding mutexlinks between everypairof nonpersistence actions. Level
costsextractedfromserialgraphsareoftenquitereasonable estimatesofactualcosts.
Toestimate the cost of a conjunction of goals, there are three simple approaches. The
max-levelheuristic simplytakesthemaximumlevelcostofanyofthegoals;thisisadmissi-
MAX-LEVEL
ble,butnotnecessarily accurate.
The level sum heuristic, following the subgoal independence assumption, returns the
LEVELSUM
sum of the level costs of the goals; this can be inadmissible but works well in practice
for problems that are largely decomposable. It is much more accurate than the number-
of-unsatisfied-goals heuristic from Section 10.2. For our problem, the level-sum heuristic
estimate for the conjunctive goal Have(Cake)∧Eaten(Cake) will be 0+1 = 1, whereas
the correct answeris 2, achieved by the plan [Eat(Cake),Bake(Cake)]. That doesn’t seem
so bad. A more serious error is that if Bake(Cake) were not in the set of actions, then the
estimatewouldstillbe1,wheninfacttheconjunctive goalwouldbeimpossible.
Finally, the set-level heuristic findsthe level at which all the literals inthe conjunctive
SET-LEVEL
goal appear in the planning graph without any pair of them being mutually exclusive. This
heuristic gives the correct values of 2 for our original problem and infinity for the problem
without Bake(Cake). It is admissible, it dominates the max-level heuristic, and it works
extremelywellontasksinwhichthereisagooddealofinteraction amongsubplans. Itisnot
perfect, ofcourse; forexample,itignoresinteractions amongthreeormoreliterals.
Asatoolforgeneratingaccurateheuristics,wecanviewtheplanninggraphasarelaxed
problem that is efficiently solvable. To understand the nature of the relaxed problem, we
need to understand exactly what it means for a literal g to appear at level S in the planning
i
graph. Ideally, wewould like itto beaguarantee that there exists aplan with iaction levels
that achieves g, and also that if g does not appear, there is no such plan. Unfortunately,
makingthatguaranteeisasdifficultassolvingtheoriginalplanningproblem. Sotheplanning
graph makes the second half of the guarantee (if g does not appear, there is no plan), but
if g does appear, then all the planning graph promises is that there is a plan that possibly
achieves g and has no “obvious” flaws. An obvious flaw is defined as a flaw that can be
detected by considering two actions or two literals at a time—in other words, by looking at
themutexrelations. Therecould bemoresubtle flawsinvolving three, four, ormoreactions,
but experience has shown that it is not worth the computational effort to keep track of these
possibleflaws. Thisissimilartoalessonlearnedfromconstraintsatisfactionproblems—that
itisoftenworthwhiletocompute2-consistencybeforesearchingforasolution,butlessoften
worthwhiletocompute3-consistency orhigher. (Seepage211.)
Oneexampleofanunsolvableproblemthatcannotberecognizedassuchbyaplanning
graphistheblocks-world problem wherethegoalistogetblockAonB,B onC,andC on
A. Thisisanimpossiblegoal;atowerwiththebottomontopofthetop. Butaplanninggraph
Section10.3. PlanningGraphs 383
cannot detect the impossibility, because any twoof the three subgoals are achievable. There
arenomutexesbetweenanypairofliterals,onlybetweenthe threeasawhole. Todetectthat
thisproblem isimpossible, wewouldhavetosearchovertheplanning graph.
10.3.2 The GRAPHPLAN algorithm
Thissubsectionshowshowtoextractaplandirectlyfromtheplanninggraph,ratherthanjust
using the graph to provide aheuristic. The GRAPHPLAN algorithm (Figure 10.9) repeatedly
adds alevel to aplanning graph with EXPAND-GRAPH. Once all the goals show up as non-
mutexinthegraph, GRAPHPLAN calls EXTRACT-SOLUTION tosearchforaplanthatsolves
the problem. If that fails, it expands another level and tries again, terminating with failure
whenthereisnoreasontogoon.
functionGRAPHPLAN(problem)returnssolutionorfailure
graph←INITIAL-PLANNING-GRAPH(problem)
goals←CONJUNCTS(problem.GOAL)
nogoods←anemptyhashtable
fortl =0to∞do
ifgoals allnon-mutexinStofgraph then
solution←EXTRACT-SOLUTION(graph,goals,NUMLEVELS(graph),nogoods)
ifsolution (cid:7)=failure thenreturnsolution
ifgraph andnogoods havebothleveledoffthenreturnfailure
graph←EXPAND-GRAPH(graph,problem)
Figure10.9 The GRAPHPLAN algorithm. GRAPHPLAN calls EXPAND-GRAPH toadda
leveluntileitherasolutionisfoundbyEXTRACT-SOLUTION,ornosolutionispossible.
LetusnowtracetheoperationofGRAPHPLANonthesparetireproblemfrompage370.
The graph is shown in Figure 10.10. The first line of GRAPHPLAN initializes the planning
graph to a one-level (S ) graph representing the initial state. The positive fluents from the
0
problem description’s initial state areshown, asaretherelevant negative fluents. Notshown
aretheunchanging positiveliterals(suchasTire(Spare))andtheirrelevantnegativeliterals.
Thegoal At(Spare,Axle)isnotpresent in S
0
,soweneed notcall EXTRACT-SOLUTION—
wearecertain that thereisnosolution yet. Instead, EXPAND-GRAPH adds into A
0
thethree
actionswhosepreconditionsexistatlevelS (i.e.,alltheactionsexceptPutOn(Spare,Axle)),
0
alongwithpersistence actionsforalltheliteralsin S . Theeffectsoftheactionsareaddedat
0
levelS
1
. EXPAND-GRAPH thenlooksformutexrelations andaddsthemtothegraph.
At(Spare,Axle)isstillnotpresentinS
1
,soagainwedonotcallEXTRACT-SOLUTION.
Wecall EXPAND-GRAPH again, adding A
1
and S
1
and giving us the planning graph shown
inFigure10.10. Nowthatwehavethefullcomplementofactions, itisworthwhiletolookat
someoftheexamplesofmutexrelations andtheircauses:
• Inconsistent effects: Remove(Spare,Trunk) is mutex with LeaveOvernight because
onehastheeffectAt(Spare,Ground)andtheotherhasitsnegation.
384 Chapter 10. ClassicalPlanning
S A S A S
0 0 1 1 2
At(Spare,Trunk) At(Spare,Trunk) At(Spare,Trunk)
Remove(Spare,Trunk)
Remove(Spare,Trunk)
¬At(Spare,Trunk) ¬At(Spare,Trunk)
Remove(Flat,Axle) Remove(Flat,Axle)
At(Flat,Axle) At(Flat,Axle) At(Flat,Axle)
LeaveOvernight ¬At(Flat,Axle) ¬At(Flat,Axle)
LeaveOvernight
¬At(Spare,Axle) ¬At(Spare,Axle) ¬At(Spare,Axle)
PutOn(Spare,Axle) At(Spare,Axle)
¬At(Flat,Ground) ¬At(Flat,Ground) ¬At(Flat,Ground)
At(Flat,Ground) At(Flat,Ground)
¬At(Spare,Ground) ¬At(Spare,Ground) ¬At(Spare,Ground)
At(Spare,Ground) At(Spare,Ground)
Figure10.10 The planninggraphforthe spare tire problemafterexpansionto levelS .
2
Mutexlinksareshownasgraylines. Notalllinksareshown,becausethegraphwouldbetoo
clutteredifweshowedthemall. Thesolutionisindicatedbyboldlinesandoutlines.
• Interference: Remove(Flat,Axle)ismutexwithLeaveOvernight becauseonehasthe
precondition At(Flat,Axle)andtheotherhasitsnegationasaneffect.
• Competing needs: PutOn(Spare,Axle) is mutex with Remove(Flat,Axle) because
onehasAt(Flat,Axle)asaprecondition andtheotherhasitsnegation.
• Inconsistentsupport: At(Spare,Axle)ismutexwithAt(Flat,Axle)inS becausethe
2
only way of achieving At(Spare,Axle) is by PutOn(Spare,Axle), and that is mutex
withthepersistence actionthatistheonlywayofachieving At(Flat,Axle). Thus,the
mutexrelations detect theimmediate conflict that arises from trying toput twoobjects
inthesameplaceatthesametime.
This time, when wego back to the start of the loop, all the literals from the goal are present
in S , and none of them is mutex with any other. That means that a solution might exist,
2
and EXTRACT-SOLUTION will try to find it. We can formulate EXTRACT-SOLUTION as a
Boolean constraint satisfaction problem (CSP) where the variables are the actions at each
level,thevaluesforeachvariableareinoroutoftheplan,andtheconstraintsarethemutexes
andtheneedtosatisfyeachgoalandprecondition.
Alternatively,wecandefineEXTRACT-SOLUTIONasabackwardsearchproblem,where
each stateinthesearch contains apointertoalevelintheplanning graph and asetofunsat-
isfiedgoals. Wedefinethissearchproblem asfollows:
• The initial state is the last level of the planning graph, S , along with the set of goals
n
fromtheplanning problem.
• The actions available in a state at level S are to select any conflict-free subset of the
i
actions inA i−1 whose effects coverthe goals in thestate. Theresulting state has level
S i−1 and has as its set of goals the preconditions for the selected set of actions. By
“conflictfree,”wemeanasetofactionssuchthatnotwoofthemaremutexandnotwo
oftheirpreconditions aremutex.
Section10.3. PlanningGraphs 385
• ThegoalistoreachastateatlevelS suchthatallthegoalsaresatisfied.
0
• Thecostofeachactionis1.
Forthisparticularproblem,westartatS withthegoalAt(Spare,Axle). Theonlychoicewe
2
haveforachievingthegoalsetisPutOn(Spare,Axle). ThatbringsustoasearchstateatS
1
with goals At(Spare,Ground) and ¬At(Flat,Axle). The former can be achieved only by
Remove(Spare,Trunk), and the latter by either Remove(Flat,Axle) or LeaveOvernight.
ButLeaveOvernight ismutexwithRemove(Spare,Trunk),sotheonlysolutionistochoose
Remove(Spare,Trunk)andRemove(Flat,Axle). ThatbringsustoasearchstateatS with
0
the goals At(Spare,Trunk) and At(Flat,Axle). Both of these are present in the state, so
we have a solution: the actions Remove(Spare, Trunk) and Remove(Flat, Axle) in level
A ,followedbyPutOn(Spare,Axle)inA .
0 1
In the case where EXTRACT-SOLUTION fails to find a solution for a set of goals at
a given level, we record the (level,goals) pair as a no-good, just as we did in constraint
learningforCSPs(page220). WheneverEXTRACT-SOLUTION iscalledagainwiththesame
levelandgoals, wecanfindtherecorded no-good andimmediately returnfailure ratherthan
searching again. Weseeshortlythatno-goods arealsousedinthetermination test.
Weknow that planning is PSPACE-complete and that constructing the planning graph
takespolynomialtime,soitmustbethecasethatsolutionextractionisintractableintheworst
case. Therefore,wewillneedsomeheuristicguidanceforchoosingamongactionsduringthe
backward search. One approach that works well in practice is a greedy algorithm based on
thelevelcostoftheliterals. Foranysetofgoals,weproceedinthefollowingorder:
1. Pickfirsttheliteralwiththehighestlevelcost.
2. Toachievethatliteral,preferactionswitheasierpreconditions. Thatis,chooseanaction
suchthatthesum(ormaximum)ofthelevelcostsofitspreconditions issmallest.
10.3.3 Terminationof GRAPHPLAN
Sofar,wehaveskatedoverthequestionoftermination. HereweshowthatGRAPHPLANwill
infactterminateandreturnfailure whenthereisnosolution.
Thefirstthingtounderstand iswhywecan’tstopexpanding thegraphassoonasithas
leveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport
A, all of which have airport B as their destination. In this version of the problem, only one
piece of cargo can fitinthe plane at atime. Thegraph willlevel offatlevel 4, reflecting the
factthatforanysinglepieceofcargo, wecanloadit,flyit,andunloaditatthedestination in
threesteps. Butthatdoesnotmeanthatasolution canbeextractedfromthegraphatlevel4;
infact asolution willrequire 4n−1steps: foreachpiece ofcargo weload, fly,and unload,
andforallbutthelastpieceweneedtoflybacktoairport Atogetthenextpiece.
Howlongdowehavetokeepexpandingafterthegraphhasleveledoff? Ifthefunction
EXTRACT-SOLUTION fails to find a solution, then there must have been at least one set of
goals that were not achievable and were marked as a no-good. So if it is possible that there
might be fewer no-goods in the next level, then we should continue. As soon as the graph
itself and the no-goods have both leveled off, withnosolution found, wecan terminate with
failurebecause thereisnopossibility ofasubsequent changethatcouldaddasolution.
386 Chapter 10. ClassicalPlanning
Nowallwehavetodoisprovethatthegraphandtheno-goodswillalwaysleveloff. The
keytothisproofisthatcertainproperties ofplanninggraphsaremonotonically increasingor
decreasing. “Xincreases monotonically” meansthat theset ofXsatleveli+1isasuperset
(notnecessarily proper)ofthesetatlevel i. Theproperties areasfollows:
• Literals increase monotonically: Once a literal appears at a given level, it will appear
atallsubsequent levels. Thisisbecause ofthepersistence actions; oncealiteral shows
up,persistence actionscauseittostayforever.
• Actionsincrease monotonically: Onceanaction appears atagiven level, itwillappear
atall subsequent levels. Thisis aconsequence of the monotonic increase ofliterals; if
thepreconditionsofanactionappearatonelevel,theywillappearatsubsequentlevels,
andthussowilltheaction.
• Mutexesdecreasemonotonically: IftwoactionsaremutexatagivenlevelA ,thenthey
i
willalsobemutexforallpreviouslevelsatwhichtheybothappear. Thesameholdsfor
mutexes between literals. It might not always appear that way in the figures, because
the figures have a simplification: they display neither literals that cannot hold at level
S nor actions that cannot be executed at level A . We can see that “mutexes decrease
i i
monotonically” istrueifyouconsiderthattheseinvisible literalsandactionsaremutex
witheverything.
The proof can be handled by cases: if actions A and B are mutex at level A , it
i
must be because of one of the three types of mutex. The first two, inconsistent effects
and interference, are properties of the actions themselves, so if the actions are mutex
at A , they will be mutex at every level. The third case, competing needs, depends on
i
conditions at level S : that level must contain a precondition of A that is mutex with
i
a precondition of B. Now, these two preconditions can be mutex if they are negations
of each other (in which case they would be mutex in every level) or if all actions for
achieving one aremutex withallactions forachieving the other. Butwealready know
that the available actions are increasing monotonically, so, by induction, the mutexes
mustbedecreasing.
• No-goods decrease monotonically: If a set of goals is not achievable at a given level,
thentheyarenotachievableinanypreviouslevel. Theproofisbycontradiction: ifthey
were achievable at some previous level, then we could just add persistence actions to
makethemachievable atasubsequent level.
Because the actions and literals increase monotonically and because there are only a finite
number of actions and literals, there must come a level that has the same number of actions
andliteralsasthepreviouslevel. Becausemutexesandno-goodsdecrease,andbecausethere
can never be fewer than zero mutexes or no-goods, there must come a level that has the
same number of mutexes and no-goods as the previous level. Once agraph has reached this
state, then if one of the goals is missing oris mutex with another goal, then wecan stop the
GRAPHPLAN algorithm and return failure. That concludes a sketch of the proof; for more
detailsseeGhallabetal.(2004).
Section10.4. OtherClassicalPlanningApproaches 387
Year Track WinningSystems(approaches)
2008 Optimal GAMER(modelchecking,bidirectionalsearch)
2008 Satisficing LAMA(fastdownwardsearchwithFFheuristic)
2006 Optimal SATPLAN,MAXPLAN(Booleansatisfiability)
2006 Satisficing SGPLAN(forwardsearch;partitionsintoindependentsubproblems)
2004 Optimal SATPLAN(Booleansatisfiability)
2004 Satisficing FASTDIAGONALLYDOWNWARD(forwardsearchwithcausalgraph)
2002 Automated LPG(localsearch,planninggraphsconvertedtoCSPs)
2002 Hand-coded TLPLAN(temporalactionlogicwithcontrolrulesforforwardsearch)
2000 Automated FF(forwardsearch)
2000 Hand-coded TALPLANNER(temporalactionlogicwithcontrolrulesforforwardsearch)
1998 Automated IPP(planninggraphs);HSP(forwardsearch)
Figure10.11 Someofthetop-performingsystemsintheInternationalPlanningCompe-
tition. Eachyearthere are varioustracks: “Optimal”means the plannersmustproducethe
shortestpossibleplan,while“Satisficing”meansnonoptimalsolutionsareaccepted. “Hand-
coded”meansdomain-specificheuristicsareallowed;“Automated”meanstheyarenot.
10.4 OTHER CLASSICAL PLANNING APPROACHES
Currentlythemostpopularandeffectiveapproaches tofullyautomated planning are:
• TranslatingtoaBooleansatisfiability (SAT)problem
• Forwardstate-space searchwithcarefully craftedheuristics (Section10.2)
• Searchusingaplanning graph(Section10.3)
These three approaches are not the only ones tried inthe 40-year history of automated plan-
ning. Figure10.11showssomeofthetopsystemsintheInternationalPlanningCompetitions,
whichhavebeenheldeveryevenyearsince1998. Inthissectionwefirstdescribethetransla-
tiontoasatisfiability problem andthendescribe threeotherinfluential approaches: planning
asfirst-orderlogicaldeduction; asconstraint satisfaction; andasplanrefinement.
10.4.1 Classicalplanning as Booleansatisfiability
InSection7.7.4wesawhowSATPLANsolvesplanningproblemsthatareexpressedinpropo-
sitional logic. Here we show how to translate a PDDL description into a form that can be
processed by SATPLAN. Thetranslation isaseriesofstraightforward steps:
• Propositionalize the actions: replace each action schema with a set of ground actions
formedbysubstitutingconstantsforeachofthevariables. Thesegroundactionsarenot
partofthetranslation, butwillbeusedinsubsequent steps.
• Define the initial state: assert F0 for every fluent F in the problem’s initial state, and
¬F foreveryfluentnotmentioned intheinitialstate.
• Propositionalize thegoal: foreveryvariableinthegoal,replacetheliteralsthatcontain
thevariablewithadisjunction overconstants. Forexample,thegoalofhavingblockA
388 Chapter 10. ClassicalPlanning
onanotherblock, On(A,x)∧Block(x)inaworldwithobjects A,B andC,wouldbe
replacedbythegoal
(On(A,A)∧Block(A))∨(On(A,B)∧Block(B))∨(On(A,C)∧Block(C)).
• Addsuccessor-state axioms: Foreachfluent F,addanaxiomoftheform
Ft+1 ⇔ ActionCausesFt∨(Ft∧¬ActionCausesNotFt),
where ActionCausesF is a disjunction of all the ground actions that have F in their
addlist, andActionCausesNotF isadisjunction ofalltheground actions thathave F
intheirdeletelist.
• Addprecondition axioms: Foreach ground action A, add the axiom At ⇒ PRE(A)t,
thatis,ifanactionistakenattimet,thenthepreconditions musthavebeentrue.
• Addactionexclusion axioms: saythateveryactionisdistinctfromeveryotheraction.
Theresulting translation isintheformthatwecanhandtoSATPLAN tofindasolution.
10.4.2 Planning asfirst-order logicaldeduction: Situation calculus
PDDLisalanguagethatcarefullybalancestheexpressiveness ofthelanguagewiththecom-
plexity ofthealgorithms thatoperate onit. Butsomeproblemsremaindifficulttoexpress in
PDDL. Forexample, we can’t express the goal “move all the cargo from A to B regardless
ofhowmanypiecesofcargothereare”inPDDL,butwecandoitinfirst-orderlogic,usinga
universal quantifier. Likewise, first-orderlogiccanconcisely express global constraints such
as“nomorethanfourrobotscanbeinthesameplaceatthesame time.” PDDLcanonlysay
thiswithrepetitious preconditions oneverypossible actionthatinvolves amove.
The propositional logic representation of planning problems also has limitations, such
as the fact that the notion of time is tied directly to fluents. For example, South2 means
“the agent is facing south at time 2.” With that representation, there is no way to say “the
agentwouldbefacingsouth attime2ifitexecuted aright turnattime1;otherwiseitwould
be facing east.” First-order logic lets us get around this limitation by replacing the notion
of linear time with a notion of branching situations, using a representation called situation
SITUATION calculusthatworkslikethis:
CALCULUS
• The initial state is called a situation. If s is a situation and a is an action, then
SITUATION
RESULT(s,a) is also a situation. There are no other situations. Thus, a situation cor-
responds to a sequence, or history, of actions. You can also think of a situation as the
resultofapplyingtheactions,butnotethattwosituationsarethesameonlyiftheirstart
and actions are the same: (RESULT(s,a) = RESULT(s (cid:2) ,a (cid:2) )) ⇔ (s = s (cid:2) ∧a = a (cid:2) ).
Someexamplesofactionsandsituations areshowninFigure10.12.
• Afunctionorrelationthatcanvaryfromonesituationtothenextisafluent. Byconven-
tion,thesituationsisalwaysthelastargumenttothefluent,forexampleAt(x,l,s)isa
relationalfluentthatistruewhenobjectxisatlocationlinsituations,andLocation isa
functionalfluentsuchthatLocation(x,s) = lholdsinthesamesituationsasAt(x,l,s).
• Each action’s preconditions are described with a possibility axiom that says when the
POSSIBILITYAXIOM
action can be taken. Ithas the form Φ(s) ⇒ Poss(a,s)where Φ(s)issome formula
Section10.4. OtherClassicalPlanningApproaches 389
Gold PIT
PIT
PIT
Gold PIT
PIT
PIT
Gold PIT
PIT
PIT
Result(Result(S, Forward),
0
Turn(Right))
Turn(Right)
Result(S, Forward)
0
Forward
S
0
Figure10.12 Situationsastheresultsofactionsinthewumpusworld.
involving sthatdescribes thepreconditions. Anexamplefromthewumpusworldsays
thatitispossible toshootiftheagentisaliveandhasanarrow:
Alive(Agent,s)∧Have(Agent,Arrow,s) ⇒ Poss(Shoot,s)
• Each fluent is described with a successor-state axiom that says what happens to the
fluent, depending on what action is taken. This is similar to the approach we took for
propositional logic. Theaxiomhastheform
Actionispossible ⇒
(Fluentistrueinresultstate ⇔ Action’seffectmadeittrue
∨Itwastruebeforeandactionleftitalone).
Forexample, theaxiom forthe relational fluent Holding says that the agent isholding
somegold g afterexecuting apossible action ifand only ifthe action wasaGrab ofg
oriftheagentwasalreadyholding g andtheactionwasnotreleasing it:
Poss(a,s) ⇒
(Holding(Agent,g,Result(a,s)) ⇔
a=Grab(g)∨(Holding(Agent,g,s)∧a(cid:7)= Release(g))).
UNIQUEACTION • We need unique action axioms so that the agent can deduce that, for example, a (cid:7)=
AXIOMS
Release(g). For each distinct pair of action names A and A we have an axiom that
i j
saystheactionsaredifferent:
A (x,...) (cid:7)= A (y,...)
i j
390 Chapter 10. ClassicalPlanning
and for each action name A wehave an axiom that says two uses of that action name
i
areequalifandonlyifalltheirarguments areequal:
A (x ,...,x )=A (y ,...,y ) ⇔ x =y ∧...∧x =y .
i 1 n i 1 n 1 1 n n
• Asolution isasituation (andhenceasequence ofactions) thatsatisfiesthegoal.
Work in situation calculus has done a lot to define the formal semantics of planning and to
open up new areas of investigation. But so far there have not been any practical large-scale
planning programs based on logical deduction over the situation calculus. This is in part
because of the difficulty of doing efficient inference in FOL, but is mainly because the field
hasnotyetdevelopedeffectiveheuristics forplanning withsituation calculus.
10.4.3 Planning asconstraint satisfaction
Wehaveseenthatconstraint satisfaction hasalotincommonwithBooleansatisfiability, and
wehaveseenthatCSPtechniquesareeffectiveforschedulingproblems,soitisnotsurprising
thatitispossibletoencodeaboundedplanningproblem(i.e.,theproblemoffindingaplanof
lengthk)asaconstraint satisfaction problem (CSP).Theencoding issimilartotheencoding
to a SAT problem (Section 10.4.1), with one important simplification: at each time step we
need only a single variable, Actiont, whose domain is the set of possible actions. We no
longerneed one variable forevery action, and wedon’t need theaction exclusion axioms. It
isalsopossibletoencodeaplanninggraphintoaCSP.ThisistheapproachtakenbyGP-CSP
(DoandKambhampati, 2003).
10.4.4 Planning asrefinement ofpartially ordered plans
Alltheapproaches wehaveseensofarconstruct totallyorderedplansconsisting ofastrictly
linear sequences of actions. This representation ignores the fact that many subproblems are
independent. A solution to an air cargo problem consists of a totally ordered sequence of
actions,yetif30packagesarebeingloadedontooneplaneinoneairportand50packagesare
beingloadedontoanotheratanotherairport,itseemspointlesstocomeupwithastrictlinear
ordering of80loadactions; thetwosubsetsofactions shouldbethought ofindependently.
An alternative is to represent plans as partially ordered structures: a plan is a set of
actions and a set of constraints of the form Before(a ,a ) saying that one action occurs
i j
beforeanother. InthebottomofFigure10.13,weseeapartiallyorderedplanthatisasolution
to the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that
Remove(Spare,Trunk)andRemove(Flat,Axle)canbedoneineitherorderaslongasthey
arebothcompleted beforethe PutOn(Spare,Axle)action.
Partially ordered plans are created by a search through the space of plans rather than
through the state space. We start with the empty plan consisting of just the initial state and
thegoal,withnoactionsinbetween,asinthetopofFigure10.13. Thesearchprocedurethen
looks for a flaw in the plan, and makes an addition to the plan to correct the flaw (or if no
FLAW
correction can be made, the search backtracks and tries something else). A flaw is anything
that keeps thepartial plan from being asolution. Forexample, oneflawintheemptyplan is
thatnoactionachievesAt(Spare,Axle). Onewaytocorrecttheflawistoinsertintotheplan
Section10.4. OtherClassicalPlanningApproaches 391
At(Spare,Trunk)
Start At(Spare,Axle) Finish
At(Flat,Axle)
(a)
At(Spare,Trunk) Remove(Spare,Trunk)
At(Spare,Trunk) At(Spare,Ground)
Start PutOn(Spare,Axle) At(Spare,Axle) Finish
At(Flat,Axle) ¬At(Flat,Axle)
(b)
At(Spare,Trunk) Remove(Spare,Trunk)
At(Spare,Trunk) At(Spare,Ground)
Start PutOn(Spare,Axle) At(Spare,Axle) Finish
At(Flat,Axle) ¬At(Flat,Axle)
At(Flat,Axle) Remove(Flat,Axle)
(c)
Figure10.13 (a)thetireproblemexpressedasanemptyplan. (b)anincompletepartially
orderedplanforthetireproblem.Boxesrepresentactionsandarrowsindicatethatoneaction
mustoccurbeforeanother.(c)acompletepartially-orderedsolution.
theactionPutOn(Spare,Axle). Ofcoursethatintroducessomenewflaws: thepreconditions
of the new action are not achieved. The search keeps adding to the plan (backtracking if
necessary) until all flaws are resolved, as in the bottom of Figure 10.13. At every step, we
make the least commitment possible to fix the flaw. For example, in adding the action
LEASTCOMMITMENT
Remove(Spare,Trunk)weneedtocommittohaving itoccurbefore PutOn(Spare,Axle),
butwemakenoothercommitmentthatplacesitbeforeorafter otheractions. Iftherewerea
variableintheactionschemathatcouldbeleftunbound, wewoulddoso.
In the 1980s and 90s, partial-order planning was seen as the best way to handle plan-
ning problems with independent subproblems—after all, it was the only approach that ex-
plicitlyrepresentsindependent branchesofaplan. Ontheotherhand,ithasthedisadvantage
of not having an explicit representation of states in the state-transition model. That makes
somecomputationscumbersome. By2000,forward-search plannershaddevelopedexcellent
heuristics thatallowedthemtoefficientlydiscovertheindependent subproblems thatpartial-
order planning was designed for. As a result, partial-order planners are not competitive on
fullyautomated classical planning problems.
However, partial-order planning remains an important part of the field. Forsome spe-
cifictasks,suchasoperationsscheduling, partial-orderplanningwithdomainspecificheuris-
tics is the technology of choice. Many of these systems use libraries of high-level plans, as
describedinSection11.2. Partial-orderplanningisalsooftenusedindomainswhereitisim-
portantforhumanstounderstand theplans. OperationalplansforspacecraftandMarsrovers
aregeneratedbypartial-orderplannersandarethencheckedbyhumanoperatorsbeforebeing
uploaded tothevehicles forexecution. Theplan refinement approach makesiteasierforthe
humanstounderstandwhattheplanningalgorithmsaredoingandverifythattheyarecorrect.
392 Chapter 10. ClassicalPlanning
10.5 ANALYSIS OF PLANNING APPROACHES
Planning combines the two major areas of AI we have covered so far: search and logic. A
planner can beseen either asaprogram that searches forasolution oras onethat (construc-
tively) proves the existence ofa solution. Thecross-fertilization of ideas from the twoareas
has led both to improvements in performance amounting to several orders of magnitude in
the last decade and toan increased use of planners in industrial applications. Unfortunately,
we do not yet have a clear understanding of which techniques work best on which kinds of
problems. Quitepossibly, newtechniques willemergethatdominateexistingmethods.
Planning is foremost an exercise in controlling combinatorial explosion. If there are n
propositions in a domain, then there are 2n states. As we have seen, planning is PSPACE-
hard. Against such pessimism, the identification of independent subproblems can be apow-
erfulweapon. Inthebestcase—fulldecomposability oftheproblem—wegetanexponential
speedup. Decomposability is destroyed, however, by negative interactions between actions.
GRAPHPLANrecordsmutexestopointoutwherethedifficultinteractionsare. SATPLANrep-
resents asimilarrange ofmutexrelations, but does sobyusing thegeneral CNFform rather
than a specific data structure. Forward search addresses the problem heuristically by trying
tofind patterns (subsets ofpropositions) that covertheindependent subproblems. Since this
approachisheuristic,itcanworkevenwhenthesubproblems arenotcompletelyindependent.
Sometimes it is possible to solve a problem efficiently by recognizing that negative
SERIALIZABLE interactions canberuledout. Wesaythataproblem has serializable subgoalsifthereexists
SUBGOAL
an order of subgoals such that the planner can achieve them in that order without having to
undo any of the previously achieved subgoals. Forexample, in the blocks world, if the goal
is to build a tower(e.g., Aon B, which in turn is on C, which in turn is on the Table, as in
Figure10.4onpage371),thenthesubgoalsareserializable bottomtotop: ifwefirstachieve
C on Table, we will never have to undo it while we are achieving the other subgoals. A
planner that uses the bottom-to-top trick can solve any problem in the blocks world without
backtracking (although itmightnotalwaysfindtheshortest plan).
Asa more complex example, for the Remote Agent planner that commanded NASA’s
Deep Space One spacecraft, it was determined that the propositions involved in command-
ing a spacecraft are serializable. This is perhaps not too surprising, because a spacecraft is
designed by its engineers to be as easy as possible to control (subject to other constraints).
Taking advantage of the serialized ordering of goals, the Remote Agent planner was able to
eliminate most of the search. This meant that it was fast enough to control the spacecraft in
realtime,something previously considered impossible.
Planners such as GRAPHPLAN, SATPLAN, and FF have moved the field of planning
forward, by raising the level of performance of planning systems, by clarifying the repre-
sentational and combinatorial issues involved, and by the development of useful heuristics.
However,thereisaquestionofhowfarthesetechniqueswillscale. Itseemslikelythatfurther
progress on larger problems cannot rely only on factored and propositional representations,
and will require some kind of synthesis of first-order and hierarchical representations with
theefficientheuristics currently inuse.
Section10.6. Summary 393
10.6 SUMMARY
In this chapter, we defined the problem of planning in deterministic, fully observable, static
environments. We described the PDDL representation for planning problems and several
algorithmic approaches forsolving them. Thepointstoremember:
• Planningsystemsareproblem-solving algorithmsthatoperateonexplicitpropositional
or relational representations of states and actions. These representations make possi-
ble the derivation of effective heuristics and the development of powerful and flexible
algorithmsforsolvingproblems.
• PDDL,thePlanningDomainDefinitionLanguage, describes theinitialandgoalstates
asconjunctions ofliterals, andactionsintermsoftheirpreconditions andeffects.
• State-space search can operate inthe forward direction (progression) orthe backward
direction (regression). Effective heuristics can be derived by subgoal independence
assumptions andbyvarious relaxations oftheplanning problem.
• Aplanninggraphcanbeconstructedincrementally,startingfromtheinitialstate. Each
layer contains a superset of all the literals or actions that could occur at that time step
andencodesmutualexclusion(mutex)relationsamongliteralsoractionsthatcannotco-
occur. Planninggraphsyieldusefulheuristicsforstate-spaceandpartial-orderplanners
andcanbeuseddirectly inthe GRAPHPLAN algorithm.
• Otherapproachesincludefirst-orderdeductionoversituationcalculusaxioms;encoding
a planning problem as a Boolean satisfiability problem or as a constraint satisfaction
problem;andexplicitlysearching through thespaceofpartially orderedplans.
• Eachofthemajorapproaches toplanning hasits adherents, and there isasyetnocon-
sensusonwhichisbest. Competitionandcross-fertilization amongtheapproacheshave
resultedinsignificantgainsinefficiencyforplanning systems.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
AI planning arose from investigations into state-space search, theorem proving, and control
theoryandfromthepracticalneedsofrobotics,scheduling,andotherdomains. STRIPS(Fikes
and Nilsson, 1971), the first majorplanning system, illustrates the interaction of these influ-
ences. STRIPS wasdesignedastheplanning componentofthesoftwarefortheShakeyrobot
projectatSRI.ItsoverallcontrolstructurewasmodeledonthatofGPS,theGeneralProblem
Solver (Newell and Simon, 1961), a state-space search system that used means–ends anal-
ysis. Bylander (1992) shows simple STRIPS planning to be PSPACE-complete. Fikes and
Nilsson (1993) give a historical retrospective on the STRIPS project and its relationship to
morerecentplanning efforts.
The representation language used by STRIPS has been far more influential than its al-
gorithmic approach; what we call the “classical” language is close to what STRIPS used.
394 Chapter 10. ClassicalPlanning
The Action Description Language, or ADL (Pednault, 1986), relaxed some of the STRIPS
restrictions and made it possible to encode more realistic problems. Nebel (2000) explores
schemes for compiling ADL into STRIPS. The Problem Domain Description Language, or
PDDL(Ghallabetal.,1998),wasintroducedasacomputer-parsable, standardizedsyntaxfor
representing planning problems and has been used as the standard language for the Interna-
tionalPlanningCompetitionsince1998. Therehavebeenseveralextensions; themostrecent
version, PDDL 3.0,includes planconstraints andpreferences (GereviniandLong,2005).
Plannersintheearly1970sgenerallyconsideredtotallyorderedactionsequences. Prob-
lemdecompositionwasachievedbycomputingasubplanforeachsubgoalandthenstringing
the subplans together in some order. This approach, called linear planning by Sacerdoti
LINEARPLANNING
(1975), was soon discovered to be incomplete. It cannot solve some very simple problems,
suchastheSussmananomaly(seeExercise10.7), found byAllenBrownduring experimen-
tation withthe HACKER system (Sussman, 1975). Acomplete plannermustallowfor inter-
leavingofactionsfromdifferentsubplanswithinasinglesequence. Thenotionofserializable
INTERLEAVING
subgoals (Korf, 1987) corresponds exactly to the set of problems for which noninterleaved
planners arecomplete.
One solution to the interleaving problem was goal-regression planning, a technique in
which steps in a totally ordered plan are reordered so as to avoid conflict between subgoals.
This was introduced by Waldinger (1975) and also used by Warren’s (1974) WARPLAN.
WARPLAN is also notable in that it was the first planner to be written in a logic program-
minglanguage (Prolog) andisoneofthebestexamples oftheremarkable economy thatcan
sometimes be gained withlogic programming: WARPLAN isonly 100 lines ofcode, asmall
fractionofthesizeofcomparable planners ofthetime.
The ideas underlying partial-order planning include the detection of conflicts (Tate,
1975a) and the protection of achieved conditions from interference (Sussman, 1975). The
construction of partially ordered plans (then called task networks) was pioneered by the
NOAHplanner(Sacerdoti, 1975,1977)andbyTate’s(1975b, 1977) NONLIN system.
Partial-order planning dominated the next 20 years of research, yet the first clear for-
mal exposition was TWEAK (Chapman, 1987), a planner that was simple enough to allow
proofs of completeness and intractability (NP-hardness and undecidability) of various plan-
ning problems. Chapman’s work led to a straightforward description of a complete partial-
orderplanner (McAllesterand Rosenblitt, 1991), then tothewidely distributed implementa-
tions SNLP (Soderland andWeld, 1991) and UCPOP (Penberthy and Weld, 1992). Partial-
order planning fell out of favor in the late 1990s as faster methods emerged. Nguyen and
Kambhampati (2001) suggest that a reconsideration is merited: with accurate heuristics de-
rived from aplanning graph, their REPOP planner scales up muchbetter than GRAPHPLAN
inparallelizable domainsandiscompetitivewiththefasteststate-space planners.
The resurgence of interest in state-space planning was pioneered by Drew McDer-
mott’sUNPOPprogram(1996),whichwasthefirsttosuggesttheignore-delete-listheuristic,
The name UNPOP was a reaction to the overwhelming concentration on partial-order plan-
ning at the time; McDermott suspected that other approaches were not getting the attention
they deserved. Bonet and Geffner’s Heuristic Search Planner (HSP) and its later deriva-
tives (Bonet and Geffner, 1999; Haslum et al., 2005; Haslum, 2006) were the first to make
Bibliographical andHistorical Notes 395
state-space search practical for large planning problems. HSP searches in the forward di-
rection while HSPR (Bonet and Geffner, 1999) searches backward. The most successful
state-space searcher todate is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,
2005), winner of the AIPS 2000 planning competition. FASTDOWNWARD (Helmert, 2006)
is a forward state-space search planner that preprocesses the action schemas into an alter-
nativerepresentation whichmakessomeoftheconstraints moreexplicit. FASTDOWNWARD
(HelmertandRichter,2004;Helmert,2006)wonthe2004planningcompetition,andLAMA
(Richter and Westphal, 2008), a planner based on FASTDOWNWARD with improved heuris-
tics,wonthe2008competition.
Bylander (1994) and Ghallab et al. (2004) discuss the computational complexity of
severalvariantsoftheplanningproblem. Helmert(2003)provescomplexityboundsformany
ofthestandard benchmark problems, andHoffmann (2005) analyzes thesearch space ofthe
ignore-delete-list heuristic. Heuristicsfortheset-covering problem arediscussed byCaprara
et al. (1995) for scheduling operations of the Italian railway. Edelkamp (2009) and Haslum
et al. (2007) describe how to construct pattern databases for planning heuristics. As we
mentionedinChapter3,Felneretal.(2004)showencouragingresultsusingpatterndatabases
forslidingblockspuzzles,whichcanbethoughtofasaplanningdomain,butHoffmannetal.
(2006)showsomelimitations ofabstraction forclassicalplanning problems.
AvrimBlumandMerrickFurst(1995, 1997)revitalized thefieldofplanningwiththeir
GRAPHPLANsystem,whichwasordersofmagnitudefasterthanthepartial-orderplannersof
the time. Othergraph-planning systems, such as IPP (Koehler etal., 1997), STAN (Fox and
Long,1998),andSGP(Weldetal.,1998),soonfollowed. Adatastructurecloselyresembling
theplanninggraphhadbeendevelopedslightlyearlierbyGhallabandLaruelle(1994),whose
IXTET partial-order planner used it to derive accurate heuristics to guide searches. Nguyen
et al. (2001) thoroughly analyze heuristics derived from planning graphs. Ourdiscussion of
planning graphs is based partly on this work and on lecture notes and articles by Subbarao
Kambhampati (Bryce and Kambhampati, 2007). As mentioned in the chapter, a planning
graph can be used in many different ways to guide the search for a solution. The winner
of the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched
planning graphsusingalocalsearchtechnique inspired by WALKSAT.
Thesituation calculus approach toplanning wasintroduced byJohnMcCarthy(1963).
Theversionweshowherewasproposed byRayReiter(1991, 2001).
Kautzet al. (1996) investigated various ways to propositionalize action schemas, find-
ing that the most compact forms did not necessarily lead to the fastest solution times. A
systematic analysis was carried out by Ernst et al. (1997), who also developed an auto-
matic “compiler” for generating propositional representations from PDDL problems. The
BLACKBOX planner, which combines ideas from GRAPHPLAN and SATPLAN, was devel-
oped by Kautz and Selman (1998). CPLAN, a planner based on constraint satisfaction, was
described byvanBeekandChen(1999).
Mostrecently, therehasbeen interest intherepresentation ofplans asbinarydecision
BINARYDECISION diagrams, compact data structures for Boolean expressions widely studied in the hardware
DIAGRAM
verificationcommunity(ClarkeandGrumberg,1987;McMillan,1993). Therearetechniques
forprovingpropertiesofbinarydecisiondiagrams,includingthepropertyofbeingasolution
396 Chapter 10. ClassicalPlanning
toaplanning problem. Cimattietal. (1998) present aplannerbased onthisapproach. Other
representations have also been used; for example, Vossen et al. (2001) survey the use of
integerprogrammingforplanning.
The jury is still out, but there are now some interesting comparisons of the various
approaches to planning. Helmert (2001) analyzes several classes of planning problems, and
showsthatconstraint-basedapproachessuchasGRAPHPLANandSATPLANarebestforNP-
hard domains, while search-based approaches do better in domains where feasible solutions
can be found without backtracking. GRAPHPLAN and SATPLAN have trouble in domains
with many objects because that means they must create many actions. In some cases the
problem can bedelayed oravoided bygenerating thepropositionalized actions dynamically,
onlyasneeded, ratherthaninstantiating themallbeforethesearchbegins.
Readings in Planning (Allen et al., 1990) is a comprehensive anthology of early work
in the field. Weld (1994, 1999) provides two excellent surveys of planning algorithms of
the 1990s. It is interesting to see the change in the five years between the two surveys:
the first concentrates on partial-order planning, and the second introduces GRAPHPLAN and
SATPLAN. AutomatedPlanning(Ghallabetal.,2004)isanexcellenttextbookonallaspects
of planning. LaValle’s text Planning Algorithms (2006) covers both classical and stochastic
planning, withextensivecoverageofrobotmotionplanning.
PlanningresearchhasbeencentraltoAIsinceitsinception, andpapersonplanningare
a staple of mainstream AI journals and conferences. There are also specialized conferences
suchastheInternational ConferenceonAIPlanningSystems,theInternationalWorkshopon
PlanningandScheduling forSpace,andtheEuropeanConference onPlanning.
EXERCISES
10.1 Describethedifferencesandsimilaritiesbetweenproblem solvingandplanning.
10.2 GiventheactionschemasandinitialstatefromFigure10.1,whatarealltheapplicable
concrete instances ofFly(p,from,to)inthestatedescribed by
At(P ,JFK)∧At(P ,SFO)∧Plane(P )∧Plane(P )
1 2 1 2
∧Airport(JFK)∧Airport(SFO)?
10.3 The monkey-and-bananas problem is faced by a monkey in a laboratory with some
bananashangingoutofreachfromtheceiling. Aboxisavailablethatwillenablethemonkey
toreachthebananasifheclimbsonit. Initially,themonkeyisatA,thebananasatB,andthe
box at C. The monkey and box have height Low, but if the monkey climbs onto the box he
willhaveheightHigh,thesameasthebananas. Theactions available tothemonkeyinclude
Go from one place to another, Push an object from one place to another, ClimbUp onto or
ClimbDown fromanobject, and Grasp orUngrasp anobject. Theresult ofaGrasp isthat
themonkeyholdstheobjectifthemonkeyandobjectareinthesameplaceatthesameheight.
a. Writedowntheinitialstatedescription.
Exercises 397
Switch 4
Door 4
Room 4
Switch 3
Door 3
Room 3
Shakey
Corridor
Switch 2
Door 2
Room 2
Switch 1
Box 3
Box 2
Door 1
Room 1
Box 4
Box 1
Figure10.14 Shakey’sworld. Shakeycanmovebetweenlandmarkswithina room,can
passthroughthedoorbetweenrooms,canclimbclimbableobjectsandpushpushableobjects,
andcanfliplightswitches.
b. Writethesixactionschemas.
c. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the
bananas, but leaving the box inits original place. Writethis as ageneral goal (i.e., not
assumingthattheboxisnecessarilyatC)inthelanguageofsituationcalculus. Canthis
goalbesolvedbyaclassical planningsystem?
d. Your schema for pushing is probably incorrect, because if the object is too heavy, its
positionwillremainthesamewhenthePushschemaisapplied. Fixyouractionschema
toaccountforheavyobjects.
10.4 The original STRIPS planner was designed to control Shakey the robot. Figure 10.14
showsaversionofShakey’s worldconsisting offourroomslinedupalongacorridor, where
eachroomhasadoorandalightswitch. TheactionsinShakey’sworldincludemovingfrom
place toplace, pushing movable objects (such asboxes), climbing ontoanddownfromrigid
398 Chapter 10. ClassicalPlanning
objects(suchasboxes),andturninglightswitchesonandoff. Therobotitselfcouldnotclimb
onaboxortoggleaswitch,buttheplannerwascapableoffindingandprintingoutplansthat
werebeyondtherobot’sabilities. Shakey’ssixactionsare thefollowing:
• Go(x,y,r), which requires that Shakey be At x and that x and y are locations In the
sameroom r. Byconvention adoorbetweentworoomsisinbothofthem.
• Pushaboxbfromlocation xtolocationywithinthesameroom: Push(b,x,y,r). You
willneedthepredicate Box andconstants fortheboxes.
• Climbontoaboxfrom position x: ClimbUp(x,b); climbdownfrom aboxtoposition
x: ClimbDown(b,x). Wewillneedthepredicate On andtheconstant Floor.
• Turn a light switch on oroff: TurnOn(s,b); TurnOff(s,b). To turn a light on oroff,
Shakeymustbeontopofaboxatthelightswitch’slocation.
WritePDDLsentences forShakey’ssixactions andtheinitialstatefromFigure10.14. Con-
structaplanforShakeytoget Box intoRoom .
2 2
10.5 AfiniteTuringmachinehasafiniteone-dimensionaltapeofcells,eachcellcontaining
one of a finite number of symbols. One cell has a read and write head above it. There is a
finitesetofstates themachine canbein, oneofwhichistheaccept state. Ateach timestep,
dependingonthesymbolonthecellundertheheadandthemachine’scurrentstate,thereare
asetofactions wecanchoose from. Eachaction involves writingasymbol tothecellunder
the head, transitioning the machine to a state, and optionally moving the head left or right.
The mapping that determines which actions are allowed is the Turing machine’s program.
Yourgoalistocontrolthemachineintotheacceptstate.
Represent the Turing machine acceptance problem as a planning problem. If you can
dothis,itdemonstratesthatdeterminingwhetheraplanningproblemhasasolutionisatleast
ashardastheTuringacceptance problem,whichisPSPACE-hard.
10.6 Explain why dropping negative effects from every action schema in a planning prob-
lemresultsinarelaxedproblem.
10.7 Figure 10.4 (page 371) shows a blocks-world problem that is known as the Sussman
anomaly. The problem was considered anomalous because the noninterleaved planners of
SUSSMANANOMALY
the early 1970s could not solve it. Write a definition of the problem and solve it, either by
handorwithaplanningprogram. Anoninterleaved planneris aplannerthat,whengiventwo
subgoals G and G , produces either aplan for G concatenated with aplan for G , orvice
1 2 1 2
versa. Explainwhyanoninterleaved plannercannotsolvethisproblem.
10.8 ProvethatbackwardsearchwithPDDLproblemsiscomplete.
10.9 Constructlevels0,1,and2oftheplanninggraphfortheprobleminFigure10.1.
10.10 Provethefollowingassertions aboutplanning graphs:
a. Aliteralthatdoesnotappearinthefinallevelofthegraphcannotbeachieved.
Exercises 399
b. Thelevelcostofaliteralinaserialgraphisnogreaterthantheactualcostofanoptimal
planforachievingit.
10.11 The set-level heuristic (see page 382) uses a planning graph to estimate the cost of
achieving a conjunctive goal from the current state. What relaxed problem is the set-level
heuristic thesolutionto?
10.12 Examinethedefinitionofbidirectionalsearch inChapter3.
a. Wouldbidirectional state-space searchbeagoodideaforplanning?
b. Whataboutbidirectional searchinthespaceofpartial-order plans?
c. Deviseaversionofpartial-orderplanninginwhichanactioncanbeaddedtoaplanifits
preconditionscanbeachievedbytheeffectsofactionsalreadyintheplan. Explainhow
todeal with conflicts and ordering constraints. Is the algorithm essentially identical to
forwardstate-space search?
10.13 We contrasted forward and backward state-space searchers with partial-order plan-
ners,sayingthatthelatterisaplan-spacesearcher. Explainhowforwardandbackwardstate-
space search can also be considered plan-space searchers, and say what the plan refinement
operators are.
10.14 Uptonowwehaveassumedthattheplanswecreatealwaysmakesurethatanaction’s
preconditions aresatisfied. Letusnowinvestigate whatpropositional successor-state axioms
such as HaveArrowt+1 ⇔ (HaveArrowt ∧ ¬Shoott) have to say about actions whose
preconditions arenotsatisfied.
a. Showthat the axioms predict that nothing will happen when anaction isexecuted in a
statewhereitspreconditions arenotsatisfied.
b. Consider aplan pthat contains the actions required toachieve agoal but also includes
illegalactions. Isitthecasethat
initialstate∧successor-state axioms∧p |= goal?
c. Withfirst-ordersuccessor-state axiomsinsituation calculus, isitpossible toprovethat
aplancontaining illegalactionswillachievethegoal?
10.15 Consider how to translate aset of action schemas into the successor-state axioms of
situation calculus.
a. Consider the schema for Fly(p,from,to). Write a logical definition for the predicate
Poss(Fly(p,from,to),s), which is true if the preconditions for Fly(p,from,to) are
satisfiedinsituation s.
b. Next, assuming that Fly(p,from,to) is the only action schema available to the agent,
write down a successor-state axiom for At(p,x,s) that captures the same information
astheactionschema.
400 Chapter 10. ClassicalPlanning
c. Now suppose there is an additional method of travel: Teleport(p,from,to). It has
theadditionalprecondition ¬Warped(p)andtheadditionaleffectWarped(p). Explain
howthesituation calculus knowledgebasemustbemodified.
d. Finally, develop a general and precisely specified procedure forcarrying out the trans-
lationfromasetofactionschemastoasetofsuccessor-state axioms.
10.16 In the SATPLAN algorithm in Figure 7.22 (page 272), each call to the satisfiabil-
ity algorithm asserts a goal gT, where T ranges from 0 to T . Suppose instead that the
max
satisfiability algorithm iscalledonlyonce,withthegoal g0∨g1∨···∨gTmax.
a. Willthisalwaysreturnaplanifoneexistswithlengthless thanorequaltoT ?
max
b. Doesthisapproach introduce anynewspurious “solutions”?
c. Discuss how one might modify a satisfiability algorithm such as WALKSAT so that it
findsshortsolutions (iftheyexist)whengivenadisjunctive goalofthisform.
11
PLANNING AND ACTING
IN THE REAL WORLD
Inwhichweseehow moreexpressive representations and moreinteractive agent
architectures leadtoplanners thatareusefulintherealworld.
Thepreviouschapterintroduced themostbasicconcepts, representations, andalgorithmsfor
planning. Planners that are are used in the real world for planning and scheduling the oper-
ations of spacecraft, factories, and military campaigns are more complex; they extend both
the representation language and the way the planner interacts with the environment. This
chapter shows how. Section 11.1 extends the classical language for planning to talk about
actions with durations and resource constraints. Section 11.2 describes methods for con-
structing plans thatareorganized hierarchically. Thisallowshuman experts tocommunicate
totheplannerwhattheyknowabouthowtosolvetheproblem. Hierarchyalsolendsitselfto
efficientplanconstructionbecausetheplannercansolveaproblematanabstractlevelbefore
delvingintodetails. Section11.3presentsagentarchitectures thatcanhandleuncertain envi-
ronments and interleave deliberation withexecution, andgivessomeexamples ofreal-world
systems. Section11.4showshowtoplanwhentheenvironment containsotheragents.
11.1 TIME, SCHEDULES, AND RESOURCES
Theclassicalplanningrepresentationtalksaboutwhattodo,andinwhatorder,buttherepre-
sentation cannot talkabout time: howlong anaction takes andwhenitoccurs. Forexample,
theplanners ofChapter10couldproduceascheduleforanairlinethatsayswhichplanesare
assignedtowhichflights,butwereallyneedtoknowdepartureandarrivaltimesaswell. This
isthesubjectmatterofscheduling. Therealworldalsoimposesmanyresourceconstraints;
forexample, anairlinehasalimitednumberofstaff—and staffwhoareononeflightcannot
be on another at the same time. This section covers methods for representing and solving
planning problemsthatincludetemporalandresource constraints.
The approach we take in this section is “plan first, schedule later”: that is, we divide
the overall problem into a planning phase in which actions are selected, with some ordering
constraints, to meet the goals of the problem, and a later scheduling phase, in which tempo-
ralinformation isaddedtotheplantoensure thatitmeetsresource anddeadline constraints.
401
402 Chapter 11. PlanningandActingintheRealWorld
Jobs({AddEngine1 ≺AddWheels1 ≺Inspect1},
{AddEngine2 ≺AddWheels2 ≺Inspect2})
Resources(EngineHoists(1),WheelStations(1),Inspectors(2),LugNuts(500))
Action(AddEngine1,DURATION:30,
USE:EngineHoists(1))
Action(AddEngine2,DURATION:60,
USE:EngineHoists(1))
Action(AddWheels1,DURATION:30,
CONSUME:LugNuts(20),USE:WheelStations(1))
Action(AddWheels2,DURATION:15,
CONSUME:LugNuts(20),USE:WheelStations(1))
Action(Inspect
i
,DURATION:10,
USE:Inspectors(1))
Figure11.1 Ajob-shopschedulingproblemforassemblingtwocars,with resourcecon-
straints. ThenotationA≺BmeansthatactionAmustprecedeactionB.
Thisapproachiscommoninreal-worldmanufacturingandlogisticalsettings,wheretheplan-
ningphase isoftenperformed byhumanexperts. Theautomated methods ofChapter10can
also be used for the planning phase, provided that they produce plans with just the minimal
ordering constraints required forcorrectness. GRAPHPLAN (Section 10.3), SATPLAN (Sec-
tion 10.4.1), and partial-order planners (Section 10.4.4) can do this; search-based methods
(Section 10.2) produce totally ordered plans, but these can easily be converted to plans with
minimalordering constraints.
11.1.1 Representing temporal andresource constraints
A typical job-shop scheduling problem, as first introduced in Section 6.1.2, consists of a
set of jobs, each of which consists a collection of actions with ordering constraints among
JOB
them. Each action has a duration and a set of resource constraints required by the action.
DURATION
Each constraint specifies a type of resource (e.g., bolts, wrenches, or pilots), the number
of that resource required, and whether that resource is consumable (e.g., the bolts are no
CONSUMABLE
longer available foruse) or reusable (e.g., apilot is occupied during a flight but is available
REUSABLE
againwhentheflightisover). Resources canalsobe produced byactionswithnegativecon-
sumption, including manufacturing, growing, andresupply actions. Asolution toajob-shop
scheduling problem mustspecify thestarttimesforeachaction andmustsatisfy allthetem-
poral ordering constraints and resource constraints. As with search and planning problems,
solutions can be evaluated according to a cost function; this can be quite complicated, with
nonlinear resource costs, time-dependent delay costs, and so on. For simplicity, we assume
thatthecostfunction isjustthetotalduration oftheplan, whichiscalledthemakespan.
MAKESPAN
Figure11.1showsasimpleexample: aprobleminvolvingtheassemblyoftwocars. The
problemconsistsoftwojobs,eachoftheform[AddEngine,AddWheels,Inspect]. Thenthe
Section11.1. Time,Schedules, andResources 403
Resources statement declares that there are four types of resources, and gives the number
of each type available at the start: 1 engine hoist, 1 wheel station, 2 inspectors, and 500 lug
nuts. The action schemas give the duration and resource needs of each action. The lug nuts
are consumed as wheels are added to the car, whereas the other resources are “borrowed” at
thestartofanactionandreleased attheaction’send.
The representation of resources as numerical quantities, such as Inspectors(2), rather
than as named entities, such as Inspector(I ) and Inspector(I ), is an example of a very
1 2
general technique called aggregation. Thecentral idea ofaggregation istogroupindividual
AGGREGATION
objects into quantities when the objects are all indistinguishable with respect to the purpose
athand. Inourassemblyproblem,itdoesnotmatterwhichinspectorinspectsthecar,sothere
is no need to make the distinction. (The same idea works in the missionaries-and-cannibals
problem in Exercise 3.9.) Aggregation is essential for reducing complexity. Consider what
happens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors
areavailable. Withinspectorsrepresentedasquantities, afailureisdetectedimmediatelyand
thealgorithm backtracks totryanotherschedule. Withinspectors represented asindividuals,
thealgorithm backtracks totryall 10!waysofassigning inspectors toactions.
11.1.2 Solvingscheduling problems
Webeginbyconsideringjustthetemporalschedulingproblem,ignoringresourceconstraints.
Tominimizemakespan(planduration),wemustfindtheearlieststarttimesforalltheactions
consistent withtheorderingconstraints suppliedwiththe problem. Itishelpfultoviewthese
orderingconstraintsasadirectedgraphrelatingtheactions,asshowninFigure11.2. Wecan
CRITICALPATH apply the critical path method (CPM) to this graph to determine the possible start and end
METHOD
times of each action. A path through a graph representing a partial-order plan is a linearly
ordered sequence of actions beginning with Start and ending with Finish. (For example,
therearetwopathsinthepartial-order planinFigure11.2.)
The critical path is that path whose total duration is longest; the path is “critical”
CRITICALPATH
because itdetermines theduration oftheentireplan—shortening otherpathsdoesn’t shorten
the plan as a whole, but delaying the start of any action on the critical path slows down the
wholeplan. Actionsthatareoffthecriticalpathhaveawindowoftimeinwhichtheycanbe
executed. Thewindowisspecifiedintermsofanearliestpossiblestarttime,ES,andalatest
possible start time, LS. The quantity LS – ES is known as the slack of an action. We can
SLACK
see in Figure 11.2 that the whole plan will take 85 minutes, that each action in the top job
has15minutesofslack, andthateachactiononthecritical pathhasnoslack(bydefinition).
TogethertheES andLS timesforalltheactions constitute aschedulefortheproblem.
SCHEDULE
ThefollowingformulasserveasadefinitionforES andLS andalsoastheoutlineofa
dynamic-programming algorithm to compute them. A and B are actions, and A≺B means
thatAcomesbefore B:
ES(Start) = 0
ES(B)= max A≺B ES(A)+Duration(A)
LS(Finish) = ES(Finish)
LS(A) = min B(cid:9)A LS(B)−Duration(A).
404 Chapter 11. PlanningandActingintheRealWorld
[0,15] [30,45] [60,75]
AddEngine1 AddWheels1 Inspect1
30 30 10
[0,0] [85,85]
Start Finish
[0,0] [60,60] [75,75]
AddEngine2 AddWheels2 Inspect2
60 15 10
AddWheels1
AddEngine1 Inspect1
AddEngine2 Inspect2
AddWheels2
0 10 20 30 40 50 60 70 80 90
Figure11.2 Top:arepresentationofthetemporalconstraintsforthejob-shopscheduling
problemofFigure11.1.Thedurationofeachactionisgivenatthebottomofeachrectangle.
In solvingthe problem,we computethe earliest andlatest start times asthe pair [ES,LS],
displayed in the upper left. The difference between these two numbers is the slack of an
action;actionswithzeroslackareonthecriticalpath,shownwithboldarrows. Bottom: the
samesolutionshownasatimeline. Greyrectanglesrepresenttimeintervalsduringwhichan
actionmaybeexecuted,providedthattheorderingconstraintsarerespected.Theunoccupied
portionofagrayrectangleindicatestheslack.
The idea is that we start by assigning ES(Start) to be 0. Then, as soon as we get an action
B such that all the actions that come immediately before B have ES values assigned, we
set ES(B) to be the maximum of the earliest finish times of those immediately preceding
actions, wheretheearliestfinishtimeofanactionisdefined astheearlieststarttimeplusthe
duration. This process repeats until every action has been assigned an ES value. The LS
valuesarecomputed inasimilarmanner,workingbackwardfromtheFinish action.
Thecomplexity ofthecritical pathalgorithm isjust O(Nb),whereN isthenumberof
actionsandbisthemaximumbranchingfactorintooroutofanaction. (Toseethis,notethat
the LS and ES computations are done once for each action, and each computation iterates
overatmostbotheractions.) Therefore,findingaminimum-durationschedule,givenapartial
ordering ontheactionsandnoresource constraints, isquiteeasy.
Mathematically speaking, critical-path problems areeasy tosolvebecause theyarede-
fined as a conjunction of linear inequalities on the start and end times. When we introduce
resource constraints, the resulting constraints on start and end times become more compli-
cated. For example, the AddEngine actions, which begin at the same time in Figure 11.2,
Section11.1. Time,Schedules, andResources 405
EngineHoists(1) AddEngine1 AddEngine2
WheelStations(1) AddWheels1 AddWheels2
Inspect1
Inspectors(2)
Inspect2
0 10 20 30 40 50 60 70 80 90 100 110 120
Figure11.3 Asolutiontothejob-shopschedulingproblemfromFigure11.1,takinginto
account resource constraints. The left-hand margin lists the three reusable resources, and
actions are shown aligned horizontally with the resources they use. There are two possi-
ble schedules, depending on which assembly uses the engine hoist first; we’ve shown the
shortest-durationsolution,whichtakes115minutes.
require the same EngineHoist and so cannot overlap. The “cannot overlap” constraint is a
disjunction of two linear inequalities, one for each possible ordering. The introduction of
disjunctions turnsouttomakescheduling withresourceconstraints NP-hard.
Figure 11.3 shows the solution with the fastest completion time, 115 minutes. This is
30 minutes longer than the 85 minutes required for a schedule without resource constraints.
Notice that there is no time at which both inspectors are required, so we can immediately
moveoneofourtwoinspectors toamoreproductive position.
The complexity of scheduling with resource constraints is often seen in practice as
well as in theory. A challenge problem posed in 1963—to find the optimal schedule for a
problem involving just 10 machines and 10 jobs of 100 actions each—went unsolved for
23 years (Lawler et al., 1993). Many approaches have been tried, including branch-and-
bound, simulated annealing, tabu search, constraint satisfaction, and other techniques from
Chapters 3 and 4. One simple but popular heuristic is the minimum slack algorithm: on
MINIMUMSLACK
each iteration, schedule for the earliest possible start whichever unscheduled action has all
itspredecessors scheduled andhastheleastslack;thenupdatetheES andLS timesforeach
affected action and repeat. The heuristic resembles the minimum-remaining-values (MRV)
heuristic in constraint satisfaction. It often works well in practice, but for our assembly
problem ityieldsa130–minute solution, notthe115–minute solution ofFigure11.3.
Up to this point, we have assumed that the set of actions and ordering constraints is
fixed. Undertheseassumptions,everyschedulingproblemcanbesolvedbyanonoverlapping
sequence that avoids all resource conflicts, provided that each action is feasible by itself. If
a scheduling problem is proving very difficult, however, it may not be a good idea to solve
itthis way—itmaybebetter toreconsider theactions and constraints, in casethat leads to a
much easier scheduling problem. Thus, it makes sense to integrate planning and scheduling
bytakingintoaccount durations andoverlapsduring theconstruction ofapartial-order plan.
SeveraloftheplanningalgorithmsinChapter10canbeaugmentedtohandlethisinformation.
For example, partial-order planners can detect resource constraint violations in much the
same way they detect conflicts with causal links. Heuristics can be devised to estimate the
totalcompletion timeofaplan. Thisiscurrently anactiveareaofresearch.
406 Chapter 11. PlanningandActingintheRealWorld
11.2 HIERARCHICAL PLANNING
Theproblem-solving andplanningmethodsoftheprecedingchaptersalloperatewithafixed
set of atomic actions. Actions can be strung together into sequences orbranching networks;
state-of-the-art algorithms cangenerate solutions containing thousands ofactions.
Forplans executed by the human brain, atomic actions are muscle activations. In very
round numbers, we have about 103 muscles to activate (639, by some counts, but many of
themhavemultiple subunits); wecanmodulatetheiractivation perhaps 10timespersecond;
and we are alive and awake for about 109 seconds in all. Thus, a human life contains about
1013 actions, give or take one or two orders of magnitude. Even if we restrict ourselves to
planning overmuchshorter timehorizons—for example, atwo-weekvacation inHawaii—a
detailed motorplanwouldcontainaround 1010 actions. Thisisalotmorethan1000.
Tobridgethisgap,AIsystemswillprobablyhavetodowhathumansappeartodo: plan
at higher levels of abstraction. A reasonable plan for the Hawaii vacation might be “Go to
SanFranciscoairport;takeHawaiianAirlinesflight11toHonolulu;dovacationstufffortwo
weeks;takeHawaiianAirlinesflight12backtoSanFrancisco;gohome.” Givensuchaplan,
the action “Go to San Francisco airport” can be viewed as a planning task in itself, with a
solution such as “Drive to the long-term parking lot; park; take the shuttle to the terminal.”
Eachofthese actions, inturn, canbedecomposed further, until wereach thelevelofactions
thatcanbeexecutedwithoutdeliberation togeneratetherequired motorcontrolsequences.
In this example, we see that planning can occur both before and during the execution
of the plan; for example, one would probably defer the problem of planning a route from a
parking spot in long-term parking to the shuttle bus stop until a particular parking spot has
been found during execution. Thus, that particular action will remain at an abstract level
prior to the execution phase. We defer discussion of this topic until Section 11.3. Here, we
HIERARCHICAL concentrate on the aspect of hierarchical decomposition, an idea that pervades almost all
DECOMPOSITION
attempts to manage complexity. Forexample, complex software is created from a hierarchy
ofsubroutines orobjectclasses;armiesoperateasahierarchyofunits;governmentsandcor-
porations have hierarchies of departments, subsidiaries, and branch offices. The key benefit
of hierarchical structure is that, at each level of the hierarchy, a computational task, military
mission,oradministrativefunctionisreducedtoasmallnumberofactivitiesatthenextlower
level, so the computational cost of finding the correct way to arrange those activities for the
current problem is small. Nonhierarchical methods, on the other hand, reduce a task to a
largenumberofindividual actions; forlarge-scale problems,thisiscompletely impractical.
11.2.1 High-levelactions
Thebasicformalismweadopttounderstandhierarchicaldecompositioncomesfromthearea
HIERARCHICALTASK of hierarchical task networksorHTNplanning. Asin classical planning (Chapter 10), we
NETWORK
assumefullobservability anddeterminism andtheavailability ofasetofactions, nowcalled
primitiveactions,withstandardprecondition–effect schemas. Thekeyadditional conceptis
PRIMITIVEACTION
the high-levelaction orHLA—forexample, theaction “GotoSanFrancisco airport” inthe
HIGH-LEVELACTION
Section11.2. Hierarchical Planning 407
Refinement(Go(Home,SFO),
STEPS:[Drive(Home,SFOLongTermParking),
Shuttle(SFOLongTermParking,SFO)])
Refinement(Go(Home,SFO),
STEPS:[Taxi(Home,SFO)])
Refinement(Navigate([a,b],[x,y]),
PRECOND:a=x ∧ b=y
STEPS:[])
Refinement(Navigate([a,b],[x,y]),
PRECOND:Connected([a,b],[a−1,b])
STEPS:[Left,Navigate([a−1,b],[x,y])])
Refinement(Navigate([a,b],[x,y]),
PRECOND:Connected([a,b],[a+1,b])
STEPS:[Right,Navigate([a+1,b],[x,y])])
...
Figure11.4 Definitionsofpossiblerefinementsfortwohigh-levelactions: goingtoSan
Franciscoairportandnavigatinginthe vacuumworld. Inthe lattercase, notethe recursive
natureoftherefinementsandtheuseofpreconditions.
example given earlier. Each HLA has one or more possible refinements, into a sequence1
REFINEMENT
of actions, each of which may be an HLA or a primitive action (which has no refinements
by definition). For example, the action “Go to San Francisco airport,” represented formally
as Go(Home,SFO), might have two possible refinements, as shown in Figure 11.4. The
same figure shows a recursive refinement for navigation in the vacuum world: to get to a
destination, takeastep,andthengotothedestination.
Theseexamplesshow thathigh-level actions andtheirrefinements embody knowledge
abouthowtodothings. Forinstance, therefinements for Go(Home,SFO)saythattogetto
the airport you can drive ortake ataxi; buying milk, sitting down, and moving the knight to
e4arenottobeconsidered.
An HLA refinement that contains only primitive actions is called an implementation
IMPLEMENTATION
of the HLA. For example, in the vacuum world, the sequences [Right,Right,Down] and
[Down,Right,Right]both implement the HLANavigate([1,3],[3,2]). Animplementation
of a high-level plan (a sequence of HLAs) is the concatenation of implementations of each
HLAinthesequence. Giventheprecondition–effect definitionsofeachprimitiveaction,itis
straightforward todeterminewhetheranygivenimplementationofahigh-levelplanachieves
the goal. We can say, then, that a high-level plan achieves the goal from a given state if at
least one ofitsimplementations achieves the goal from that state. The“atleast one” in this
definitioniscrucial—notallimplementationsneedtoachievethegoal,becausetheagentgets
1 HTN planners often allow refinement into partially ordered plans, and they allow the refinements of two
differentHLAsinaplantoshareactions.Weomittheseimportantcomplicationsintheinterestofunderstanding
thebasicconceptsofhierarchicalplanning.
408 Chapter 11. PlanningandActingintheRealWorld
todecidewhichimplementation itwillexecute. Thus,thesetofpossible implementations in
HTN planning—each of which may have a different outcome—is not the same as the set of
possible outcomes innondeterministic planning. There, we required that aplan work for all
outcomesbecause theagentdoesn’t gettochoosetheoutcome;naturedoes.
The simplest case is an HLA that has exactly one implementation. In that case, we
can compute the preconditions and effects of the HLA from those of the implementation
(see Exercise 11.3) and then treat the HLA exactly as if it were a primitive action itself. It
can be shown that the right collection of HLAs can result in the time complexity of blind
search dropping from exponential in the solution depth to linear in the solution depth, al-
though devising such a collection of HLAs may be a nontrivial task in itself. When HLAs
have multiple possible implementations, there are two options: one is to search among the
implementations foronethatworks,asinSection11.2.2;theotheristoreasondirectlyabout
theHLAs—despitethemultiplicityofimplementations—as explainedinSection11.2.3. The
latter method enables the derivation of provably correct abstract plans, without the need to
considertheirimplementations.
11.2.2 Searching forprimitivesolutions
HTNplanningisoftenformulatedwithasingle“toplevel”actioncalledAct,wheretheaimis
tofindanimplementationofActthatachievesthegoal. Thisapproachisentirelygeneral. For
example,classicalplanning problemscanbedefinedasfollows: foreachprimitiveaction a ,
i
provide one refinement of Act with steps [a ,Act]. That creates a recursive definition of Act
i
thatletsusaddactions. Butweneedsomewaytostoptherecursion;wedothatbyproviding
one more refinement for Act, one with an empty list of steps and with a precondition equal
to the goal of the problem. This says that if the goal is already achieved, then the right
implementation istodonothing.
The approach leads to a simple algorithm: repeatedly choose an HLA in the current
planandreplace itwithoneofitsrefinements, untiltheplan achievesthegoal. Onepossible
implementation based onbreadth-first treesearch isshowninFigure11.5. Plansareconsid-
eredinorderofdepth ofnesting oftherefinements, ratherthannumberofprimitivesteps. It
isstraightforward todesignagraph-search versionofthealgorithm aswellasdepth-firstand
iterativedeepening versions.
Inessence,thisformofhierarchicalsearchexploresthespaceofsequencesthatconform
totheknowledgecontained intheHLAlibraryabouthowthingsaretobedone. Agreatdeal
ofknowledgecanbeencoded,notjustintheactionsequencesspecifiedineachrefinementbut
also in the preconditions for the refinements. For some domains, HTN planners have been
able to generate huge plans with very little search. For example, O-PLAN (Bell and Tate,
1985),whichcombinesHTNplanning withscheduling, hasbeenusedtodevelopproduction
plans for Hitachi. A typical problem involves a product line of 350 different products, 35
assembly machines, and over 2000 different operations. The planner generates a 30-day
schedulewiththree8-hourshiftsaday,involvingtensofmillionsofsteps. Anotherimportant
aspect of HTN plans is that they are, by definition, hierarchically structured; usually this
makesthemeasyforhumanstounderstand.
Section11.2. Hierarchical Planning 409
functionHIERARCHICAL-SEARCH(problem,hierarchy)returnsasolution,orfailure
frontier←aFIFOqueuewith[Act]astheonlyelement
loopdo
ifEMPTY?(frontier)thenreturnfailure
plan←POP(frontier) /*choosestheshallowestplaninfrontier */
hla←thefirstHLAinplan,ornull ifnone
prefix,suffix←theactionsubsequencesbeforeandafterhla inplan
outcome←RESULT(problem.INITIAL-STATE,prefix)
ifhla isnullthen /*soplan isprimitiveandoutcome isitsresult*/
ifoutcome satisfiesproblem.GOAL thenreturnplan
elseforeachsequence inREFINEMENTS(hla,outcome,hierarchy)do
frontier←INSERT(APPEND(prefix,sequence,suffix),frontier)
Figure11.5 Abreadth-firstimplementationofhierarchicalforwardplanningsearch. The
initial plan suppliedto the algorithmis [Act]. The REFINEMENTS functionreturnsa set of
actionsequences,oneforeachrefinementoftheHLAwhosepreconditionsaresatisfied by
thespecifiedstate,outcome.
The computational benefits of hierarchical search can be seen by examining an ide-
alized case. Suppose that a planning problem has a solution with d primitive actions. For
a nonhierarchical, forward state-space planner with b allowable actions at each state, the
cost is O(bd), as explained in Chapter 3. For an HTN planner, let us suppose a very reg-
ular refinement structure: each nonprimitive action has r possible refinements, each into
k actions at the next lower level. We want to know how many different refinement trees
there are with this structure. Now, if there are d actions at the primitive level, then the
number of levels below the root is log d, so the number of internal refinement nodes is
k
1+k +k2 +···+klogkd−1 = (d−1)/(k −1). Each internal node has r possible refine-
ments,sor(d−1)/(k−1)
possibleregulardecompositiontreescouldbeconstructed. Examining
this formula, we see that keeping r small and k large can result in huge savings: essentially
wearetakingthekthrootofthenonhierarchical cost,if bandr arecomparable. Smallrand
large k means a library of HLAs with a small number of refinements each yielding a long
action sequence (that nonetheless allows us to solve any problem). This is not always pos-
sible: long action sequences that are usable across a wide range of problems are extremely
precious.
ThekeytoHTNplanning, then, isthe construction ofaplan library containing known
methods for implementing complex, high-level actions. One method of constructing the li-
brary is to learn the methods from problem-solving experience. After the excruciating ex-
perience of constructing a plan from scratch, the agent can save the plan in the library as a
methodforimplementingthehigh-levelactiondefinedbythetask. Inthisway,theagentcan
becomemoreandmorecompetentovertimeasnewmethodsarebuiltontopofoldmethods.
One important aspect of this learning process is the ability to generalize the methods that
are constructed, eliminating detail that is specific to the problem instance (e.g., the name of
410 Chapter 11. PlanningandActingintheRealWorld
the builder or the address of the plot of land) and keeping just the key elements of the plan.
Methodsforachievingthiskindofgeneralization aredescribed inChapter19. Itseemstous
inconceivable thathumanscouldbeascompetentastheyarewithoutsomesuchmechanism.
11.2.3 Searching forabstractsolutions
Thehierarchicalsearchalgorithmintheprecedingsection refinesHLAsallthewaytoprimi-
tiveactionsequencestodetermineifaplanisworkable. Thiscontradictscommonsense: one
shouldbeabletodetermine thatthetwo-HLAhigh-level plan
[Drive(Home,SFOLongTermParking),Shuttle(SFOLongTermParking,SFO)]
gets one to the airport without having to determine a precise route, choice of parking spot,
andsoon. Thesolution seemsobvious: writeprecondition–effect descriptions oftheHLAs,
just as we write down what the primitive actions do. From the descriptions, it ought to be
easytoprovethatthehigh-level planachieves thegoal. Thisistheholygrail,sotospeak, of
hierarchical planning because if wederive ahigh-level plan that provably achieves the goal,
working in a small search space of high-level actions, then we can commit to that plan and
workontheproblemofrefiningeachstepoftheplan. Thisgivesustheexponentialreduction
we seek. For this to work, it has to be the case that every high-level plan that “claims” to
achieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal in
thesensedefinedearlier: itmusthaveatleastoneimplementation thatdoesachievethegoal.
DOWNWARD
Thisproperty hasbeencalledthe downwardrefinementpropertyforHLAdescriptions.
REFINEMENT
PROPERTY
Writing HLA descriptions that satisfy the downward refinement property is, in princi-
ple,easy: aslongasthedescriptions are true,thenanyhigh-level planthatclaimstoachieve
the goal must in fact do so—otherwise, the descriptions are making some false claim about
whatthe HLAsdo. Wehave already seen howto writetrue descriptions forHLAsthathave
exactly one implementation (Exercise 11.3); a problem arises when the HLA has multiple
implementations. How can we describe the effects of an action that can be implemented in
manydifferent ways?
Onesafeanswer(atleastforproblemswhereallpreconditionsandgoalsarepositive)is
toincludeonlythepositiveeffectsthatareachievedbyeveryimplementationoftheHLAand
the negative effects of any implementation. Then the downward refinement property would
besatisfied. Unfortunately,thissemanticsforHLAsismuchtooconservative. Consideragain
the HLA Go(Home,SFO), which has two refinements, and suppose, for the sake of argu-
ment,asimpleworldinwhichonecanalwaysdrivetotheairport andpark, buttaking ataxi
requires Cash as a precondition. In that case, Go(Home,SFO) doesn’t always get you to
theairport. Inparticular, itfailsifCash isfalse,andsowecannotassertAt(Agent,SFO)as
aneffectoftheHLA.Thismakesnosense, however;iftheagentdidn’t haveCash,itwould
driveitself. Requiringthataneffectholdfor everyimplementation isequivalent toassuming
that someone else—an adversary—will choose the implementation. Ittreats theHLA’smul-
tiple outcomes exactly asiftheHLAwereanondeterministicaction, asinSection 4.3. For
ourcase,theagentitselfwillchoosetheimplementation.
The programming languages community has coined the term demonic nondetermin-
DEMONIC ismforthecasewhereanadversary makesthechoices, contrasting thiswithangelicnonde-
NONDETERMINISM
Section11.2. Hierarchical Planning 411
(a) (b)
Figure11.6 Schematicexamplesofreachablesets. Thesetofgoalstatesisshaded.Black
andgrayarrowsindicatepossibleimplementationsofh andh ,respectively.(a)Thereach-
1 2
ablesetofanHLAh inastates. (b)Thereachablesetforthesequence[h ,h ]. Because
1 1 2
thisintersectsthegoalset,thesequenceachievesthegoal.
ANGELIC terminism, wherethe agent itself makes the choices. Weborrow this term to defineangelic
NONDETERMINISM
semantics for HLA descriptions. The basic concept required for understanding angelic se-
ANGELICSEMANTICS
mantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h,
REACHABLESET
written as REACH(s,h), is the set of states reachable by any of the HLA’simplementations.
The key idea is that the agent can choose which element of the reachable set it ends up in
whenitexecutes theHLA;thus, anHLAwithmultiple refinements ismore“powerful” than
thesameHLAwithfewerrefinements. Wecanalsodefinethereachablesetofasequencesof
HLAs. Forexample,thereachable setofasequence [h ,h ]istheunionofallthereachable
1 2
setsobtained byapplyingh ineachstateinthereachable setofh :
2 (cid:15) 1
(cid:2)
REACH(s,[h
1
,h
2
])= REACH(s,h
2
).
s(cid:3)∈REACH(s,h1)
Given these definitions, a high-level plan—a sequence of HLAs—achieves the goal if its
reachable set intersects the set of goal states. (Compare this to the much stronger condition
for demonic semantics, where every member of the reachable set has to be a goal state.)
Conversely, if the reachable set doesn’t intersect the goal, then the plan definitely doesn’t
work. Figure11.6illustrates theseideas.
The notion of reachable sets yields a straightforward algorithm: search among high-
level plans, looking for one whose reachable set intersects the goal; once that happens, the
algorithm can commit to that abstract plan, knowing that it works, and focus on refining
the plan further. We will come back to the algorithmic issues later; first, we consider the
question ofhowtheeffects ofanHLA—thereachable setforeachpossible initialstate—are
represented. As with the classical action schemas of Chapter 10, we represent the changes
412 Chapter 11. PlanningandActingintheRealWorld
madetoeachfluent. Thinkofafluentasastatevariable. Aprimitiveactioncanaddordelete
a variable or leave it unchanged. (With conditional effects (see Section 11.3.1) there is a
fourthpossibility: flippingavariabletoitsopposite.)
An HLA under angelic semantics can do more: it can control the value of a variable,
setting ittotrueorfalse depending onwhichimplementation ischosen. Infact, anHLAcan
have nine different effects on a variable: if the variable starts out true, it can always keep
it true, always make it false, or have a choice; if the variable starts out false, it can always
keep it false, always make it true, or have a choice; and the three choices for each case can
becombined arbitrarily, making nine. Notationally, this isabitchallenging. We’llusethe (cid:23)
(cid:23)
symboltomean“possibly, iftheagentsochooses.” Thus,aneffect+Ameans“possibly add
A,” that is, either leave Aunchanged ormake it true. Similarly,
−(cid:23)
A means “possibly delete
A” and
±(cid:23)
A means “possibly add or delete A.” For example, the HLA Go(Home,SFO),
withthetworefinementsshowninFigure11.4,possiblydeletesCash (iftheagentdecidesto
take ataxi), soitshould havethe effect
−(cid:23)
Cash. Thus, weseethat thedescriptions ofHLAs
arederivable,inprinciple,fromthedescriptionsoftheirrefinements—infact,thisisrequired
if wewanttrue HLAdescriptions, such that the downward refinement property holds. Now,
suppose wehavethefollowingschemasfortheHLAsh andh :
1 2
Action(h
1
,PRECOND:¬A,EFFECT:A∧−(cid:23)
B),
Action(h
2
,PRECOND:¬B,EFFECT:+ (cid:23) A∧±(cid:23) C).
Thatis,h addsAandpossibledeletesB,whileh possiblyaddsAandhasfullcontrolover
1 2
C. Now,ifonlyB istrueintheinitial stateandthegoalisA∧C thenthesequence [h ,h ]
1 2
achieves the goal: we choose an implementation of h that makes B false, then choose an
1
implementation ofh thatleavesAtrueandmakesC true.
2
The preceding discussion assumes that the effects of an HLA—the reachable set for
anygiveninitial state—can bedescribed exactlybydescribing theeffect oneachvariable. It
would be nice if this were always true, but in many cases we can only approximate the ef-
fectsbecauseanHLAmayhaveinfinitelymanyimplementationsandmayproducearbitrarily
wiggly reachable sets—rather like the wiggly-belief-state problem illustrated in Figure 7.21
on page 271. For example, we said that Go(Home,SFO) possibly deletes Cash; it also
possibly adds At(Car,SFOLongTermParking); but it cannot do both—in fact, it must do
exactly one. As with belief states, we may need to write approximate descriptions. Wewill
OPTIMISTIC usetwokindsofapproximation: anoptimisticdescriptionREACH +(s,h)ofanHLAhmay
DESCRIPTION
−
PESSIMISTIC overstate the reachable set, while a pessimistic description REACH (s,h) may understate
DESCRIPTION
thereachable set. Thus,wehave
REACH − (s,h) ⊆ REACH(s,h) ⊆ REACH +(s,h).
Forexample,anoptimisticdescriptionofGo(Home,SFO)saysthatitpossibledeletesCash
and possibly adds At(Car,SFOLongTermParking). Another good example arises in the
8-puzzle, half of whose states are unreachable from any given state (see Exercise 3.4 on
page 113): the optimistic description of Act might well include the whole state space, since
theexactreachable setisquitewiggly.
With approximate descriptions, the test for whether a plan achieves the goal needs to
be modified slightly. If the optimistic reachable set for the plan doesn’t intersect the goal,
Section11.2. Hierarchical Planning 413
(a) (b)
Figure 11.7 Goal achievementfor high-levelplans with approximate descriptions. The
setofgoalstatesisshaded.Foreachplan,thepessimistic(solidlines)andoptimistic(dashed
lines)reachablesetsareshown.(a)Theplanindicatedbytheblackarrowdefinitelyachieves
thegoal,whiletheplanindicatedbythegrayarrowdefinitelydoesn’t.(b)Aplanthatwould
needtoberefinedfurthertodetermineifitreallydoesachievethegoal.
then the plan doesn’t work; if the pessimistic reachable set intersects the goal, then the plan
does work (Figure 11.7(a)). With exact descriptions, a plan either works or it doesn’t, but
with approximate descriptions, there is a middle ground: if the optimistic set intersects the
goal but the pessimistic set doesn’t, then we cannot tell if the plan works (Figure 11.7(b)).
When this circumstance arises, the uncertainty can be resolved by refining the plan. This is
a very common situation in human reasoning. Forexample, in planning the aforementioned
two-week Hawaii vacation, one might propose to spend two days on each of seven islands.
Prudence would indicate that this ambitious plan needs to be refined by adding details of
inter-island transportation.
Analgorithm forhierarchical planning withapproximate angelicdescriptions isshown
in Figure 11.8. For simplicity, we have kept to the same overall scheme used previously in
Figure 11.5, that is, abreadth-first search in the space of refinements. Asjust explained, the
algorithmcandetectplansthatwillandwon’tworkbycheckingtheintersections oftheopti-
misticandpessimisticreachablesetswiththegoal. (Thedetailsofhowtocomputethereach-
ablesetsofaplan,givenapproximatedescriptionsofeachstep,arecoveredinExercise11.5.)
Whenaworkableabstractplanisfound, thealgorithm decomposes theoriginal probleminto
subproblems, one for each step of the plan. The initial state and goal for each subproblem
are obtained by regressing a guaranteed-reachable goal state through the action schemas for
each step of the plan. (See Section 10.2.2 for a discussion of how regression works.) Fig-
ure 11.6(b) illustrates the basic idea: the right-hand circled state is the guaranteed-reachable
goal state, and the left-hand circled state is the intermediate goal obtained by regressing the
414 Chapter 11. PlanningandActingintheRealWorld
functionANGELIC-SEARCH(problem,hierarchy,initialPlan)returnssolutionorfail
frontier←aFIFOqueuewithinitialPlan astheonlyelement
loopdo
ifEMPTY?(frontier)thenreturnfail
plan←POP(frontier) /*choosestheshallowestnodeinfrontier */
ifREACH +(problem.INITIAL-STATE,plan)intersectsproblem.GOALthen
ifplan isprimitivethenreturnplan /*REACH + isexactforprimitiveplans*/
guaranteed←REACH − (problem.INITIAL-STATE,plan) ∩ problem.GOAL
ifguaranteed(cid:7)={}andMAKING-PROGRESS(plan,initialPlan)then
finalState←anyelementofguaranteed
returnDECOMPOSE(hierarchy,problem.INITIAL-STATE,plan,finalState)
hla←someHLAinplan
prefix,suffix←theactionsubsequencesbeforeandafterhla inplan
foreachsequence inREFINEMENTS(hla,outcome,hierarchy)do
frontier←INSERT(APPEND(prefix,sequence,suffix),frontier)
functionDECOMPOSE(hierarchy,s0,plan,sf)returnsasolution
solution←anemptyplan
whileplan isnotemptydo
action←REMOVE-LAST(plan)
si ←astateinREACH − (s0,plan)suchthatsf ∈REACH − (si,action)
problem←aproblemwithINITIAL-STATE=si andGOAL=sf
solution←APPEND(ANGELIC-SEARCH(problem,hierarchy,action),solution)
sf ←si
returnsolution
Figure11.8 Ahierarchicalplanningalgorithmthatusesangelicsemanticstoidentifyand
committohigh-levelplansthatworkwhileavoidinghigh-levelplansthatdon’t. Thepredi-
cateMAKING-PROGRESS checkstomakesurethatwearen’tstuckinaninfiniteregression
ofrefinements.Attoplevel,callANGELIC-SEARCHwith[Act]astheinitialPlan.
goalthrough thefinalaction.
The ability to commit to or reject high-level plans can give ANGELIC-SEARCH a sig-
nificant computational advantage over HIERARCHICAL-SEARCH, which in turn may have
a large advantage over plain old BREADTH-FIRST-SEARCH. Consider, for example, clean-
ing up a large vacuum world consisting of rectangular rooms connected by narrow corri-
dors. It makes sense to have an HLA for Navigate (as shown in Figure 11.4) and one for
CleanWholeRoom. (Cleaning the room could be implemented with the repeated application
of another HLA to clean each row.) Since there are five actions in this domain, the cost
for BREADTH-FIRST-SEARCH grows as 5d, where d is the length of the shortest solution
(roughly twice the total number of squares); the algorithm cannot manage even two 2×2
rooms. HIERARCHICAL-SEARCH ismoreefficient,butstillsuffersfromexponential growth
becauseittriesallwaysofcleaningthatareconsistentwiththehierarchy. ANGELIC-SEARCH
scales approximately linearly inthe numberofsquares—it commits toagood high-level se-
Section11.3. PlanningandActinginNondeterministic Domains 415
quence and prunes away the other options. Notice that cleaning a set of rooms by cleaning
each room in turn is hardly rocket science: it is easy for humans precisely because of the
hierarchical structure of the task. When we consider how difficult humans find it to solve
small puzzles such asthe 8-puzzle, itseems likely that the human capacity forsolving com-
plex problems derives to a great extent from their skill in abstracting and decomposing the
problem toeliminatecombinatorics.
The angelic approach can be extended to find least-cost solutions by generalizing the
notion of reachable set. Instead of a state being reachable or not, it has a cost for the most
efficient way to get there. (The cost is ∞ for unreachable states.) The optimistic and pes-
simisticdescriptions boundthesecosts. Inthisway,angelicsearchcanfindprovablyoptimal
abstract planswithoutconsidering theirimplementations. Thesameapproach canbeusedto
∗
HIERARCHICAL obtain effective hierarchical lookahead algorithms for online search, in the style of LRTA
LOOKAHEAD
(page152). Insomeways,suchalgorithmsmirroraspectsofhumandeliberationintaskssuch
asplanningavacationtoHawaii—consideration ofalternativesisdoneinitiallyatanabstract
leveloverlongtimescales;somepartsoftheplanareleftquiteabstractuntilexecution time,
suchashowtospendtwolazydaysonMolokai,whileotherspartsareplannedindetail,such
as the flights to be taken and lodging to be reserved—without these refinements, there is no
guarantee thattheplanwouldbefeasible.
11.3 PLANNING AND ACTING IN NONDETERMINISTIC DOMAINS
In this section we extend planning to handle partially observable, nondeterministic, and un-
known environments. Chapter 4 extended search in similar ways, and the methods here are
also similar: sensorless planning (also known as conformant planning) for environments
with no observations; contingency planning for partially observable and nondeterministic
environments; andonlineplanningandreplanningforunknownenvironments.
While the basic concepts are the same as in Chapter 4, there are also significant dif-
ferences. These arise because planners deal withfactored representations ratherthan atomic
representations. Thisaffectsthewaywerepresenttheagent’scapabilityforactionandobser-
vation and the way we represent belief states—the sets of possible physical states the agent
might be in—for unobservable and partially observable environments. We can also take ad-
vantage of many of the domain-independent methods given in Chapter 10 for calculating
searchheuristics.
Considerthisproblem: givenachairandatable,thegoalistohavethemmatch—have
the same color. In the initial state wehave two cans of paint, but the colors of the paint and
thefurniture areunknown. Onlythetableisinitiallyinthe agent’s fieldofview:
Init(Object(Table)∧Object(Chair)∧Can(C )∧Can(C )∧InView(Table))
1 2
Goal(Color(Chair,c)∧Color(Table,c))
There are two actions: removing the lid from a paint can and painting an object using the
paintfromanopencan. Theactionschemasarestraightforward, withoneexception: wenow
allow preconditions and effects to contain variables that are not part of the action’s variable
416 Chapter 11. PlanningandActingintheRealWorld
list. That is, Paint(x,can) does not mention the variable c, representing the color of the
paint in the can. In the fully observable case, this is not allowed—we would have to name
the action Paint(x,can,c). But in the partially observable case, we might or might not
knowwhatcolorisinthecan. (Thevariable cisuniversally quantified, justlikealltheother
variables inanactionschema.)
Action(RemoveLid(can),
PRECOND:Can(can)
EFFECT:Open(can))
Action(Paint(x,can),
PRECOND:Object(x)∧Can(can)∧Color(can,c)∧Open(can)
EFFECT:Color(x,c))
Tosolveapartiallyobservableproblem,theagentwillhavetoreasonabouttheperceptsitwill
obtainwhenitisexecutingtheplan. Theperceptwillbesuppliedbytheagent’ssensorswhen
itisactually acting, butwhenitisplanning itwillneed amodel ofitssensors. InChapter4,
this model was given by a function, PERCEPT(s). For planning, we augment PDDL with a
newtypeofschema,theperceptschema:
PERCEPTSCHEMA
Percept(Color(x,c),
PRECOND:Object(x)∧InView(x)
Percept(Color(can,c),
PRECOND:Can(can)∧InView(can)∧Open(can)
The first schema says that whenever an object is in view, the agent will perceive the color
of the object (that is, for the object x, the agent will learn the truth value of Color(x,c) for
all c). The second schema says that if an open can is in view, then the agent perceives the
color of the paint in the can. Because there are no exogenous events in this world, the color
of an object will remain the same, even if it is not being perceived, until the agent performs
an action to change the object’s color. Of course, the agent will need an action that causes
objects(oneatatime)tocomeintoview:
Action(LookAt(x),
PRECOND:InView(y)∧(x(cid:7)= y)
EFFECT:InView(x)∧¬InView(y))
Fora fully observable environment, we would have a Percept axiom with no preconditions
for each fluent. A sensorless agent, on the other hand, has no Percept axioms at all. Note
that evenasensorless agent can solvethe painting problem. Onesolution istoopen anycan
of paint and apply it to both chair and table, thus coercing them to be the same color (even
thoughtheagentdoesn’tknowwhatthecoloris).
A contingent planning agent with sensors can generate a better plan. First, look at the
table and chair to obtain their colors; if they are already the same then the plan is done. If
not, look at the paint cans; if the paint in a can is the same color as one piece of furniture,
thenapplythatpainttotheotherpiece. Otherwise,paintbothpieceswithanycolor.
Finally,anonlineplanningagentmightgenerateacontingent planwithfewerbranches
at first—perhaps ignoring the possibility that no cans match any of the furniture—and deal
Section11.3. PlanningandActinginNondeterministic Domains 417
with problems when they arise by replanning. It could also deal with incorrectness of its
action schemas. Whereas a contingent planner simply assumes that the effects of an action
always succeed—that painting the chair does the job—a replanning agent would check the
resultandmakeanadditional plantofixanyunexpectedfailure,suchasanunpaintedareaor
theoriginalcolorshowingthrough.
Intherealworld,agentsuseacombinationofapproaches. Carmanufacturerssellspare
tires and air bags, which are physical embodiments of contingent plan branches designed
to handle punctures or crashes. On the other hand, most car drivers never consider these
possibilities; when a problem arises they respond as replanning agents. In general, agents
plan only for contingencies that have important consequences and a nonnegligible chance
of happening. Thus, a car driver contemplating a trip across the Sahara desert should make
explicit contingency plans for breakdowns, whereas a trip to the supermarket requires less
advanceplanning. Wenextlookateachofthethreeapproaches inmoredetail.
11.3.1 Sensorless planning
Section 4.4.1 (page 138) introduced the basic idea of searching in belief-state space to find
asolution forsensorless problems. Conversion ofasensorless planning problem to abelief-
state planning problem works muchthe samewayasit didinSection 4.4.1; the maindiffer-
encesarethattheunderlyingphysicaltransitionmodelisrepresentedbyacollectionofaction
schemas and the belief state can be represented by a logical formula instead of an explicitly
enumerated set of states. Forsimplicity, weassume that the underlying planning problem is
deterministic.
The initial belief state for the sensorless painting problem can ignore InView fluents
because the agent has no sensors. Furthermore, we take as given the unchanging facts
Object(Table) ∧ Object(Chair) ∧ Can(C ) ∧ Can(C ) because these hold in every be-
1 2
lief state. The agent doesn’t know the colors of the cans or the objects, or whether the cans
areopenorclosed, butitdoesknowthatobjects andcanshave colors: ∀x ∃c Color(x,c).
AfterSkolemizing, (seeSection9.5),weobtaintheinitial beliefstate:
b = Color(x,C(x)).
0
In classical planning, where the closed-world assumption is made, we would assume that
any fluentnot mentioned inastate is false, but in sensorless (and partially observable) plan-
ning we have to switch to an open-world assumption in which states contain both positive
and negative fluents, and if a fluent does not appear, its value is unknown. Thus, the belief
state corresponds exactly to the set of possible worlds that satisfy the formula. Given this
initialbeliefstate,thefollowingactionsequence isasolution:
[RemoveLid(Can ),Paint(Chair,Can ),Paint(Table,Can )].
1 1 1
We now show how to progress the belief state through the action sequence to show that the
finalbeliefstatesatisfiesthegoal.
First, note that in a given belief state b, the agent can consider any action whose pre-
conditions aresatisfied by b. (Theotheractions cannot beused because thetransition model
doesn’t define the effects of actions whose preconditions might be unsatisfied.) According
418 Chapter 11. PlanningandActingintheRealWorld
to Equation (4.4) (page 139), the general formula for updating the belief state b given an
applicable actionainadeterministic worldisasfollows:
b (cid:2) = RESULT(b,a) = {s (cid:2) : s (cid:2) =RESULTP (s,a)ands ∈ b}
whereRESULTP definesthephysicaltransitionmodel. Forthetimebeing,weassumethatthe
initial belief state is always a conjunction of literals, that is, a 1-CNF formula. To construct
(cid:2)
thenewbeliefstateb,wemustconsiderwhathappens toeachliteral (cid:3)ineachphysicalstate
sinbwhenactionaisapplied. Forliteralswhosetruthvalueisalreadyknowninb,thetruth
(cid:2)
value in b is computed from the current value and the add list and delete list of the action.
(For example, if (cid:3) is in the delete list of the action, then ¬(cid:3) is added to b (cid:2) .) What about a
literalwhosetruthvalueisunknowninb? Therearethreecases:
(cid:2)
1. Iftheactionadds(cid:3),then(cid:3)willbetrueinb regardless ofitsinitialvalue.
(cid:2)
2. Iftheactiondeletes(cid:3),then(cid:3)willbefalseinb regardlessofitsinitialvalue.
3. Iftheactiondoesnotaffect(cid:3),then(cid:3)willretainitsinitialvalue(whichisunknown)and
(cid:2)
willnotappearinb.
(cid:2)
Hence,weseethatthecalculation ofb isalmostidentical totheobservable case, whichwas
specifiedbyEquation(10.1)onpage368:
b (cid:2) = RESULT(b,a) = (b−DEL(a))∪ADD(a).
(cid:2)
We cannot quite use the set semantics because (1) we must make sure that b does not con-
tain both (cid:3) and ¬(cid:3), and (2) atoms may contain unbound variables. But it is still the case
that RESULT(b,a) is computed by starting with b, setting any atom that appears in DEL(a)
to false, and setting any atom that appears in ADD(a) to true. For example, if we apply
RemoveLid(Can )totheinitialbeliefstateb ,weget
1 0
b = Color(x,C(x))∧Open(Can ).
1 1
WhenweapplytheactionPaint(Chair,Can ),thepreconditionColor(Can ,c)issatisfied
1 1
bytheknownliteralColor(x,C(x))withbinding{x/Can ,c/C(Can )}andthenewbelief
1 1
stateis
b = Color(x,C(x))∧Open(Can )∧Color(Chair,C(Can )).
2 1 1
Finally,weapplytheactionPaint(Table,Can )toobtain
1
b = Color(x,C(x))∧Open(Can )∧Color(Chair,C(Can ))
3 1 1
∧Color(Table,C(Can )).
1
Thefinalbeliefstatesatisfiesthegoal,Color(Table,c)∧Color(Chair,c),withthevariable
cboundtoC(Can ).
1
The preceding analysis of the update rule has shown a very important fact: the family
of belief states defined as conjunctions of literals is closed under updates defined by PDDL
actionschemas. Thatis,ifthebelief statestartsasaconjunction ofliterals, thenanyupdate
will yield a conjunction of literals. That means that in a world with n fluents, any belief
state can be represented by a conjunction of size O(n). This is a very comforting result,
considering that there are 2n states in the world. It says we can compactly represent all the
subsetsofthose2n statesthatwewilleverneed. Moreover,theprocessofcheckingforbelief
Section11.3. PlanningandActinginNondeterministic Domains 419
states that are subsets or supersets of previously visited belief states is also easy, at least in
thepropositional case.
Theflyinthe ointment of this pleasant picture isthat itonly works foraction schemas
that have the same effects for all states in which their preconditions are satisfied. It is this
propertythatenablesthepreservationofthe1-CNFbelief-staterepresentation. Assoonasthe
effect can depend on the state, dependencies are introduced between fluents and the 1-CNF
property is lost. Consider, for example, the simple vacuum world defined in Section 3.2.1.
Let the fluents be AtL and AtR for the location of the robot and CleanL and CleanR for
the state of the squares. According to the definition of the problem, the Suck action has no
precondition—itcanalwaysbedone. Thedifficultyisthatitseffectdependsontherobot’slo-
cation: whentherobotisAtL,theresultisCleanL,butwhenitisAtR,theresultisCleanR.
CONDITIONAL Forsuch actions, ouraction schemas will need something new: a conditional effect. These
EFFECT
have the syntax “when condition: effect,” where condition is a logical formula to be com-
pared against the current state, and effect is a formula describing the resulting state. Forthe
vacuumworld,wehave
Action(Suck,
EFFECT:whenAtL:CleanL∧whenAtR:CleanR).
When applied to the initial belief state True, the resulting belief state is (AtL∧CleanL)∨
(AtR ∧CleanR), which is no longer in 1-CNF. (This transition can be seen in Figure 4.14
on page 141.) In general, conditional effects can induce arbitrary dependencies among the
fluentsinabeliefstate, leadingtobeliefstatesofexponential sizeintheworstcase.
It is important to understand the difference between preconditions and conditional ef-
fects. Allconditionaleffectswhoseconditionsaresatisfiedhavetheireffectsappliedtogener-
atetheresultingstate;ifnonearesatisfied,thentheresultingstateisunchanged. Ontheother
hand, if a precondition is unsatisfied, then the action is inapplicable and the resulting state
is undefined. From the point of view of sensorless planning, it is better to have conditional
effects than an inapplicable action. Forexample, we could split Suck into two actions with
unconditional effectsasfollows:
Action(SuckL,
PRECOND:AtL; EFFECT:CleanL)
Action(SuckR,
PRECOND:AtR; EFFECT:CleanR).
Nowwehaveonly unconditional schemas, sothebelief states allremainin1-CNF;unfortu-
nately, wecannot determinetheapplicability ofSuckLandSuckR intheinitialbeliefstate.
Itseemsinevitable, then,thatnontrivial problemswillinvolve wigglybeliefstates, just
like those encountered when we considered the problem of state estimation for the wumpus
world(seeFigure7.21onpage271). Thesolution suggested thenwastouseaconservative
approximation to the exact belief state; for example, the belief state can remain in 1-CNF
if it contains all literals whose truth values can be determined and treats all other literals as
unknown. While this approach is sound, in that it never generates an incorrect plan, it is
incomplete because it may be unable to find solutions to problems that necessarily involve
interactions among literals. To give a trivial example, if the goal is for the robot to be on
420 Chapter 11. PlanningandActingintheRealWorld
a clean square, then [Suck] is a solution but a sensorless agent that insists on 1-CNF belief
stateswillnotfindit.
Perhaps a better solution is to look for action sequences that keep the belief state
as simple as possible. For example, in the sensorless vacuum world, the action sequence
[Right,Suck,Left,Suck]generates thefollowingsequence ofbeliefstates:
b = True
0
b = AtR
1
b = AtR∧CleanR
2
b = AtL∧CleanR
3
b = AtL∧CleanR∧CleanL
4
That is, the agent can solve the problem while retaining a 1-CNF belief state, even though
some sequences (e.g., those beginning with Suck) go outside 1-CNF. The general lesson is
not lost on humans: we are always performing little actions (checking the time, patting our
pocketstomakesurewehavethecarkeys,readingstreetsignsaswenavigatethroughacity)
toeliminate uncertainty andkeepourbeliefstatemanageable.
There is another, quite different approach to the problem of unmanageably wiggly be-
lief states: don’t bother computing them at all. Suppose the initial belief state is b and we
0
would like to know the belief state resulting from the action sequence [a ,...,a ]. Instead
1 m
of computing it explicitly, just represent it as “b then [a ,...,a ].” This is a lazy but un-
0 1 m
ambiguous representation of the belief state, and it’s quite concise—O(n +m) where n is
the size of the initial belief state (assumed to be in 1-CNF) and m is the maximum length
of an action sequence. As a belief-state representation, it suffers from one drawback, how-
ever: determining whether the goal is satisfied, or an action is applicable, may require a lot
ofcomputation.
Thecomputationcanbeimplementedasanentailmenttest: ifA representsthecollec-
m
tion of successor-state axioms required to define occurrences of the actions a ,...,a —as
1 m
explainedforSATPLANinSection10.4.1—andG
m
assertsthatthegoalistrueaftermsteps,
thentheplanachievesthegoalifb ∧A |= G ,thatis,ifb ∧A ∧¬G isunsatisfiable.
0 m m 0 m m
GivenamodernSATsolver,itmaybepossibletodothismuchmorequicklythancomputing
thefull belief state. Forexample, ifnoneofthe actions inthe sequence hasaparticular goal
fluent in its add list, the solver will detect this immediately. It also helps if partial results
aboutthebeliefstate—forexample,fluentsknowntobetrueorfalse—arecachedtosimplify
subsequent computations.
The final piece of the sensorless planning puzzle is a heuristic function to guide the
search. The meaning of the heuristic function is the same as for classical planning: an esti-
mate(perhaps admissible) ofthecostofachieving thegoal from thegivenbelief state. With
belief states, we have one additional fact: solving any subset of a belief state is necessarily
easierthansolvingthebeliefstate:
ifb ⊆ b thenh ∗ (b ) ≤ h ∗ (b ).
1 2 1 2
Hence,anyadmissibleheuristiccomputedforasubsetisadmissibleforthebeliefstateitself.
Themostobvious candidates arethesingleton subsets, thatis,individual physicalstates. We
Section11.3. PlanningandActinginNondeterministic Domains 421
can take any random collection of states s ,...,s that are in the belief state b, apply any
1 N
admissible heuristic hfromChapter10,andreturn
H(b) = max{h(s ),...,h(s )}
1 N
astheheuristicestimateforsolving b. Wecouldalsouseaplanninggraphdirectlyonbitself:
if it is a conjunction of literals (1-CNF), simply set those literals to be the initial state layer
ofthegraph. Ifbisnotin1-CNF,itmaybepossibletofindsetsofliteralsthattogetherentail
b. For example, if b is in disjunctive normal form (DNF), each term of the DNF formula is
a conjunction of literals that entails b and can form the initial layer of a planning graph. As
before,wecantakethemaximumoftheheuristics obtainedfromeachsetofliterals. Wecan
also use inadmissible heuristics such as the ignore-delete-lists heuristic (page 377), which
seemstoworkquitewellinpractice.
11.3.2 Contingent planning
We saw in Chapter 4 that contingent planning—the generation of plans with conditional
branchingbasedonpercepts—isappropriateforenvironmentswithpartialobservability,non-
determinism, orboth. Forthepartially observable painting problem withthepercept axioms
givenearlier, onepossible contingent solution isasfollows:
[LookAt(Table),LookAt(Chair),
ifColor(Table,c)∧Color(Chair,c)thenNoOp
else[RemoveLid(Can ),LookAt(Can ),RemoveLid(Can ),LookAt(Can ),
1 1 2 2
ifColor(Table,c)∧Color(can,c)thenPaint(Chair,can)
elseifColor(Chair,c)∧Color(can,c)thenPaint(Table,can)
else[Paint(Chair,Can ),Paint(Table,Can )]]]
1 1
Variables in this plan should be considered existentially quantified; the second line says
that if there exists some color c that is the color of the table and the chair, then the agent
need not do anything to achieve the goal. When executing this plan, a contingent-planning
agent can maintain its belief state as a logical formula and evaluate each branch condition
by determining if the belief state entails the condition formula or its negation. (It is up to
the contingent-planning algorithm to make sure that the agent will never end up in a be-
lief state where the condition formula’s truth value is unknown.) Note that with first-order
conditions, the formula may be satisfied in more than one way; for example, the condition
Color(Table,c)∧Color(can,c)mightbesatisfiedby{can/Can }andby{can/Can }if
1 2
both cans are the same color as the table. In that case, the agent can choose any satisfying
substitution toapplytotherestoftheplan.
As shown in Section 4.4.2, calculating the new belief state after an action and subse-
quentperceptisdoneintwostages. Thefirststagecalculates thebeliefstateaftertheaction,
justasforthesensorless agent:
ˆb =(b−DEL(a))∪ADD(a)
where,asbefore,wehaveassumedabeliefstaterepresented asaconjunction ofliterals. The
second stage is a little trickier. Suppose that percept literals p ,...,p are received. One
1 k
might think that we simply need to add these into the belief state; in fact, we can also infer
422 Chapter 11. PlanningandActingintheRealWorld
that the preconditions for sensing are satisfied. Now, if a percept p has exactly one percept
axiom, Percept(p,PRECOND:c), where c is a conjunction of literals, then those literals can
bethrownintothebeliefstatealongwithp. Ontheotherhand,ifphasmorethanonepercept
axiomwhosepreconditions mightholdaccordingtothepredictedbeliefstate
ˆb,thenwehave
to add in the disjunction of the preconditions. Obviously, this takes the belief state outside
1-CNF and brings up the same complications as conditional effects, with much the same
classesofsolutions.
Givenamechanism forcomputing exact orapproximate belief states, wecan generate
contingent plans with an extension of the AND–OR forward search over belief states used
in Section 4.4. Actions with nondeterministic effects—which are defined simply by using a
disjunction inthe EFFECT oftheactionschema—can beaccommodated withminorchanges
tothebelief-stateupdatecalculationandnochangetothesearchalgorithm.2 Fortheheuristic
function, many of the methods suggested for sensorless planning are also applicable in the
partially observable, nondeterministic case.
11.3.3 Onlinereplanning
Imagine watching aspot-welding robot inacarplant. Therobot’s fast, accurate motions are
repeated over and over again as each car passes down the line. Although technically im-
pressive, the robot probably does not seem at all intelligent because the motion is a fixed,
preprogrammed sequence; the robot obviously doesn’t “know what it’s doing” inany mean-
ingful sense. Now suppose that a poorly attached door falls off the car just as the robot is
about to apply a spot-weld. The robot quickly replaces its welding actuator with a gripper,
picks up the door, checks it forscratches, reattaches it to the car, sends an email to the floor
supervisor, switches back to the welding actuator, and resumes its work. All of a sudden,
the robot’s behavior seems purposive rather than rote; we assume it results not from a vast,
precomputed contingent plan but from an online replanning process—which means that the
robot doesneedtoknowwhatit’stryingtodo.
EXECUTION Replanningpresupposessomeformofexecutionmonitoringtodeterminetheneedfor
MONITORING
a new plan. One such need arises when a contingent planning agent gets tired of planning
forevery little contingency, such as whether the sky might fall on its head.3 Some branches
ofapartiallyconstructed contingent plancansimplysayReplan;ifsuchabranchisreached
during execution, the agent reverts to planning mode. Aswementioned earlier, the decision
as to how much of the problem to solve in advance and how much to leave to replanning
is one that involves tradeoffs among possible events with different costs and probabilities of
occurring. NobodywantstohavetheircarbreakdowninthemiddleoftheSaharadesertand
onlythenthinkabouthavingenough water.
2 Ifcyclicsolutionsarerequiredforanondeterministicproblem,AND–ORsearchmustbegeneralizedtoaloopy
versionsuchasLAO∗(HansenandZilberstein,2001).
3 In1954,aMrs.HodgesofAlabamawashitbymeteoritethatcrashedthroughherroof. In1992, apieceof
theMbalemeteoritehitasmallboyonthehead;fortunately,itsdescentwasslowedbybananaleaves(Jenniskens
etal.,1994).Andin2009,aGermanboyclaimedtohavebeenhitinthehandbyapea-sizedmeteorite.Noserious
injuriesresultedfromanyoftheseincidents,suggestingthattheneedforpreplanningagainstsuchcontingencies
issometimesoverstated.
Section11.3. PlanningandActinginNondeterministic Domains 423
whole plan
plan
S P E G
continuation
repair
O
Figure11.9 Beforeexecution,theplannercomesupwithaplan,herecalledwhole plan,
togetfromS toG. TheagentexecutesstepsoftheplanuntilitexpectstobeinstateE,but
observesitisactuallyinO. Theagentthenreplansfortheminimalrepairpluscontinuation
toreachG.
Replanningmayalsobeneedediftheagent’smodeloftheworldisincorrect. Themodel
MISSING foran action may have a missing precondition—for example, the agent may not know that
PRECONDITION
removing the lid of a paint can often requires a screwdriver; the model may have a missing
effect—forexample, painting anobjectmaygetpaintontheflooraswell;orthemodelmay
MISSINGEFFECT
MISSINGSTATE have a missing state variable—for example, the model given earlier has no notion of the
VARIABLE
amountofpaint inacan,ofhowitsactions affectthisamount, oroftheneedfortheamount
to be nonzero. The model may also lack provision for exogenous events such as someone
EXOGENOUSEVENT
knocking over the paint can. Exogenous events can also include changes in the goal, such
as the addition of the requirement that the table and chair not be painted black. Without the
ability to monitor and replan, an agent’s behavior is likely to be extremely fragile if it relies
onabsolutecorrectness ofitsmodel.
Theonline agent hasachoice ofhow carefully tomonitortheenvironment. Wedistin-
guishthreelevels:
• Actionmonitoring: beforeexecutinganaction,theagentverifiesthatalltheprecondi-
ACTIONMONITORING
tionsstillhold.
• Planmonitoring: beforeexecutinganaction,theagentverifiesthattheremainingplan
PLANMONITORING
willstillsucceed.
• Goalmonitoring: beforeexecutinganaction,theagentcheckstoseeifthereisabetter
GOALMONITORING
setofgoalsitcouldbetryingtoachieve.
In Figure 11.9 we see a schematic of action monitoring. The agent keeps track of both its
original plan, wholeplan, and the part of the plan that has not been executed yet, which is
denoted by plan. After executing the first few steps of the plan, the agent expects to be in
state E. But the agent observes it is actually in state O. It then needs to repair the plan by
findingsomepointP ontheoriginalplanthatitcangetbackto. (ItmaybethatP isthegoal
state,G.) Theagenttriestominimizethetotalcostoftheplan: therepairpart(from OtoP)
plusthecontinuation (from P toG).
424 Chapter 11. PlanningandActingintheRealWorld
Now let’s return to the example problem of achieving a chair and table of matching
color. Supposetheagentcomesupwiththisplan:
[LookAt(Table),LookAt(Chair),
ifColor(Table,c)∧Color(Chair,c)thenNoOp
else[RemoveLid(Can ),LookAt(Can ),
1 1
ifColor(Table,c)∧Color(Can ,c)thenPaint(Chair,Can )
1 1
else REPLAN]].
Now the agent is ready to execute the plan. Suppose the agent observes that the table and
can of paint are white and the chair is black. It then executes Paint(Chair,Can ). At this
1
point a classical planner would declare victory; the plan has been executed. But an online
execution monitoring agent needs tocheck thepreconditions ofthe remaining emptyplan—
that the table and chair are the same color. Suppose the agent perceives that they do not
have the same color—in fact, the chair is now a mottled gray because the black paint is
showing through. The agent then needs to figure out a position in whole plan to aim for
and arepair action sequence toget there. Theagent notices thatthe current state isidentical
to the precondition before the Paint(Chair,Can ) action, so the agent chooses the empty
1
sequence for repair and makes its plan be thesame [Paint]sequence that it just attempted.
With this new plan in place, execution monitoring resumes, and the Paint action is retried.
Thisbehavior willloop until thechairisperceived tobecompletely painted. Butnotice that
the loop is created by a process of plan–execute–replan, rather than by an explicit loop in a
plan. Notealso that the original plan need notcoverevery contingency. Ifthe agent reaches
thestepmarked REPLAN,itcanthengenerate anewplan(perhaps involving Can
2
).
Action monitoring is a simple method of execution monitoring, but it can sometimes
lead to less than intelligent behavior. Forexample, suppose there is no black orwhite paint,
and the agent constructs a plan to solve the painting problem by painting both the chair and
table red. Suppose that there isonly enough red paint forthe chair. With action monitoring,
theagentwouldgoaheadandpaintthechairred,thennoticethatitisoutofpaintandcannot
paintthetable,atwhichpointitwouldreplanarepair—perhaps paintingbothchairandtable
green. Aplan-monitoring agent candetect failure whenever the current state issuch that the
remaining plan no longer works. Thus, it would not waste time painting the chair red. Plan
monitoring achieves this by checking the preconditions for success of the entire remaining
plan—that is, the preconditions of each step in the plan, except those preconditions that are
achieved by another step in the remaining plan. Plan monitoring cuts off execution of a
doomed plan as soon as possible, rather than continuing until the failure actually occurs.4
Plan monitoring also allows for serendipity—accidental success. If someone comes along
andpaintsthetableredatthesametimethattheagentispaintingthechairred,thenthefinal
planpreconditionsaresatisfied(thegoalhasbeenachieved),andtheagentcangohomeearly.
It is straightforward to modify a planning algorithm so that each action in the plan
is annotated with the action’s preconditions, thus enabling action monitoring. It is slightly
4 Planmonitoringmeansthatfinally,after424pages,wehaveanagentthatissmarterthanadungbeetle(see
page39). Aplan-monitoringagentwouldnoticethatthedungballwasmissingfromitsgraspandwouldreplan
togetanotherballandplugitshole.
Section11.4. Multiagent Planning 425
more complex to enable plan monitoring. Partial-order and planning-graph planners have
the advantage that they have already built up structures that contain the relations necessary
forplanmonitoring. Augmentingstate-space planners with thenecessary annotations canbe
donebycarefulbookkeeping asthegoalfluentsareregressed throughtheplan.
Now that we have described a method for monitoring and replanning, we need to ask,
“Does it work?” This is a surprisingly tricky question. If we mean, “Can we guarantee that
the agent will always achieve the goal?” then the answer is no, because the agent could
inadvertently arrive at a dead end from which there is no repair. For example, the vacuum
agent might have a faulty model of itself and not know that its batteries can run out. Once
theydo,itcannot repairanyplans. Ifweruleoutdeadends—assume thatthereexists aplan
to reach the goal from any state in the environment—and assume that the environment is
really nondeterministic, in the sense that such a plan always has some chance of success on
anygivenexecutionattempt,thentheagentwilleventually reachthegoal.
Trouble occurs when an action is actually not nondeterministic, but rather depends on
some precondition that the agent does not know about. For example, sometimes a paint
canmaybeempty,sopaintingfromthatcanhasnoeffect. Noamountofretrying isgoingto
changethis.5 Onesolutionistochooserandomlyfromamongthesetofpossiblerepairplans,
ratherthantotrythesameoneeachtime. Inthiscase,therepairplanofopening anothercan
might work. A better approach is to learn a better model. Every prediction failure is an
opportunity forlearning; anagent should be able tomodify itsmodel ofthe worldto accord
withitspercepts. Fromthen on,thereplanner willbeable to comeupwitharepairthatgets
attherootproblem,ratherthanrelyingonlucktochooseagoodrepair. Thiskindoflearning
isdescribed inChapters18and19.
11.4 MULTIAGENT PLANNING
So far, we have assumed that only one agent is doing the sensing, planning, and acting.
Whentherearemultiple agentsintheenvironment, eachagent facesamultiagentplanning
MULTIAGENT probleminwhichittriestoachieveitsowngoalswiththehelporhindrance ofothers.
PLANNINGPROBLEM
Betweenthepurelysingle-agent andtrulymultiagentcases isawidespectrumofprob-
lems that exhibit various degrees of decomposition of the monolithic agent. An agent with
multiple effectors that can operate concurrently—for example, a human who can type and
MULTIEFFECTOR speak atthe same time—needs to do multieffector planningto manage each effector while
PLANNING
handling positive and negative interactions among the effectors. When the effectors are
physically decoupled into detached units—as in a fleet of delivery robots in a factory—
MULTIBODY multieffector planning becomes multibody planning. A multibody problem is still a “stan-
PLANNING
dard”single-agent problemaslongastherelevantsensorinformationcollectedbyeachbody
can be pooled—either centrally or within each body—to form a common estimate of the
worldstatethattheninformstheexecution oftheoverallplan;inthiscase, themultiplebod-
ies act as a single body. When communication constraints make this impossible, we have
5 Futilerepetitionofaplanrepairisexactlythebehaviorexhibitedbythesphexwasp(page39).
426 Chapter 11. PlanningandActingintheRealWorld
DECENTRALIZED whatissometimescalledadecentralizedplanningproblem;thisisperhapsamisnomer,be-
PLANNING
causetheplanningphaseiscentralizedbuttheexecutionphaseisatleastpartiallydecoupled.
Inthiscase,thesubplanconstructed foreachbodymayneedtoincludeexplicitcommunica-
tiveactions withotherbodies. Forexample, multiple reconnaissance robots covering awide
areamayoftenbeoutofradiocontactwitheachotherandshouldsharetheirfindingsduring
timeswhencommunication isfeasible.
When a single entity is doing the planning, there is really only one goal, which all the
bodiesnecessarilyshare. Whenthebodiesaredistinctagentsthatdotheirownplanning,they
may still share identical goals; for example, two human tennis players who form a doubles
team share the goal of winning the match. Even with shared goals, however, the multibody
and multiagent cases are quite different. In a multibody robotic doubles team, a single plan
dictates which body willgo where on the court and which body willhit the ball. Ina multi-
agent doubles team,ontheotherhand, eachagent decides whattodo;without somemethod
for coordination, both agents may decide to cover the same part of the court and each may
COORDINATION
leavetheballfortheothertohit.
Theclearest caseofamultiagent problem, ofcourse, iswhen the agents havedifferent
goals. In tennis, the goals of two opposing teams are in direct conflict, leading to the zero-
sum situation of Chapter 5. Spectators could be viewed as agents if their support ordisdain
is a significant factor and can be influenced by the players’ conduct; otherwise, they can be
treated asanaspect ofnature—just liketheweather—that is assumedtobeindifferent tothe
players’ intentions.6
Finally, some systems are a mixture of centralized and multiagent planning. For ex-
ample, a delivery company may do centralized, offline planning for the routes of its trucks
and planes each day, but leave some aspects open for autonomous decisions by drivers and
pilots who can respond individually to traffic and weather situations. Also, the goals of the
company and its employees are brought into alignment, to some extent, by the payment of
incentives(salariesandbonuses)—a suresignthatthisisatruemultiagentsystem.
INCENTIVE
The issues involved in multiagent planning can be divided roughly into two sets. The
first, covered in Section 11.4.1, involves issues of representing and planning for multiple
simultaneous actions;theseissuesoccurinallsettingsfrommultieffectortomultiagentplan-
ning. The second, covered in Section 11.4.2, involves issues of cooperation, coordination,
andcompetition arisingintruemultiagentsettings.
11.4.1 Planning withmultiplesimultaneous actions
Forthe time being, wewill treat the multieffector, multibody, and multiagent settings in the
same way, labeling them generically as multiactor settings, using the generic term actor to
MULTIACTOR
cover effectors, bodies, and agents. The goal of this section is to work out how to define
ACTOR
transition models, correct plans, and efficient planning algorithms forthe multiactor setting.
Acorrectplanisonethat,ifexecutedbytheactors,achievesthegoal. (Inthetruemultiagent
setting, of course, the agents may not agree to execute any particular plan, but at least they
6 We apologize to residents of the United Kingdom, where the mere act of contemplating a game of tennis
guaranteesrain.
Section11.4. Multiagent Planning 427
Actors(A,B)
Init(At(A,LeftBaseline) ∧ At(B,RightNet) ∧
Approaching(Ball,RightBaseline)) ∧ Partner(A,B) ∧ Partner(B,A)
Goal(Returned(Ball) ∧ (At(a,RightNet) ∨ At(a,LeftNet))
Action(Hit(actor,Ball),
PRECOND:Approaching(Ball,loc) ∧ At(actor,loc)
EFFECT:Returned(Ball))
Action(Go(actor,to),
PRECOND:At(actor,loc) ∧ to (cid:7)= loc,
EFFECT:At(actor,to) ∧ ¬At(actor,loc))
Figure11.10 Thedoublestennisproblem. TwoactorsAandB areplayingtogetherand
canbeinoneoffourlocations: LeftBaseline,RightBaseline,LeftNet,andRightNet. The
ballcanbereturnedonlyifaplayerisintherightplace. Notethateachactionmustinclude
theactorasanargument.
will know what plans would work if they did agree to execute them.) For simplicity, we
assume perfect synchronization: each action takes the same amount of time and actions at
SYNCHRONIZATION
eachpointinthejointplanaresimultaneous.
We begin with the transition model; for the deterministic case, this is the function
RESULT(s,a). In the single-agent setting, there might be b different choices for the action;
b can be quite large, especially for first-order representations with many objects to act on,
but action schemas provide a concise representation nonetheless. In the multiactor setting
with n actors, the single action a is replaced by a joint action (cid:16)a ,...,a (cid:17), where a is the
JOINTACTION 1 n i
action taken by the ith actor. Immediately, we see two problems: first, we have to describe
the transition model for bn different joint actions; second, we have a joint planning problem
withabranching factorofbn.
Having put the actors together into a multiactor system with a huge branching factor,
the principal focus of research on multiactor planning has been to decouple the actors to
the extent possible, so that the complexity of the problem grows linearly with n rather than
exponentially. Iftheactorshavenointeractionwithoneanother—forexample,nactorseach
playing agameofsolitaire—then wecansimplysolve nseparate problems. Iftheactors are
loosely coupled,canweattainsomething closetothisexponential improvement? Thisis,of
LOOSELYCOUPLED
course, a central question in many areas of AI. We have seen it explicitly in the context of
CSPs,where “tree like” constraint graphs yielded efficient solution methods (see page 225),
as well as in the context of disjoint pattern databases (page 106) and additive heuristics for
planning (page378).
Thestandardapproachtolooselycoupledproblemsistopretendtheproblemsarecom-
pletelydecoupledandthenfixuptheinteractions. Forthetransitionmodel,thismeanswriting
actionschemasasiftheactorsactedindependently. Let’sseehowthisworksforthedoubles
tennisproblem. Let’ssupposethatatonepointinthegame,theteamhasthegoalofreturning
the ball that has been hit to them and ensuring that at least one of them is covering the net.
428 Chapter 11. PlanningandActingintheRealWorld
A firstpass at amultiactor definition might look like Figure 11.10. With this definition, itis
easytoseethatthefollowingjointplanplanworks:
JOINTPLAN
PLAN 1:
A: [Go(A,RightBaseline),Hit(A,Ball)]
B : [NoOp(B),NoOp(B)].
Problemsarise,however,whenaplanhasbothagentshitting theballatthesametime. Inthe
real world, this won’t work, but the action schema for Hit says that the ball will be returned
successfully. Technically, the difficulty is that preconditions constrain the state in which an
action canbeexecuted successfully, butdonotconstrain otheractions thatmightmessitup.
CONCURRENT Wesolvethisbyaugmenting action schemaswithonenewfeature: a concurrentaction list
ACTIONLIST
statingwhichactionsmustormustnotbeexecutedconcurrently. Forexample,theHit action
couldbedescribed asfollows:
Action(Hit(a,Ball),
CONCURRENT:b(cid:7)= a ⇒ ¬Hit(b,Ball)
PRECOND:Approaching(Ball,loc)∧At(a,loc)
EFFECT:Returned(Ball)).
In other words, the Hit action has its stated effect only if no other Hit action by another
agent occurs at the same time. (In the SATPLAN approach, this would be handled by a
partial action exclusion axiom.) Forsome actions, the desired effect is achieved only when
anotheractionoccursconcurrently. Forexample,twoagentsareneededtocarryacoolerfull
ofbeverages tothetenniscourt:
Action(Carry(a,cooler,here,there),
CONCURRENT:b (cid:7)= a∧Carry(b,cooler,here,there)
PRECOND:At(a,here)∧At(cooler,here)∧Cooler(cooler)
EFFECT:At(a,there)∧At(cooler,there)∧¬At(a,here)∧¬At(cooler,here)).
With these kinds of action schemas, any of the planning algorithms described in Chapter 10
canbeadaptedwithonlyminormodificationstogeneratemultiactorplans. Totheextentthat
thecouplingamongsubplansisloose—meaning thatconcurrency constraints comeintoplay
only rarely during plan search—one would expect the various heuristics derived for single-
agent planning to also be effective in the multiactor context. Wecould extend this approach
withtherefinementsofthelasttwochapters—HTNs,partialobservability, conditionals, exe-
cutionmonitoring, andreplanning—but thatisbeyondthescopeofthisbook.
11.4.2 Planning withmultipleagents: Cooperationand coordination
Now let us consider the true multiagent setting in which each agent makes its own plan. To
start with, let us assume that the goals and knowledge base are shared. One might think
that this reduces to the multibody case—each agent simply computes the joint solution and
executes its own part of that solution. Alas, the “the” in “the joint solution” is misleading.
Forourdoublesteam,morethanonejointsolutionexists:
PLAN 2:
A: [Go(A,LeftNet),NoOp(A)]
B : [Go(B,RightBaseline),Hit(B,Ball)].
Section11.4. Multiagent Planning 429
Ifbothagentscanagreeoneitherplan1orplan2,thegoalwillbeachieved. ButifAchooses
plan2andBchoosesplan1,thennobodywillreturntheball. Conversely,ifAchooses1and
B chooses 2,then theywillboth trytohittheball. Theagents mayrealize this, but howcan
theycoordinate tomakesuretheyagreeontheplan?
Oneoption is to adopt aconvention before engaging in joint activity. A convention is
CONVENTION
anyconstraintontheselectionofjointplans. Forexample, theconvention “sticktoyourside
ofthecourt” wouldrule outplan 1,causing thedoubles partners toselect plan 2. Driverson
aroadfacetheproblemofnotcollidingwitheachother;this is(partially)solvedbyadopting
the convention “stay on the right side of the road” in most countries; the alternative, “stay
on the left side,” works equally well as long as all agents in an environment agree. Similar
considerations applytothedevelopmentofhumanlanguage, wheretheimportantthingisnot
which language each individual should speak, but the fact that a community all speaks the
samelanguage. Whenconventions arewidespread, theyarecalled sociallaws.
SOCIALLAWS
In the absence of a convention, agents can use communication to achieve common
knowledge of a feasible joint plan. For example, a tennis player could shout “Mine!” or
“Yours!” toindicateapreferredjointplan. Wecovermechanismsforcommunicationinmore
depth in Chapter 22, where we observe that communication does not necessarily involve a
verbalexchange. Forexample,oneplayercancommunicateapreferredjointplantotheother
simply by executing the first part of it. If agent Aheads forthe net, then agent B is obliged
togobacktothebaselinetohittheball,becauseplan2istheonlyjointplanthatbeginswith
A’s heading forthe net. This approach to coordination, sometimes called planrecognition,
PLANRECOGNITION
workswhenasingleaction(orshortsequence ofactions)isenoughtodetermineajointplan
unambiguously. Note that communication can workas wellwith competitive agents aswith
cooperative ones.
Conventions can also arise through evolutionary processes. For example, seed-eating
harvester ants are social creatures that evolved from the less social wasps. Colonies of ants
execute very elaborate joint plans without any centralized control—the queen’s job is to re-
produce, not to do centralized planning—and with very limited computation, communica-
tion,andmemorycapabilities ineachant(Gordon, 2000,2007). Thecolonyhasmanyroles,
including interior workers, patrollers, and foragers. Each ant chooses to perform a role ac-
cording tothe local conditions it observes. Forexample, foragers travel awayfrom the nest,
searchforaseed,andwhentheyfindone, bringitbackimmediately. Thus,therateatwhich
foragers return to the nest is an approximation of the availability of food today. When the
rateishigh, otherantsabandon theircurrent roleandtakeontheroleofscavenger. Theants
appeartohaveaconventionontheimportanceofroles—foragingisthemostimportant—and
antswilleasilyswitchintothemoreimportantroles, butnotintothelessimportant. Thereis
somelearningmechanism: acolonylearnstomakemoresuccessfulandprudentactionsover
thecourseofitsdecades-long life,eventhoughindividual antsliveonlyaboutayear.
Onefinalexampleofcooperative multiagent behavior appears intheflocking behavior
of birds. We can obtain a reasonable simulation of a flock if each bird agent (sometimes
BOID called a boid) observes the positions of its nearest neighbors and then chooses the heading
andacceleration thatmaximizestheweightedsumofthesethreecomponents:
430 Chapter 11. PlanningandActingintheRealWorld
(a) (b) (c)
Figure11.11 (a)Asimulatedflockofbirds,usingReynold’sboidsmodel.Imagecourtesy
GiuseppeRandazzo,novastructura.net. (b)An actualflock ofstarlings. ImagebyEduardo
(pastaboy sleeps on flickr). (c) Two competitiveteams of agents attempting to capture the
towersintheNEROgame.ImagecourtesyRistoMiikkulainen.
1. Cohesion: apositivescoreforgettingclosertotheaverageposition oftheneighbors
2. Separation: anegativescoreforgetting tooclosetoanyoneneighbor
3. Alignment: apositivescoreforgetting closertotheaverageheadingoftheneighbors
EMERGENT If all the boids execute this policy, the flock exhibits the emergent behavior of flying as a
BEHAVIOR
pseudorigid body with roughly constant density that does not disperse over time, and that
occasionally makessudden swooping motions. Youcanseeastillimages inFigure11.11(a)
and compare it to an actual flock in (b). As with ants, there is no need for each agent to
possessajointplanthatmodelstheactionsofotheragents.
Themostdifficultmultiagentproblemsinvolvebothcooperationwithmembersofone’s
own team and competition against members of opposing teams, all without centralized con-
trol. WeseethisingamessuchasroboticsoccerortheNEROgameshowninFigure11.11(c),
inwhich two teamsof software agents compete to capture the control towers. Asyet, meth-
ods for efficient planning in these kinds of environments—for example, taking advantage of
loosecoupling—are intheirinfancy.
11.5 SUMMARY
Thischapterhasaddressedsomeofthecomplicationsofplanningandactingintherealworld.
Themainpoints:
• Manyactionsconsumeresources,suchasmoney,gas,orrawmaterials. Itisconvenient
to treat these resources as numeric measures in a pool rather than try to reason about,
say, each individual coin and bill in the world. Actions can generate and consume
resources, anditisusually cheapandeffective tocheck partialplans forsatisfaction of
resourceconstraints beforeattemptingfurtherrefinements.
• Timeisoneofthemostimportant resources. Itcanbehandled byspecialized schedul-
ingalgorithms, orscheduling canbeintegrated withplanning.
Bibliographical andHistorical Notes 431
• Hierarchical task network (HTN) planning allows the agent to take advice from the
domain designer inthe form of high-level actions (HLAs)that can beimplemented in
variouswaysbylower-levelactionsequences. TheeffectsofHLAscanbedefinedwith
angelic semantics, allowing provably correct high-level plans to be derived without
consideration of lower-level implementations. HTN methods can create the very large
plansrequired bymanyreal-worldapplications.
• Standard planning algorithms assume complete and correct information and determin-
istic,fullyobservable environments. Manydomainsviolatethisassumption.
• Contingent plans allow the agent to sense the world during execution to decide what
branchoftheplantofollow. Insomecases,sensorlessorconformantplanningcanbe
used to construct a plan that works without the need for perception. Both conformant
andcontingentplanscanbeconstructedbysearchinthespaceofbeliefstates. Efficient
representation orcomputation ofbeliefstatesisakeyproblem.
• An online planning agent uses execution monitoring and splices in repairs as needed
to recover from unexpected situations, which can be due to nondeterministic actions,
exogenous events,orincorrect modelsoftheenvironment.
• Multiagentplanning isnecessary whenthere areotheragents intheenvironment with
whichtocooperate orcompete. Jointplanscanbeconstructed, butmustbeaugmented
withsomeformofcoordinationiftwoagentsaretoagreeonwhichjointplantoexecute.
• This chapter extends classic planning to cover nondeterministic environments (where
outcomes of actions are uncertain), but it is not the last word on planning. Chapter 17
describes techniques for stochastic environments (in which outcomes of actions have
probabilities associated with them): Markov decision processes, partially observable
Markovdecisionprocesses,andgametheory. InChapter21weshowthatreinforcement
learningallowsanagenttolearnhowtobehavefrompastsuccesses andfailures.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Planning with time constraints was first dealt with by DEVISER (Vere, 1983). The repre-
sentation of time in plans was addressed by Allen (1984) and by Dean et al. (1990) in the
FORBIN system. NONLIN+ (Tate and Whiter, 1984) and SIPE (Wilkins, 1988, 1990) could
reason about the allocation of limited resources to various plan steps. O-PLAN (Bell and
Tate, 1985), an HTN planner, had a uniform, general representation for constraints on time
and resources. In addition to the Hitachi application mentioned in the text, O-PLAN has
been applied tosoftware procurement planning atPriceWaterhouse andback-axle assembly
planning atJaguarCars.
The two planners SAPA (Do and Kambhampati, 2001) and T4 (Haslum and Geffner,
2001) both used forward state-space search with sophisticated heuristics to handle actions
with durations and resources. An alternative is to use very expressive action languages, but
guide them by human-written domain-specific heuristics, as is done by ASPEN (Fukunaga
etal.,1997), HSTS(Jonssonetal.,2000), andIxTeT(GhallabandLaruelle,1994).
432 Chapter 11. PlanningandActingintheRealWorld
A number of hybrid planning-and-scheduling systems have been deployed: ISIS (Fox
et al., 1982; Fox, 1990) has been used forjob shop scheduling at Westinghouse, GARI (De-
scotte and Latombe, 1985) planned the machining and construction of mechanical parts,
FORBIN was used for factory control, and NONLIN+ was used for naval logistics planning.
Wechosetopresentplanningandschedulingastwoseparateproblems;(Cushingetal.,2007)
show that this can lead to incompleteness on certain problems. There is a long history of
scheduling inaerospace. T-SCHED (Drabble, 1990)wasusedtoschedule mission-command
sequencesfortheUOSAT-II satellite. OPTIMUM-AIV (Aarupetal.,1994)andPLAN-ERS1
(Fuchs et al., 1990), both based on O-PLAN, were used for spacecraft assembly and obser-
vation planning, respectively, at the European Space Agency. SPIKE (Johnston and Adorf,
1992) was used for observation planning at NASA for the Hubble Space Telescope, while
the Space Shuttle Ground Processing Scheduling System (Deale et al., 1994) does job-shop
scheduling of up to 16,000 worker-shifts. Remote Agent (Muscettola et al., 1998) became
thefirstautonomousplanner–scheduler tocontrolaspacecraftwhenitflewonboardtheDeep
SpaceOneprobein1999. Spaceapplications havedriventhedevelopment ofalgorithms for
resourceallocations; seeLaborie(2003)andMuscettola(2002). Theliteratureonscheduling
is presented in a classic survey article (Lawler et al., 1993), a recent book (Pinedo, 2008),
andaneditedhandbook (Blazewiczetal.,2007).
MACROPS
Thefacility inthe STRIPS program forlearning macrops—“macro-operators” consist-
ingofasequence ofprimitive steps—could beconsidered the firstmechanism forhierarchi-
cal planning (Fikes et al., 1972). Hierarchy was also used in the LAWALY system (Siklossy
and Dreussi, 1973). The ABSTRIPS system (Sacerdoti, 1974) introduced the idea of an ab-
ABSTRACTION straction hierarchy, whereby planning at higher levels was permitted to ignore lower-level
HIERARCHY
preconditions of actions in order to derive the general structure of a working plan. Austin
Tate’s Ph.D. thesis (1975b) and work by Earl Sacerdoti (1977) developed the basic ideas of
HTN planning in its modern form. Many practical planners, including O-PLAN and SIPE,
are HTNplanners. Yang (1990) discusses properties of actions that make HTNplanning ef-
ficient. Erol, Hendler, and Nau (1994, 1996) present a complete hierarchical decomposition
planner aswellasarange ofcomplexity results forpure HTNplanners. Ourpresentation of
HLAsandangelicsemanticsisduetoMarthietal.(2007,2008). Kambhampatietal.(1998)
haveproposedanapproachinwhichdecompositionsarejustanotherformofplanrefinement,
similartotherefinementsfornon-hierarchical partial-order planning.
Beginningwiththeworkonmacro-operatorsinSTRIPS,oneofthegoalsofhierarchical
planninghasbeenthereuseofpreviousplanningexperience intheformofgeneralizedplans.
The technique of explanation-based learning, described in depth in Chapter 19, has been
applied in several systems as a means of generalizing previously computed plans, including
SOAR (Laird etal.,1986) and PRODIGY (Carbonell etal.,1989). Analternative approach is
to store previously computed plans in their original form and then reuse them to solve new,
similarproblems by analogy to the original problem. Thisis theapproach taken by thefield
CASE-BASED called case-based planning (Carbonell, 1983; Alterman, 1988; Hammond, 1989). Kamb-
PLANNING
hampati (1994) argues that case-based planning should be analyzed as a form of refinement
planning andprovidesaformalfoundation forcase-based partial-order planning.
Bibliographical andHistorical Notes 433
Early planners lacked conditionals and loops, but some could use coercion to form
conformant plans. Sacerdoti’s NOAH solved the“keysandboxes”problem, aplanning chal-
lenge problem in which the planner knows little about the initial state, using coercion. Ma-
son (1993) argued that sensing often can and should be dispensed with in robotic planning,
and described a sensorless plan that can move a tool into a specific position on a table by a
sequence oftiltingactions, regardless oftheinitialposition.
GoldmanandBoddy(1996)introducedthetermconformantplanning,notingthatsen-
sorless plans are often effective even if the agent has sensors. The first moderately efficient
conformant planner was Smith and Weld’s (1998) Conformant Graphplan or CGP. Ferraris
andGiunchiglia(2000)andRintanen(1999)independently developedSATPLAN-basedcon-
formantplanners. BonetandGeffner(2000)describeaconformantplannerbasedonheuristic
searchinthespaceofbeliefstates,drawingonideasfirstdevelopedinthe1960sforpartially
observable Markovdecisionprocesses, orPOMDPs(seeChapter17).
Currently, there are three main approaches to conformant planning. The first two use
heuristic search in belief-state space: HSCP (Bertoli et al., 2001a) uses binary decision
diagrams (BDDs) to represent belief states, whereas Hoffmann and Brafman (2006) adopt
the lazy approach of computing precondition and goal tests on demand using a SAT solver.
The third approach, championed primarily by Jussi Rintanen (2007), formulates the entire
sensorless planning problem as a quantified Boolean formula (QBF) and solves it using a
general-purpose QBFsolver. Currentconformantplannersarefiveordersofmagnitudefaster
than CGP. The winner of the 2006 conformant-planning track at the International Planning
Competition wasT (PalaciosandGeffner, 2007), whichusesheuristic search inbelief-state
0
space while keeping the belief-state representation simple by defining derived literals that
coverconditional effects. BryceandKambhampati(2007)discusshowaplanning graphcan
begeneralized togenerategoodheuristics forconformant andcontingent planning.
There has been some confusion in the literature between the terms “conditional” and
“contingent” planning. Following Majercik and Littman (2003), we use “conditional” to
mean a plan (or action) that has different effects depending on the actual state of the world,
and “contingent” to mean a plan in which the agent can choose different actions depending
on the results of sensing. The problem of contingent planning received more attention after
thepublication ofDrewMcDermott’s(1978a)influential article, PlanningandActing.
The contingent-planning approach described in the chapter is based on Hoffmann and
Brafman (2005), and was influenced by the efficient search algorithms for cyclic AND–OR
graphs developed by Jimenez and Torras (2000) and Hansen and Zilberstein (2001). Bertoli
et al. (2001b) describe MBP (Model-Based Planner), which uses binary decision diagrams
todoconformant andcontingent planning.
Inretrospect, itisnow possible to see how the majorclassical planning algorithms led
toextendedversionsforuncertaindomains. Fast-forwardheuristicsearchthroughstatespace
led to forward search in belief space (Bonet and Geffner, 2000; Hoffmann and Brafman,
2005); SATPLAN ledtostochastic SATPLAN (MajercikandLittman,2003)andtoplanning
withquantified Boolean logic (Rintanen, 2007); partial order planning led to UWL (Etzioni
et al., 1992) and CNLP (Peot and Smith, 1992); GRAPHPLAN led to Sensory Graphplan or
SGP (Weldetal.,1998).
434 Chapter 11. PlanningandActingintheRealWorld
The first online planner with execution monitoring was PLANEX (Fikes et al., 1972),
which worked with the STRIPS planner to control the robot Shakey. The NASL planner
(McDermott, 1978a) treated a planning problem simply as a specification for carrying out a
complex action, so that execution and planning were completely unified. SIPE (System for
Interactive Planning and Execution monitoring) (Wilkins, 1988, 1990) was the first planner
to deal systematically with the problem of replanning. It has been used in demonstration
projects in several domains, including planning operations on the flight deck of an aircraft
carrier, job-shop scheduling foran Australian beerfactory, and planning the construction of
multistory buildings (KartamandLevitt,1990).
In the mid-1980s, pessimism about the slow run times of planning systems led to the
proposal of reflex agents called reactive planning systems (Brooks, 1986; Agre and Chap-
REACTIVEPLANNING
man, 1987). PENGI (Agre and Chapman, 1987) could play a (fully observable) video game
by using Boolean circuits combined with a “visual” representation of current goals and the
agent’sinternalstate. “Universalplans”(Schoppers,1987,1989)weredevelopedasalookup-
table method forreactive planning, but turned out to be arediscovery ofthe idea of policies
POLICY
thathadlongbeenusedinMarkovdecision processes (seeChapter17). Auniversal plan(or
a policy) contains a mapping from any state to the action that should be taken in that state.
Koenig(2001)surveysonlineplanning techniques, underthenameAgent-Centered Search.
Multiagent planning has leaped in popularity in recent years, although it does have
a long history. Konolige (1982) formalizes multiagent planning in first-order logic, while
Pednault (1986) givesa STRIPS-style description. Thenotion ofjoint intention, whichis es-
sentialifagentsaretoexecuteajointplan, comesfromwork oncommunicative acts(Cohen
and Levesque, 1990; Cohen et al., 1990). Boutilier and Brafman (2001) show how to adapt
partial-order planning to a multiactor setting. Brafman and Domshlak (2008) devise a mul-
tiactor planning algorithm whose complexity growsonly linearly withthe number ofactors,
providedthatthedegreeofcoupling(measuredpartlybythe treewidthofthegraphofinter-
actionsamongagents)isbounded. PetrikandZilberstein(2009)showthatanapproachbased
onbilinearprogramming outperforms thecover-setapproach weoutlinedinthechapter.
We have barely skimmed the surface of work on negotiation in multiagent planning.
Durfeeand Lesser(1989) discuss how tasks canbeshared out amongagents bynegotiation.
Krausetal.(1991)describe asystemforplaying Diplomacy,aboardgame requiring negoti-
ation, coalition formation, and dishonesty. Stone (2000) shows how agents can cooperate as
teammatesinthecompetitive,dynamic,partiallyobservableenvironmentofroboticsoccer. In
a later article, Stone (2003) analyzes two competitive multiagent environments—RoboCup,
a robotic soccer competition, and TAC, the auction-based Trading Agents Competition—
and finds that the computational intractability of our current theoretically well-founded ap-
proaches hasledtomanymultiagent systemsbeingdesigned byadhocmethods.
Inhishighly influential Society ofMindtheory, Marvin Minsky(1986, 2007) proposes
thathumanmindsareconstructed fromanensemble ofagents. LivnatandPippenger (2006)
provethat,fortheproblemofoptimalpath-finding,andgivenalimitationonthetotalamount
ofcomputing resources, the best architecture foranagent isanensemble ofsubagents, each
ofwhichtriestooptimizeitsownobjective,andallofwhichareinconflictwithoneanother.
Exercises 435
Theboid model on page 429 is due to Reynolds (1987), whowon an Academy Award
foritsapplication toswarmsofpenguins inBatmanReturns. TheNERO gameandthemeth-
odsforlearningstrategies aredescribed byBryantandMiikkulainen (2007).
Recent book on multiagent systems include those by Weiss (2000a), Young (2004),
Vlassis (2008), and Shoham and Leyton-Brown (2009). There is an annual conference on
autonomous agentsandmultiagent systems(AAMAS).
EXERCISES
11.1 Thegoals wehave considered sofarall ask the planner to make the world satisfy the
goal at just one time step. Not all goals can be expressed this way: you do not achieve the
goal of suspending a chandelier above the ground by throwing it in the air. More seriously,
you wouldn’t want your spacecraft life-support system to supply oxygen one day but not
the next. A maintenance goal is achieved when the agent’s plan causes a condition to hold
continuouslyfromagivenstateonward. Describehowtoextendtheformalismofthischapter
tosupport maintenance goals.
11.2 You have a number of trucks with which to deliver a set of packages. Each package
startsatsomelocationonagridmap,andhasadestinationsomewhereelse. Eachtruckisdi-
rectlycontrolled bymovingforwardandturning. Construct ahierarchy ofhigh-level actions
forthisproblem. Whatknowledge aboutthesolution doesyourhierarchyencode?
11.3 Suppose that a high-level action has exactly one implementation as a sequence of
primitive actions. Give an algorithm for computing its preconditions and effects, given the
completerefinementhierarchyandschemasfortheprimitive actions.
11.4 Suppose thatthe optimistic reachable set ofahigh-level plan isasuperset ofthegoal
set; can anything be concluded about whether the plan achieves the goal? What if the pes-
simisticreachable setdoesn’t intersect thegoalset? Explain.
11.5 Writeanalgorithmthattakesaninitialstate(specifiedbyasetofpropositionalliterals)
and a sequence of HLAs (each defined by preconditions and angelic specifications of opti-
mistic and pessimistic reachable sets) and computes optimistic and pessimistic descriptions
ofthereachable setofthesequence.
11.6 In Figure 11.2 we showed how to describe actions in a scheduling problem by using
separate fields for DURATION, USE, and CONSUME. Now suppose we wanted to combine
scheduling withnondeterministic planning, whichrequires nondeterministic andconditional
effects. Considereachofthethreefieldsandexplainifthey shouldremainseparate fields,or
iftheyshouldbecomeeffectsoftheaction. Giveanexampleforeachofthethree.
11.7 Someoftheoperationsinstandardprogramminglanguagescanbemodeledasactions
that change the state of the world. For example, the assignment operation changes the con-
tentsofamemorylocation, andtheprintoperation changes thestate oftheoutput stream. A
program consisting oftheseoperations canalsobeconsidered asaplan, whosegoalisgiven
436 Chapter 11. PlanningandActingintheRealWorld
bythespecification oftheprogram. Therefore, planning algorithms canbeusedtoconstruct
programsthatachieveagivenspecification.
a. Writeanactionschemafortheassignmentoperator(assigningthevalueofonevariable
toanother). Rememberthattheoriginal valuewillbeoverwritten!
b. Show how object creation can be used by a planner to produce a plan for exchanging
thevaluesoftwovariablesbyusingatemporaryvariable.
11.8 Suppose the Flip action always changes the truth value of variable L. Show how
to define its effects by using an action schema with conditional effects. Show that, despite
the use of conditional effects, a 1-CNF belief state representation remains in 1-CNF after a
Flip.
11.9 In the blocks world we were forced to introduce two action schemas, Move and
MoveToTable, in order to maintain the Clear predicate properly. Show how conditional
effectscanbeusedtorepresent bothofthesecaseswithasingleaction.
11.10 ConditionaleffectswereillustratedfortheSuck actioninthevacuumworld—which
squarebecomescleandependsonwhichsquaretherobotisin. Canyouthinkofanewsetof
propositional variables todefinestatesofthevacuum world, suchthatSuck hasanuncondi-
tional description? Writeout thedescriptions of Suck,Left,andRight,using yourproposi-
tions,anddemonstrate thattheysufficetodescribeallpossible statesoftheworld.
11.11 Find a suitably dirty carpet, free of obstacles, and vacuum it. Draw the path taken
by the vacuum cleaner as accurately as you can. Explain it, with reference to the forms of
planning discussed inthischapter.
11.12 To the medication problem in the previous exercise, add a Test action that has the
conditional effect CultureGrowth when Disease is true and in any case has the perceptual
effect Known(CultureGrowth). Diagram a conditional plan that solves the problem and
minimizestheuseoftheMedicate action.
12
KNOWLEDGE
REPRESENTATION
In which we show how to use first-order logic to represent the most important
aspectsoftherealworld,suchasaction, space,time,thoughts, andshopping.
The previous chapters described the technology for knowledge-based agents: the syntax,
semantics, andproof theoryofpropositional andfirst-orderlogic, andtheimplementation of
agents that use these logics. In this chapter we address the question of what content to put
intosuchanagent’sknowledge base—howtorepresent factsabouttheworld.
Section 12.1 introduces the idea of a general ontology, which organizes everything in
the world into a hierarchy of categories. Section 12.2 covers the basic categories of objects,
substances, andmeasures;Section12.3coversevents,andSection12.4discussesknowledge
about beliefs. We then return to consider the technology for reasoning with this content:
Section 12.5 discusses reasoning systems designed for efficient inference with categories,
and Section 12.6 discusses reasoning with default information. Section 12.7 brings all the
knowledgetogetherinthecontextofanInternet shopping environment.
12.1 ONTOLOGICAL ENGINEERING
In“toy”domains,thechoiceofrepresentation isnotthatimportant;manychoiceswillwork.
Complex domains such as shopping on the Internet or driving a car in traffic require more
general andflexiblerepresentations. Thischaptershowshowtocreatethese representations,
concentrating on general concepts—such as Events, Time, Physical Objects, and Beliefs—
that occur in many different domains. Representing these abstract concepts is sometimes
ONTOLOGICAL calledontological engineering.
ENGINEERING
Theprospect of representing everything in the world is daunting. Ofcourse, wewon’t
actually write a complete description of everything—that would be far too much for even a
1000-page textbook—but we will leave placeholders where new knowledge for any domain
canfitin. Forexample,wewilldefinewhatitmeanstobeaphysicalobject,andthedetailsof
differenttypesofobjects—robots,televisions,books,orwhatever—canbefilledinlater. This
isanalogoustothewaythatdesignersofanobject-oriented programmingframework(suchas
theJavaSwinggraphicalframework)definegeneralconcepts likeWindow,expectingusersto
437
438 Chapter 12. KnowledgeRepresentation
Anything
AbstractObjects GeneralizedEvents
Sets Numbers RepresentationalObjects Interval Places PhysicalObjects Processes
Categories Sentences Measurements Moments Things Stuff
Times Weights Animals Agents Solid Liquid Gas
Humans
Figure 12.1 The upperontologyof the world, showingthe topics to be coveredlater in
thechapter. Each linkindicatesthatthe lowerconceptisa specializationof the upperone.
Specializations are not necessarily disjoint; a human is both an animal and an agent, for
example.WewillseeinSection12.3.3whyphysicalobjectscomeundergeneralizedevents.
use these to define more specific concepts like SpreadsheetWindow. The general framework
of concepts is called an upper ontology because of the convention of drawing graphs with
UPPERONTOLOGY
thegeneralconceptsatthetopandthemorespecificconcepts belowthem,asinFigure12.1.
Before considering the ontology further, we should state one important caveat. We
have elected to use first-order logic to discuss the content and organization of knowledge,
althoughcertainaspectsoftherealworldarehardtocaptureinFOL.Theprincipaldifficulty
isthatmostgeneralizations haveexceptions orholdonly to adegree. Forexample, although
“tomatoes are red” is a useful rule, some tomatoes are green, yellow, or orange. Similar
exceptionscanbefoundtoalmostalltherulesinthischapter. Theabilitytohandleexceptions
and uncertainty is extremely important, but is orthogonal to the task of understanding the
generalontology. Forthisreason,wedelaythediscussionofexceptionsuntilSection12.5of
thischapter, andthemoregeneraltopicofreasoning withuncertainty untilChapter13.
Ofwhat use is an upper ontology? Consider the ontology for circuits inSection 8.4.2.
Itmakesmanysimplifyingassumptions: timeisomittedcompletely; signalsarefixedanddo
not propagate; the structure of the circuit remains constant. Amore general ontology would
consider signals at particular times, and would include the wire lengths and propagation de-
lays. This would allow us to simulate the timing properties of the circuit, and indeed such
simulations are often carried out by circuit designers. We could also introduce more inter-
esting classes of gates, for example, by describing the technology (TTL,CMOS,and so on)
aswellastheinput–output specification. Ifwewantedtodiscuss reliability ordiagnosis, we
would include the possibility that the structure of the circuit or the properties of the gates
might change spontaneously. Toaccount forstray capacitances, wewould need to represent
wherethewiresareontheboard.
Section12.1. Ontological Engineering 439
Ifwelookatthewumpusworld,similarconsiderationsapply. Althoughwedorepresent
time, ithasasimplestructure: Nothing happens except when theagent acts, andallchanges
areinstantaneous. Amoregeneral ontology, bettersuitedfortherealworld,wouldallowfor
simultaneouschangesextendedovertime. WealsousedaPit predicatetosaywhichsquares
have pits. We could have allowed for different kinds of pits by having several individuals
belonging to the class of pits, each having different properties. Similarly, we might want to
allow for other animals besides wumpuses. It might not be possible to pin down the exact
species from the available percepts, so we would need to build up a biological taxonomy to
helptheagentpredictthebehaviorofcave-dwellers fromscantyclues.
For any special-purpose ontology, it is possible to make changes like these to move
toward greater generality. Anobvious question then arises: doall these ontologies converge
on a general-purpose ontology? After centuries of philosophical and computational inves-
tigation, the answer is “Maybe.” In this section, we present one general-purpose ontology
that synthesizes ideas from those centuries. Two major characteristics of general-purpose
ontologies distinguish themfromcollections ofspecial-purpose ontologies:
• A general-purpose ontology should be applicable in more or less any special-purpose
domain (with the addition of domain-specific axioms). This means that no representa-
tionalissuecanbefinessedorbrushed underthecarpet.
• In any sufficiently demanding domain, different areas of knowledge must be unified,
because reasoning and problem solving could involve several areas simultaneously. A
robotcircuit-repairsystem,forinstance,needstoreason aboutcircuitsintermsofelec-
trical connectivity and physical layout, and about time, both forcircuit timing analysis
and estimating labor costs. The sentences describing time therefore must be capable
ofbeing combinedwiththosedescribing spatial layoutandmustworkequally wellfor
nanoseconds andminutesandforangstroms andmeters.
We should say up front that the enterprise of general ontological engineering has so far had
only limited success. None of the top AI applications (as listed in Chapter 1) make use
of a shared ontology—they all use special-purpose knowledge engineering. Social/political
considerations can make it difficult for competing parties to agree on an ontology. As Tom
Gruber (2004) says, “Every ontology is a treaty—a social agreement—among people with
some common motive in sharing.” When competing concerns outweigh the motivation for
sharing, therecanbenocommonontology. Thoseontologies thatdoexisthavebeencreated
alongfourroutes:
1. Byateamoftrainedontologist/logicians, whoarchitect theontologyandwriteaxioms.
TheCYC systemwasmostlybuiltthisway(LenatandGuha,1990).
2. Byimporting categories, attributes, and values from an existing database ordatabases.
DBPEDIA wasbuiltbyimportingstructured factsfromWikipedia (Bizeretal.,2007).
3. Byparsing text documents and extracting information from them. TEXTRUNNER was
builtbyreadingalargecorpusofWebpages(BankoandEtzioni,2008).
4. By enticing unskilled amateurs to enter commonsense knowledge. The OPENMIND
system was built by volunteers who proposed facts in English (Singh et al., 2002;
ChklovskiandGil,2005).
440 Chapter 12. KnowledgeRepresentation
12.2 CATEGORIES AND OBJECTS
The organization of objects into categories is a vital part of knowledge representation. Al-
CATEGORY
thoughinteraction withtheworldtakesplaceatthelevelof individual objects, muchreason-
ing takes place at the level of categories. For example, a shopper would normally have the
goalofbuying abasketball, ratherthanaparticular basketball suchasBB . Categoriesalso
9
serve to make predictions about objects once they are classified. One infers the presence of
certainobjectsfromperceptualinput,inferscategorymembershipfromtheperceivedproper-
tiesoftheobjects, andthenusescategory information tomakepredictions abouttheobjects.
For example, from its green and yellow mottled skin, one-foot diameter, ovoid shape, red
flesh,blackseeds,andpresenceinthefruitaisle,onecaninferthatanobjectisawatermelon;
fromthis,oneinfersthatitwouldbeusefulforfruitsalad.
There are two choices for representing categories in first-order logic: predicates and
objects. That is, we can use the predicate Basketball(b), or we can reify1 the category as
REIFICATION
an object, Basketballs. We could then say Member(b,Basketballs), which we will abbre-
viate as b∈Basketballs, to say that b is a member of the category of basketballs. We say
Subset(Basketballs,Balls), abbreviated as Basketballs ⊂ Balls, tosay that Basketballs is
asubcategoryofBalls. Wewillusesubcategory, subclass, andsubsetinterchangeably.
SUBCATEGORY
Categories serve to organize and simplify the knowledge base through inheritance. If
INHERITANCE
we say that all instances of the category Food are edible, and if we assert that Fruit is a
subclass of Food and Apples is a subclass of Fruit, then we can infer that every apple is
edible. We say that the individual apples inherit the property of edibility, in this case from
theirmembership inthe Food category.
Subclassrelationsorganizecategoriesintoataxonomy,ortaxonomichierarchy. Tax-
TAXONOMY
onomieshavebeenusedexplicitlyforcenturiesintechnicalfields. Thelargestsuchtaxonomy
organizesabout10millionlivingandextinctspecies,manyofthembeetles,2 intoasinglehi-
erarchy;librarysciencehasdeveloped ataxonomyofallfieldsofknowledge, encoded asthe
Dewey Decimal system; and tax authorities and other government departments have devel-
opedextensivetaxonomiesofoccupationsandcommercialproducts. Taxonomiesarealsoan
importantaspectofgeneralcommonsense knowledge.
First-order logic makes it easy to state facts about categories, either by relating ob-
jects to categories orby quantifying overtheir members. Here are some types of facts, with
examplesofeach:
• Anobjectisamemberofacategory.
BB ∈Basketballs
9
• Acategoryisasubclassofanothercategory.
Basketballs ⊂ Balls
• Allmembersofacategoryhavesomeproperties.
(x∈Basketballs) ⇒ Spherical(x)
1 Turningapropositionintoanobjectiscalledreification,fromtheLatinwordres,orthing. JohnMcCarthy
proposedtheterm“thingification,”butitnevercaughton.
2 ThefamousbiologistJ.B.S.Haldanededuced“Aninordinatefondnessforbeetles”onthepartoftheCreator.
Section12.2. CategoriesandObjects 441
• Membersofacategory canberecognized bysomeproperties.
Orange(x)∧Round(x)∧Diameter(x)=9.5 (cid:2)(cid:2)∧x∈Balls ⇒ x∈Basketballs
• Acategoryasawholehassomeproperties.
Dogs∈DomesticatedSpecies
Notice that because Dogs is acategory and is amemberof DomesticatedSpecies, the latter
must be a category of categories. Of course there are exceptions to many of the above rules
(punctured basketballs arenotspherical); wedealwiththeseexceptions later.
Although subclass and member relations are the most important ones for categories,
we also want to be able to state relations between categories that are not subclasses of each
other. Forexample, if wejust say that Males and Females are subclasses of Animals, then
we have not said that a male cannot be a female. We say that two or more categories are
disjoint if they have no members in common. Andeven if weknow that males and females
DISJOINT
are disjoint, we will not know that an animal that is not a male must be a female, unless
EXHAUSTIVE we say that males and females constitute an exhaustive decomposition of the animals. A
DECOMPOSITION
disjointexhaustivedecompositionisknownasapartition. Thefollowingexamplesillustrate
PARTITION
thesethreeconcepts:
Disjoint({Animals,Vegetables})
ExhaustiveDecomposition({Americans,Canadians,Mexicans},
NorthAmericans)
Partition({Males,Females},Animals).
(Note that the ExhaustiveDecomposition of NorthAmericans is not a Partition, because
somepeoplehavedualcitizenship.) Thethreepredicates aredefinedasfollows:
Disjoint(s) ⇔ (∀c ,c c ∈s∧c ∈s∧c (cid:7)= c ⇒ Intersection(c ,c )={ })
1 2 1 2 1 2 1 2
ExhaustiveDecomposition(s,c) ⇔ (∀i i∈c ⇔ ∃c c ∈s∧i∈c )
2 2 2
Partition(s,c) ⇔ Disjoint(s)∧ExhaustiveDecomposition(s,c).
Categories can also be defined by providing necessary and sufficient conditions for
membership. Forexample,abachelorisanunmarriedadultmale:
x∈Bachelors ⇔ Unmarried(x)∧x∈Adults ∧x∈Males .
Aswediscuss in thesidebar onnatural kinds onpage 443, strict logical definitions forcate-
goriesareneitheralwayspossible noralwaysnecessary.
12.2.1 Physicalcomposition
The idea that one object can be part of another is a familiar one. One’s nose is part of one’s
head, Romania is part of Europe, and this chapter is part of this book. We use the general
PartOf relationtosaythatonethingispartofanother. Objectscan begroupedintoPartOf
hierarchies, reminiscent ofthe Subset hierarchy:
PartOf(Bucharest,Romania)
PartOf(Romania,EasternEurope)
PartOf(EasternEurope,Europe)
PartOf(Europe,Earth).
442 Chapter 12. KnowledgeRepresentation
ThePartOf relation istransitive andreflexive;thatis,
PartOf(x,y)∧PartOf(y,z) ⇒ PartOf(x,z).
PartOf(x,x).
Therefore, wecanconclude PartOf(Bucharest,Earth).
Categories of composite objects are often characterized by structural relations among
COMPOSITEOBJECT
parts. Forexample,abipedhastwolegsattached toabody:
Biped(a) ⇒ ∃l ,l ,b Leg(l )∧Leg(l )∧Body(b) ∧
1 2 1 2
PartOf(l ,a)∧PartOf(l ,a)∧PartOf(b,a) ∧
1 2
Attached(l ,b)∧Attached(l ,b) ∧
1 2
l (cid:7)=l ∧[∀l Leg(l )∧PartOf(l ,a) ⇒ (l =l ∨l =l )].
1 2 3 3 3 3 1 3 2
The notation for “exactly two” is a little awkward; we are forced to say that there are two
legs, that they are not the same, and that if anyone proposes a third leg, it must be the same
as one of the other two. In Section 12.5.2, we describe a formalism called description logic
makesiteasiertorepresentconstraints like“exactly two.”
We can define a PartPartition relation analogous to the Partition relation for cate-
gories. (SeeExercise12.8.) AnobjectiscomposedofthepartsinitsPartPartition andcan
beviewedasderiving someproperties fromthoseparts. Forexample,themassofacompos-
iteobjectisthesumofthemassesoftheparts. Noticethatthisisnotthecasewithcategories,
whichhavenomass,eventhoughtheirelementsmight.
It is also useful to define composite objects with definite parts but no particular struc-
ture. For example, we might want to say “The apples in this bag weigh two pounds.” The
temptation would be to ascribe this weight to the set of apples in the bag, but this would be
amistake because thesetisan abstract mathematical concept thathas elements but does not
have weight. Instead, we need a new concept, which we will call a bunch. Forexample, if
BUNCH
theapplesareApple ,Apple ,andApple ,then
1 2 3
BunchOf({Apple ,Apple ,Apple })
1 2 3
denotesthecompositeobjectwiththethreeapplesasparts(notelements). Wecanthenusethe
bunchasanormal,albeitunstructured,object. NoticethatBunchOf({x})=x. Furthermore,
BunchOf(Apples)isthecompositeobjectconsisting ofallapples—not tobeconfusedwith
Apples,thecategory orsetofallapples.
Wecan define BunchOf in terms of the PartOf relation. Obviously, each element of
sispartofBunchOf(s):
∀x x∈s ⇒ PartOf(x,BunchOf(s)).
Furthermore, BunchOf(s) is the smallest object satisfying this condition. In other words,
BunchOf(s)mustbepartofanyobjectthathasalltheelementsofsasparts:
∀y [∀x x∈s ⇒ PartOf(x,y)] ⇒ PartOf(BunchOf(s),y).
LOGICAL These axioms are an example of a general technique called logical minimization, which
MINIMIZATION
meansdefininganobjectasthesmallestonesatisfying certain conditions.
Section12.2. CategoriesandObjects 443
NATURAL KINDS
Some categories have strict definitions: an object is a triangle if and only if it is
a polygon with three sides. On the other hand, most categories in the real world
havenoclear-cutdefinition;thesearecallednaturalkindcategories. Forexample,
tomatoestendtobeadullscarlet; roughly spherical; withanindentation atthetop
where the stem was; about two to four inches in diameter; with a thin but tough
skin; and with flesh, seeds, and juice inside. There is, however, variation: some
tomatoes are yellow or orange, unripe tomatoes are green, some are smaller or
larger than average, and cherry tomatoes are uniformly small. Rather than having
acomplete definition of tomatoes, wehave aset offeatures that serves toidentify
objects that are clearly typical tomatoes, but might not be able to decide forother
objects. (Couldtherebeatomatothatisfuzzylikeapeach?)
This poses a problem for a logical agent. The agent cannot be sure that an
object it has perceived is a tomato, and even if it were sure, it could not be cer-
tain which of the properties of typical tomatoes this one has. This problem is an
inevitable consequence ofoperating inpartially observable environments.
One useful approach is to separate what is true of all instances of a cate-
gory from what is true only of typical instances. So in addition to the category
Tomatoes,wewillalsohavethecategoryTypical(Tomatoes). Here,theTypical
functionmapsacategory tothesubclass thatcontainsonlytypicalinstances:
Typical(c) ⊆ c.
Mostknowledge aboutnatural kindswillactuallybeabouttheirtypicalinstances:
x∈Typical(Tomatoes) ⇒ Red(x)∧Round(x).
Thus, we can write down useful facts about categories without exact defini-
tions. Thedifficulty ofproviding exact definitions formost natural categories was
explainedindepthbyWittgenstein (1953). Heusedtheexampleofgamestoshow
that members of a category shared “family resemblances” rather than necessary
and sufficient characteristics: what strict definition encompasses chess, tag, soli-
taire,anddodgeball?
The utility of the notion of strict definition was also challenged by
Quine (1953). He pointed out that even the definition of “bachelor” as an un-
married adult male is suspect; one might, for example, question a statement such
as “the Pope is a bachelor.” While not strictly false, this usage is certainly infe-
licitous because it induces unintended inferences on the part of the listener. The
tension could perhaps be resolved by distinguishing between logical definitions
suitable for internal knowledge representation and the more nuanced criteria for
felicitous linguistic usage. Thelattermaybeachieved by“filtering” theassertions
derivedfromtheformer. Itisalsopossiblethatfailuresof linguistic usageserveas
feedbackformodifyinginternaldefinitions,sothatfilteringbecomesunnecessary.
444 Chapter 12. KnowledgeRepresentation
12.2.2 Measurements
In both scientific and commonsense theories of the world, objects have height, mass, cost,
and so on. The values that we assign for these properties are called measures. Ordi-
MEASURE
nary quantitative measures are quite easy to represent. We imagine that the universe in-
cludes abstract “measure objects,” such as the length that is the length of this line seg-
ment: . Wecancallthislength1.5inchesor3.81centimeters. Thus,
the same length has different names in our language.We represent the length with a units
function that takes a number as argument. (An alternative scheme is explored in Exer-
UNITSFUNCTION
cise12.9.) IfthelinesegmentiscalledL ,wecanwrite
1
Length(L )=Inches(1.5)=Centimeters(3.81).
1
Conversion betweenunitsisdonebyequatingmultiples ofoneunittoanother:
Centimeters(2.54×d)=Inches(d).
Similaraxioms can be written for pounds and kilograms, seconds and days, and dollars and
cents. Measurescanbeusedtodescribe objectsasfollows:
Diameter(Basketball )=Inches(9.5).
12
ListPrice(Basketball )=$(19).
12
d∈Days ⇒ Duration(d)=Hours(24).
Notethat$(1) isnotadollarbill! Onecanhavetwodollarbills, butthere isonly one object
named $(1). Note also that, while Inches(0) and Centimeters(0) refer to the same zero
length, theyarenotidentical tootherzeromeasures, suchasSeconds(0).
Simple, quantitative measures areeasy torepresent. Other measures present moreof a
problem,becausetheyhavenoagreedscaleofvalues. Exerciseshavedifficulty,dessertshave
deliciousness,andpoemshavebeauty,yetnumberscannotbeassignedtothesequalities. One
might,inamomentofpureaccountancy,dismisssuchpropertiesasuselessforthepurposeof
logical reasoning; or, stillworse, attempttoimpose anumerical scaleonbeauty. Thiswould
be agrave mistake, because it isunnecessary. Themost important aspect of measures is not
theparticularnumericalvalues, butthefactthatmeasures canbeordered.
Although measures are not numbers, we can still compare them, using an ordering
symbol such as >. For example, we might well believe that Norvig’s exercises are tougher
thanRussell’s, andthatonescoreslessontougherexercises:
e ∈Exercises ∧e ∈Exercises ∧Wrote(Norvig,e )∧Wrote(Russell,e ) ⇒
1 2 1 2
Difficulty(e )> Difficulty(e ).
1 2
e ∈Exercises ∧e ∈Exercises ∧Difficulty(e ) > Difficulty(e ) ⇒
1 2 1 2
ExpectedScore(e )< ExpectedScore(e ).
1 2
Thisisenoughtoallowonetodecidewhichexercisestodo,eventhoughnonumericalvalues
for difficulty were ever used. (One does, however, have to discover who wrote which exer-
cises.) Thesesortsofmonotonicrelationships amongmeasuresformthebasisforthefieldof
qualitative physics, a subfield of AI that investigates how to reason about physical systems
without plunging into detailed equations and numerical simulations. Qualitative physics is
discussed inthehistorical notessection.
Section12.2. CategoriesandObjects 445
12.2.3 Objects: Things andstuff
The real world can be seen as consisting of primitive objects (e.g., atomic particles) and
composite objects built from them. Byreasoning at the level of large objects such as apples
andcars,wecanovercomethecomplexityinvolvedindealingwithvastnumbersofprimitive
objectsindividually. Thereis,however,asignificantportionofrealitythatseemstodefyany
obviousindividuation—division intodistinctobjects. Wegivethisportionthegenericname
INDIVIDUATION
stuff. For example, suppose I have some butter and an aardvark in front of me. I can say
STUFF
thereisoneaardvark, butthereisnoobviousnumberof“butter-objects,” becauseanypartof
abutter-object isalso abutter-object, atleast until wegettovery smallparts indeed. Thisis
the major distinction between stuff and things. If we cut an aardvark in half, we do not get
twoaardvarks (unfortunately).
TheEnglish language distinguishes clearly between stuff andthings. Wesay “anaard-
vark,” but, except in pretentious California restaurants, one cannot say “a butter.” Linguists
distinguish between countnouns,such asaardvarks, holes, and theorems, and mass nouns,
COUNTNOUNS
such as butter, water, and energy. Several competing ontologies claim tohandle this distinc-
MASSNOUN
tion. Herewedescribe justone;theothersarecoveredinthe historical notessection.
To represent stuff properly, we begin with the obvious. We need to have as objects in
our ontology at least the gross “lumps” of stuff we interact with. For example, we might
recognize a lump of butter as the one left on the table the night before; wemight pick it up,
weigh it, sell it, or whatever. In these senses, it is an object just like the aardvark. Let us
callitButter . WealsodefinethecategoryButter. Informally,itselementswillbeallthose
3
thingsofwhichonemightsay“It’sbutter,”includingButter . Withsomecaveatsaboutvery
3
smallpartsthatwewomitfornow,anypartofabutter-object isalsoabutter-object:
b∈Butter ∧PartOf(p,b) ⇒ p∈Butter .
Wecannowsaythatbuttermeltsataround30degreescentigrade:
b∈Butter ⇒ MeltingPoint(b,Centigrade(30)).
Wecould goontosaythatbutterisyellow,islessdensethanwater, issoftatroomtempera-
ture,hasahighfatcontent,andsoon. Ontheotherhand,butterhasnoparticularsize,shape,
or weight. We can define more specialized categories of butter such as UnsaltedButter,
which is also a kind of stuff. Note that the category PoundOfButter, which includes as
members all butter-objects weighing one pound, is not a kind of stuff. If we cut a pound of
butterinhalf,wedonot,alas,gettwopoundsofbutter.
Whatisactuallygoingonisthis: somepropertiesare intrinsic: theybelongtothevery
INTRINSIC
substance of the object, rather than to the object as a whole. When you cut an instance of
stuff in half, the twopieces retain the intrinsic properties—things like density, boiling point,
flavor, color, ownership, and so on. On the other hand, their extrinsic properties—weight,
EXTRINSIC
length, shape, and so on—are not retained under subdivision. A category of objects that
includes in its definition only intrinsic properties is then a substance, or mass noun; a class
that includes any extrinsic properties in its definition is acount noun. Thecategory Stuff is
the mostgeneral substance category, specifying nointrinsic properties. Thecategory Thing
isthemostgeneraldiscreteobjectcategory, specifying no extrinsicproperties.
446 Chapter 12. KnowledgeRepresentation
12.3 EVENTS
In Section 10.4.2, we showed how situation calculus represents actions and their effects.
Situationcalculus islimitedinitsapplicability: itwasdesigned todescribe aworldinwhich
actions are discrete, instantaneous, and happen one at a time. Consider a continuous action,
suchasfillingabathtub. Situationcalculuscansaythatthetubisemptybeforetheactionand
full when the action is done, but it can’t talk about what happens during the action. It also
can’t describe two actions happening at the same time—such as brushing one’s teeth while
waitingforthetubtofill. Tohandlesuchcasesweintroduce analternativeformalismknown
aseventcalculus,whichisbasedonpointsoftimeratherthanonsituations.3
EVENTCALCULUS
Event calculus reifies fluents and events. The fluent At(Shankar,Berkeley) is an ob-
ject that refers to the fact of Shankar being in Berkeley, but does not by itself say anything
about whether it is true. To assert that a fluent is actually true at some point in time we use
thepredicate T,asinT(At(Shankar,Berkeley),t).
Eventsaredescribed asinstances ofeventcategories.4 TheeventE ofShankarflying
1
fromSanFranciscotoWashington, D.C.isdescribed as
E ∈ Flyings∧Flyer(E ,Shankar)∧Origin(E ,SF)∧Destination(E ,DC).
1 1 1 1
If this is too verbose, we can define an alternative three-argument version of the category of
flyingeventsandsay
E ∈ Flyings(Shankar,SF,DC).
1
WethenuseHappens(E ,i)tosaythattheeventE tookplaceoverthetimeinterval i,and
1 1
we say the same thing in functional form with Extent(E )=i. We represent time intervals
1
bya(start,end)pairoftimes;thatis, i = (t ,t )isthetimeintervalthatstartsatt andends
1 2 1
att . Thecompletesetofpredicates foroneversionoftheeventcalculus is
2
T(f,t) Fluentf istrueattimet
Happens(e,i) Eventehappens overthetimeinterval i
Initiates(e,f,t) Eventecausesfluentf tostarttoholdattimet
Terminates(e,f,t) Eventecausesfluentf toceasetoholdattimet
Clipped(f,i) Fluentf ceasestobetrueatsomepointduringtimeinterval i
Restored(f,i) Fluentf becomestruesometimeduringtimeinterval i
Weassumeadistinguishedevent,Start,thatdescribestheinitialstatebysayingwhichfluents
areinitiatedorterminatedatthestarttime. WedefineT bysayingthatafluentholdsatapoint
intimeifthefluentwasinitiatedbyaneventatsometimeinthepastandwasnotmadefalse
(clipped)byaninterveningevent. Afluentdoesnotholdifitwasterminatedbyaneventand
3 Theterms“event”and“action”maybeusedinterchangeably. Informally,“action”connotesanagentwhile
“event”connotesthepossibilityofagentlessactions.
4 Someversionsofeventcalculusdonotdistinguisheventcategoriesfrominstancesofthecategories.
Section12.3. Events 447
notmadetrue(restored) byanotherevent. Formally,theaxiomsare:
Happens(e,(t ,t ))∧Initiates(e,f,t )∧¬Clipped(f,(t ,t))∧t < t ⇒
1 2 1 1 1
T(f,t)
Happens(e,(t ,t ))∧Terminates(e,f,t )∧¬Restored(f,(t ,t))∧t < t ⇒
1 2 1 1 1
¬T(f,t)
whereClipped andRestored aredefinedby
Clipped(f,(t ,t )) ⇔
1 2
∃e,t,t Happens(e,(t,t ))∧t ≤t < t ∧Terminates(e,f,t)
3 3 1 2
Restored(f,(t ,t )) ⇔
1 2
∃e,t,t Happens(e,(t,t ))∧t ≤t < t ∧Initiates(e,f,t)
3 3 1 2
Itisconvenient toextendT toworkoverintervals aswellastimepoints; afluentholds over
anintervalifitholdsoneverypointwithintheinterval:
T(f,(t ,t )) ⇔ [∀t (t ≤ t < t ) ⇒ T(f,t)]
1 2 1 2
Fluents and actions are defined with domain-specific axioms that are similar to successor-
state axioms. For example, we can say that the only way a wumpus-world agent gets an
arrowisatthestart,andtheonlywaytouseupanarrowistoshootit:
Initiates(e,HaveArrow(a),t) ⇔ e= Start
Terminates(e,HaveArrow(a),t) ⇔ e ∈ Shootings(a)
By reifying events we make it possible to add any amount of arbitrary information about
them. For example, we can say that Shankar’s flight was bumpy with Bumpy(E ). In an
1
ontology whereevents are n-arypredicates, there would benowaytoadd extra information
likethis;movingtoann+1-arypredicate isn’tascalablesolution.
Wecanextendeventcalculustomakeitpossibletorepresentsimultaneousevents(such
astwopeoplebeingnecessarytorideaseesaw),exogenousevents(suchasthewindblowing
and changing the location of an object), continuous events (such as the level of water in the
bathtubcontinuously rising)andothercomplications.
12.3.1 Processes
Theeventswehaveseensofararewhatwecall discrete events—theyhave adefinitestruc-
DISCRETEEVENTS
ture. Shankar’striphasabeginning, middle,andend. Ifinterruptedhalfway,theeventwould
besomethingdifferent—itwouldnotbeatripfromSanFranciscotoWashington,butinstead
a trip from San Francisco to somewhere over Kansas. On the other hand, the category of
events denoted by Flyings has a different quality. If we take a small interval of Shankar’s
flight, say, the third 20-minute segment (while he waitsanxiously forabag of peanuts), that
eventisstillamemberofFlyings. Infact,thisistrueforanysubinterval.
Categories of events with this property are called process categories or liquid event
PROCESS
categories. Anyprocess ethathappens overanintervalalsohappensoveranysubinterval:
LIQUIDEVENT
(e∈Processes)∧Happens(e,(t ,t ))∧(t <t <t <t ) ⇒ Happens(e,(t ,t )).
1 4 1 2 3 4 2 3
The distinction between liquid and nonliquid events is exactly analogous to the difference
between substances, or stuff, and individual objects, or things. In fact, some have called
TEMPORAL liquideventstemporalsubstances,whereassubstances likebutterare spatialsubstances.
SUBSTANCE
SPATIALSUBSTANCE
448 Chapter 12. KnowledgeRepresentation
12.3.2 Timeintervals
Event calculus opens us up to the possibility of talking about time, and time intervals. We
willconsidertwokindsoftimeintervals: momentsandextendedintervals. Thedistinction is
thatonlymomentshavezeroduration:
Partition({Moments,ExtendedIntervals},Intervals)
i∈Moments ⇔ Duration(i)=Seconds(0).
Next we invent a time scale and associate points on that scale with moments, giving us ab-
solute times. The time scale is arbitrary; we measure it in seconds and say that the moment
at midnight (GMT) on January 1, 1900, has time 0. The functions Begin and End pick out
theearliestandlatestmomentsinaninterval,andthefunctionTime deliversthepointonthe
time scale for a moment. The function Duration gives the difference between the end time
andthestarttime.
Interval(i) ⇒ Duration(i)=(Time(End(i))−Time(Begin(i))).
Time(Begin(AD1900))=Seconds(0).
Time(Begin(AD2001))=Seconds(3187324800).
Time(End(AD2001))=Seconds(3218860800).
Duration(AD2001)=Seconds(31536000).
To make these numbers easier to read, we also introduce a function Date, which takes six
arguments (hours, minutes,seconds, day,month,andyear)andreturnsatimepoint:
Time(Begin(AD2001))=Date(0,0,0,1,Jan,2001)
Date(0,20,21,24,1,1995)=Seconds(3000000000).
Twointervals Meet if the end timeof the firstequals the start timeof the second. Thecom-
pletesetofintervalrelations,asproposedbyAllen(1983),isshowngraphicallyinFigure12.2
andlogically below:
Meet(i,j) ⇔ End(i)=Begin(j)
Before(i,j) ⇔ End(i) < Begin(j)
After(j,i) ⇔ Before(i,j)
During(i,j) ⇔ Begin(j) < Begin(i) < End(i) < End(j)
Overlap(i,j) ⇔ Begin(i) < Begin(j) < End(i) < End(j)
Begins(i,j) ⇔ Begin(i) = Begin(j)
Finishes(i,j) ⇔ End(i) = End(j)
Equals(i,j) ⇔ Begin(i) = Begin(j)∧End(i) = End(j)
These all have their intuitive meaning, with the exception of Overlap: we tend to think of
overlap as symmetric (if i overlaps j then j overlaps i), but in this definition, Overlap(i,j)
onlyholdsifibeginsbeforej. TosaythatthereignofElizabethIIimmediatelyfollowedthat
ofGeorgeVI,andthereignofElvisoverlapped withthe1950s, wecanwritethefollowing:
Meets(ReignOf(GeorgeVI),ReignOf(ElizabethII)).
Overlap(Fifties,ReignOf(Elvis)).
Begin(Fifties)=Begin(AD1950).
End(Fifties)=End(AD1959).
Section12.3. Events 449
Figure12.2 Predicatesontimeintervals.
1801
1797 time
1789
W a sh in g to n
A d a m s Je ffe rso n
Figure12.3 AschematicviewoftheobjectPresident(USA)forthefirst15yearsofits
existence.
12.3.3 Fluents and objects
Physical objects can be viewed as generalized events, in the sense that a physical object is
a chunk of space–time. For example, USA can be thought of as an event that began in,
say, 1776 as a union of 13 states and is still in progress today as a union of 50. We can
describe the changing properties of USA using state fluents, such as Population(USA). A
property oftheUSAthatchanges everyfouroreightyears, barring mishaps, isitspresident.
One might propose that President(USA) is a logical term that denotes a different object
at different times. Unfortunately, this is not possible, because a term denotes exactly one
objectinagivenmodelstructure. (Theterm President(USA,t)candenotedifferentobjects,
depending onthevalueoft,butourontology keeps timeindices separate from fluents.) The
450 Chapter 12. KnowledgeRepresentation
only possibility is that President(USA) denotes a single object that consists of different
people atdifferent times. Itistheobject thatisGeorgeWashington from1789to1797, John
Adamsfrom1797to1801,andsoon,asinFigure12.3. TosaythatGeorgeWashingtonwas
president throughout 1790,wecanwrite
T(Equals(President(USA),GeorgeWashington),AD1790).
We use the function symbol Equals rather than the standard logical predicate =, because
we cannot have a predicate as an argument to T, and because the interpretation is not that
GeorgeWashington and President(USA)are logically identical in1790; logical identity is
notsomethingthatcanchangeovertime. Theidentityisbetweenthesubeventsofeachobject
thataredefinedbytheperiod1790.
12.4 MENTAL EVENTS AND MENTAL OBJECTS
The agents we have constructed so far have beliefs and can deduce new beliefs. Yet none
of them has any knowledge about beliefs or about deduction. Knowledge about one’s own
knowledgeandreasoningprocessesisusefulforcontrollinginference. Forexample,suppose
Aliceasks “whatisthesquare root of1764” and Bobreplies “I don’t know.” IfAlice insists
“think harder,” Bob should realize that with some more thought, this question can in fact
be answered. On the other hand, if the question were “Is your mother sitting down right
now?” then Bob should realize that thinking harder is unlikely to help. Knowledge about
the knowledge of other agents is also important; Bob should realize that his mother knows
whethersheissittingornot,andthataskingherwouldbeawaytofindout.
What we need is a model of the mental objects that are in someone’s head (or some-
thing’s knowledge base) and of the mental processes that manipulate those mental objects.
The model does not have to be detailed. We do not have to be able to predict how many
milliseconds itwilltake foraparticular agent tomakeadeduction. Wewillbehappy justto
beabletoconclude thatmotherknowswhetherornotsheissitting.
PROPOSITIONAL We begin with the propositional attitudes that an agent can have toward mental ob-
ATTITUDE
jects: attitudes such as Believes, Knows, Wants, Intends, and Informs. The difficulty is
that these attitudes do not behave like “normal” predicates. Forexample, suppose we try to
assertthatLoisknowsthatSupermancanfly:
Knows(Lois,CanFly(Superman)).
OneminorissuewiththisisthatwenormallythinkofCanFly(Superman)asasentence,but
hereitappearsasaterm. ThatissuecanbepatchedupjustbereifyingCanFly(Superman);
making it a fluent. A more serious problem isthat, ifit is true that Superman is Clark Kent,
thenwemustconcludethatLoisknowsthatClarkcanfly:
(Superman = Clark)∧Knows(Lois,CanFly(Superman))
|= Knows(Lois,CanFly(Clark)).
This is a consequence of the fact that equality reasoning is built into logic. Normally that is
agood thing; ifouragent knowsthat 2+2 = 4and4 < 5, then wewantouragent toknow
Section12.4. MentalEventsandMentalObjects 451
REFERENTIAL that 2 + 2 < 5. This property is called referential transparency—it doesn’t matter what
TRANSPARENCY
termalogicusestorefertoanobject,whatmattersistheobjectthatthetermnames. Butfor
propositionalattitudeslikebelievesandknows,wewouldliketohavereferentialopacity—the
termsuseddomatter,becausenotallagentsknowwhichtermsareco-referential.
Modallogicisdesignedtoaddressthisproblem. Regularlogicisconcernedwithasin-
MODALLOGIC
gle modality, the modality of truth, allowing us to express “P is true.” Modal logic includes
special modal operators that take sentences (rather than terms) as arguments. For example,
“AknowsP”isrepresentedwiththenotationK P,whereKisthemodaloperatorforknowl-
A
edge. Ittakes twoarguments, anagent (written asthe subscript) and asentence. The syntax
ofmodallogicisthesameasfirst-orderlogic, except thatsentences canalsobeformedwith
modaloperators.
The semantics of modal logic is more complicated. In first-order logic a model con-
tains a set of objects and an interpretation that maps each name to the appropriate object,
relation, orfunction. In modal logic wewant to be able to consider both the possibility that
Superman’s secret identity is Clark and that it isn’t. Therefore, we will need a more com-
plicated model, one that consists of a collection of possible worlds rather than just one true
POSSIBLEWORLD
ACCESSIBILITY world. Theworlds areconnected inagraph by accessibility relations, one relation foreach
RELATIONS
modaloperator. Wesaythatworld w isaccessible fromworld w withrespect tothemodal
1 0
operator K if everything in w is consistent with what A knows in w , and we write this
A 1 0
asAcc(K ,w ,w ). Indiagrams such asFigure 12.4 weshow accessibility asan arrow be-
A 0 1
tweenpossibleworlds. Asanexample,intherealworld,BucharestisthecapitalofRomania,
but for an agent that did not know that, other possible worlds are accessible, including ones
wherethecapital ofRomaniaisSibiuorSofia. Presumably aworldwhere 2+2 = 5would
notbeaccessible toanyagent.
Ingeneral, a knowledge atom K P is true in world w if and only if P istrue in every
A
worldaccessible from w. Thetruthofmorecomplexsentences isderivedbyrecursive appli-
cation ofthis rule and the normal rules offirst-order logic. Thatmeans that modal logic can
be used to reason about nested knowledge sentences: what one agent knows about another
agent’s knowledge. For example, we can say that, even though Lois doesn’t know whether
Superman’ssecretidentityisClarkKent,shedoesknowthatClarkknows:
K [K Identity(Superman,Clark)∨K ¬Identity(Superman,Clark)]
Lois Clark Clark
Figure12.4showssomepossibleworldsforthisdomain,withaccessibility relationsforLois
andSuperman.
IntheTOP-LEFT diagram,itiscommonknowledgethatSupermanknowshisowniden-
tity, and neither henorLoishasseen the weatherreport. Soinw theworlds w and w are
0 0 2
accessible toSuperman;mayberainispredicted, maybenot. ForLoisallfourworldsareac-
cessiblefromeachother;shedoesn’tknowanythingaboutthereportorifClarkisSuperman.
ButshedoesknowthatSuperman knowswhetherheisClark, because ineveryworldthatis
accessible to Lois, eitherSuperman knows I, orhe knows ¬I. Loisdoes not know which is
thecase,buteitherwaysheknowsSupermanknows.
In the TOP-RIGHT diagram it is common knowledge that Lois has seen the weather
report. So in w she knows rain is predicted and in w she knows rain is not predicted.
4 6
452 Chapter 12. KnowledgeRepresentation
w: I,R w: ¬I,R w: I,R w: ¬I,R
0 1 4 5
w: I,¬R w: ¬I,¬R w: I,¬R w: ¬I,¬R
2 3 6 7
(a) (b)
w: I,R w: ¬I,R
4 5
w: I,R w: ¬I,R
0 1
w: I,¬R w: ¬I,¬R
2 3
w: I,¬R w: ¬I,¬R
6 7
(c)
Figure 12.4 Possible worlds with accessibility relations KSuperman (solid arrows) and
KLois (dottedarrows). Theproposition Rmeans“theweatherreportfortomorrowisrain”
andI means“Superman’ssecretidentityisClarkKent.” Allworldsareaccessibletothem-
selves;thearrowsfromaworldtoitselfarenotshown.
Superman does not know the report, but he knows that Lois knows, because in every world
thatisaccessible tohim,eithersheknowsRorsheknows¬R.
Inthe BOTTOM diagram werepresent thescenario whereitiscommonknowledge that
Supermanknowshisidentity, andLoismightormightnothave seentheweatherreport. We
representthisbycombiningthetwotopscenarios, andaddingarrowstoshowthatSuperman
does not know which scenario actually holds. Lois does know, so we don’t need to add any
arrowsforher. In w Superman still knows I but notR,and nowhedoes notknow whether
0
Lois knows R. From what Superman knows, he might be in w or w , in which case Lois
0 2
doesnotknowwhetherRistrue,orhecouldbeinw ,inwhichcasesheknowsR,orw ,in
4 6
whichcasesheknows¬R.
Thereareaninfinitenumberofpossibleworlds,sothetrickistointroducejusttheones
you need to represent what you are trying to model. A new possible world is needed to talk
about different possible facts (e.g., rain is predicted or not), or to talk about different states
ofknowledge (e.g., does Loisknow that rain ispredicted). Thatmeanstwopossible worlds,
suchasw andw inFigure12.4,mighthavethesamebasefactsabout theworld, butdiffer
4 0
intheiraccessibility relations, andthereforeinfactsaboutknowledge.
Modallogic solvessometricky issues withtheinterplay ofquantifiers and knowledge.
TheEnglishsentence“Bondknowsthatsomeoneisaspy”isambiguous. Thefirstreadingis
Section12.5. Reasoning SystemsforCategories 453
thatthereisaparticularsomeonewhoBondknowsisaspy;wecanwritethisas
∃x K Spy(x),
Bond
which in modal logic means that there is an x that, in all accessible worlds, Bond knows to
beaspy. Thesecondreading isthatBondjustknowsthatthere isatleastonespy:
K ∃x Spy(x).
Bond
Themodallogic interpretation isthat ineach accessible worldthere isan xthat isaspy, but
itneednotbethesamexineachworld.
Now that we have a modal operator for knowledge, we can write axioms for it. First,
we can say that agents are able to draw deductions; if an agent knows P and knows that P
impliesQ,thentheagentknowsQ:
(K P ∧K (P ⇒ Q)) ⇒ K Q.
a a a
Fromthis(and afewotherrules about logical identities) we canestablish thatK (P ∨¬P)
A
is a tautology; every agent knows every proposition P is either true or false. On the other
hand,(K P)∨(K ¬P)isnotatautology; ingeneral, therewillbelotsofpropositions that
A A
anagentdoesnotknowtobetrueanddoesnotknowtobefalse.
It is said (going back to Plato) that knowledge is justified true belief. That is, if it is
true, if you believe it, and if you have an unassailably good reason, then you know it. That
meansthatifyouknowsomething, itmustbetrue,andwehavetheaxiom:
K P ⇒ P .
a
Furthermore, logical agents should be able to introspect on their own knowledge. If they
knowsomething, thentheyknowthattheyknowit:
K P ⇒ K (K P).
a a a
Wecandefinesimilaraxiomsforbelief(oftendenotedbyB)andothermodalities. However,
LOGICAL one problem with the modal logic approach is that it assumes logical omniscience on the
OMNISCIENCE
part of agents. Thatis, ifan agent knows aset of axioms, then it knows allconsequences of
those axioms. Thisisonshaky ground evenforthesomewhatabstract notion ofknowledge,
butitseemsevenworseforbelief, becausebeliefhasmoreconnotation ofreferringtothings
that are physically represented in the agent, not just potentially derivable. There have been
attempts to define a form of limited rationality for agents; to say that agents believe those
assertions that can be derived with the application of no more than k reasoning steps, or no
morethansseconds ofcomputation. Theseattemptshavebeengenerally unsatisfactory.
12.5 REASONING SYSTEMS FOR CATEGORIES
Categoriesaretheprimarybuilding blocksoflarge-scale knowledgerepresentation schemes.
This section describes systems specially designed for organizing and reasoning with cate-
gories. Therearetwocloselyrelatedfamiliesofsystems: semanticnetworksprovidegraph-
ical aids for visualizing a knowledge base and efficient algorithms for inferring properties
454 Chapter 12. KnowledgeRepresentation
of an object on the basis of its category membership; and description logics provide a for-
mal language for constructing and combining category definitions and efficient algorithms
fordeciding subsetandsupersetrelationships betweencategories.
12.5.1 Semantic networks
In1909,CharlesS.Peirceproposedagraphicalnotationofnodesandedgescalledexistential
EXISTENTIAL graphs that he called “the logic of the future.” Thus began a long-running debate between
GRAPHS
advocates of “logic” and advocates of “semantic networks.” Unfortunately, the debate ob-
scured the fact that semantics networks—at least those with well-defined semantics—are a
form of logic. The notation that semantic networks provide for certain kinds of sentences
is often more convenient, but if we strip away the “human interface” issues, the underlying
concepts—objects, relations, quantification, andsoon—arethesame.
There are many variants of semantic networks, but all are capable of representing in-
dividual objects, categories of objects, and relations among objects. A typical graphical no-
tation displays object or category names in ovals or boxes, and connects them with labeled
links. Forexample, Figure 12.5 hasa MemberOf link between Mary and FemalePersons,
corresponding tothelogical assertion Mary∈FemalePersons;similarly, the SisterOf link
between Mary andJohn corresponds totheassertion SisterOf(Mary,John). Wecancon-
nect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows
that one can get carried away. For example, we know that persons have female persons as
mothers, socanwedrawa HasMother link from Persons toFemalePersons? Theanswer
isno,becauseHasMother isarelationbetweenapersonandhisorhermother,andcategories
donothavemothers.5
Forthisreason,wehaveusedaspecialnotation—thedouble-boxedlink—inFigure12.5.
Thislinkassertsthat
∀x x∈Persons ⇒ [∀y HasMother(x,y) ⇒ y∈FemalePersons].
Wemightalsowanttoassertthatpersonshavetwolegs—thatis,
∀x x∈Persons ⇒ Legs(x,2).
Asbefore, weneed tobe careful not to assert that acategory has legs; the single-boxed link
inFigure12.5isusedtoassertproperties ofeverymemberof acategory.
The semantic network notation makes it convenient to perform inheritance reasoning
ofthekindintroducedinSection12.2. Forexample,byvirtueofbeingaperson,Maryinherits
the property of having two legs. Thus, to find out how many legs Mary has, the inheritance
algorithm follows the MemberOf link from Mary to the category she belongs to, and then
follows SubsetOf links up the hierarchy until it finds a category for which there is a boxed
Legs link—inthiscase,thePersons category. Thesimplicityandefficiencyofthisinference
5 Severalearlysystemsfailedtodistinguishbetweenpropertiesofmembersofacategoryandpropertiesofthe
categoryasawhole. Thiscanleaddirectlytoinconsistencies,aspointedoutbyDrewMcDermott(1976)inhis
article“ArtificialIntelligenceMeetsNaturalStupidity.” AnothercommonproblemwastheuseofIsAlinksfor
bothsubsetandmembershiprelations,incorrespondencewithEnglishusage: “acatisamammal”and“Fifiisa
cat.”SeeExercise12.22formoreontheseissues.
Section12.5. Reasoning SystemsforCategories 455
Mammals
SubsetOf
HasMother Legs
Persons 2
SubsetOf SubsetOf
Female Male
Persons Persons
MemberOf MemberOf
SisterOf Legs
Mary John 1
Figure12.5 Asemanticnetworkwithfourobjects(John,Mary,1, and2)andfourcate-
gories.Relationsaredenotedbylabeledlinks.
FlyEvents
MemberOf
Fly
17
Agent During
Origin Destination
Shankar NewYork NewDelhi Yesterday
Figure12.6 Afragmentofasemanticnetworkshowingtherepresentation ofthelogical
assertionFly(Shankar,NewYork,NewDelhi,Yesterday).
mechanism, compared withlogical theorem proving, hasbeen oneofthemainattractions of
semanticnetworks.
Inheritancebecomescomplicatedwhenanobjectcanbelongtomorethanonecategory
orwhenacategorycanbeasubsetofmorethanoneothercategory;thisiscalledmultiplein-
MULTIPLE heritance. Insuchcases,theinheritance algorithmmightfindtwoormoreconflictingvalues
INHERITANCE
answeringthequery. Forthisreason,multipleinheritance isbannedinsomeobject-oriented
programming (OOP)languages, such asJava, that useinheritance inaclass hierarchy. Itis
usuallyallowedinsemanticnetworks, butwedeferdiscussion ofthatuntilSection12.6.
Thereadermighthavenoticedanobviousdrawbackofsemanticnetworknotation,com-
paredtofirst-orderlogic: thefactthatlinks betweenbubbles represent only binary relations.
For example, the sentence Fly(Shankar,NewYork,NewDelhi,Yesterday) cannot be as-
serted directly in a semantic network. Nonetheless, we can obtain the effect of n-ary asser-
tionsbyreifyingthepropositionitselfasaneventbelongingtoanappropriateeventcategory.
Figure 12.6 shows the semantic network structure for this particular event. Notice that the
restriction tobinaryrelations forcesthecreation ofarichontology ofreifiedconcepts.
Reification of propositions makes it possible to represent every ground, function-free
atomicsentenceoffirst-orderlogicinthesemanticnetworknotation. Certainkindsofuniver-
456 Chapter 12. KnowledgeRepresentation
sallyquantifiedsentencescanbeassertedusinginverselinksandthesinglyboxedanddoubly
boxed arrowsapplied tocategories, butthatstillleaves us alongwayshort offullfirst-order
logic. Negation, disjunction, nested function symbols, and existential quantification are all
missing. Nowitispossibletoextendthenotationtomakeitequivalenttofirst-orderlogic—as
in Peirce’s existential graphs—but doing so negates one ofthe main advantages of semantic
networks,whichisthesimplicityandtransparency oftheinference processes. Designerscan
build alarge networkandstillhaveagood ideaabout whatqueries willbeefficient, because
(a)itiseasytovisualizethestepsthattheinferenceprocedurewillgothroughand(b)insome
cases the query language is so simple that difficult queries cannot be posed. In cases where
the expressive powerproves to be too limiting, many semantic network systems provide for
procedural attachment to fill in the gaps. Procedural attachment is a technique whereby
a query about (or sometimes an assertion of) a certain relation results in a call to a special
procedure designed forthatrelationratherthanageneral inference algorithm.
One of the most important aspects of semantic networks is their ability to represent
defaultvaluesforcategories. ExaminingFigure12.5carefully,onenoticesthatJohnhasone
DEFAULTVALUE
leg,despitethefactthatheisapersonandallpersonshavetwolegs. InastrictlylogicalKB,
this would be a contradiction, but in a semantic network, the assertion that all persons have
two legs has only default status; that is, a person is assumed to have two legs unless this is
contradictedbymorespecificinformation. Thedefaultsemanticsisenforcednaturallybythe
inheritance algorithm, because it follows links upwards from the object itself (John in this
case)andstopsassoonasitfindsavalue. Wesaythatthedefaultisoverriddenbythemore
OVERRIDING
specific value. Notice that we could also override the default number of legs by creating a
categoryofOneLeggedPersons,asubsetofPersons ofwhichJohn isamember.
Wecanretainastrictly logical semantics forthenetworkif wesaythattheLegs asser-
tionforPersons includes anexception forJohn:
∀x x∈Persons ∧x (cid:7)= John ⇒ Legs(x,2).
For a fixed network, this is semantically adequate but will be much less concise than the
networknotationitselfiftherearelotsofexceptions. Foranetworkthatwillbeupdatedwith
moreassertions, however, such anapproach fails—wereally wanttosay thatanypersons as
yetunknownwithonelegareexceptionstoo. Section12.6goesintomoredepthonthisissue
andondefaultreasoning ingeneral.
12.5.2 Descriptionlogics
The syntax of first-order logic is designed to make it easy to say things about objects. De-
scription logics are notations that are designed to make it easier to describe definitions and
DESCRIPTIONLOGIC
properties of categories. Description logic systems evolved from semantic networks in re-
sponse to pressure to formalize what the networks mean while retaining the emphasis on
taxonomic structureasanorganizing principle.
The principal inference tasks for description logics are subsumption (checking if one
SUBSUMPTION
category is a subset of another by comparing their definitions) and classification (checking
CLASSIFICATION
whetheranobject belongs toacategory).. Somesystems also include consistency ofacate-
gorydefinition—whether themembershipcriteria arelogically satisfiable.
Section12.5. Reasoning SystemsforCategories 457
Concept → Thing| ConceptName
| And(Concept,...)
| All(RoleName,Concept)
| AtLeast(Integer,RoleName)
| AtMost(Integer,RoleName)
| Fills(RoleName,IndividualName,...)
| SameAs(Path,Path)
| OneOf(IndividualName,...)
Path → [RoleName,...]
Figure12.7 ThesyntaxofdescriptionsinasubsetoftheCLASSIClanguage.
TheCLASSIC language(Borgida etal.,1989)isatypicaldescription logic. Thesyntax
of CLASSIC descriptions is shown in Figure 12.7.6 For example, to say that bachelors are
unmarriedadultmaleswewouldwrite
Bachelor = And(Unmarried,Adult,Male).
Theequivalent infirst-orderlogicwouldbe
Bachelor(x) ⇔ Unmarried(x)∧Adult(x)∧Male(x).
Notice that the description logic has an an algebra of operations on predicates, which of
coursewecan’tdoinfirst-orderlogic. Anydescription in CLASSIC canbetranslated intoan
equivalent first-order sentence, but some descriptions are more straightforward in CLASSIC.
For example, to describe the set of men with at least three sons who are all unemployed
and married to doctors, and at mosttwodaughters who areall professors in physics ormath
departments, wewoulduse
And(Man,AtLeast(3,Son),AtMost(2,Daughter),
All(Son,And(Unemployed,Married,All(Spouse,Doctor))),
All(Daughter,And(Professor,Fills(Department,Physics,Math)))).
Weleaveitasanexercisetotranslatethisintofirst-orderlogic.
Perhapsthemostimportantaspectofdescriptionlogicsistheiremphasisontractability
ofinference. Aproblem instance issolved bydescribing itandthen asking ifitissubsumed
byoneofseveralpossiblesolutioncategories. Instandardfirst-orderlogicsystems,predicting
thesolutiontimeisoftenimpossible. Itisfrequently lefttotheusertoengineertherepresen-
tation to detour around sets of sentences that seem to be causing the system to take several
weekstosolveaproblem. Thethrustindescriptionlogics,ontheotherhand,istoensurethat
subsumption-testing canbesolvedintimepolynomial inthesizeofthedescriptions.7
6 Notice that the language does not allow one to simply state that one concept, or category, is a subset of
another. Thisisadeliberatepolicy:subsumptionbetweencategoriesmustbederivablefromsomeaspectsofthe
descriptionsofthecategories.Ifnot,thensomethingismissingfromthedescriptions.
7 CLASSICprovidesefficientsubsumptiontestinginpractice,buttheworst-caseruntimeisexponential.
458 Chapter 12. KnowledgeRepresentation
This sounds wonderful in principle, until one realizes that it can only have one of two
consequences: either hard problems cannot be stated at all, or they require exponentially
large descriptions! However,thetractability results doshed lightonwhatsorts ofconstructs
cause problems and thus help the user to understand how different representations behave.
For example, description logics usually lack negation and disjunction. Each forces first-
orderlogicalsystemstogothrough apotentially exponential caseanalysis inordertoensure
completeness. CLASSIC allows only a limited form of disjunction in the Fills and OneOf
constructs, which permit disjunction overexplicitly enumerated individuals but not overde-
scriptions. Withdisjunctive descriptions, nested definitions canleadeasily toanexponential
numberofalternative routesbywhichonecategory cansubsumeanother.
12.6 REASONING WITH DEFAULT INFORMATION
Inthepreceding section,wesawasimpleexampleofanassertionwithdefaultstatus: people
have two legs. This default can be overridden by more specific information, such as that
LongJohn Silverhasone leg. Wesaw thatthe inheritance mechanism insemantic networks
implements the overriding of defaults in a simple and natural way. In this section, we study
defaults more generally, with a view toward understanding the semantics of defaults rather
thanjustproviding aprocedural mechanism.
12.6.1 Circumscription anddefault logic
Wehaveseentwoexamplesofreasoningprocessesthatviolatethemonotonicitypropertyof
logic that was proved in Chapter 7.8 In this chapter we saw that a property inherited by all
membersofacategory inasemanticnetworkcouldbeoverridden bymorespecificinforma-
tionforasubcategory. InSection 9.4.5,wesawthat undertheclosed-world assumption, ifa
proposition αisnotmentioned inKB thenKB |= ¬α,butKB ∧α |= α.
Simple introspection suggests that these failures of monotonicity are widespread in
commonsense reasoning. It seems that humans often “jump to conclusions.” For example,
when one sees a car parked on the street, one is normally willing to believe that it has four
wheels even though only three are visible. Now, probability theory can certainly provide a
conclusion thatthefourth wheelexists withhighprobability, yet,formostpeople, thepossi-
bility of the car’s not having four wheels does not arise unless some new evidence presents
itself. Thus, it seems that the four-wheel conclusion is reached by default, in the absence of
anyreason todoubt it. Ifnewevidence arrives—forexample, ifonesees theownercarrying
awheelandnoticesthatthecarisjackedup—thentheconclusioncanberetracted. Thiskind
of reasoning is said to exhibit nonmonotonicity, because the set of beliefs does not grow
NONMONOTONICITY
NONMONOTONIC monotonically over time as new evidence arrives. Nonmonotonic logics have been devised
LOGIC
withmodifiednotionsoftruthandentailmentinordertocapturesuchbehavior. Wewilllook
attwosuchlogicsthathavebeenstudied extensively: circumscription anddefaultlogic.
8 Recallthatmonotonicityrequiresallentailedsentencestoremainentailedafternewsentencesareaddedtothe
KB.Thatis,ifKB |=αthenKB ∧β |=α.
Section12.6. Reasoning withDefaultInformation 459
Circumscription can be seen as a more powerful and precise version of the closed-
CIRCUMSCRIPTION
worldassumption. Theideaistospecifyparticularpredicatesthatareassumedtobe“asfalse
aspossible”—that is,falseforeveryobjectexceptthoseforwhichtheyareknowntobetrue.
Forexample, suppose wewanttoassert thedefault rule thatbirds fly. Wewouldintroduce a
predicate, sayAbnormal (x),andwrite
1
Bird(x)∧¬Abnormal (x) ⇒ Flies(x).
1
If we say that Abnormal is to be circumscribed, a circumscriptive reasoner is entitled to
1
assume ¬Abnormal (x) unless Abnormal (x) is known to be true. This allows the con-
1 1
clusion Flies(Tweety) to be drawn from the premise Bird(Tweety), but the conclusion no
longerholdsifAbnormal (Tweety)isasserted.
1
MODEL Circumscription can be viewed as an example of a model preference logic. In such
PREFERENCE
logics,asentenceisentailed(withdefaultstatus)ifitistrueinallpreferredmodelsoftheKB,
as opposed to the requirement of truth in all models in classical logic. For circumscription,
one modelispreferred toanother ifithas fewerabnormal objects.9 Letusseehow thisidea
worksinthecontextofmultipleinheritance insemanticnetworks. Thestandardexamplefor
which multiple inheritance is problematic is called the “Nixon diamond.” It arises from the
observation that Richard Nixon was both a Quaker (and hence by default a pacifist) and a
Republican (andhencebydefault notapacifist). Wecanwritethisasfollows:
Republican(Nixon)∧Quaker(Nixon).
Republican(x)∧¬Abnormal (x) ⇒ ¬Pacifist(x).
2
Quaker(x)∧¬Abnormal (x) ⇒ Pacifist(x).
3
If we circumscribe Abnormal and Abnormal , there are two preferred models: one in
2 3
whichAbnormal (Nixon)andPacifist(Nixon)holdandoneinwhichAbnormal (Nixon)
2 3
and¬Pacifist(Nixon)hold. Thus,thecircumscriptivereasonerremainsproperly agnosticas
to whether Nixon was a pacifist. If we wish, in addition, to assert that religious beliefs take
PRIORITIZED precedenceoverpoliticalbeliefs,wecanuseaformalismcalledprioritizedcircumscription
CIRCUMSCRIPTION
togivepreference tomodelswhere Abnormal isminimized.
3
Defaultlogic is aformalism in which default rules can be written togenerate contin-
DEFAULTLOGIC
gent,nonmonotonic conclusions. Adefaultrulelookslikethis:
DEFAULTRULES
Bird(x) :Flies(x)/Flies(x).
ThisrulemeansthatifBird(x)istrue,andifFlies(x)isconsistentwiththeknowledgebase,
thenFlies(x)maybeconcluded bydefault. Ingeneral, adefault rulehastheform
P :J ,...,J /C
1 n
where P is called the prerequisite, C is the conclusion, and J are the justifications—if any
i
one of them can be proven false, then the conclusion cannot be drawn. Any variable that
9 Fortheclosed-worldassumption,onemodelispreferredtoanotherifithasfewertrueatoms—thatis,preferred
modelsareminimalmodels. Thereisanaturalconnectionbetweentheclosed-worldassumptionanddefinite-
clauseKBs,becausethefixedpointreachedbyforwardchainingondefinite-clauseKBsistheuniqueminimal
model.Seepage258formoreonthispoint.
460 Chapter 12. KnowledgeRepresentation
appears in J or C must also appear in P. The Nixon-diamond example can be represented
i
indefault logicwithonefactandtwodefaultrules:
Republican(Nixon)∧Quaker(Nixon).
Republican(x) :¬Pacifist(x)/¬Pacifist(x).
Quaker(x) :Pacifist(x)/Pacifist(x).
To interpret what the default rules mean, we define the notion of an extension of a default
EXTENSION
theory to be a maximal set of consequences of the theory. That is, an extension S consists
of the original known facts and a set of conclusions from the default rules, such that no
additionalconclusions canbedrawnfromS andthejustificationsofeverydefaultconclusion
inS areconsistentwithS. Asinthecaseofthepreferredmodelsincircumscription, wehave
twopossibleextensionsfortheNixondiamond: onewhereinheisapacifistandonewherein
heisnot. Prioritizedschemesexistinwhichsomedefaultrulescanbegivenprecedenceover
others, allowingsomeambiguities toberesolved.
Since 1980, when nonmonotonic logics were first proposed, a great deal of progress
has been made in understanding their mathematical properties. There are still unresolved
questions, however. For example, if “Cars have four wheels” is false, what does it mean
to have it in one’s knowledge base? What is a good set of default rules to have? If we
cannot decide, for each rule separately, whether it belongs in our knowledge base, then we
haveaseriousproblem ofnonmodularity. Finally, howcanbeliefsthathavedefaultstatusbe
used to make decisions? This is probably the hardest issue for default reasoning. Decisions
often involve tradeoffs, and one therefore needs to compare the strengths of belief in the
outcomes ofdifferent actions, and the costs of making awrong decision. In cases where the
same kinds of decisions are being made repeatedly, it is possible to interpret default rules
as “threshold probability” statements. For example, the default rule “My brakes are always
OK” really means “The probability that my brakes are OK, given no other information, is
sufficiently high that the optimal decision is for me to drive without checking them.” When
thedecisioncontextchanges—forexample,whenoneisdrivingaheavilyladentruckdowna
steep mountain road—the default rulesuddenly becomes inappropriate, eventhough thereis
nonewevidenceoffaultybrakes. Theseconsiderationshaveledsomeresearcherstoconsider
howtoembeddefaultreasoning withinprobability theoryor utilitytheory.
12.6.2 Truth maintenance systems
Wehave seen that many of the inferences drawn by a knowledge representation system will
have only default status, rather than being absolutely certain. Inevitably, some of these in-
ferredfactswillturnouttobewrongandwillhavetoberetractedinthefaceofnewinforma-
tion. This process is called belief revision.10 Suppose that a knowledge base KB contains
BELIEFREVISION
a sentence P—perhaps a default conclusion recorded by a forward-chaining algorithm, or
perhaps just an incorrect assertion—and we want to execute TELL(KB, ¬P). Toavoid cre-
ating a contradiction, we must first execute RETRACT(KB, P). This sounds easy enough.
10 Beliefrevisionisoftencontrastedwithbeliefupdate,whichoccurswhenaknowledgebaseisrevisedtoreflect
achangeintheworldratherthannewinformationaboutafixed world. Beliefupdatecombinesbeliefrevision
withreasoningabouttimeandchange;itisalsorelatedtotheprocessoffilteringdescribedinChapter15.
Section12.6. Reasoning withDefaultInformation 461
Problems arise, however, if any additional sentences were inferred from P and asserted in
theKB.Forexample,theimplicationP ⇒ QmighthavebeenusedtoaddQ. Theobvious
“solution”—retracting allsentencesinferredfrom P—failsbecausesuchsentencesmayhave
other justifications besides P. For example, if R and R ⇒ Q are also in the KB, then Q
TRUTH
does not have to be removed after all. Truth maintenance systems, orTMSs, are designed
MAINTENANCE
SYSTEM
tohandle exactlythesekindsofcomplications.
One simple approach to truth maintenance is to keep track of the order in which sen-
tences are told to the knowledge base by numbering them from P to P . When the call
1 n
RETRACT(KB,P
i
)ismade,thesystem revertstothestatejustbefore P
i
wasadded,thereby
removingbothP andanyinferences thatwerederivedfrom P . Thesentences P through
i i i+1
P can then be added again. This is simple, and it guarantees that the knowledge base will
n
be consistent, but retracting P requires retracting and reasserting n−isentences as wellas
i
undoing and redoing all the inferences drawn from those sentences. For systems to which
manyfactsarebeingadded—such aslargecommercialdatabases—this isimpractical.
Amoreefficientapproachisthejustification-basedtruthmaintenancesystem,orJTMS.
JTMS
InaJTMS,each sentence inthe knowledge base isannotated withajustification consisting
JUSTIFICATION
of the set of sentences from which it was inferred. For example, if the knowledge base
already contains P ⇒ Q, then TELL(P) will cause Q to be added with the justification
{P, P ⇒ Q}. In general, a sentence can have any number of justifications. Justifica-
tions make retraction efficient. Given the call RETRACT(P), the JTMS will delete exactly
those sentences for which P is a member of every justification. So, if a sentence Q had
the single justification {P, P ⇒ Q}, it would be removed; if it had the additional justi-
fication {P, P ∨ R ⇒ Q}, it would still be removed; but if it also had the justification
{R, P ∨R ⇒ Q},thenitwouldbespared. Inthisway,thetimerequiredforretractionofP
depends onlyonthenumberofsentences derived from P ratherthanonthenumberofother
sentences addedsinceP enteredtheknowledge base.
TheJTMSassumesthatsentencesthatareconsideredoncewillprobablybeconsidered
again, so rather than deleting a sentence from the knowledge base entirely when it loses
all justifications, we merely mark the sentence as being out of the knowledge base. If a
subsequent assertion restores one of the justifications, then we mark the sentence as being
back in. In this way, the JTMS retains all the inference chains that it uses and need not
rederivesentences whenajustification becomesvalidagain.
In addition to handling the retraction of incorrect information, TMSs can be used to
speed up the analysis of multiple hypothetical situations. Suppose, for example, that the
Romanian Olympic Committee is choosing sites for the swimming, athletics, and eques-
trian events at the 2048 Games to be held in Romania. For example, let the first hypothe-
sisbeSite(Swimming,Pitesti),Site(Athletics,Bucharest),andSite(Equestrian,Arad).
A great deal of reasoning must then be done to work out the logistical consequences and
hence the desirability of this selection. If we want to consider Site(Athletics,Sibiu) in-
stead, the TMS avoids the need to start again from scratch. Instead, we simply retract
Site(Athletics,Bucharest)andassert Site(Athletics,Sibiu)andtheTMStakescareofthe
necessary revisions. Inference chains generated from the choice of Bucharest can be reused
withSibiu,providedthattheconclusions arethesame.
462 Chapter 12. KnowledgeRepresentation
Anassumption-based truthmaintenancesystem,orATMS,makesthistypeofcontext-
ATMS
switching between hypothetical worldsparticularly efficient. InaJTMS,themaintenance of
justifications allows you to move quickly from one state to another by making a few retrac-
tionsandassertions,butatanytimeonlyonestateisrepresented. AnATMSrepresentsallthe
states that have everbeen considered at the same time. Whereas aJTMS simply labels each
sentence as being in or out, an ATMS keeps track, for each sentence, of which assumptions
wouldcausethesentencetobetrue. Inotherwords,eachsentencehasalabelthatconsistsof
aset ofassumption sets. Thesentence holds justinthose cases inwhichalltheassumptions
inoneoftheassumption setshold.
Truth maintenance systems also provide a mechanism for generating explanations.
EXPLANATION
Technically, an explanation of a sentence P is a set of sentences E such that E entails P.
If the sentences in E are already known to be true, then E simply provides a sufficient ba-
sis for proving that P must be the case. But explanations can also include assumptions—
ASSUMPTION
sentences that are not known to be true, but would suffice to prove P if they were true. For
example, one might not have enough information to prove that one’s car won’t start, but a
reasonableexplanationmightincludetheassumptionthatthebatteryisdead. This,combined
with knowledge of how cars operate, explains the observed nonbehavior. In most cases, we
willpreferanexplanation E thatisminimal,meaningthatthereisnopropersubsetofE that
isalsoanexplanation. AnATMScangenerateexplanations forthe“carwon’tstart”problem
bymakingassumptions (such as“gasincar”or“battery dead”) inanyorderwelike, evenif
some assumptions are contradictory. Then we look at the label for the sentence “car won’t
start”toreadoffthesetsofassumptions thatwouldjustify thesentence.
Theexact algorithms used toimplement truth maintenance systems arealittle compli-
cated,andwedonotcoverthemhere. Thecomputationalcomplexityofthetruthmaintenance
problem is at least as great as that of propositional inference—that is, NP-hard. Therefore,
you should not expect truth maintenance to be a panacea. When used carefully, however, a
TMS can provide a substantial increase in the ability of a logical system to handle complex
environments andhypotheses.
12.7 THE INTERNET SHOPPING WORLD
Inthisfinalsection weputtogether allwehave learned toencode knowledge forashopping
research agent that helps a buyer find product offers on the Internet. The shopping agent is
given a product description by the buyer and has the task of producing a list of Web pages
that offer such a product for sale, and ranking which offers are best. In some cases the
buyer’s product description will be precise, as in Canon Rebel XTi digital camera, and the
taskisthentofindthestore(s) withthebestoffer. Inothercases thedescription willbeonly
partially specified, as in digital camera for under $300, and the agent will have to compare
differentproducts.
Theshoppingagent’senvironmentistheentireWorldWideWebinitsfullcomplexity—
not atoy simulated environment. Theagent’s percepts are Webpages, but whereas ahuman
Section12.7. TheInternet ShoppingWorld 463
Example Online Store
Selectfromourfinelineofproducts:
•Computers
•Cameras
•Books
•Videos
•Music
<h1>Example Online Store</h1>
<i>Select</i> from our fine line of products:
<ul>
<li> <a href="http://example.com/compu">Computers</a>
<li> <a href="http://example.com/camer">Cameras</a>
<li> <a href="http://example.com/books">Books</a>
<li> <a href="http://example.com/video">Videos</a>
<li> <a href="http://example.com/music">Music</a>
</ul>
Figure12.8 AWebpagefromagenericonlinestoreintheformperceivedby thehuman
userofabrowser(top),andthecorrespondingHTMLstringas perceivedbythebrowseror
theshoppingagent(bottom). InHTML,charactersbetween<and>aremarkupdirectives
that specify how the page is displayed. For example, the string <i>Select</i> means
toswitch to italic font, displaytheword Select, andthenendtheuse ofitalic font. A page
identifiersuchashttp://example.com/booksiscalledauniformresourcelocator
(URL).Themarkup<a href="url">Books</a>meanstocreateahypertextlinkto url
withtheanchortextBooks.
Web user would see pages displayed as an array of pixels on a screen, the shopping agent
willperceive apage asacharacter string consisting ofordinary words interspersed withfor-
matting commands in the HTML markup language. Figure 12.8 shows a Web page and a
corresponding HTML character string. The perception problem for the shopping agent in-
volvesextracting usefulinformation frompercepts ofthis kind.
Clearly, perception on Webpages iseasier than, say, perception while driving ataxi in
Cairo. Nonetheless, therearecomplications totheInternetperception task. TheWebpagein
Figure12.8issimplecomparedtorealshoppingsites,whichmayincludeCSS,cookies,Java,
Javascript, Flash,robotexclusionprotocols, malformedHTML,soundfiles,movies,andtext
that appears only as part of a JPEG image. An agent that can deal with all of the Internet is
almost as complex as a robot that can move in the real world. We concentrate on a simple
agentthatignoresmostofthesecomplications.
Theagent’sfirsttaskistocollectproductoffersthatarerelevanttoaquery. Ifthequery
is “laptops,” then a Web page with a review of the latest high-end laptop would be relevant,
butifitdoesn’t provideawaytobuy, itisn’t anoffer. Fornow,wecansayapageisanoffer
ifitcontainsthewords“buy”or“price”or“addtocart”withinanHTMLlinkorformonthe
464 Chapter 12. KnowledgeRepresentation
page. Forexample,ifthepagecontainsastringoftheform“<a...add to cart...</a”
thenitisanoffer. Thiscouldberepresentedinfirst-orderlogic,butitismorestraightforward
toencodeitintoprogramcode. Weshowhowtodomoresophisticatedinformationextraction
inSection22.4.
12.7.1 Followinglinks
Thestrategy istostartatthehomepage ofanonline store and considerallpagesthatcanbe
reached byfollowing relevantlinks.11 Theagent willhaveknowledge ofanumberofstores,
forexample:
Amazon∈OnlineStores ∧Homepage(Amazon,“amazon.com”).
Ebay∈OnlineStores ∧Homepage(Ebay,“ebay.com”).
ExampleStore ∈OnlineStores ∧Homepage(ExampleStore,“example.com”).
These stores classify their goods into product categories, and provide links tothe major cat-
egories from their home page. Minor categories can be reached through a chain of relevant
links, andeventually wewillreachoffers. Inotherwords,a pageisrelevanttothequeryifit
can be reached by achain of zero ormorerelevant category links from astore’s home page,
andthenfromonemorelinktotheproductoffer. Wecandefinerelevance:
Relevant(page,query) ⇔
∃store,home store∈OnlineStores ∧Homepage(store,home)
∧∃url,url RelevantChain(home,url ,query)∧Link(url ,url)
2 2 2
∧page = Contents(url).
Here the predicate Link(from,to) means that there is a hyperlink from the from URL to
the to URL. To define what counts as a RelevantChain, we need to follow not just any old
hyperlinks,butonlythoselinkswhoseassociatedanchortextindicatesthatthelinkisrelevant
to the product query. Forthis, we use LinkText(from,to,text) to mean that there is a link
between from and to with text as the anchor text. A chain of links between two URLs, start
and end, is relevant to a description d if the anchor text of each link is a relevant category
nameford. Theexistenceofthechainitselfisdetermined byarecursive definition, withthe
emptychain(start=end)asthebasecase:
RelevantChain(start,end,query) ⇔ (start = end)
∨(∃u,text LinkText(start,u,text)∧RelevantCategoryName(query,text)
∧RelevantChain(u,end,query)).
Now we must define what it means for text to be a RelevantCategoryName for query.
First, weneed to relate strings to the categories they name. This is done using the predicate
Name(s,c),whichsaysthatstring sisanameforcategory c—forexample,wemightassert
that Name(“laptops”,LaptopComputers). Some more examples of the Name predicate
appear in Figure 12.9(b). Next, we define relevance. Suppose that query is “laptops.” Then
RelevantCategoryName(query,text)istruewhenoneofthefollowingholds:
• Thetextandquerynamethesamecategory—e.g., “notebooks” and“laptops.”
11 Analternativetothelink-followingstrategyistouseanInternetsearchengine;thetechnologybehindInternet
search,informationretrieval,willbecoveredinSection22.3.
Section12.7. TheInternet ShoppingWorld 465
Name(“books”,Books)
Books ⊂Products
Name(“music”,MusicRecordings)
MusicRecordings ⊂Products
Name(“CDs”,MusicCDs)
MusicCDs ⊂MusicRecordings
Name(“electronics”,Electronics)
Electronics ⊂Products
Name(“digitalcameras”,DigitalCameras)
DigitalCameras ⊂Electronics
Name(“stereos”,StereoEquipment)
StereoEquipment ⊂Electronics
Name(“computers”,Computers)
Computers ⊂Electronics
Name(“desktops”,DesktopComputers)
DesktopComputers ⊂Computers
Name(“laptops”,LaptopComputers)
LaptopComputers ⊂Computers
Name(“notebooks”,LaptopComputers)
...
...
(a) (b)
Figure12.9 (a)Taxonomyofproductcategories.(b)Namesforthosecategories.
• Thetextnamesasupercategory suchas“computers.”
• Thetextnamesasubcategory suchas“ultralight notebooks.”
Thelogicaldefinition ofRelevantCategoryName isasfollows:
RelevantCategoryName(query,text) ⇔
∃c ,c Name(query,c )∧Name(text,c )∧(c ⊆ c ∨c ⊆ c ). (12.1)
1 2 1 2 1 2 2 1
Otherwise, theanchortextisirrelevant because itnamesacategory outside thisline, suchas
“clothes” or“lawn&garden.”
To follow relevant links, then, it is essential to have a rich hierarchy of product cate-
gories. ThetoppartofthishierarchymightlooklikeFigure 12.9(a). Itwillnotbefeasibleto
list all possible shopping categories, because a buyer could always come up with some new
desire and manufacturers will always come out with new products to satisfy them (electric
kneecap warmers?). Nonetheless, anontology ofabout athousand categories willserve asa
veryusefultoolformostbuyers.
In addition to the product hierarchy itself, we also need to have a rich vocabulary of
names for categories. Life would be much easier if there were a one-to-one correspon-
dence between categories and the character strings that name them. We have already seen
the problem of synonymy—two names for the same category, such as “laptop computers”
and“laptops.” Thereisalso theproblem of ambiguity—one namefortwoormoredifferent
categories. Forexample,ifweaddthesentence
Name(“CDs”,CertificatesOfDeposit)
totheknowledge baseinFigure12.9(b), then“CDs”willnametwodifferent categories.
Synonymy and ambiguity can cause a significant increase in the number of paths that
the agent has to follow, and can sometimes make it difficult to determine whether a given
pageisindeedrelevant. Amuchmoreseriousproblemistheverybroadrangeofdescriptions
thatausercantypeandcategorynamesthatastorecanuse. Forexample, thelinkmightsay
“laptop” whenthe knowledge basehas only “laptops” orthe usermight askfor“acomputer
466 Chapter 12. KnowledgeRepresentation
I can fit on the tray table of an economy-class airline seat.” It is impossible to enumerate in
advance all the ways a category can be named, so the agent will have to be able to do addi-
tionalreasoninginsomecasestodetermineiftheNamerelationholds. Intheworstcase,this
requiresfullnaturallanguageunderstanding, atopicthatwewilldefertoChapter22. Inprac-
tice,afewsimplerules—suchasallowing“laptop”tomatchacategorynamed“laptops”—go
alongway. Exercise12.10asksyoutodevelopasetofsuchrules afterdoing someresearch
intoonlinestores.
Given the logical definitions from the preceding paragraphs and suitable knowledge
bases of product categories and naming conventions, are we ready to apply an inference
algorithm to obtain a set of relevant offers for our query? Not quite! The missing element
is the Contents(url) function, which refers to the HTML page at a given URL. The agent
doesn’t havethepagecontents ofeveryURLinitsknowledge base;nordoesithaveexplicit
rulesfordeducing whatthosecontents mightbe. Instead, wecanarrange fortherightHTTP
procedure to be executed whenever a subgoal involves the Contents function. In this way, it
appears tothe inference engine as ifthe entire Webis inside the knowledge base. This isan
PROCEDURAL exampleofageneraltechniquecalledproceduralattachment,wherebyparticularpredicates
ATTACHMENT
andfunctions canbehandledbyspecial-purpose methods.
12.7.2 Comparing offers
Let us assume that the reasoning processes of the preceding section have produced a set of
offerpagesforour“laptops” query. Tocomparethoseoffers, theagentmustextracttherele-
vantinformation—price, speed,disksize,weight,andsoon—fromtheofferpages. Thiscan
beadifficult taskwithrealWebpages, forallthereasons mentioned previously. Acommon
wayof dealing withthis problem is touse programs called wrappersto extract information
WRAPPER
from a page. The technology of information extraction is discussed in Section 22.4. For
now weassume that wrappers exist, and when given apage and a knowledge base, they add
assertions to the knowledge base. Typically, a hierarchy of wrappers would be applied to a
page: a very general one to extract dates and prices, a morespecific one to extract attributes
forcomputer-related products, andifnecessary asite-specific onethatknowstheformatofa
particularstore. Givenapageontheexample.com sitewiththetext
IBM ThinkBook 970. Our price: $399.00
followed byvarious technical specifications, wewouldlike awrapper toextract information
suchasthefollowing:
∃c,offer c∈LaptopComputers ∧offer∈ProductOffers ∧
Manufacturer(c,IBM)∧Model(c,ThinkBook970)∧
ScreenSize(c,Inches(14))∧ScreenType(c,ColorLCD)∧
MemorySize(c,Gigabytes(2))∧CPUSpeed(c,GHz(1.2))∧
OfferedProduct(offer,c)∧Store(offer,GenStore)∧
URL(offer,“example.com/computers/34356.html”)∧
Price(offer,$(399))∧Date(offer,Today).
Thisexampleillustratesseveralissuesthatarisewhenwetakeseriouslythetaskofknowledge
engineering forcommercialtransactions. Forexample, notice thatthepriceisanattribute of
Section12.8. Summary 467
the offer, not the product itself. This is important because the offer at a given store may
change from day to day even for the same individual laptop; for some categories—such as
houses and paintings—the same individual object may even be offered simultaneously by
different intermediaries at different prices. There are still more complications that we have
not handled, such asthe possibility that the price depends onthe method ofpayment and on
the buyer’s qualifications for certain discounts. The final task is to compare the offers that
havebeenextracted. Forexample,considerthesethreeoffers:
A :1.4GHzCPU,2GBRAM,250GBdisk,$299.
B : 1.2GHzCPU,4GBRAM,350GBdisk,$500.
C : 1.2GHzCPU,2GBRAM,250GBdisk,$399.
C is dominated by A; that is, A is cheaper and faster, and they are otherwise the same. In
general, X dominates Y ifX hasabettervalueonatleast oneattribute, andisnotworseon
any attribute. But neither A nor B dominates the other. To decide which is better we need
to know how the buyer weighs CPU speed and price against memory and disk space. The
general topic ofpreferences among multiple attributes isaddressed inSection16.4; fornow,
our shopping agent will simply return a list of all undominated offers that meet the buyer’s
description. Inthisexample, bothAandB areundominated. Noticethatthisoutcomerelies
on the assumption that everyone prefers cheaper prices, fasterprocessors, and more storage.
Someattributes,suchasscreensizeonanotebook,dependontheuser’sparticularpreference
(portability versusvisibility); forthese,theshopping agentwilljusthavetoasktheuser.
The shopping agent we have described here is a simple one; many refinements are
possible. Still, it has enough capability that with the right domain-specific knowledge it can
actually be of use to a shopper. Because of its declarative construction, it extends easily to
more complex applications. The main point of this section is to show that some knowledge
representation—in particular, theproducthierarchy—isnecessaryforsuchanagent,andthat
oncewehavesomeknowledgeinthisform,therestfollowsnaturally.
12.8 SUMMARY
By delving into the details of how one represents a variety of knowledge, we hope we have
given the reader a sense of how real knowledge bases are constructed and a feeling for the
interesting philosophical issuesthatarise. Themajorpointsareasfollows:
• Large-scale knowledge representation requires ageneral-purpose ontology to organize
andtietogetherthevariousspecificdomainsofknowledge.
• Ageneral-purpose ontology needs tocoverawidevariety ofknowledge andshould be
capable, inprinciple, ofhandling anydomain.
• Building a large, general-purpose ontology is a significant challenge that has yet to be
fullyrealized, although currentframeworksseemtobequiterobust.
• We presented an upper ontology based on categories and the event calculus. We
covered categories, subcategories, parts, structured objects, measurements, substances,
events,timeandspace,change, andbeliefs.
468 Chapter 12. KnowledgeRepresentation
• Naturalkindscannotbedefinedcompletelyinlogic,butpropertiesofnaturalkindscan
berepresented.
• Actions, events, and time can be represented either in situation calculus or in more
expressiverepresentations suchaseventcalculus. Suchrepresentations enableanagent
toconstruct plansbylogicalinference.
• WepresentedadetailedanalysisoftheInternetshoppingdomain,exercisingthegeneral
ontology andshowinghowthedomainknowledge canbeusedbyashopping agent.
• Special-purpose representation systems, such as semantic networks and description
logics, have been devised to help in organizing a hierarchy of categories. Inheritance
isanimportantformofinference,allowingthepropertiesofobjectstobededucedfrom
theirmembershipincategories.
• The closed-world assumption, as implemented in logic programs, provides a simple
way to avoid having to specify lots of negative information. It is best interpreted as a
defaultthatcanbeoverriddenbyadditional information.
• Nonmonotoniclogics,suchascircumscriptionanddefaultlogic,areintendedtocap-
turedefaultreasoning ingeneral.
• Truthmaintenancesystemshandleknowledge updatesandrevisions efficiently.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Briggs(1985) claimsthatformalknowledge representation research beganwithclassical In-
dian theorizing about thegrammarofShastric Sanskrit, which dates back to thefirstmillen-
nium B.C. In the West, the use of definitions of terms in ancient Greek mathematics can be
regardedastheearliestinstance: Aristotle’s Metaphysics(literally,whatcomesafterthebook
onphysics) isanear-synonym for Ontology. Indeed, thedevelopment oftechnical terminol-
ogyinanyfieldcanberegardedasaformofknowledge representation.
Early discussions of representation in AI tended to focus on “problem representation”
ratherthan“knowledgerepresentation.” (See,forexample,Amarel’s(1968)discussionofthe
Missionaries andCannibals problem.) Inthe1970s, AIemphasized thedevelopment of“ex-
pert systems” (also called “knowledge-based systems”) that could, if given the appropriate
domain knowledge, matchorexceed theperformance ofhumanexperts onnarrowlydefined
tasks. For example, the first expert system, DENDRAL (Feigenbaum et al., 1971; Lindsay
etal.,1980),interpreted theoutputofamassspectrometer(atypeofinstrumentusedtoana-
lyzethestructureoforganicchemicalcompounds)asaccuratelyasexpertchemists. Although
the success of DENDRAL was instrumental in convincing the AI research community of the
importance ofknowledge representation, therepresentational formalisms used in DENDRAL
are highly specific to the domain of chemistry. Over time, researchers became interested in
standardized knowledge representation formalisms and ontologies that could streamline the
process of creating new expert systems. In so doing, they ventured into territory previously
explored by philosophers of science and of language. The discipline imposed in AI by the
needforone’stheoriesto“work”hasledtomorerapidanddeeperprogressthanwasthecase
Bibliographical andHistorical Notes 469
whenthese problems weretheexclusive domain ofphilosophy (although ithasattimesalso
ledtotherepeated reinvention ofthewheel).
Thecreationofcomprehensivetaxonomiesorclassificationsdatesbacktoancienttimes.
Aristotle(384–322 B.C.) stronglyemphasizedclassificationandcategorization schemes. His
Organon,acollection ofworksonlogicassembledbyhisstudentsafterhisdeath,included a
treatisecalledCategoriesinwhichheattemptedtoconstructwhatwewouldnowcallanupper
ontology. Healso introduced thenotions of genusand species forlower-level classification.
Ourpresentsystemofbiologicalclassification,includingtheuseof“binomialnomenclature”
(classification via genus and species in the technical sense), was invented by the Swedish
biologist Carolus Linnaeus, or Carl von Linne (1707–1778). The problems associated with
natural kinds and inexact category boundaries have been addressed by Wittgenstein (1953),
Quine(1953), Lakoff(1987),andSchwartz(1977),amongothers.
Interest in larger-scale ontologies is increasing, as documented by the Handbook on
Ontologies (Staab, 2004). The OPENCYC project (Lenat and Guha, 1990; Matuszek et al.,
2006) hasreleased a150,000-concept ontology, withanupperontology similartotheonein
Figure 12.1aswellasspecificconcepts like“OLEDDisplay” and“iPhone,” whichisatype
of “cellular phone,” which in turn is a type of “consumer electronics,” “phone,” “wireless
communication device,” and other concepts. The DBPEDIA project extracts structured data
from Wikipedia; specifically from Infoboxes: the boxes of attribute/value pairs that accom-
pany many Wikipedia articles (Wu and Weld, 2008; Bizer et al., 2007). As of mid-2009,
DBPEDIA contains 2.6million concepts, withabout 100 facts perconcept. TheIEEEwork-
inggroupP1600.1createdtheSuggestedUpperMergedOntology(SUMO)(NilesandPease,
2001; Pease and Niles, 2002), which contains about 1000 terms in the upper ontology and
links to over 20,000 domain-specific terms. Stoffel et al. (1997) describe algorithms for ef-
ficiently managing a very large ontology. A survey of techniques for extracting knowledge
fromWebpagesisgivenbyEtzioni etal.(2008).
On the Web, representation languages are emerging. RDF (Brickley and Guha, 2004)
allows for assertions to be made in the form of relational triples, and provides some means
forevolvingthemeaningofnamesovertime. OWL(Smithetal.,2004)isadescriptionlogic
thatsupportsinferencesoverthesetriples. Sofar,usageseemstobeinverselyproportionalto
representational complexity: thetraditionalHTMLandCSSformatsaccountforover99%of
Webcontent,followedbythesimplestrepresentation schemes,suchasmicroformats(Khare,
2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup to
add attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet
widespread, and the full vision of the Semantic Web (Berners-Lee et al., 2001) has not yet
been realized. The conferences on Formal Ontology in Information Systems (FOIS)contain
manyinteresting papersonbothgeneralanddomain-specific ontologies.
The taxonomy used in this chapter was developed by the authors and is based in part
on their experience in the CYC project and in part on work by Hwang and Schubert (1993)
and Davis (1990, 2005). An inspirational discussion of the general project of commonsense
knowledgerepresentation appearsinHayes’s(1978, 1985b) “NaivePhysicsManifesto.”
Successful deep ontologies within a specific field include the Gene Ontology project
(Consortium, 2008)andCML,theChemicalMarkupLanguage(Murray-Rust etal.,2003).
470 Chapter 12. KnowledgeRepresentation
Doubts about the feasibility of a single ontology for all knowledge are expressed by
Doctorow (2001), Gruber (2004), Halevy et al. (2009), and Smith (2004), who states, “the
initialprojectofbuilding onesingleontology ...has...largelybeenabandoned.”
TheeventcalculuswasintroducedbyKowalskiandSergot(1986)tohandlecontinuous
time,andtherehavebeenseveralvariations(SadriandKowalski,1995;Shanahan,1997)and
overviews (Shanahan, 1999; Mueller, 2006). van Lambalgen and Hamm (2005) show how
thelogic ofeventsmapsonto thelanguage weusetotalkabout events. Analternative tothe
event and situation calculi is the fluent calculus (Thielscher, 1999). James Allen introduced
timeintervalsforthesamereason(Allen,1984),arguingthatintervalsweremuchmorenatu-
ralthansituations forreasoning aboutextended andconcurrent events. PeterLadkin(1986a,
1986b) introduced “concave” time intervals (intervals with gaps; essentially, unions of ordi-
nary“convex” timeintervals) andapplied thetechniques of mathematical abstract algebra to
time representation. Allen (1991) systematically investigates the wide variety of techniques
availablefortimerepresentation; vanBeekandManchak(1996)analyzealgorithmsfortem-
poralreasoning. Therearesignificantcommonalitiesbetweentheevent-basedontologygiven
in this chapter and an analysis of events due to the philosopher Donald Davidson (1980).
ThehistoriesinPatHayes’s(1985a) ontology ofliquids andthechronicles inMcDermott’s
(1985)theoryofplanswerealsoimportantinfluencesonthefieldandthischapter.
Thequestion ofthe ontological status of substances has along history. Plato proposed
that substances were abstract entities entirely distinct from physical objects; he would say
MadeOf(Butter ,Butter) rather than Butter ∈Butter. This leads to a substance hierar-
3 3
chyinwhich,forexample,UnsaltedButter isamorespecificsubstancethanButter. Thepo-
sitionadoptedinthischapter, inwhichsubstances arecategoriesofobjects,waschampioned
byRichardMontague(1973). IthasalsobeenadoptedintheCYC project. Copeland(1993)
mountsaserious, butnotinvincible, attack. Thealternative approach mentioned inthechap-
ter,inwhichbutterisoneobjectconsistingofallbutteryobjectsintheuniverse,wasproposed
originallybythePolishlogicianLes´niewski(1916). Hismereology(thenameisderivedfrom
MEREOLOGY
the Greek word for “part”) used the part–whole relation as a substitute for mathematical set
theory, withtheaimofeliminating abstract entities suchassets. Amorereadable exposition
of these ideas is given by Leonard and Goodman (1940), and Goodman’s The Structure of
Appearance(1977)appliestheideastovariousproblemsinknowledgerepresentation. While
some aspects of the mereological approach are awkward—for example, the need for asepa-
rateinheritance mechanism based onpart–whole relations—the approach gainedthesupport
of Quine (1960). Harry Bunt (1985) has provided an extensive analysis of its use in knowl-
edgerepresentation. CasatiandVarzi(1999)coverparts,wholes,andthespatiallocations.
Mental objects have been the subject of intensive study in philosophy and AI. There
arethreemainapproaches. Theonetaken inthis chapter, based onmodallogic andpossible
worlds, is the classical approach from philosophy (Hintikka, 1962; Kripke, 1963; Hughes
andCresswell, 1996). Thebook Reasoning about Knowledge(Faginetal.,1995)provides a
thorough introduction. The second approach is a first-order theory in which mental objects
arefluents. Davis(2005) andDavisandMorgenstern (2005)describe thisapproach. Itrelies
on the possible-worlds formalism, and builds on work by Robert Moore (1980, 1985). The
third approach is a syntactic theory, in which mental objects are represented by character
SYNTACTICTHEORY
Bibliographical andHistorical Notes 471
strings. A string is just a complex term denoting a list of symbols, so CanFly(Clark) can
be represented by the list of symbols [C,a,n,F,l,y,(,C,l,a,r,k,)]. The syntactic theory
of mental objects was first studied in depth by Kaplan and Montague (1960), who showed
that it led to paradoxes if not handled carefully. Ernie Davis (1990) provides an excellent
comparison ofthesyntactic andmodaltheories ofknowledge.
The Greek philosopher Porphyry (c. 234–305 A.D.), commenting on Aristotle’s Cat-
egories, drew what might qualify as the first semantic network. Charles S. Peirce (1909)
developed existential graphs as the first semantic network formalism using modern logic.
Ross Quillian (1961), driven by an interest in human memory and language processing, ini-
tiated workon semantic networks within AI.Aninfluential paperby Marvin Minsky (1975)
presented a version of semantic networks called frames; a frame was a representation of
an object or category, with attributes and relations to other objects or categories. The ques-
tionofsemanticsarosequiteacutely withrespecttoQuillian’ssemanticnetworks (andthose
of others who followed his approach), with their ubiquitous and very vague “IS-A links”
Woods’s(1975)famousarticle“What’sInaLink?” drewtheattentionofAIresearcherstothe
need forprecise semantics in knowledge representation formalisms. Brachman (1979) elab-
orated on this point and proposed solutions. Patrick Hayes’s (1979) “The Logic of Frames”
cut even deeper, claiming that “Most of ‘frames’ is just a new syntax for parts of first-order
logic.” Drew McDermott’s (1978b) “Tarskian Semantics, or, No Notation without Denota-
tion!” argued that themodel-theoretic approach tosemantics used infirst-order logicshould
be applied to all knowledge representation formalisms. This remains a controversial idea;
notably, McDermott himself has reversed his position in “A Critique of Pure Reason” (Mc-
Dermott, 1987). Selman and Levesque (1993) discuss the complexity of inheritance with
exceptions, showingthatinmostformulations itisNP-complete.
The development of description logics is the most recent stage in a long line of re-
search aimed at finding useful subsets of first-order logic for which inference is computa-
tionally tractable. Hector Levesque and Ron Brachman (1987) showed that certain logical
constructs—notably, certain uses of disjunction and negation—were primarily responsible
for the intractability of logical inference. Building on the KL-ONE system (Schmolze and
Lipkis, 1983), several researchers developed systems that incorporate theoretical complex-
ity analysis, most notably KRYPTON (Brachman et al., 1983) and Classic (Borgida et al.,
1989). The result has been a marked increase in the speed of inference and a much better
understanding of the interaction between complexity and expressiveness in reasoning sys-
tems. Calvaneseetal.(1999)summarizethestateoftheart,andBaader etal.(2007)present
a comprehensive handbook of description logic. Against this trend, Doyle and Patil (1991)
have argued that restricting the expressiveness of a language either makes it impossible to
solvecertainproblemsorencouragestheusertocircumvent thelanguagerestrictionsthrough
nonlogical means.
Thethreemainformalismsfordealingwithnonmonotonic inference—circumscription
(McCarthy, 1980), default logic (Reiter, 1980), and modal nonmonotonic logic (McDermott
andDoyle,1980)—wereallintroduced inonespecialissueoftheAIJournal. Delgrandeand
Schaub (2003) discuss the merits of the variants, given 25 years of hindsight. Answer set
programmingcanbeseenasanextensionofnegationasfailureorasarefinementofcircum-
472 Chapter 12. KnowledgeRepresentation
scription; the underlying theory of stable model semantics was introduced by Gelfond and
Lifschitz(1988),andtheleadinganswersetprogrammingsystemsareDLV(Eiteretal.,1998)
andSMODELS(Niemela¨etal.,2000). ThediskdriveexamplecomesfromtheSMODELSuser
manual (Syrja¨nen, 2000). Lifschitz (2001) discusses the use of answer set programming for
planning. Brewkaetal.(1997)giveagoodoverviewofthevariousapproaches tononmono-
tonic logic. Clark (1978) covers the negation-as-failure approach to logic programming and
Clarkcompletion. VanEmdenandKowalski(1976)showthateveryPrologprogramwithout
negation has a unique minimal model. Recent years have seen renewed interest in applica-
tionsofnonmonotonic logicstolarge-scale knowledgerepresentation systems. TheBENINQ
systemsforhandling insurance-benefit inquiries wasperhaps thefirstcommercially success-
fulapplication ofanonmonotonic inheritance system (Morgenstern, 1998). Lifschitz (2001)
discussestheapplicationofanswersetprogrammingtoplanning. Avarietyofnonmonotonic
reasoning systems based on logic programming are documented in the proceedings of the
conferences onLogicProgrammingandNonmonotonic Reasoning(LPNMR).
The study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP
(McAllester, 1980) systems, both of which were essentially JTMSs. Forbus and de Kleer
(1993) explain in depth how TMSs can be used in AI applications. Nayak and Williams
(1997) show howanefficientincremental TMScalled anITMSmakes itfeasible toplan the
operations ofaNASAspacecraft inrealtime.
Thischaptercouldnotcovereveryareaofknowledgerepresentationindepth. Thethree
principal topicsomittedarethefollowing:
QUALITATIVE Qualitativephysics: Qualitativephysicsisasubfieldofknowledgerepresentationconcerned
PHYSICS
specificallywithconstructingalogical,nonnumerictheoryofphysicalobjectsandprocesses.
The term was coined by Johan de Kleer (1975), although the enterprise could be said to
have started in Fahlman’s (1974) BUILD, a sophisticated planner for constructing complex
towers of blocks. Fahlman discovered in the process of designing it that most of the effort
(80%, by his estimate) went into modeling the physics of the blocks world to calculate the
stability of various subassemblies of blocks, rather than into planning per se. He sketches a
hypothetical naive-physics-like processtoexplainwhyyoungchildrencansolve BUILD-like
problemswithoutaccesstothehigh-speedfloating-pointarithmeticusedinBUILD’sphysical
modeling. Hayes (1985a) uses “histories”—four-dimensional slices of space-time similarto
Davidson’s events—to construct a fairly complex naive physics of liquids. Hayes was the
firsttoprovethatabathwiththepluginwilleventuallyoverflowifthetapkeepsrunningand
that a person who falls into a lake will get wet all over. Davis (2008) gives an update to the
ontology ofliquids thatdescribes thepouringofliquidsintocontainers.
De Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) inde-
pendently and almost simultaneously developed systems that can reason about a physical
system based on qualitative abstractions of the underlying equations. Qualitative physics
soon developed to the point where it became possible to analyze an impressive variety of
complex physical systems (Yip, 1991). Qualitative techniques have been used to construct
noveldesignsforclocks,windshieldwipers,andsix-leggedwalkers(SubramanianandWang,
1994). Thecollection Readings inQualitative Reasoning about Physical Systems (Weldand
Exercises 473
de Kleer, 1990) an encyclopedia article by Kuipers (2001), and ahandbook article by Davis
(2007)introduce tothefield.
Spatialreasoning: Thereasoning necessary tonavigate inthe wumpusworld and shopping
SPATIALREASONING
world is trivial in comparison to the rich spatial structure of the real world. The earliest
seriousattempttocapturecommonsensereasoningaboutspaceappearsintheworkofErnest
Davis(1986,1990). Theregionconnection calculus ofCohnetal.(1997)supportsaformof
qualitative spatial reasoning and has led to new kinds of geographical information systems;
seealso (Davis, 2006). Aswithqualitative physics, anagent cangoalong way, sotospeak,
without resorting to a full metric representation. When such a representation is necessary,
techniques developedinrobotics(Chapter25)canbeused.
PSYCHOLOGICAL Psychological reasoning: Psychological reasoning involves the development of a working
REASONING
psychology for artificial agents to use in reasoning about themselves and other agents. This
is often based on so-called folk psychology, the theory that humans in general are believed
to use in reasoning about themselves and other humans. When AI researchers provide their
artificialagentswithpsychological theoriesforreasoning aboutotheragents,thetheoriesare
frequentlybasedontheresearchers’descriptionofthelogicalagents’owndesign. Psycholog-
icalreasoning iscurrently mostuseful within thecontext ofnatural language understanding,
wherediviningthespeaker’s intentions isofparamount importance.
Minker(2001)collectspapersbyleadingresearchersinknowledgerepresentation,sum-
marizing 40 years of work in the field. The proceedings of the international conferences on
Principles ofKnowledgeRepresentation andReasoningprovide themostup-to-date sources
for work in this area. Readings in Knowledge Representation (Brachman and Levesque,
1985) and Formal Theories of the Commonsense World (Hobbs and Moore, 1985) are ex-
cellent anthologies on knowledge representation; the former focuses more on historically
important papers in representation languages and formalisms, the latter onthe accumulation
of the knowledge itself. Davis (1990), Stefik (1995), and Sowa (1999) provide textbook in-
troductions toknowledgerepresentation, vanHarmelen etal.(2007)contributes ahandbook,
andaspecialissueofAIJournal coversrecentprogress(DavisandMorgenstern, 2004). The
biennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers
applications ofthetheoryofknowledgeinAI,economics, anddistributed systems.
EXERCISES
12.1 Define an ontology in first-order logic for tic-tac-toe. The ontology should contain
situations,actions,squares,players,marks(X,O,orblank),andthenotionofwinning,losing,
ordrawing agame. Also define the notion ofaforced win(ordraw): aposition from which
a player can force a win (or draw) with the right sequence of actions. Write axioms for the
domain. (Note: The axioms that enumerate the different squares and that characterize the
winning positions are rather long. You need not write these out in full, but indicate clearly
whattheylooklike.)
474 Chapter 12. KnowledgeRepresentation
12.2 Figure 12.1 shows the top levels of a hierarchy for everything. Extend it to include
as many real categories as possible. A good way to do this is to cover all the things in your
everyday life. This includes objects and events. Start with waking up, and proceed in an
orderly fashion noting everything that you see, touch, do, and think about. For example,
a random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,
talking, ProfessorFateman,chicken curry,tongue, $7,sun,thedailynewspaper, andsoon.
You should produce both a single hierarchy chart (on a large sheet of paper) and a
listing of objects and categories with the relations satisfied by members of each category.
Everyobjectshould beinacategory, andeverycategoryshouldbeinthehierarchy.
12.3 Develop a representational system for reasoning about windows in a window-based
computerinterface. Inparticular, yourrepresentation shouldbeabletodescribe:
• Thestateofawindow: minimized,displayed, ornonexistent.
• Whichwindow(ifany)istheactivewindow.
• Thepositionofeverywindowatagiventime.
• Theorder(fronttoback)ofoverlapping windows.
• Theactions of creating, destroying, resizing, and moving windows; changing the state
ofawindow;andbringing awindowtothefront. Treattheseactions asatomic;thatis,
do not deal with the issue of relating them to mouse actions. Give axioms describing
theeffectsofactionsonfluents. Youmayuseeithereventorsituation calculus.
Assume an ontology containing situations, actions, integers (for x and y coordinates) and
windows. Definealanguage overthisontology; thatis, alistofconstants, function symbols,
andpredicates withanEnglishdescription ofeach. Ifyouneedtoaddmorecategories tothe
ontology(e.g.,pixels),youmaydoso,butbesuretospecifytheseinyourwrite-up. Youmay
(andshould) usesymbolsdefinedinthetext,butbesuretolisttheseexplicitly.
12.4 Statethefollowinginthelanguage youdevelopedforthepreviousexercise:
a. In situation S , window W is behind W but sticks out on the left and right. Do not
0 1 2
stateexactcoordinates forthese;describe the generalsituation.
b. Ifawindowisdisplayed, thenitstopedgeishigherthanitsbottomedge.
c. Afteryoucreateawindow w,itisdisplayed.
d. Awindowcanbeminimizedifitisdisplayed.
12.5 (Adapted from an example by Doug Lenat.) Your mission is to capture, in logical
form,enoughknowledgetoansweraseriesofquestionsabout thefollowingsimplescenario:
YesterdayJohnwenttotheNorthBerkeleySafewaysupermarketandboughttwo
poundsoftomatoesandapoundofgroundbeef.
Start by trying to represent the content of the sentence as a series of assertions. You should
writesentences that have straightforward logical structure (e.g., statements that objects have
certainproperties,thatobjectsarerelatedincertainways,thatallobjectssatisfyingoneprop-
ertysatisfyanother). Thefollowingmighthelpyougetstarted:
Exercises 475
• Whichclasses, objects, and relations would youneed? Whatare theirparents, siblings
andsoon? (Youwillneedeventsandtemporalordering, among otherthings.)
• Wherewouldtheyfitinamoregeneralhierarchy?
• Whataretheconstraints andinterrelationships amongthem?
• Howdetailedmustyoubeabouteachofthevariousconcepts?
To answer the questions below, your knowledge base must include background knowledge.
You’ll have to deal with what kind of things are at a supermarket, what is involved with
purchasingthethingsoneselects,whatthepurchaseswillbeusedfor,andsoon. Trytomake
your representation as general as possible. Togive a trivial example: don’t say “People buy
foodfromSafeway,”becausethatwon’thelpyouwiththosewhoshopatanothersupermarket.
Also,don’tturnthequestionsintoanswers;forexample,question(c)asks“DidJohnbuyany
meat?”—not“DidJohnbuyapoundofgroundbeef?”
Sketch the chains of reasoning that would answer the questions. If possible, use a
logicalreasoningsystemtodemonstratethesufficiencyofyourknowledgebase. Manyofthe
things you write might be only approximately correct in reality, but don’t worry too much;
the idea is to extract the common sense that lets you answer these questions at all. A truly
completeanswertothisquestionisextremelydifficult,probablybeyondthestateoftheartof
current knowledge representation. But you should beable to put together aconsistent set of
axiomsforthelimitedquestions posedhere.
a. IsJohnachildoranadult? [Adult]
b. DoesJohnnowhaveatleasttwotomatoes? [Yes]
c. DidJohnbuyanymeat? [Yes]
d. IfMarywasbuyingtomatoesatthesametimeasJohn,didheseeher? [Yes]
e. Arethetomatoesmadeinthesupermarket? [No]
f. WhatisJohngoingtodowiththetomatoes? [Eatthem]
g. DoesSafewayselldeodorant? [Yes]
h. DidJohnbringsomemoneyoracreditcardtothesupermarket? [Yes]
i. DoesJohnhavelessmoneyaftergoingtothesupermarket? [Yes]
12.6 Make the necessary additions or changes to your knowledge base from the previous
exercisesothatthequestionsthatfollowcanbeanswered. Includeinyourreportadiscussion
of your changes, explaining why they were needed, whether they were minor or major, and
whatkindsofquestions wouldnecessitate furtherchanges.
a. ArethereotherpeopleinSafewaywhileJohnisthere? [Yes—staff!]
b. IsJohnavegetarian? [No]
c. Whoownsthedeodorant inSafeway? [SafewayCorporation]
d. DidJohnhaveanounceofgroundbeef? [Yes]
e. DoestheShellstationnextdoorhaveanygas? [Yes]
f. DothetomatoesfitinJohn’scartrunk? [Yes]
476 Chapter 12. KnowledgeRepresentation
12.7 Represent the following seven sentences using and extending the representations de-
velopedinthechapter:
a. Waterisaliquidbetween0and100degrees.
b. Waterboilsat100degrees.
c. ThewaterinJohn’swaterbottleisfrozen.
d. Perrierisakindofwater.
e. JohnhasPerrierinhiswaterbottle.
f. Allliquids haveafreezing point.
g. Aliterofwaterweighsmorethanaliterofalcohol.
12.8 Writedefinitionsforthefollowing:
a. ExhaustivePartDecomposition
b. PartPartition
c. PartwiseDisjoint
Theseshould beanalogous tothedefinitions forExhaustiveDecomposition,Partition,and
Disjoint. Is it the case that PartPartition(s,BunchOf(s))? If so, prove it; if not, give a
counterexample anddefinesufficientconditions underwhich itdoeshold.
12.9 Analternative scheme forrepresenting measures involves applying the units function
to an abstract length object. In such a scheme, one would write Inches(Length(L )) =
1
1.5. How does this scheme compare with the one in the chapter? Issues include conversion
axioms, names for abstract quantities (such as “50 dollars”), and comparisons of abstract
measuresindifferent units(50inchesismorethan50centimeters).
12.10 Add sentences to extend the definition of the predicate Name(s,c) so that a string
such as “laptop computer” matches the appropriate category names from a variety of stores.
Trytomakeyourdefinitiongeneral. Testitbylookingattenonlinestores,andatthecategory
names they give for three different categories. Forexample, for the category of laptops, we
found the names “Notebooks,” “Laptops,” “Notebook Computers,” “Notebook,” “Laptops
andNotebooks,”and“NotebookPCs.” SomeofthesecanbecoveredbyexplicitName facts,
whileotherscouldbecoveredbysentences forhandling plurals, conjunctions, etc.
12.11 Writeeventcalculus axiomstodescribetheactionsinthewumpusworld.
12.12 Statetheinterval-algebra relationthatholdsbetweeneverypairofthefollowingreal-
worldevents:
LK: ThelifeofPresidentKennedy.
IK: TheinfancyofPresident Kennedy.
PK: Thepresidency ofPresident Kennedy.
LJ: ThelifeofPresident Johnson.
PJ: Thepresidency ofPresidentJohnson.
LO: ThelifeofPresidentObama.
Exercises 477
12.13 Investigate ways to extend the event calculus to handle simultaneous events. Is it
possible toavoidacombinatorial explosion ofaxioms?
12.14 Constructarepresentationforexchangeratesbetweencurrenciesthatallowsfordaily
fluctuations.
12.15 Define the predicate Fixed, where Fixed(Location(x)) means that the location of
objectxisfixedovertime.
12.16 Describe the event of trading something for something else. Describe buying as a
kindoftradinginwhichoneoftheobjectstradedisasumofmoney.
12.17 The two preceding exercises assume a fairly primitive notion of ownership. Forex-
ample, the buyer starts by owning the dollar bills. This picture begins to break down when,
for example, one’s money is in the bank, because there is no longer any specific collection
of dollar bills that one owns. The picture is complicated still further by borrowing, leasing,
renting,andbailment. Investigatethevariouscommonsenseandlegalconceptsofownership,
andpropose aschemebywhichtheycanberepresented formally.
12.18 (Adapted from Fagin et al. (1995).) Consider a game played with a deck of just 8
cards,4acesand4kings. Thethreeplayers,Alice,Bob,andCarlos,aredealttwocardseach.
Withoutlookingatthem,theyplacethecardsontheirforeheads sothattheotherplayerscan
see them. Then the players take turns either announcing that they know what cards are on
their own forehead, thereby winning the game, or saying “I don’t know.” Everyone knows
theplayersaretruthfulandareperfectatreasoning aboutbeliefs.
a. Game1. Alice and Bob have both said “I don’t know.” Carlos sees that Alice has two
aces(A-A)andBobhastwokings (K-K).Whatshould Carlossay? (Hint: consider all
threepossible casesforCarlos: A-A,K-K,A-K.)
b. DescribeeachstepofGame1usingthenotation ofmodallogic.
c. Game2. Carlos, Alice, and Boballsaid “I don’t know” ontheirfirstturn. Aliceholds
K-KandBobholdsA-K.Whatshould Carlossayonhissecondturn?
d. Game3. Alice,Carlos,andBoballsay“Idon’tknow”ontheirfirstturn,asdoesAlice
onhersecondturn. AliceandBobbothholdA-K.WhatshouldCarlossay?
e. Provethattherewillalwaysbeawinnertothisgame.
12.19 Theassumption oflogical omniscience, discussed on page453, isofcourse not true
of any actual reasoners. Rather, it is an idealization of the reasoning process that may be
more or less acceptable depending on the applications. Discuss the reasonableness of the
assumption foreachofthefollowingapplications ofreasoning aboutknowledge:
a. Partial knowledge adversary games, such as card games. Here one player wants to
reasonaboutwhathisopponent knowsaboutthestateofthegame.
b. Chess with a clock. Here the player may wish to reason about the limits of his oppo-
nent’s or his own ability to find the best move in the time available. For instance, if
player A has much more time left than player B, then A will sometimes make a move
that greatly complicates thesituation, in thehopes ofgaining an advantage because he
hasmoretimetoworkouttheproperstrategy.
478 Chapter 12. KnowledgeRepresentation
c. Ashopping agentinanenvironment inwhichtherearecostsofgatheringinformation.
d. Reasoning about public key cryptography, which rests on the intractability of certain
computational problems.
12.20 Translate the following description logic expression (from page 457) into first-order
logic,andcommentontheresult:
And(Man,AtLeast(3,Son),AtMost(2,Daughter),
All(Son,And(Unemployed,Married,All(Spouse,Doctor))),
All(Daughter,And(Professor,Fills(Department,Physics,Math)))).
12.21 Recall that inheritance information in semantic networks can be captured logically
by suitable implication sentences. This exercise investigates the efficiency of using such
sentences forinheritance.
a. Consider the information in a used-car catalog such as Kelly’s Blue Book—for exam-
ple, that 1973 Dodge vans are (or perhaps were once) worth $575. Suppose all this
information (for 11,000 models) is encoded as logical sentences, as suggested in the
chapter. Write down three such sentences, including that for 1973 Dodge vans. How
would you use the sentences to find the value of a particular car, given a backward-
chainingtheorem proversuchasProlog?
b. Comparethetimeefficiencyofthebackward-chaining methodforsolvingthisproblem
withtheinheritance methodusedinsemanticnets.
c. Explain how forward chaining allows a logic-based system to solve the same problem
efficiently,assuming thattheKBcontainsonlythe11,000sentences aboutprices.
d. Describe a situation in which neither forward nor backward chaining on the sentences
willallowthepricequeryforanindividual cartobehandled efficiently.
e. Can you suggest a solution enabling this type of query to be solved efficiently in all
cases in logic systems? (Hint: Remember that two cars of the same year and model
havethesameprice.)
12.22 One might suppose that the syntactic distinction between unboxed links and singly
boxed links in semantic networks is unnecessary, because singly boxed links are always at-
tached to categories; an inheritance algorithm could simply assume that an unboxed link
attached to a category is intended to apply to all members of that category. Show that this
argumentisfallacious, givingexamplesoferrorsthatwouldarise.
12.23 One part of the shopping process that was not covered in this chapter is checking
forcompatibility between items. Forexample, ifadigital cameraisordered, whataccessory
batteries, memorycards,andcasesarecompatiblewiththecamera? Writeaknowledgebase
thatcandetermine thecompatibility ofasetofitemsandsuggest replacements oradditional
itemsiftheshoppermakesachoicethatisnotcompatible. Theknowledgebaseshouldworks
withatleastonelineofproductsandextendeasilytootherlines.
12.24 A complete solution to the problem of inexact matches to the buyer’s description
in shopping is very difficult and requires a full array of natural language processing and
Exercises 479
information retrieval techniques. (See Chapters 22 and 23.) One small step is to allow the
usertospecifyminimumandmaximumvaluesforvariousattributes. Thebuyermustusethe
followinggrammarforproductdescriptions:
Description → Category [Connector Modifier]∗
Connector → “with” | “and” | “,”
Modifier → Attribute | Attribute Op Value
Op → “=” | “>”| “<”
Here, Category names a product category, Attribute is some feature such as “CPU” or
“price,”andValue isthetargetvaluefortheattribute. Sothequery“computer withatleasta
2.5GHzCPUforunder$500”mustbere-expressed as“computer withCPU>2.5GHzand
price<$500.” Implementashopping agentthatacceptsdescriptions inthislanguage.
12.25 OurdescriptionofInternetshoppingomittedtheall-importantstepofactuallybuying
the product. Provide a formal logical description of buying, using event calculus. That is,
define the sequence of events that occurs when a buyer submits a credit-card purchase and
theneventually getsbilledandreceivestheproduct.
13
QUANTIFYING
UNCERTAINTY
Inwhichweseehowanagentcantameuncertainty withdegrees ofbelief.
13.1 ACTING UNDER UNCERTAINTY
Agents may need to handle uncertainty, whether due to partial observability, nondetermin-
UNCERTAINTY
ism, ora combination of the two. An agent may never know for certain what state it’s in or
whereitwillendupafterasequence ofactions.
Wehaveseenproblem-solvingagents(Chapter4)andlogicalagents(Chapters7and11)
designed tohandleuncertainty bykeeping trackofabeliefstate—arepresentation oftheset
of all possible world states that it might be in—and generating a contingency plan that han-
dleseverypossibleeventualitythatitssensorsmayreport duringexecution. Despiteitsmany
virtues, however,thisapproach hassignificantdrawbacks whentakenliterally asarecipefor
creating agentprograms:
• When interpreting partial sensor information, a logical agent must consider every log-
ically possible explanation for the observations, no matter how unlikely. This leads to
impossiblelargeandcomplexbelief-state representations.
• Acorrect contingent plan that handles every eventuality can grow arbitrarily large and
mustconsiderarbitrarily unlikely contingencies.
• Sometimes there is no plan that is guaranteed to achieve the goal—yet the agent must
act. Itmusthavesomewaytocomparethemeritsofplansthatarenotguaranteed.
Suppose, for example, that an automated taxi!automated has the goal of delivering a pas-
senger to the airport on time. The agent forms a plan, A , that involves leaving home 90
90
minutes before the flight departs and driving at a reasonable speed. Even though the airport
is only about 5 miles away, a logical taxi agent will not be able to conclude with certainty
that “Plan A will get us to the airport in time.” Instead, it reaches the weaker conclusion
90
“Plan A willget usto theairport in time, aslong asthe cardoesn’t break down orrun out
90
ofgas,andIdon’tgetintoanaccident,andtherearenoaccidentsonthebridge,andtheplane
doesn’t leave early, and nometeorite hits the car, and ....” None ofthese conditions can be
480
Section13.1. ActingunderUncertainty 481
deducedforsure,sotheplan’ssuccesscannotbeinferred. Thisisthequalificationproblem
(page268),forwhichwesofarhaveseennorealsolution.
Nonetheless, in some sense A is in fact the right thing to do. What do we mean by
90
this? Aswediscussed inChapter2,wemeanthatoutofalltheplansthatcouldbeexecuted,
A isexpected tomaximize theagent’s performance measure (wheretheexpectation isrel-
90
ative to the agent’s knowledge about the environment). The performance measure includes
getting to the airport in time for the flight, avoiding a long, unproductive wait at the airport,
andavoidingspeedingticketsalongtheway. Theagent’sknowledgecannotguaranteeanyof
these outcomes for A , but it can provide some degree of belief that they will be achieved.
90
Other plans, such as A , might increase the agent’s belief that it will get to the airport on
180
time, but also increase the likelihood of a long wait. The right thing to do—the rational
decision—therefore depends on both the relative importance of various goals and the likeli-
hood that, and degree to which, they will be achieved. The remainder of this section hones
theseideas,inpreparation forthedevelopmentofthegeneraltheories ofuncertainreasoning
andrationaldecisions thatwepresent inthisandsubsequent chapters.
13.1.1 Summarizing uncertainty
Let’s consider an example of uncertain reasoning: diagnosing a dental patient’s toothache.
Diagnosis—whether for medicine, automobile repair, or whatever—almost always involves
uncertainty. Letustrytowriterulesfordentaldiagnosisusingpropositional logic,sothatwe
canseehowthelogicalapproach breaksdown. Considerthefollowingsimplerule:
Toothache ⇒ Cavity .
The problem is that this rule is wrong. Not all patients with toothaches have cavities; some
ofthemhavegumdisease, anabscess, oroneofseveralotherproblems:
Toothache ⇒ Cavity ∨GumProblem ∨Abscess...
Unfortunately, in order to make the rule true, we have to add an almost unlimited list of
possible problems. Wecouldtryturningtheruleintoacausalrule:
Cavity ⇒ Toothache .
But this rule is not right either; not all cavities cause pain. The only way to fix the rule
is to make it logically exhaustive: to augment the left-hand side with all the qualifications
required for a cavity to cause a toothache. Trying to use logic to cope with a domain like
medicaldiagnosis thusfailsforthreemainreasons:
• Laziness: It is too much work to list the complete set of antecedents or consequents
LAZINESS
neededtoensureanexceptionless ruleandtoohardtousesuchrules.
THEORETICAL • Theoreticalignorance: Medicalsciencehasnocompletetheoryforthedomain.
IGNORANCE
PRACTICAL • Practical ignorance: Even if we know all the rules, we might be uncertain about a
IGNORANCE
particularpatientbecausenotallthenecessary testshave beenorcanberun.
The connection between toothaches and cavities is just not a logical consequence in either
direction. This is typical of the medical domain, as well as most other judgmental domains:
law,business,design,automobilerepair,gardening,dating,andsoon. Theagent’sknowledge
482 Chapter 13. Quantifying Uncertainty
can at best provide only a degree of belief in the relevant sentences. Our main tool for
DEGREEOFBELIEF
PROBABILITY dealing with degrees of belief is probability theory. In the terminology of Section 8.1, the
THEORY
ontological commitments of logic and probability theory are the same—that the world is
composed of facts that do or do not hold in any particular case—but the epistemological
commitments are different: a logical agent believes each sentence to be true or false or has
no opinion, whereas a probabilistic agent may have a numerical degree of belief between 0
(forsentences thatarecertainly false)and1(certainly true).
Probability provides a way of summarizing the uncertainty that comes from our lazi-
ness and ignorance, thereby solving the qualification problem. We might not know forsure
what afflicts a particular patient, but we believe that there is, say, an 80% chance—that is,
a probability of 0.8—that the patient who has a toothache has a cavity. That is, we expect
that outofallthesituations that areindistinguishable from the current situation asfarasour
knowledge goes, the patient will have acavity in 80% of them. This belief could be derived
from statistical data—80% of the toothache patients seen so far have had cavities—or from
somegeneraldentalknowledge, orfromacombination ofevidencesources.
One confusing point is that at the time of our diagnosis, there is no uncertainty in the
actual world: the patient either has a cavity or doesn’t. So what does it mean to say the
probability of a cavity is 0.8? Shouldn’t it be either 0 or 1? The answer is that probability
statementsaremadewithrespecttoaknowledgestate,notwithrespecttotherealworld. We
say“Theprobabilitythatthepatienthasacavity,giventhatshehasatoothache,is0.8.” Ifwe
later learn that the patient has a history of gum disease, we can make a different statement:
“Theprobability that the patient has acavity, given that she has atoothache and ahistory of
gum disease, is 0.4.” If we gather further conclusive evidence against a cavity, we can say
“Theprobability thatthepatient hasacavity, givenallwenowknow,isalmost0.” Notethat
these statements do not contradict each other; each is a separate assertion about a different
knowledgestate.
13.1.2 Uncertainty andrational decisions
Consider again the A plan for getting to the airport. Suppose it gives us a 97% chance
90
of catching our flight. Does this mean it is a rational choice? Not necessarily: there might
be other plans, such as A , with higher probabilities. If it is vital not to miss the flight,
180
then itisworth risking the longer waitattheairport. What about A ,aplan that involves
1440
leavinghome24hoursinadvance? Inmostcircumstances, thisisnotagoodchoice,because
although it almost guarantees getting there on time, it involves an intolerable wait—not to
mentionapossibly unpleasant dietofairportfood.
Tomakesuchchoices, anagent mustfirsthave preferences between thedifferent pos-
PREFERENCE
sible outcomes of the various plans. An outcome is a completely specified state, including
OUTCOME
suchfactorsaswhethertheagentarrivesontimeandthelengthofthewaitattheairport. We
useutilitytheorytorepresent andreason withpreferences. (Theterm utilityisusedherein
UTILITYTHEORY
the sense of “the quality of being useful,” not in the sense of the electric company or water
works.) Utility theory says that every state has a degree of usefulness, orutility, to an agent
andthattheagentwillpreferstateswithhigherutility.
Section13.2. BasicProbability Notation 483
Theutility of astate isrelative toan agent. Forexample, the utility ofastate in which
WhitehascheckmatedBlackinagameofchessisobviouslyhighfortheagentplayingWhite,
butlowfortheagentplayingBlack. Butwecan’tgostrictlybythescoresof1,1/2,and0that
aredictatedbytherulesoftournamentchess—someplayers(includingtheauthors)mightbe
thrilledwithadrawagainsttheworldchampion,whereasotherplayers(includingtheformer
worldchampion)mightnot. Thereisnoaccounting fortasteorpreferences: youmightthink
that anagent whoprefers jalapen˜o bubble-gum ice cream to chocolate chocolate chip is odd
orevenmisguided,butyoucouldnotsaytheagentisirrational. Autilityfunctioncanaccount
foranysetofpreferences—quirkyortypical,nobleorperverse. Notethatutilitiescanaccount
foraltruism,simplybyincluding thewelfareofothersasoneofthefactors.
Preferences, as expressed by utilities, are combined with probabilities in the general
theoryofrational decisions called decisiontheory:
DECISIONTHEORY
Decisiontheory = probability theory+utilitytheory.
The fundamental idea of decision theory is that an agent is rational if and only if it chooses
the action that yields the highest expected utility, averaged over all the possible outcomes
MAXIMUMEXPECTED of the action. This is called the principle of maximum expected utility (MEU). Note that
UTILITY
“expected” might seem like avague, hypothetical term, but asitisused here ithas aprecise
meaning: it means the “average,” or “statistical mean” of the outcomes, weighted by the
probability of the outcome. We saw this principle in action in Chapter 5 when we touched
brieflyonoptimaldecisions inbackgammon;itisinfactacompletely generalprinciple.
Figure13.1sketchesthestructureofanagentthatusesdecisiontheorytoselectactions.
The agent is identical, at an abstract level, to the agents described in Chapters 4 and 7 that
maintain a belief state reflecting the history of percepts to date. The primary difference is
that the decision-theoretic agent’s belief state represents not just the possibilities for world
states but also their probabilities. Given the belief state, the agent can make probabilistic
predictions ofactionoutcomesandhenceselecttheactionwithhighestexpectedutility. This
chapterandthenextconcentrateonthetaskofrepresenting andcomputingwithprobabilistic
information ingeneral. Chapter 15 deals with methods forthe specific tasks of representing
and updating the belief state over time and predicting the environment. Chapter 16 covers
utility theory in more depth, and Chapter 17 develops algorithms for planning sequences of
actionsinuncertain environments.
13.2 BASIC PROBABILITY NOTATION
For our agent to represent and use probabilistic information, we need a formal language.
The language of probability theory has traditionally been informal, written by human math-
ematicians to other human mathematicians. Appendix Aincludes astandard introduction to
elementary probability theory; here, wetake anapproach moresuited totheneeds ofAIand
moreconsistent withtheconcepts offormallogic.
484 Chapter 13. Quantifying Uncertainty
functionDT-AGENT(percept)returnsanaction
persistent: belief state,probabilisticbeliefsaboutthecurrentstateoftheworld
action,theagent’saction
updatebelief state basedonaction andpercept
calculateoutcomeprobabilitiesforactions,
givenactiondescriptionsandcurrentbelief state
selectaction withhighestexpectedutility
givenprobabilitiesofoutcomesandutilityinformation
returnaction
Figure13.1 Adecision-theoreticagentthatselectsrationalactions.
13.2.1 Whatprobabilities areabout
Like logical assertions, probabilistic assertions are about possible worlds. Whereas logical
assertions saywhichpossible worldsarestrictly ruled out (allthoseinwhichtheassertion is
false), probabilistic assertions talkabouthowprobable thevariousworldsare. Inprobability
theory, the set of all possible worlds is called the sample space. The possible worlds are
SAMPLESPACE
mutually exclusive and exhaustive—two possible worlds cannot both be the case, and one
possible world must be the case. For example, if we are about to roll two (distinguishable)
dice, there are 36 possible worlds to consider: (1,1), (1,2), ..., (6,6). The Greek letter Ω
(uppercase omega) is used to refer to the sample space, and ω (lowercase omega) refers to
elementsofthespace, thatis,particularpossible worlds.
Afullyspecifiedprobabilitymodelassociatesanumericalprobability P(ω)witheach
PROBABILITYMODEL
possible world.1 The basic axioms of probability theory say that every possible world has a
probability between0and1andthatthetotalprobability ofthesetofpossible worldsis1:
(cid:12)
0 ≤ P(ω) ≤ 1forevery ω and P(ω) = 1. (13.1)
ω∈Ω
Forexample, if we assume that each die is fair and the rolls don’t interfere with each other,
then each of the possible worlds (1,1), (1,2), ..., (6,6) has probability 1/36. On the other
hand,ifthediceconspiretoproducethesamenumber,thentheworlds(1,1),(2,2),(3,3),etc.,
mighthavehigherprobabilities, leaving theotherswithlowerprobabilities.
Probabilisticassertionsandqueriesarenotusuallyabout particularpossibleworlds,but
about setsofthem. Forexample, wemightbeinterested inthe cases wherethetwodice add
up to 11, the cases where doubles are rolled, and so on. In probability theory, these sets are
called events—a term already used extensively in Chapter 12for adifferent concept. In AI,
EVENT
the sets are always described by propositions in a formal language. (One such language is
described in Section 13.2.2.) Foreach proposition, the corresponding set contains just those
possibleworldsinwhichthepropositionholds. Theprobabilityassociatedwithaproposition
1 Fornow,weassumeadiscrete,countablesetofworlds.Thepropertreatmentofthecontinuouscasebringsin
certaincomplicationsthatarelessrelevantformostpurposesinAI.
Section13.2. BasicProbability Notation 485
isdefinedtobethesumoftheprobabilities oftheworldsinwhichitholds:
(cid:12)
Foranyproposition φ, P(φ) = P(ω). (13.2)
ω∈φ
For example, when rolling fair dice, we have P(Total =11) = P((5,6)) + P((6,5)) =
1/36 + 1/36 = 1/18. Note that probability theory does not require complete knowledge
of the probabilities of each possible world. For example, if we believe the dice conspire to
producethesamenumber,wemightassertthatP(doubles) = 1/4withoutknowingwhether
thedicepreferdouble 6todouble 2. Just aswithlogical assertions, thisassertion constrains
theunderlying probability modelwithoutfullydetermining it.
UNCONDITIONAL ProbabilitiessuchasP(Total =11)andP(doubles)arecalledunconditionalorprior
PROBABILITY
probabilities(andsometimesjust“priors”forshort);theyrefertodegreesofbeliefinpropo-
PRIORPROBABILITY
sitions in the absence of any other information. Most of the time, however, we have some
information, usually called evidence, that has already been revealed. For example, the first
EVIDENCE
die may already be showing a 5 and we are waiting with bated breath for the other one to
stop spinning. In that case, we are interested not in the unconditional probability of rolling
CONDITIONAL doubles, buttheconditionalorposteriorprobability (orjust“posterior” forshort)ofrolling
PROBABILITY
POSTERIOR doublesgiventhatthefirstdieisa5. ThisprobabilityiswrittenP(doubles|Die =5),where
PROBABILITY 1
the “|” is pronounced “given.” Similarly, if I am going to the dentist for a regular checkup,
the probability P(cavity)=0.2might beofinterest; but ifIgo tothe dentist because Ihave
atoothache, it’sP(cavity|toothache)=0.6thatmatters. Notethattheprecedence of“|”is
suchthatanyexpression oftheform P(...|...)alwaysmeansP((...)|(...)).
It is important to understand that P(cavity)=0.2 is still valid after toothache is ob-
served; it just isn’t especially useful. When making decisions, an agent needs to condition
on all the evidence it has observed. It is also important to understand the difference be-
tween conditioning and logical implication. The assertion that P(cavity|toothache)=0.6
does not mean “Whenever toothache is true, conclude that cavity is true with probabil-
ity 0.6” rather it means “Whenever toothache is true and we have no further information,
conclude that cavity is true with probability 0.6.” The extra condition is important; for ex-
ample, if we had the further information that the dentist found no cavities, we definitely
would not want to conclude that cavity is true with probability 0.6; instead we need to use
P(cavity|toothache ∧¬cavity)=0.
Mathematically speaking, conditional probabilities are defined in terms of uncondi-
tionalprobabilities asfollows: foranypropositions aandb,wehave
P(a∧b)
P(a|b) = , (13.3)
P(b)
whichholdswheneverP(b) > 0. Forexample,
P(doubles ∧Die =5)
P(doubles|Die =5) = 1 .
1
P(Die =5)
1
The definition makes sense if you remember that observing b rules out all those possible
worldswhere bisfalse,leaving asetwhosetotalprobability isjustP(b). Withinthatset,the
a-worldssatisfy a∧bandconstitute afraction P(a∧b)/P(b).
486 Chapter 13. Quantifying Uncertainty
Thedefinition of conditional probability, Equation (13.3), can be written in a different
formcalledtheproductrule:
PRODUCTRULE
P(a∧b)= P(a|b)P(b),
Theproduct ruleisperhaps easiertoremember: itcomesfrom thefactthat,foraandbtobe
true,weneedbtobetrue,andwealsoneed atobetruegivenb.
13.2.2 The languageofpropositions inprobability assertions
In this chapter and the next, propositions describing sets of possible worlds are written in a
notationthatcombineselementsofpropositionallogicandconstraintsatisfactionnotation. In
the terminology of Section 2.4.7, it is a factored representation, in which apossible world
isrepresented byasetofvariable/value pairs.
Variablesinprobabilitytheoryarecalledrandomvariablesandtheirnamesbeginwith
RANDOMVARIABLE
anuppercase letter. Thus, inthe dice example, Total and Die are random variables. Every
1
random variable has a domain—the set of possible values it can take on. The domain of
DOMAIN
Total for two dice is the set {2,...,12} and the domain of Die is {1,...,6}. A Boolean
1
random variable has the domain {true,false} (notice that values are always lowercase); for
example, the proposition that doubles are rolled can be written as Doubles=true. By con-
vention, propositions of the form A=true are abbreviated simply as a, while A=false is
abbreviated as¬a. (Theusesofdoubles,cavity,andtoothache inthepreceding section are
abbreviations of this kind.) As in CSPs, domains can be sets of arbitrary tokens; we might
choose the domain of Age to be {juvenile,teen,adult} and the domain of Weather might
be{sunny,rain,cloudy,snow}. Whennoambiguityispossible,itiscommontouseavalue
byitselftostandfortheproposition thataparticularvariablehasthatvalue;thus, sunny can
standforWeather =sunny.
The preceding examples all have finite domains. Variables can have infinite domains,
too—eitherdiscrete(liketheintegers)orcontinuous(likethereals). Foranyvariablewithan
ordereddomain,inequalitiesarealsoallowed,suchasNumberOfAtomsInUniverse ≥ 1070.
Finally, we can combine these sorts of elementary propositions (including the abbre-
viated forms for Boolean variables) by using the connectives of propositional logic. For
example, we can express “The probability that the patient has a cavity, given that she is a
teenagerwithnotoothache, is0.1”asfollows:
P(cavity|¬toothache ∧teen) = 0.1.
Sometimeswewillwanttotalkabouttheprobabilities ofallthepossible valuesofarandom
variable. Wecouldwrite:
P(Weather =sunny) = 0.6
P(Weather =rain)= 0.1
P(Weather =cloudy)= 0.29
P(Weather =snow)= 0.01,
butasanabbreviation wewillallow
P(Weather)=(cid:16)0.6,0.1,0.29,0.01(cid:17),
Section13.2. BasicProbability Notation 487
wheretheboldPindicatesthattheresultisavectorofnumbers,andwherewe assumeapre-
defined ordering (cid:16)sunny,rain,cloudy,snow(cid:17) on the domain of Weather. We say that the
PROBABILITY PstatementdefinesaprobabilitydistributionfortherandomvariableWeather. ThePnota-
DISTRIBUTION
tionisalsousedforconditionaldistributions: P(X|Y)givesthevaluesofP(X=x |Y =y )
i j
foreachpossible i,j pair.
Forcontinuousvariables,itisnotpossibletowriteouttheentiredistributionasavector,
becausethereareinfinitelymanyvalues. Instead,wecandefinetheprobabilitythatarandom
variabletakesonsomevalue xasaparameterized function of x. Forexample,thesentence
P(NoonTemp=x) = Uniform (x)
[18C,26C]
expresses thebelief that the temperature at noon isdistributed uniformly between 18and 26
PROBABILITY degreesCelsius. Wecallthisaprobabilitydensityfunction.
DENSITYFUNCTION
Probability density functions (sometimes called pdfs) differ in meaning from discrete
distributions. Saying that the probability density is uniform from 18C to 26C means that
there is a 100% chance that the temperature will fall somewhere in that 8C-wide region
anda50%chancethatitwillfallinany4C-wideregion,andsoon. Wewritetheprobability
densityforacontinuousrandomvariableX atvaluexasP(X=x)orjustP(x);theintuitive
definition ofP(x)istheprobability that X fallswithinanarbitrarily smallregion beginning
atx,dividedbythewidthoftheregion:
P(x) = lim P(x ≤ X ≤ x+dx)/dx .
dx→0
ForNoonTemp wehave
(cid:24)
1 if18C ≤ x ≤ 26C
P(NoonTemp=x) = Uniform (x) = 8C ,
[18C,26C] 0otherwise
where C stands forcentigrade (not for aconstant). In P(NoonTemp=20.18C)= 1 , note
8C
that 1 is not a probability, it is a probability density. The probability that NoonTemp is
8C
exactly 20.18C is zero, because 20.18C is a region of width 0. Some authors use different
symbolsfordiscretedistributions anddensityfunctions; weuseP inbothcases,sinceconfu-
sionseldomarisesandtheequationsareusuallyidentical. Notethatprobabilities areunitless
numbers,whereasdensityfunctionsaremeasuredwithaunit,inthiscasereciprocaldegrees.
In addition to distributions on single variables, we need notation for distributions on
multiple variables. Commas are used for this. For example, P(Weather,Cavity) denotes
the probabilities of all combinations of the values of Weather and Cavity. This is a 4×2
JOINTPROBABILITY table of probabilities called the joint probability distribution ofWeather and Cavity. We
DISTRIBUTION
can also mix variables with and without values; P(sunny,Cavity) would be a two-element
vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.
ThePnotation makes certain expressions much moreconcise than they might otherwise be.
Forexample,theproduct rulesforallpossible valuesof Weather andCavity canbewritten
asasingleequation:
P(Weather,Cavity) = P(Weather |Cavity)P(Cavity),
488 Chapter 13. Quantifying Uncertainty
insteadofasthese4×2=8equations (usingabbreviations W andC):
P(W =sunny∧C=true)=P(W =sunny|C=true)P(C=true)
P(W =rain ∧C=true)=P(W =rain|C=true)P(C=true)
P(W =cloudy ∧C=true)=P(W =cloudy|C=true)P(C=true)
P(W =snow ∧C=true)=P(W =snow|C=true)P(C=true)
P(W =sunny∧C=false)=P(W =sunny|C=false)P(C=false)
P(W =rain ∧C=false)=P(W=rain|C=false)P(C=false)
P(W =cloudy ∧C=false)=P(W =cloudy|C=false)P(C=false)
P(W =snow ∧C=false)=P(W =snow|C=false)P(C=false).
As a degenerate case, P(sunny,cavity) has no variables and thus is a one-element vec-
tor that is the probability of a sunny day with a cavity, which could also be written as
P(sunny,cavity)orP(sunny∧cavity). WewillsometimesusePnotationtoderiveresults
aboutindividualP values,andwhenwesay“P(sunny)=0.6”itisreallyanabbreviationfor
“P(sunny)istheone-element vector(cid:16)0.6(cid:17),whichmeansthatP(sunny)=0.6.”
Nowwehave defined asyntax forpropositions andprobability assertions andwehave
givenpartofthesemantics: Equation(13.2)definestheprobabilityofapropositionasthesum
of the probabilities of worlds in which it holds. To complete the semantics, we need to say
whattheworldsareandhowtodeterminewhetherapropositionholdsinaworld. Weborrow
this part directly from the semantics of propositional logic, as follows. A possible world is
definedtobeanassignmentofvaluestoalloftherandomvariablesunderconsideration. Itis
easytoseethatthisdefinitionsatisfiesthebasicrequirementthatpossibleworldsbemutually
exclusive and exhaustive (Exercise 13.5). For example, if the random variables are Cavity,
Toothache, and Weather, then there are 2×2×4=16 possible worlds. Furthermore, the
truth of any given proposition, no matter how complex, can be determined easily in such
worldsusingthesamerecursive definitionoftruthasforformulasinpropositional logic.
Fromthepreceding definition ofpossible worlds, itfollows that aprobability modelis
completelydeterminedbythejointdistributionforalloftherandomvariables—theso-called
FULLJOINT full joint probability distribution. For example, if the variables are Cavity, Toothache,
PROBABILITY
DISTRIBUTION
and Weather, then the full joint distribution is given by P(Cavity,Toothache,Weather).
Thisjoint distribution canberepresented asa 2×2×4tablewith16entries. Because every
proposition’s probability is a sum over possible worlds, a full joint distribution suffices, in
principle, forcalculating theprobability ofanyproposition.
13.2.3 Probabilityaxiomsand theirreasonableness
The basic axioms of probability (Equations (13.1) and (13.2)) imply certain relationships
amongthedegreesofbeliefthatcanbeaccordedtologically relatedpropositions. Forexam-
ple, wecan derive the familiar relationship between the probability of a proposition and the
probability ofitsnegation:
(cid:2)
P(¬a) = P(ω) byEquation(13.2)
(cid:2)ω∈¬a (cid:2) (cid:2)
= P(ω)+ P(ω)− P(ω)
(cid:2)ω∈¬a (cid:2) ω∈a ω∈a
= P(ω)− P(ω) grouping thefirsttwoterms
ω∈Ω ω∈a
= 1−P(a) by(13.1)and(13.2).
Section13.2. BasicProbability Notation 489
We can also derive the well-known formula for the probability of a disjunction, sometimes
INCLUSION–
calledtheinclusion–exclusion principle:
EXCLUSION
PRINCIPLE
P(a∨b)= P(a)+P(b)−P(a∧b). (13.4)
Thisruleiseasilyrememberedbynotingthatthecaseswhereaholds,togetherwiththecases
where b holds, certainly cover all the cases where a∨b holds; but summing the two sets of
casescountstheirintersection twice,soweneedtosubtract P(a∧b). Theproofisleftasan
exercise(Exercise13.6).
KOLMOGOROV’S Equations(13.1)and(13.4)areoftencalledKolmogorov’saxiomsinhonoroftheRus-
AXIOMS
sianmathematician AndreiKolmogorov, whoshowedhowtobuilduptherestofprobability
theory from this simple foundation and how to handle the difficulties caused by continuous
variables.2 While Equation (13.2) has a definitional flavor, Equation (13.4) reveals that the
axioms really do constrain the degrees of belief an agent can have concerning logically re-
lated propositions. This is analogous to the fact that a logical agent cannot simultaneously
believe A, B, and ¬(A∧B), because there is no possible world in which all three are true.
Withprobabilities, however,statementsrefernottotheworlddirectly, buttotheagent’s own
stateofknowledge. Why,then,cananagentnotholdthefollowingsetofbeliefs(eventhough
theyviolateKolmogorov’s axioms)?
P(a) = 0.4 P(a∧b) = 0.0
(13.5)
P(b) = 0.3 P(a∨b) = 0.8.
This kind of question has been the subject of decades of intense debate between those who
advocate the use of probabilities as the only legitimate form for degrees of belief and those
whoadvocate alternative approaches.
One argument for the axioms of probability, first stated in 1931 by Bruno de Finetti
(andtranslatedintoEnglishindeFinetti(1993)),isasfollows: Ifanagenthassomedegreeof
belief inaproposition a,then theagentshould beable tostate oddsatwhichitisindifferent
to a bet for or against a.3 Think of it as a game between two agents: Agent 1 states, “my
degree of belief in event a is 0.4.” Agent 2 is then free to choose whether to wager for or
against aatstakes thatareconsistent withthestated degreeofbelief. Thatis, Agent2could
choose toaccept Agent 1’sbet that awilloccur, offering $6against Agent 1’s$4. OrAgent
2 could accept Agent 1’s bet that ¬a will occur, offering $4 against Agent 1’s $6. Then we
observe the outcome of a, and whoever is right collects the money. If an agent’s degrees of
belief do not accurately reflect the world, then you would expect that it would tend to lose
moneyoverthelongruntoanopposing agentwhosebeliefsmoreaccurately reflectthestate
oftheworld.
But de Finetti proved something much stronger: If Agent 1 expresses a set of degrees
of belief that violate the axioms of probability theory then there is a combination of bets by
Agent 2that guarantees that Agent 1 will lose money every time. Forexample, suppose that
Agent1hasthesetofdegreesofbelieffromEquation(13.5). Figure13.2showsthatifAgent
2 ThedifficultiesincludetheVitaliset,awell-definedsubsetoftheinterval[0,1]withnowell-definedsize.
3 Onemightarguethattheagent’spreferencesfordifferentbankbalancesaresuchthatthepossibilityoflosing
$1isnotcounterbalancedbyanequalpossibilityofwinning$1.Onepossibleresponseistomakethebetamounts
smallenoughtoavoidthisproblem.Savage’sanalysis(1954)circumventstheissuealtogether.
490 Chapter 13. Quantifying Uncertainty
2 chooses to bet $4 on a, $3 on b, and $2 on ¬(a∨ b), then Agent 1 always loses money,
regardless of the outcomes for a and b. De Finetti’s theorem implies that no rational agent
canhavebeliefsthatviolatetheaxiomsofprobability.
Agent1 Agent2 Outcomesandpayoffs toAgent1
Proposition Belief Bet Stakes a,b a,¬b ¬a,b ¬a,¬b
a 0.4 a 4to6 –6 –6 4 4
b 0.3 b 3to7 –7 3 –7 3
a∨b 0.8 ¬(a∨b) 2to8 2 2 2 –8
–11 –1 –1 –1
Figure13.2 Because Agent1 hasinconsistentbeliefs, Agent2is able to devisea set of
betsthatguaranteesalossforAgent1,nomatterwhattheoutcomeofaandb.
One common objection to de Finetti’s theorem is that this betting game is rather con-
trived. Forexample, what ifone refuses to bet? Does that end the argument? Theansweris
that the betting game is an abstract model for the decision-making situation in which every
agent is unavoidably involved at every moment. Every action (including inaction) is a kind
ofbet, andeveryoutcome canbeseenasapayoff ofthebet. Refusing tobetislikerefusing
toallowtimetopass.
Otherstrongphilosophicalargumentshavebeenputforwardfortheuseofprobabilities,
mostnotably those ofCox(1946), Carnap (1950), andJaynes (2003). Theyeach construct a
set of axioms for reasoning with degrees of beliefs: no contradictions, correspondence with
ordinary logic (forexample, ifbelief in Agoes up, thenbelief in¬Amustgodown), andso
on. The only controversial axiom is that degrees of belief must be numbers, or at least act
likenumbersinthattheymustbetransitive(ifbeliefinAisgreaterthanbeliefinB,whichis
greaterthan beliefin C,then belief inAmustbegreater than C)andcomparable (thebelief
inAmustbeoneofequalto,greaterthan,orlessthanbeliefinB). Itcanthenbeprovedthat
probability istheonlyapproach thatsatisfiestheseaxioms.
The world being the way it is, however, practical demonstrations sometimes speak
louder than proofs. The success of reasoning systems based on probability theory has been
muchmoreeffectiveinmakingconverts. Wenowlookathowtheaxiomscanbedeployedto
makeinferences.
13.3 INFERENCE USING FULL JOINT DISTRIBUTIONS
PROBABILISTIC Inthissectionwedescribeasimplemethodforprobabilisticinference—that is,thecompu-
INFERENCE
tation of posterior probabilities for query propositions given observed evidence. We use the
fulljointdistributionasthe“knowledgebase”fromwhichanswerstoallquestionsmaybede-
rived. Alongthewaywealsointroduce severalusefultechniques formanipulating equations
involving probabilities.
Section13.3. Inference UsingFullJointDistributions 491
WHERE DO PROBABILITIES COME FROM?
There has been endless debate over the source and status of probability numbers.
The frequentist position is that the numbers can come only from experiments: if
we test 100 people and find that 10 of them have a cavity, then we can say that
the probability of a cavity is approximately 0.1. In this view, the assertion “the
probability ofacavityis0.1”meansthat0.1isthefraction thatwouldbeobserved
in the limit of infinitely many samples. From any finite sample, we can estimate
thetruefractionandalsocalculate howaccurate ourestimateislikelytobe.
The objectivist view is that probabilities are real aspects of the universe—
propensities of objects to behave in certain ways—rather than being just descrip-
tionsofanobserver’s degreeofbelief. Forexample,thefactthatafaircoincomes
up heads with probability 0.5 is a propensity of the coin itself. In this view, fre-
quentist measurements areattempts toobserve these propensities. Mostphysicists
agreethatquantum phenomena areobjectively probabilistic, butuncertainty atthe
macroscopic scale—e.g., in coin tossing—usually arises from ignorance of initial
conditions anddoesnotseemconsistent withthepropensity view.
The subjectivist view describes probabilities as a way of characterizing an
agent’s beliefs, rather than as having any external physical significance. The sub-
jectiveBayesianviewallowsanyself-consistent ascriptionofpriorprobabilitiesto
propositions, buttheninsistsonproperBayesianupdating asevidence arrives.
In the end, even a strict frequentist position involves subjective analysis be-
causeofthereferenceclassproblem: intryingtodeterminetheoutcomeprobabil-
ityofaparticular experiment, thefrequentist hastoplaceitinareference classof
“similar” experiments with known outcome frequencies. I. J. Good (1983, p. 27)
wrote, “every event in life is unique, and every real-life probability that we esti-
mate in practice is that of an event that has never occurred before.” For example,
given a particular patient, a frequentist who wants to estimate the probability of a
cavitywillconsiderareferenceclassofotherpatientswho aresimilarinimportant
ways—age, symptoms, diet—and seewhatproportion ofthem hadacavity. Ifthe
dentistconsiderseverythingthatisknownaboutthepatient—weight tothenearest
gram,haircolor,mother’smaidenname—thenthereferenceclassbecomesempty.
Thishasbeenavexingproblem inthephilosophy ofscience.
The principle of indifference attributed to Laplace (1816) states that propo-
sitions that are syntactically “symmetric” with respect to the evidence should be
accorded equal probability. Various refinements have been proposed, culminating
in the attempt by Carnap and others to develop a rigorous inductive logic, capa-
bleofcomputingthecorrectprobabilityforanyproposition fromanycollectionof
observations. Currently, itisbelieved thatno unique inductive logic exists; rather,
any such logic rests on a subjective prior probability distribution whose effect is
diminished asmoreobservations arecollected.
492 Chapter 13. Quantifying Uncertainty
toothache ¬toothache
catch ¬catch catch ¬catch
cavity 0.108 0.012 0.072 0.008
¬cavity 0.016 0.064 0.144 0.576
Figure13.3 AfulljointdistributionfortheToothache,Cavity,Catch world.
Webeginwithasimpleexample: adomainconsistingofjustthethreeBooleanvariables
Toothache,Cavity,andCatch (thedentist’snastysteelprobecatchesinmytooth). Thefull
jointdistribution isa2×2×2tableasshowninFigure13.3.
Noticethattheprobabilities inthejointdistributionsumto1,asrequiredbytheaxioms
ofprobability. NoticealsothatEquation(13.2)givesusadirectwaytocalculatetheprobabil-
ityofanyproposition, simpleorcomplex: simplyidentifythosepossibleworldsinwhichthe
proposition istrue and add uptheir probabilities. Forexample, there are six possible worlds
inwhichcavity ∨toothache holds:
P(cavity ∨toothache)= 0.108+0.012+0.072+0.008+0.016+0.064 = 0.28.
One particularly common task is to extract the distribution over some subset of variables or
a single variable. Forexample, adding the entries in the first row gives the unconditional or
MARGINAL marginalprobability4 ofcavity:
PROBABILITY
P(cavity) = 0.108+0.012+0.072+0.008 = 0.2.
Thisprocess iscalled marginalization, orsummingout—because wesumuptheprobabil-
MARGINALIZATION
ities for each possible value of the other variables, thereby taking them out of the equation.
Wecanwritethefollowinggeneralmarginalization rulefor anysetsofvariables YandZ:
(cid:12)
P(Y)= P(Y,z), (13.6)
(cid:2)
z∈Z
where
z∈Z
meanstosumoverallt(cid:2)hepossiblecombinationsofvaluesofthesetofvariables
Z. Wesometimesabbreviate thisas ,leaving Zimplicit. Wejustusedtheruleas
(cid:12) z
P(Cavity)= P(Cavity,z). (13.7)
z∈{Catch,Toothache}
A variant of this rule involves conditional probabilities instead of joint probabilities, using
theproductrule:
(cid:12)
P(Y)= P(Y|z)P(z). (13.8)
z
Thisruleiscalledconditioning. Marginalization andconditioning turnouttobeusefulrules
CONDITIONING
forallkindsofderivations involving probability expressions.
In most cases, we are interested in computing conditional probabilities of some vari-
ables, given evidence about others. Conditional probabilities can be found by first using
4 Socalledbecauseofacommonpracticeamongactuariesofwritingthesumsofobservedfrequenciesinthe
marginsofinsurancetables.
Section13.3. Inference UsingFullJointDistributions 493
Equation(13.3)toobtainanexpressionintermsofunconditional probabilities andtheneval-
uating the expression from the full joint distribution. For example, we can compute the
probability ofacavity, givenevidence ofatoothache, asfollows:
P(cavity ∧toothache)
P(cavity|toothache) =
P(toothache)
0.108+0.012
= = 0.6.
0.108+0.012+0.016+0.064
Justtocheck,wecanalsocomputetheprobability thatthere isnocavity, givenatoothache:
P(¬cavity ∧toothache)
P(¬cavity|toothache) =
P(toothache)
0.016+0.064
= =0.4.
0.108+0.012+0.016+0.064
The two values sum to 1.0, as they should. Notice that in these two calculations the term
1/P(toothache) remains constant, no matter which value of Cavity we calculate. In fact,
it can be viewed as a normalization constant for the distribution P(Cavity|toothache),
NORMALIZATION
ensuring that it adds up to 1. Throughout the chapters dealing with probability, we use α to
denotesuchconstants. Withthisnotation, wecanwritethetwopreceding equations inone:
P(Cavity|toothache)= αP(Cavity,toothache)
= α[P(Cavity,toothache,catch)+P(Cavity,toothache,¬catch)]
= α[(cid:16)0.108,0.016(cid:17)+(cid:16)0.012,0.064(cid:17)] = α(cid:16)0.12,0.08(cid:17) = (cid:16)0.6,0.4(cid:17).
In other words, we can calculate P(Cavity|toothache) even if we don’t know the value of
P(toothache)! Wetemporarilyforgetaboutthefactor1/P(toothache)andaddupthevalues
forcavity and¬cavity,getting0.12and0.08. Thosearethecorrect relativeproportions, but
they don’t sum to 1, so we normalize them by dividing each one by 0.12 + 0.08, getting
thetrue probabilities of0.6and 0.4. Normalization turns outtobeauseful shortcut inmany
probabilitycalculations,bothtomakethecomputationeasierandtoallowustoproceedwhen
someprobability assessment (suchasP(toothache))isnotavailable.
From the example, we can extract a general inference procedure. We begin with the
case in whichthe query involves asingle variable, X (Cavity in theexample). LetEbe the
listofevidencevariables(justToothache intheexample),letebethelistofobservedvalues
forthem, and let Ybe theremaining unobserved variables (just Catch intheexample). The
queryisP(X|e)andcanbeevaluatedas
(cid:12)
P(X|e)= αP(X,e)= α P(X,e,y), (13.9)
y
where the summation is over all possible ys (i.e., all possible combinations of values of the
unobserved variables Y). Notice that together the variables X, E, andYconstitute the com-
pletesetofvariablesforthedomain,soP(X,e,y)issimplyasubsetofprobabilities fromthe
fulljointdistribution.
Giventhe fulljoint distribution toworkwith, Equation (13.9) can answerprobabilistic
queries for discrete variables. It does not scale well, however: for a domain described by n
Booleanvariables,itrequiresaninputtableofsizeO(2n)andtakesO(2n)timetoprocessthe
494 Chapter 13. Quantifying Uncertainty
table. Ina realistic problem wecould easily have n > 100, making O(2n) impractical. The
fulljointdistributionintabularformisjustnotapracticaltoolforbuildingreasoningsystems.
Instead,itshouldbeviewedasthetheoreticalfoundationonwhichmoreeffectiveapproaches
maybebuilt,justastruthtablesformedatheoreticalfoundationformorepracticalalgorithms
like DPLL. The remainder of this chapter introduces some of the basic ideas required in
preparation forthedevelopment ofrealistic systemsinChapter14.
13.4 INDEPENDENCE
Letusexpandthefulljointdistribution inFigure13.3byaddingafourthvariable, Weather.
Thefulljointdistribution thenbecomes P(Toothache,Catch,Cavity,Weather),whichhas
2×2×2×4 = 32 entries. It contains four “editions” of the table shown in Figure 13.3,
one for each kind of weather. What relationship do these editions have to each other and to
theoriginalthree-variable table? Forexample,howare P(toothache,catch,cavity,cloudy)
andP(toothache,catch,cavity)related? Wecanusetheproductrule:
P(toothache,catch,cavity,cloudy)
= P(cloudy|toothache,catch,cavity)P(toothache,catch,cavity).
Now, unless one is in the deity business, one should not imagine that one’s dental problems
influence theweather. Andforindoordentistry, atleast, it seemssafetosaythattheweather
doesnotinfluencethedentalvariables. Therefore, thefollowingassertion seemsreasonable:
P(cloudy|toothache,catch,cavity) = P(cloudy). (13.10)
Fromthis,wecandeduce
P(toothache,catch,cavity,cloudy) = P(cloudy)P(toothache,catch,cavity).
AsimilarequationexistsforeveryentryinP(Toothache,Catch,Cavity,Weather). Infact,
wecanwritethegeneralequation
P(Toothache,Catch,Cavity,Weather)=P(Toothache,Catch,Cavity)P(Weather).
Thus, the 32-element table for four variables can be constructed from one 8-element table
andone4-elementtable. Thisdecomposition isillustrated schematically inFigure13.4(a).
The property we used in Equation (13.10) is called independence (also marginal in-
INDEPENDENCE
dependenceandabsoluteindependence). Inparticular, theweatherisindependent ofone’s
dentalproblems. Independence betweenpropositions aandbcanbewrittenas
P(a|b)=P(a) or P(b|a)=P(b) or P(a∧b)=P(a)P(b). (13.11)
All these forms are equivalent (Exercise 13.12). Independence between variables X and Y
canbewrittenasfollows(again, theseareallequivalent):
P(X|Y)=P(X) or P(Y |X)=P(Y) or P(X,Y)=P(X)P(Y).
Independence assertions are usually based on knowledge of the domain. As the toothache–
weather example illustrates, they can dramatically reduce the amount of information nec-
essary to specify the full joint distribution. If the complete set of variables can be divided
Section13.5. Bayes’RuleandItsUse 495
Cavity Coin Coin
1 n
Toothache Catch
Weather
decomposes
decomposes
into
into
Cavity
Toothache Catch Weather
Coin Coin
1 n
(a) (b)
Figure13.4 Twoexamplesoffactoringalargejointdistributionintosmallerdistributions,
using absolute independence. (a) Weather and dental problems are independent. (b) Coin
flipsareindependent.
into independent subsets, then the full joint distribution can be factored into separate joint
distributions on those subsets. For example, the full joint distribution on the outcome of n
independent coin flips, P(C ,...,C ), has 2n entries, but it can be represented as the prod-
1 n
uct of n single-variable distributions P(C ). In a more practical vein, the independence of
i
dentistry andmeteorology isagood thing, because otherwise the practice ofdentistry might
requireintimateknowledgeofmeteorology, andviceversa.
Whentheyareavailable, then,independence assertions can helpinreducingthesizeof
thedomainrepresentation andthecomplexityoftheinference problem. Unfortunately, clean
separation of entire sets of variables by independence is quite rare. Whenever a connection,
however indirect, exists between two variables, independence will fail to hold. Moreover,
evenindependent subsetscanbequitelarge—forexample,dentistrymightinvolve dozensof
diseases and hundreds ofsymptoms, allof which areinterrelated. Tohandle such problems,
weneedmoresubtlemethodsthanthestraightforward conceptofindependence.
13.5 BAYES’ RULE AND ITS USE
Onpage486,wedefinedtheproductrule. Itcanactuallybewrittenintwoforms:
P(a∧b)= P(a|b)P(b) and P(a∧b)= P(b|a)P(a).
Equatingthetworight-hand sidesanddividing byP(a),weget
P(a|b)P(b)
P(b|a) = . (13.12)
P(a)
This equation is known as Bayes’ rule (also Bayes’ law or Bayes’ theorem). This simple
BAYES’RULE
equation underlies mostmodernAIsystemsforprobabilistic inference.
496 Chapter 13. Quantifying Uncertainty
Themore general case ofBayes’ rule formultivalued variables can bewritten in the P
notation asfollows:
P(X|Y)P(Y)
P(Y |X) = ,
P(X)
Asbefore,thisistobetakenasrepresentingasetofequations,eachdealingwithspecificval-
uesofthevariables. Wewillalsohaveoccasiontouseamoregeneralversionconditionalized
onsomebackground evidence e:
P(X|Y,e)P(Y |e)
P(Y |X,e)= . (13.13)
P(X|e)
13.5.1 ApplyingBayes’rule: The simplecase
On the surface, Bayes’ rule does not seem very useful. It allows us to compute the single
term P(b|a) in terms of three terms: P(a|b), P(b), and P(a). That seems like two steps
backwards, but Bayes’ rule is useful in practice because there are many cases where we do
have good probability estimates for these three numbers and need to compute the fourth.
Often, we perceive as evidence the effect of some unknown cause and we would like to
determinethatcause. Inthatcase,Bayes’rulebecomes
P(effect|cause)P(cause)
P(cause|effect)= .
P(effect)
The conditional probability P(effect|cause) quantifies the relationship in the causal direc-
CAUSAL
tion, whereas P(cause|effect)describes the diagnostic direction. In atask such as medical
DIAGNOSTIC
diagnosis, weoften have conditional probabilities on causal relationships (that is, the doctor
knowsP(symptoms|disease))andwanttoderiveadiagnosis, P(disease|symptoms). For
example, a doctor knows that the disease meningitis causes the patient to have a stiff neck,
say, 70% of the time. The doctor also knows some unconditional facts: the prior probabil-
ity that a patient has meningitis is 1/50,000, and the prior probability that any patient has a
stiff neck is 1%. Letting s be the proposition that the patient has a stiff neck and m be the
proposition thatthepatienthasmeningitis, wehave
P(s|m) = 0.7
P(m) = 1/50000
P(s) = 0.01
P(s|m)P(m) 0.7×1/50000
P(m|s) = = = 0.0014. (13.14)
P(s) 0.01
Thatis,weexpectlessthan1in700patientswithastiffnecktohavemeningitis. Noticethat
even though a stiff neck is quite strongly indicated by meningitis (with probability 0.7), the
probabilityofmeningitisinthepatientremainssmall. Thisisbecausethepriorprobabilityof
stiffnecksismuchhigherthanthatofmeningitis.
Section13.3illustratedaprocessbywhichonecanavoidassessingthepriorprobability
of the evidence (here, P(s)) by instead computing a posterior probability for each value of
Section13.5. Bayes’RuleandItsUse 497
thequeryvariable(here, mand¬m)andthennormalizingtheresults. Thesameprocesscan
beappliedwhenusingBayes’rule. Wehave
P(M|s)= α(cid:16)P(s|m)P(m),P(s|¬m)P(¬m)(cid:17).
Thus, to use this approach we need to estimate P(s|¬m) instead of P(s). There is no free
lunch—sometimesthisiseasier,sometimesitisharder. ThegeneralformofBayes’rulewith
normalization is
P(Y |X) = αP(X|Y)P(Y), (13.15)
whereαisthenormalization constant neededtomaketheentriesinP(Y |X)sumto1.
One obvious question to ask about Bayes’ rule is why one might have available the
conditional probability inonedirection, butnottheother. Inthemeningitis domain, perhaps
thedoctorknowsthatastiffneckimpliesmeningitisin1outof5000cases;thatis,thedoctor
has quantitative information in the diagnostic direction from symptoms to causes. Such a
doctor has no need to use Bayes’ rule. Unfortunately, diagnostic knowledge is often more
fragilethancausalknowledge. Ifthereisasuddenepidemicofmeningitis, theunconditional
probability of meningitis, P(m), will go up. The doctor who derived the diagnostic proba-
bility P(m|s)directly from statistical observation ofpatients before theepidemic willhave
noidea howto update the value, butthe doctorwhocomputes P(m|s)from theotherthree
values will see that P(m|s) should go up proportionately with P(m). Most important, the
causalinformation P(s|m)isunaffected bytheepidemic,becauseitsimplyreflectstheway
meningitis works. The use of this kind of direct causal ormodel-based knowledge provides
thecrucialrobustness neededtomakeprobabilistic systemsfeasibleintherealworld.
13.5.2 UsingBayes’rule: Combining evidence
Wehave seen that Bayes’ rule can be useful for answering probabilistic queries conditioned
on one piece of evidence—for example, the stiff neck. In particular, we have argued that
probabilisticinformationisoftenavailableintheformP(effect|cause). Whathappenswhen
we have two or more pieces of evidence? For example, what can a dentist conclude if her
nastysteelprobecatchesintheachingtoothofapatient? Ifweknowthefulljointdistribution
(Figure13.3),wecanreadofftheanswer:
P(Cavity|toothache ∧catch) = α(cid:16)0.108,0.016(cid:17) ≈ (cid:16)0.871,0.129(cid:17).
We know, however, that such an approach does not scale up to larger numbers of variables.
WecantryusingBayes’ruletoreformulate theproblem:
P(Cavity|toothache ∧catch)
= αP(toothache ∧catch|Cavity)P(Cavity). (13.16)
Forthisreformulation towork,weneedtoknowtheconditional probabilities oftheconjunc-
tiontoothache∧catch foreachvalueofCavity. Thatmightbefeasibleforjusttwoevidence
variables, but again it does not scale up. If there are n possible evidence variables (X rays,
diet,oralhygiene,etc.),thenthereare2n possiblecombinationsofobservedvaluesforwhich
we would need to know conditional probabilities. We might as well go back to using the
full joint distribution. Thisiswhatfirstledresearchers awayfrom probability theory toward
498 Chapter 13. Quantifying Uncertainty
approximate methodsforevidence combination that, whilegiving incorrect answers, require
fewernumberstogiveanyansweratall.
Rather than taking this route, we need to find some additional assertions about the
domain that will enable ustosimplify the expressions. The notion of independenceinSec-
tion13.4provides aclue, butneeds refining. Itwouldbenice ifToothache andCatch were
independent, buttheyarenot: iftheprobe catches inthetooth, thenitislikely thatthetooth
has a cavity and that the cavity causes a toothache. These variables are independent, how-
ever, given thepresence ortheabsence ofacavity. Eachisdirectly caused bythecavity, but
neither has a direct effect on the other: toothache depends on the state of the nerves in the
tooth, whereas the probe’s accuracy depends on the dentist’s skill, to which the toothache is
irrelevant.5 Mathematically, thisproperty iswrittenas
P(toothache ∧catch|Cavity) = P(toothache |Cavity)P(catch|Cavity). (13.17)
CONDITIONAL Thisequationexpressestheconditionalindependenceoftoothache andcatchgivenCavity.
INDEPENDENCE
WecanplugitintoEquation(13.16)toobtaintheprobability ofacavity:
P(Cavity|toothache ∧catch)
= αP(toothache |Cavity)P(catch|Cavity)P(Cavity). (13.18)
Now the information requirements are the same as for inference, using each piece of evi-
dence separately: the prior probability P(Cavity) for the query variable and the conditional
probability ofeacheffect, givenitscause.
ThegeneraldefinitionofconditionalindependenceoftwovariablesX andY,givena
thirdvariable Z,is
P(X,Y |Z)= P(X|Z)P(Y |Z).
Inthedentistdomain,forexample,itseemsreasonabletoassertconditional independence of
thevariables Toothache andCatch,givenCavity:
P(Toothache,Catch|Cavity) = P(Toothache |Cavity)P(Catch|Cavity). (13.19)
NoticethatthisassertionissomewhatstrongerthanEquation(13.17),whichassertsindepen-
dence only for specific values of Toothache and Catch. As with absolute independence in
Equation(13.11), theequivalent forms
P(X|Y,Z)=P(X|Z) and P(Y |X,Z)=P(Y |Z)
can also be used (see Exercise 13.17). Section 13.4 showed that absolute independence as-
sertionsallowadecomposition ofthefulljointdistribution intomuchsmallerpieces. Itturns
out that the same is true for conditional independence assertions. For example, given the
assertion inEquation(13.19), wecanderiveadecomposition asfollows:
P(Toothache,Catch,Cavity)
= P(Toothache,Catch|Cavity)P(Cavity) (productrule)
= P(Toothache |Cavity)P(Catch|Cavity)P(Cavity) (using13.19).
(Thereadercaneasilycheck thatthisequation doesinfacthold inFigure13.3.) Inthisway,
theoriginal large table isdecomposed intothree smallertables. Theoriginal table has seven
5 Weassumethatthepatientanddentistaredistinctindividuals.
Section13.6. TheWumpusWorldRevisited 499
independent numbers (23=8 entries in the table, but they must sum to 1, so 7 are indepen-
dent). The smaller tables contain five independent numbers (for a conditional probability
distributions such as P(T|C there are tworowsof twonumbers, and each row sums to1, so
that’stwoindependent numbers; forapriordistribution likeP(C)thereisonlyoneindepen-
dent number). Going from seven to five might not seem like a major triumph, but the point
is that, for n symptoms that are all conditionally independent given Cavity, the size of the
representation grows as O(n) instead of O(2n). That means that conditional independence
assertions can allow probabilistic systems to scale up; moreover, they are much more com-
monly available than absolute independence assertions. Conceptually, Cavity separates
SEPARATION
Toothache and Catch because it is a direct cause of both of them. The decomposition of
largeprobabilistic domainsintoweaklyconnectedsubsets throughconditional independence
isoneofthemostimportantdevelopments intherecenthistoryofAI.
Thedentistryexampleillustratesacommonlyoccurringpatterninwhichasinglecause
directly influences anumberofeffects, allofwhichareconditionally independent, giventhe
cause. Thefulljointdistribution canbewrittenas
(cid:25)
P(Cause,Effect ,...,Effect ) = P(Cause) P(Effect |Cause).
1 n i
i
Such a probability distribution is called a naive Bayes model—“naive” because it is often
NAIVEBAYES
used (as a simplifying assumption) in cases where the “effect” variables are not actually
conditionally independent given the cause variable. (The naive Bayes model is sometimes
called a Bayesian classifier, a somewhat careless usage that has prompted true Bayesians
to call it the idiot Bayes model.) In practice, naive Bayes systems can work surprisingly
well, even when the conditional independence assumption is not true. Chapter 20 describes
methodsforlearningnaiveBayesdistributions fromobservations.
13.6 THE WUMPUS WORLD REVISITED
Wecan combine of the ideas in this chapter to solve probabilistic reasoning problems in the
wumpusworld. (SeeChapter7foracompletedescriptionofthewumpusworld.) Uncertainty
arises in the wumpus world because the agent’s sensors give only partial information about
the world. For example, Figure 13.5 shows a situation in which each of the three reachable
squares—[1,3], [2,2], and [3,1]—might contain a pit. Pure logical inference can conclude
nothing aboutwhichsquare ismostlikelytobesafe,soalogicalagentmighthavetochoose
randomly. Wewillseethataprobabilistic agentcandomuchbetterthanthelogicalagent.
Ouraimistocalculatetheprobabilitythateachofthethree squarescontainsapit. (For
this example we ignore the wumpus and the gold.) The relevant properties of the wumpus
world are that (1) a pit causes breezes in all neighboring squares, and (2) each square other
than [1,1] contains a pit with probability 0.2. The first step is to identify the set of random
variables weneed:
• As in the propositional logic case, we want one Boolean variable P for each square,
ij
whichistrueiffsquare[i,j]actuallycontains apit.
500 Chapter 13. Quantifying Uncertainty
1,4 2,4 3,4 4,4 1,4 2,4 3,4 4,4
1,3 2,3 3,3 4,3 1,3 2,3 3,3 4,3
OTHER
QUERY
1,2 2,2 3,2 4,2 1,2 2,2 3,2 4,2
B
OK
FRONTIER
1,1 2,1 3,1 4,1 1,1 2,1 3,1 4,1
KNOWN
B
OK OK
(a) (b)
Figure13.5 (a)Afterfindingabreezeinboth[1,2]and[2,1],theagentisstuck—thereis
nosafeplacetoexplore. (b)DivisionofthesquaresintoKnown,Frontier,andOther,for
aqueryabout[1,3].
• Wealso have Boolean variables B that are true iffsquare [i,j] is breezy; weinclude
ij
thesevariables onlyfortheobservedsquares—in thiscase, [1,1],[1,2],and[2,1].
Thenextstep istospecify thefulljoint distribution, P(P ,...,P ,B ,B ,B ). Ap-
1,1 4,4 1,1 1,2 2,1
plyingtheproductrule,wehave
P(P ,...,P ,B ,B ,B )=
1,1 4,4 1,1 1,2 2,1
P(B ,B ,B | P ,...,P )P(P ,...,P ).
1,1 1,2 2,1 1,1 4,4 1,1 4,4
This decomposition makes it easy to see what the joint probability values should be. The
first term is the conditional probability distribution of a breeze configuration, given a pit
configuration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise. The
second term is the prior probability of a pit configuration. Each square contains a pit with
probability 0.2,independently oftheothersquares; hence,
(cid:25)4,4
P(P ,...,P )= P(P ). (13.20)
1,1 4,4 i,j
i,j=1,1
Foraparticularconfiguration withexactly npits,P(P ,...,P
)=0.2n×0.816−n.
1,1 4,4
In the situation in Figure 13.5(a), the evidence consists of the observed breeze (or its
absence)ineachsquarethatisvisited, combinedwiththefactthateachsuchsquarecontains
nopit. Weabbreviatethesefactsasb=¬b ∧b ∧b andknown=¬p ∧¬p ∧¬p .
1,1 1,2 2,1 1,1 1,2 2,1
Weareinterested inanswering queries such as P(P |known,b): how likely isit that [1,3]
1,3
contains apit,giventheobservations sofar?
Toanswerthisquery, wecanfollow thestandard approach ofEquation (13.9),namely,
summing over entries from the full joint distribution. Let Unknown be the set of P vari-
i,j
Section13.6. TheWumpusWorldRevisited 501
ables forsquares otherthan the Known squares and the query square [1,3]. Then, by Equa-
tion(13.9),wehave
(cid:12)
P(P |known,b) = α P(P ,unknown,known,b).
1,3 1,3
unknown
The full joint probabilities have already been specified, so we are done—that is, unless we
care about computation. There are 12 unknown squares; hence the summation contains
212=4096terms. Ingeneral,thesummationgrowsexponentiallywiththenumberofsquares.
Surely, one might ask, aren’t the other squares irrelevant? How could [4,4] affect
whether [1,3] has a pit? Indeed, this intuition is correct. Let Frontier be the pit variables
(other than the query variable) that are adjacent to visited squares, in this case just [2,2] and
[3,1]. Also,letOther bethepitvariablesfortheotherunknownsquares;inthiscase,thereare
10othersquares,asshowninFigure13.5(b). Thekeyinsight isthattheobservedbreezesare
conditionally independent of the other variables, given the known, frontier, and query vari-
ables. Touse the insight, wemanipulate the query formula into aform inwhich the breezes
areconditioned onalltheothervariables, andthenweapply conditional independence:
P(P |known,b)
1,3 (cid:12)
= α P(P ,known,b,unknown) (byEquation(13.9))
1,3
un(cid:12)known
= α P(b|P ,known,unknown)P(P ,known,unknown)
1,3 1,3
unknown
(bytheproductrule)
(cid:12) (cid:12)
= α P(b|known,P ,frontier,other)P(P ,known,frontier,other)
1,3 1,3
frontierother
(cid:12) (cid:12)
= α P(b|known,P ,frontier)P(P ,known,frontier,other),
1,3 1,3
frontierother
wherethe finalstepuses conditional independence: bisindependent ofother givenknown,
P , and frontier. Now, the first term in this expression does not depend on the Other
1,3
variables, sowecanmovethesummationinward:
P(P |known,b)
1,3 (cid:12) (cid:12)
= α P(b|known,P ,frontier) P(P ,known,frontier,other).
1,3 1,3
frontier other
By independence, as in Equation (13.20), the prior term can be factored, and then the terms
canbereordered:
P(P |known,b)
1,3 (cid:12) (cid:12)
= α P(b|known,P ,frontier) P(P )P(known)P(frontier)P(other)
1,3 1,3
frontier other
(cid:12) (cid:12)
= αP(known)P(P ) P(b|known,P ,frontier)P(frontier) P(other)
1,3 1,3
frontier other
(cid:12)
= α (cid:2) P(P ) P(b|known,P ,frontier)P(frontier),
1,3 1,3
frontier
502 Chapter 13. Quantifying Uncertainty
1,3 1,3 1,3 1,3 1,3
1,2 2,2 1,2 2,2 1,2 2,2 1,2 2,2 1,2 2,2
B B B B B
OK OK OK OK OK
1,1 2,1 3,1 1,1 2,1 3,1 1,1 2,1 3,1 1,1 2,1 3,1 1,1 2,1 3,1
B B B B B
OK OK OK OK OK OK OK OK OK OK
0.2 x 0.2 = 0.04 0.2 x 0.8 = 0.16 0.8 x 0.2 = 0.16 0.2 x 0.2 = 0.04 0.2 x 0.8 = 0.16
(a) (b)
Figure 13.6 Consistent models for the frontier variables P 2,2 and P 3,1 , showing
P(frontier) for each model: (a) three models with P 1,3 =true showing two or three pits,
and(b)twomodelswithP 1,3 =false showingoneortwopits.
where the last step folds P(known) into the normalizing constant and uses the fact that
(cid:2)
P(other)equals1.
other
Now, there are just four terms in the summation over the frontier variables P and
2,2
P . The use of independence and conditional independence has completely eliminated the
3,1
othersquaresfromconsideration.
Noticethattheexpression P(b|known,P ,frontier)is1whenthefrontierisconsis-
1,3
tentwiththebreezeobservations, and0otherwise. Thus,foreachvalueofP ,wesumover
1,3
the logical models for the frontier variables that are consistent with the known facts. (Com-
pare with the enumeration over models in Figure 7.5 on page 241.) The models and their
associated priorprobabilities—P(frontier)—areshowninFigure13.6. Wehave
P(P |known,b) = α (cid:2)(cid:16)0.2(0.04+0.16+0.16), 0.8(0.04+0.16)(cid:17)≈ (cid:16)0.31,0.69(cid:17).
1,3
Thatis,[1,3](and[3,1]bysymmetry)containsapitwithroughly31%probability. Asimilar
calculation, which the reader might wish to perform, shows that [2,2] contains a pit with
roughly 86% probability. The wumpus agent should definitely avoid [2,2]! Note that our
logicalagentfromChapter7didnotknowthat[2,2]wasworse thantheothersquares. Logic
cantellusthatitisunknownwhetherthereisapitin[2,2],butweneedprobability totellus
howlikelyitis.
What this section has shown is that even seemingly complicated problems can be for-
mulated precisely in probability theory and solved with simple algorithms. To get efficient
solutions, independence and conditional independence relationships can be used to simplify
the summations required. These relationships often correspond to ournatural understanding
ofhowtheproblemshouldbedecomposed. Inthenextchapter, wedevelopformalrepresen-
tations for such relationships as well as algorithms that operate on those representations to
perform probabilistic inference efficiently.
Section13.7. Summary 503
13.7 SUMMARY
Thischapterhassuggestedprobabilitytheoryasasuitablefoundationforuncertainreasoning
andprovided agentleintroduction toitsuse.
• Uncertaintyarisesbecauseofbothlazinessandignorance. Itisinescapableincomplex,
nondeterministic, orpartiallyobservable environments.
• Probabilitiesexpresstheagent’sinabilitytoreachadefinitedecisionregardingthetruth
ofasentence. Probabilities summarizetheagent’sbeliefs relativetotheevidence.
• Decisiontheorycombinestheagent’sbeliefsanddesires,definingthebestactionasthe
onethatmaximizesexpected utility.
• Basicprobabilitystatementsincludepriorprobabilitiesandconditionalprobabilities
oversimpleandcomplexpropositions.
• Theaxiomsofprobability constrain thepossibleassignments ofprobabilities topropo-
sitions. Anagentthatviolates theaxiomsmustbehaveirrationally insomecases.
• The full joint probability distribution specifies the probability of each complete as-
signment of values to random variables. It is usually too large to create or use in its
explicitform,butwhenitisavailableitcanbeusedtoanswerqueriessimplybyadding
upentriesforthepossible worldscorresponding tothequerypropositions.
• Absoluteindependencebetween subsets ofrandom variables allowsthe fulljoint dis-
tribution tobefactored intosmallerjointdistributions, greatly reducing itscomplexity.
Absoluteindependence seldomoccursinpractice.
• Bayes’ rule allows unknown probabilities to be computed from known conditional
probabilities, usuallyinthecausaldirection. ApplyingBayes’rulewithmanypiecesof
evidencerunsintothesamescalingproblemsasdoesthefull jointdistribution.
• Conditionalindependencebrought about bydirect causal relationships inthedomain
might allow the full joint distribution to be factored into smaller, conditional distri-
butions. The naive Bayes model assumes the conditional independence of all effect
variables, givenasinglecausevariable, andgrowslinearly withthenumberofeffects.
• Awumpus-worldagentcancalculateprobabilities forunobservedaspectsoftheworld,
therebyimprovingonthedecisionsofapurelylogicalagent. Conditionalindependence
makesthesecalculations tractable.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Probability theory was invented as a way of analyzing games of chance. In about 850 A.D.
theIndianmathematicianMahaviracaryadescribedhowtoarrangeasetofbetsthatcan’tlose
(what we now call a Dutch book). In Europe, the first significant systematic analyses were
produced by Girolamo Cardano around 1565, although publication wasposthumous (1663).
Bythattime,probability hadbeenestablished asamathematical discipline duetoaseriesof
504 Chapter 13. Quantifying Uncertainty
results established in a famous correspondence between Blaise Pascal and Pierre de Fermat
in1654. Aswithprobabilityitself,theresultswereinitiallymotivatedbygamblingproblems
(see Exercise 13.9). The first published textbook on probability was De Ratiociniis in Ludo
Aleae (Huygens, 1657). The “laziness and ignorance” view of uncertainty was described
by John Arbuthnot in the preface of his translation of Huygens (Arbuthnot, 1692): “It is
impossibleforaDie,withsuchdetermin’dforceanddirection,nottofallonsuchdetermin’d
side, only I don’t know the force and direction which makes it fall on such determin’d side,
andtherefore IcallitChance,whichisnothing butthewantofart...”
Laplace(1816) gaveanexceptionally accurate andmodernoverview ofprobability; he
was the first to use the example “take two urns, A and B,the first containing fourwhite and
twoblackballs,...” TheRev.ThomasBayes(1702–1761) introduced theruleforreasoning
about conditional probabilities that was named after him (Bayes, 1763). Bayes only con-
sidered the case of uniform priors; it was Laplace who independently developed the general
case. Kolmogorov (1950, firstpublished inGermanin1933) presented probability theory in
a rigorously axiomatic framework for the first time. Re´nyi (1970) later gave an axiomatic
presentation thattookconditional probability, ratherthanabsolute probability, asprimitive.
Pascalusedprobabilityinwaysthatrequiredboththeobjectiveinterpretation,asaprop-
erty ofthe world based onsymmetry orrelative frequency, and the subjective interpretation,
basedondegreeofbelief—theformerinhisanalysesofprobabilitiesingamesofchance,the
latter in the famous “Pascal’s wager” argument about the possible existence of God. How-
ever, Pascal did not clearly realize the distinction between these two interpretations. The
distinction wasfirstdrawnclearlybyJamesBernoulli(1654–1705).
Leibnizintroduced the“classical” notion ofprobability asaproportion ofenumerated,
equally probable cases,whichwasalsousedbyBernoulli, although itwasbrought topromi-
nence byLaplace (1749–1827). Thisnotion isambiguous between thefrequency interpreta-
tionandthesubjective interpretation. Thecasescanbethought tobeequally probable either
because of a natural, physical symmetry between them, or simply because we do not have
any knowledge that would lead us to consider one more probable than another. The use of
this latter, subjective consideration to justify assigning equal probabilities is known as the
PRINCIPLEOF principle ofindifference. Theprinciple is often attributed to Laplace, but he neverisolated
INDIFFERENCE
the principle explicitly. George Boole and John Venn both referred to it as the principle of
PRINCIPLEOF
insufficientreason;themodernnameisduetoKeynes(1921).
INSUFFICIENT
REASON
The debate between objectivists and subjectivists became sharper in the 20th century.
Kolmogorov (1963), R. A. Fisher (1922), and Richard von Mises (1928) were advocates of
therelativefrequencyinterpretation. KarlPopper’s(1959,firstpublishedinGermanin1934)
“propensity” interpretation traces relative frequencies to an underlying physical symmetry.
Frank Ramsey (1931), Bruno de Finetti (1937), R. T. Cox (1946), Leonard Savage (1954),
Richard Jeffrey (1983), and E. T. Jaynes (2003) interpreted probabilities as the degrees of
belief of specific individuals. Their analyses of degree of belief were closely tied to utili-
tiesandtobehavior—specifically, tothewillingness toplacebets. RudolfCarnap, following
Leibniz and Laplace, offered a different kind of subjective interpretation of probability—
not as any actual individual’s degree of belief, but as the degree of belief that an idealized
individual should have in a particular proposition a, given a particular body of evidence e.
Bibliographical andHistorical Notes 505
Carnap attempted to go further than Leibniz or Laplace by making this notion of degree of
confirmationmathematicallyprecise,asalogicalrelationbetweenaande. Thestudyofthis
CONFIRMATION
relation was intended to constitute a mathematical discipline called inductive logic, analo-
INDUCTIVELOGIC
gous to ordinary deductive logic (Carnap, 1948, 1950). Carnap was not able to extend his
inductivelogicmuchbeyondthepropositional case,andPutnam(1963)showedbyadversar-
ialargumentsthatsomefundamentaldifficultieswouldpreventastrictextensiontolanguages
capableofexpressing arithmetic.
Cox’stheorem (1946) showsthatanysystem foruncertain reasoning thatmeetshisset
of assumptions is equivalent to probability theory. This gave renewed confidence to those
whoalready favored probability, but others werenot convinced, pointing to theassumptions
(primarily thatbeliefmustberepresented byasinglenumber, andthusthebelief in¬pmust
be a function of the belief in p). Halpern (1999) describes the assumptions and shows some
gaps in Cox’s original formulation. Horn (2003) shows how to patch up the difficulties.
Jaynes(2003)hasasimilarargument thatiseasiertoread.
Thequestionofreferenceclassesiscloselytiedtotheattempttofindaninductivelogic.
Theapproach ofchoosing the “mostspecific” reference class ofsufficient size wasformally
proposed by Reichenbach (1949). Various attempts have been made, notably by Henry Ky-
burg (1977, 1983), to formulate more sophisticated policies in order to avoid some obvious
fallacies that arise with Reichenbach’s rule, but such approaches remain somewhat ad hoc.
MorerecentworkbyBacchus,Grove,Halpern,andKoller(1992)extendsCarnap’smethods
to first-order theories, thereby avoiding many of the difficulties associated with the straight-
forward reference-class method. Kyburg and Teng (2006) contrast probabilistic inference
withnonmonotonic logic.
Bayesian probabilistic reasoning has been used in AI since the 1960s, especially in
medicaldiagnosis. Itwasusednotonlytomakeadiagnosisfromavailableevidence,butalso
to select further questions and tests by using the theory of information value (Section 16.6)
when available evidence was inconclusive (Gorry, 1968; Gorry et al., 1973). One system
outperformed humanexpertsinthediagnosisofacuteabdominalillnesses(deDombaletal.,
1974). Lucas etal.(2004) givesanoverview. Theseearly Bayesian systems suffered from a
number of problems, however. Because they lacked any theoretical model of the conditions
they were diagnosing, they were vulnerable to unrepresentative data occurring in situations
forwhichonlyasmallsamplewasavailable(deDombaletal.,1981). Evenmorefundamen-
tally,becausetheylackedaconciseformalism(suchastheonetobedescribedinChapter14)
for representing and using conditional independence information, they depended on the ac-
quisition, storage, and processing ofenormous tables ofprobabilistic data. Because ofthese
difficulties, probabilistic methodsforcopingwithuncertainty felloutoffavorinAIfromthe
1970stothemid-1980s. Developmentssincethelate1980saredescribedinthenextchapter.
The naive Bayes model for joint distributions has been studied extensively in the pat-
ternrecognitionliteraturesincethe1950s(DudaandHart, 1973). Ithasalsobeenused,often
unwittingly, in information retrieval, beginning with the work of Maron (1961). The proba-
bilisticfoundations ofthistechnique, describedfurther inExercise13.22,wereelucidatedby
Robertson and Sparck Jones (1976). Domingos and Pazzani (1997) provide an explanation
506 Chapter 13. Quantifying Uncertainty
for the surprising success of naive Bayesian reasoning even in domains where the indepen-
denceassumptions areclearlyviolated.
There are many good introductory textbooks on probability theory, including those by
Bertsekas and Tsitsiklis (2008) and Grinstead and Snell (1997). DeGroot and Schervish
(2001) offer a combined introduction to probability and statistics from a Bayesian stand-
point. Richard Hamming’s (1991) textbook gives a mathematically sophisticated introduc-
tiontoprobabilitytheoryfromthestandpointofapropensityinterpretation basedonphysical
symmetry. Hacking (1975) and Hald (1990) coverthe early history of theconcept of proba-
bility. Bernstein(1996)givesanentertaining popularaccount ofthestoryofrisk.
EXERCISES
13.1 Showfromfirstprinciples that P(a|b∧a)= 1.
13.2 Using the axioms of probability, prove that any probability distribution on a discrete
randomvariable mustsumto1.
13.3 Foreachofthefollowingstatements, eitherproveitistrue orgiveacounterexample.
a. IfP(a|b,c) = P(b|a,c),thenP(a|c) = P(b|c)
b. IfP(a|b,c) = P(a),thenP(b|c)= P(b)
c. IfP(a|b)= P(a),thenP(a|b,c) = P(a|c)
13.4 WoulditberationalforanagenttoholdthethreebeliefsP(A)=0.4,P(B)=0.3,and
P(A∨B)=0.5? Ifso,whatrangeofprobabilities wouldberationalforthe agenttoholdfor
A∧B? MakeupatableliketheoneinFigure13.2,andshowhowitsupportsyourargument
about rationality. Then draw another version of the table where P(A ∨B)=0.7. Explain
whyitisrational tohave thisprobability, eventhough thetable showsonecase thatisaloss
and three that just break even. (Hint: what is Agent 1committed to about the probability of
eachofthefourcases,especially thecasethatisaloss?)
13.5 This question deals with the properties of possible worlds, defined on page 488 as
assignments to all random variables. We will work with propositions that correspond to
exactly one possible world because they pin down the assignments of all the variables. In
probability theory, such propositions are called atomic events. For example, with Boolean
ATOMICEVENT
variables X ,X ,X ,theproposition x ∧¬x ∧¬x fixestheassignment ofthevariables;
1 2 3 1 2 3
inthelanguage ofpropositional logic,wewouldsayithasexactlyonemodel.
a. Prove, for the case of n Boolean variables, that any two distinct atomic events are
mutuallyexclusive;thatis,theirconjunction isequivalent tofalse.
b. Provethatthedisjunction ofallpossible atomiceventsislogically equivalent totrue.
c. Provethatanypropositionislogicallyequivalenttothedisjunctionoftheatomicevents
thatentailitstruth.
Exercises 507
13.6 ProveEquation(13.4)fromEquations(13.1)and(13.2).
13.7 Considerthesetofallpossiblefive-cardpokerhandsdealtfairlyfromastandarddeck
offifty-twocards.
a. Howmany atomic events are there in the joint probability distribution (i.e., how many
five-cardhandsarethere)?
b. Whatistheprobability ofeachatomicevent?
c. Whatistheprobability ofbeingdealtaroyalstraight flush? Fourofakind?
13.8 Giventhefulljointdistribution showninFigure13.3,calculatethefollowing:
a. P(toothache).
b. P(Cavity).
c. P(Toothache |cavity).
d. P(Cavity|toothache ∨catch).
13.9 InhisletterofAugust24,1654,Pascalwastryingtoshowhowapotofmoneyshould
beallocated whenagamblinggamemustendprematurely. Imagine agamewhereeachturn
consists of the roll of a die, player E gets a point when the die is even, and player O gets a
point when thedie isodd. Thefirstplayer toget 7points winsthe pot. Suppose thegameis
interrupted with E leading 4–2. How should the money be fairly split in this case? What is
thegeneralformula? (FermatandPascalmadeseveralerrors beforesolvingtheproblem, but
youshouldbeabletogetitrightthefirsttime.)
13.10 Deciding to put probability theory to good use, we encounter a slot machine with
three independent wheels, each producing one of the four symbols BAR, BELL, LEMON, or
CHERRY withequal probability. Theslotmachinehasthefollowingpayout schemeforabet
of1coin(where“?” denotes thatwedon’tcarewhatcomesupforthatwheel):
BAR/BAR/BARpays20coins
BELL/BELL/BELL pays15coins
LEMON/LEMON/LEMON pays5coins
CHERRY/CHERRY/CHERRY pays3coins
CHERRY/CHERRY/? pays2coins
CHERRY/?/? pays1coin
a. Compute the expected “payback” percentage of the machine. In other words, foreach
coinplayed, whatistheexpected coinreturn?
b. Computetheprobability thatplaying theslotmachineonce willresultinawin.
c. Estimate the mean and median number of plays you can expect to make until you go
broke, ifyou start with10coins. Youcanrun asimulation toestimate this, ratherthan
tryingtocomputeanexactanswer.
13.11 Wewishtotransmitann-bitmessagetoareceivingagent. Thebitsinthemessageare
independently corrupted (flipped)during transmission with(cid:2)probability each. Withanextra
paritybitsentalongwiththeoriginalinformation,amessagecanbecorrectedbythereceiver
508 Chapter 13. Quantifying Uncertainty
ifatmostonebitintheentiremessage(includingtheparity bit)hasbeencorrupted. Suppose
wewanttoensurethatthecorrectmessageisreceivedwithprobabilityatleast1−δ. Whatis
themaximumfeasiblevalueofn? Calculatethisvalueforthecase(cid:2)=0.001,δ=0.01.
13.12 Showthatthethreeformsofindependence inEquation(13.11)areequivalent.
13.13 Consider two medical tests, A and B, for a virus. Test A is 95% effective at recog-
nizing theviruswhenitispresent, buthasa10%falsepositiverate(indicating thatthevirus
ispresent,whenitisnot). TestBis90%effectiveatrecognizing thevirus,buthasa5%false
positive rate. The two tests use independent methods of identifying the virus. The virus is
carriedby1%ofallpeople. Saythatapersonistestedforthevirususingonlyoneofthetests,
andthattestcomesbackpositiveforcarryingthevirus. Whichtestreturningpositiveismore
indicative ofsomeonereallycarryingthevirus? Justifyyouranswermathematically.
13.14 Suppose you are given a coin that lands heads with probability x and tails with
probability 1 − x. Are the outcomes of successive flips of the coin independent of each
other given that you know the value of x? Are the outcomes of successive flips of the coin
independent ofeachotherifyoudonotknowthevalueofx? Justifyyouranswer.
13.15 After your yearly checkup, the doctor has bad news and good news. The bad news
is that you tested positive for a serious disease and that the test is 99% accurate (i.e., the
probability of testing positive when you do have the disease is 0.99, as is the probability of
testingnegativewhenyoudon’thavethedisease). Thegoodnewsisthatthisisararedisease,
striking only 1 in 10,000 people of your age. Why is it good news that the disease is rare?
Whatarethechances thatyouactuallyhavethedisease?
13.16 It is quite often useful to consider the effect of some specific propositions in the
context ofsomegeneral background evidence thatremainsfixed,ratherthaninthecomplete
absence of information. The following questions ask you to prove more general versions of
theproductruleandBayes’rule,withrespecttosomebackground evidencee:
a. Provetheconditionalized versionofthegeneralproduct rule:
P(X,Y |e) = P(X|Y,e)P(Y |e).
b. Provetheconditionalized versionofBayes’ruleinEquation(13.13).
13.17 Showthatthestatementofconditional independence
P(X,Y |Z)= P(X|Z)P(Y |Z)
isequivalent toeachofthestatements
P(X|Y,Z) = P(X|Z) and P(B|X,Z) = P(Y |Z).
13.18 Supposeyouaregivenabagcontaining nunbiased coins. Youaretoldthatn−1of
these coins are normal, with heads on one side and tails on the other, whereas one coin is a
fake,withheadsonbothsides.
a. Supposeyoureachintothebag,pickoutacoinatrandom,flipit,andgetahead. What
isthe(conditional) probability thatthecoinyouchoseisthefakecoin?
Exercises 509
b. Suppose you continue flipping the coin for atotal of k times after picking it and see k
heads. Nowwhatistheconditional probability thatyoupickedthefakecoin?
c. Supposeyouwantedtodecidewhetherthechosencoinwasfakebyflippingitk times.
The decision procedure returns fake if all k flips come up heads; otherwise it returns
normal. Whatisthe(unconditional) probability thatthisprocedure makesanerror?
13.19 In this exercise, you will complete the normalization calculation for the meningitis
example. First,makeupasuitable valuefor P(s|¬m),anduseittocalculate unnormalized
valuesforP(m|s)andP(¬m|s)(i.e.,ignoringtheP(s)termintheBayes’ruleexpression,
Equation(13.14)). Nownormalize thesevaluessothattheyaddto1.
13.20 Let X, Y, Z be Boolean random variables. Label the eight entries in the joint dis-
tribution P(X,Y,Z) as a through h. Express the statement that X and Y are conditionally
independent given Z, as a set of equations relating a through h. How many nonredundant
equations arethere?
13.21 (Adapted from Pearl (1988).) Suppose you are a witness to a nighttime hit-and-run
accident involving a taxi in Athens. Alltaxis in Athens are blue orgreen. You swear, under
oath, that the taxi wasblue. Extensive testing showsthat, under thedim lighting conditions,
discrimination betweenblueandgreenis75%reliable.
a. Isitpossible tocalculate themostlikelycolorforthetaxi? (Hint: distinguish carefully
betweentheproposition thatthetaxiisblueandtheproposition thatitappearsblue.)
b. Whatifyouknowthat9outof10Atheniantaxisaregreen?
13.22 Textcategorization isthe taskofassigning agivendocument toone ofafixedsetof
categories on the basis of the text it contains. Naive Bayes models are often used for this
task. Inthesemodels,thequeryvariable isthedocument category, andthe“effect”variables
arethepresenceorabsenceofeachwordinthelanguage; theassumption isthatwordsoccur
independently indocuments, withfrequencies determinedbythedocumentcategory.
a. Explain precisely how such a model can be constructed, given as “training data” a set
ofdocuments thathavebeenassigned tocategories.
b. Explainprecisely howtocategorize anewdocument.
c. Istheconditional independence assumption reasonable? Discuss.
13.23 In our analysis of the wumpus world, we used the fact that each square contains a
pit withprobability 0.2, independently ofthe contents of the othersquares. Suppose instead
that exactly N/5 pits are scattered at random among the N squares other than [1,1]. Are
the variables P and P still independent? What isthe joint distribution P(P ,...,P )
i,j k,l 1,1 4,4
now? Redothecalculation fortheprobabilities ofpitsin[1,3]and[2,2].
13.24 Redotheprobabilitycalculationforpitsin[1,3]and[2,2],assumingthateachsquare
contains a pit with probability 0.01, independent of the other squares. What can you say
abouttherelativeperformance ofalogicalversusaprobabilistic agentinthiscase?
13.25 Implement a hybrid probabilistic agent for the wumpus world, based on the hybrid
agentinFigure7.20andtheprobabilistic inference procedure outlined inthischapter.
14
PROBABILISTIC
REASONING
In which we explain how to build network models to reason under uncertainty
according tothelawsofprobability theory.
Chapter 13 introduced the basic elements of probability theory and noted the importance of
independence and conditional independence relationships in simplifying probabilistic repre-
sentations ofthe world. Thischapter introduces asystematic waytorepresent such relation-
ships explicitly in the form of Bayesian networks. We define the syntax and semantics of
these networks and show how they can be used to capture uncertain knowledge in a natu-
ral and efficient way. We then show how probabilistic inference, although computationally
intractable in the worst case, can be done efficiently in many practical situations. We also
describe a variety of approximate inference algorithms that are often applicable when exact
inferenceisinfeasible. Weexplorewaysinwhichprobabilitytheorycanbeappliedtoworlds
withobjectsandrelations—thatis,tofirst-order,asopposedtopropositional,representations.
Finally,wesurveyalternative approaches touncertain reasoning.
14.1 REPRESENTING KNOWLEDGE IN AN UNCERTAIN DOMAIN
InChapter13,wesawthatthefulljointprobabilitydistributioncanansweranyquestionabout
thedomain,butcanbecomeintractablylargeasthenumberofvariablesgrows. Furthermore,
specifying probabilities forpossible worldsonebyoneisunnatural andtedious.
Wealsosawthatindependenceandconditionalindependencerelationshipsamongvari-
ablescangreatlyreducethenumberofprobabilitiesthatneedtobespecifiedinordertodefine
thefulljointdistribution. ThissectionintroducesadatastructurecalledaBayesiannetwork1
BAYESIANNETWORK
torepresent the dependencies among variables. Bayesian networks can represent essentially
anyfulljointprobability distribution andinmanycasescandosoveryconcisely.
1 Thisisthemostcommonname,buttherearemanysynonyms,includingbeliefnetwork,probabilisticnet-
work, causal network, and knowledge map. In statistics, the term graphical model refers to a somewhat
broaderclassthatincludesBayesiannetworks.AnextensionofBayesiannetworkscalledadecisionnetworkor
influencediagramiscoveredinChapter16.
510
Section14.1. Representing KnowledgeinanUncertainDomain 511
ABayesiannetworkisadirected graph inwhicheachnodeisannotated withquantita-
tiveprobability information. Thefullspecification isasfollows:
1. Eachnodecorresponds toarandomvariable, whichmaybediscrete orcontinuous.
2. Asetofdirectedlinksorarrowsconnectspairsofnodes. Ifthereisanarrowfromnode
X tonodeY,X issaidtobeaparentofY. Thegraphhasnodirectedcycles(andhence
isadirected acyclicgraph, orDAG.
3. EachnodeX hasaconditionalprobabilitydistributionP(X |Parents(X ))thatquan-
i i i
tifiestheeffectoftheparentsonthenode.
Thetopology ofthenetwork—the setofnodesandlinks—specifies theconditional indepen-
dence relationships that hold in the domain, in a way that will be made precise shortly. The
intuitive meaning ofanarrow istypically that X hasadirect influence onY,whichsuggests
thatcausesshouldbeparentsofeffects. Itisusuallyeasyforadomainexperttodecidewhat
directinfluencesexistinthedomain—mucheasier,infact,thanactuallyspecifyingtheprob-
abilities themselves. Once the topology of the Bayesian network is laid out, we need only
specify a conditional probability distribution for each variable, given its parents. We will
see that the combination of the topology and the conditional distributions suffices tospecify
(implicitly) thefulljointdistribution forallthevariables.
RecallthesimpleworlddescribedinChapter13,consisting ofthevariablesToothache,
Cavity, Catch, and Weather. We argued that Weather is independent of the other vari-
ables; furthermore, we argued that Toothache and Catch are conditionally independent,
given Cavity. These relationships are represented by the Bayesian network structure shown
in Figure 14.1. Formally, the conditional independence of Toothache and Catch, given
Cavity,isindicated bytheabsence ofalinkbetweenToothache andCatch. Intuitively, the
network represents the fact that Cavity is a direct cause of Toothache and Catch, whereas
nodirectcausalrelationship existsbetween Toothache andCatch.
Now consider the following example, which is just a little more complex. You have
a new burglar alarm installed at home. It is fairly reliable at detecting a burglary, but also
responds on occasion to minor earthquakes. (This example is due to Judea Pearl, a resident
ofLosAngeles—hencetheacuteinterestinearthquakes.) Youalsohavetwoneighbors,John
and Mary, who have promised to call you at work when they hear the alarm. John nearly
always calls when he hears the alarm, but sometimes confuses the telephone ringing with
Cavity
Weather
Toothache Catch
Figure 14.1 A simple Bayesian network in which Weather is independentof the other
threevariablesandToothache andCatch areconditionallyindependent,givenCavity.
512 Chapter 14. Probabilistic Reasoning
P(B) P(E)
Burglary Earthquake
.001 .002
B E P(A)
t t .95
Alarm
t f .94
f t .29
f f .001
A P(J) A P(M)
JohnCalls MaryCalls
t .90 t .70
f .05 f .01
Figure14.2 AtypicalBayesiannetwork,showingboththetopologyandtheconditional
probabilitytables(CPTs). IntheCPTs, theletters B, E, A, J, andM standforBurglary,
Earthquake,Alarm,JohnCalls,andMaryCalls,respectively.
the alarm and calls then, too. Mary, on the other hand, likes rather loud music and often
misses thealarm altogether. Giventhe evidence ofwhohasor hasnot called, wewouldlike
toestimatetheprobability ofaburglary.
A Bayesian network for this domain appears in Figure 14.2. The network structure
shows that burglary and earthquakes directly affect the probability of the alarm’s going off,
but whether John and Mary call depends only on the alarm. The network thus represents
ourassumptions thattheydonotperceive burglaries directly, theydonotnotice minorearth-
quakes, andtheydonotconferbeforecalling.
The conditional distributions in Figure 14.2 are shown as a conditional probability
CONDITIONAL table, or CPT. (This form of table can be used for discrete variables; other representations,
PROBABILITYTABLE
including those suitable for continuous variables, are described in Section 14.2.) Each row
in a CPT contains the conditional probability of each node value for a conditioning case.
CONDITIONINGCASE
A conditioning case is just a possible combination of values for the parent nodes—a minia-
ture possible world, if you like. Each row must sum to 1, because the entries represent an
exhaustive setofcasesforthevariable. ForBooleanvariables, onceyouknowthattheprob-
abilityofatruevalueisp,theprobability offalsemustbe1–p,soweoftenomitthesecond
number, asinFigure 14.2. Ingeneral, atable foraBoolean variable with k Boolean parents
contains2k independentlyspecifiableprobabilities. Anodewithnoparentshasonlyonerow,
representing thepriorprobabilities ofeachpossible valueofthevariable.
NoticethatthenetworkdoesnothavenodescorrespondingtoMary’scurrentlylistening
toloud musicortothetelephone ringing andconfusing John. Thesefactors aresummarized
intheuncertainty associated withthelinks from Alarm toJohnCalls andMaryCalls. This
showsbothlazinessandignoranceinoperation: itwouldbealotofworktofindoutwhythose
factorswouldbemoreorlesslikelyinanyparticularcase,andwehavenoreasonablewayto
obtain the relevant information anyway. The probabilities actually summarize a potentially
Section14.2. TheSemanticsofBayesianNetworks 513
infinite set of circumstances in which the alarm might fail to go off (high humidity, power
failure, dead battery, cut wires, a dead mouse stuck inside the bell, etc.) or John or Mary
mightfailtocallandreportit(outtolunch,onvacation,temporarilydeaf,passinghelicopter,
etc.). Inthisway,asmallagentcancopewithaverylargeworld,atleastapproximately. The
degreeofapproximation canbeimprovedifweintroduce additional relevantinformation.
14.2 THE SEMANTICS OF BAYESIAN NETWORKS
The previous section described what a network is, but not what it means. There are two
ways in which one can understand the semantics of Bayesian networks. The first is to see
the network as a representation of the joint probability distribution. The second is to view
itas an encoding ofacollection ofconditional independence statements. Thetwoviews are
equivalent, but the first turns out to be helpful in understanding how to construct networks,
whereasthesecond ishelpfulindesigning inference procedures.
14.2.1 Representing the full jointdistribution
Viewed as a piece of “syntax,” a Bayesian network is a directed acyclic graph with some
numeric parameters attached to each node. One way to define what the network means—its
semantics—istodefinethewayinwhichitrepresentsaspecificjointdistribution overallthe
variables. Todothis, wefirstneedtoretract (temporarily) whatwesaidearlieraboutthepa-
rametersassociated witheachnode. Wesaidthatthoseparameterscorrespond toconditional
probabilities P(X |Parents(X )); this is atrue statement, but until weassign semantics to
i i
thenetworkasawhole,weshould thinkofthemjustasnumbers θ(X |Parents(X )).
i i
Ageneric entry inthejointdistribution istheprobability ofaconjunction ofparticular
assignments to each variable, such as P(X =x ∧ ... ∧ X =x ). We use the notation
1 1 n n
P(x ,...,x )asanabbreviation forthis. Thevalueofthisentryisgivenbytheformula
1 n
(cid:25)n
P(x ,...,x ) = θ(x |parents(X )), (14.1)
1 n i i
i=1
where parents(X ) denotes the values of Parents(X ) that appear in x ,...,x . Thus,
i i 1 n
each entry in the joint distribution is represented by the product of the appropriate elements
oftheconditional probability tables(CPTs)intheBayesiannetwork.
From this definition, it is easy to prove that the parameters θ(X |Parents(X )) are
i i
exactly the conditional probabilities P(X |Parents(X )) implied by the joint distribution
i i
(seeExercise14.2). Hence,wecanrewriteEquation(14.1)as
(cid:25)n
P(x ,...,x ) = P(x |parents(X )). (14.2)
1 n i i
i=1
Inotherwords, thetables wehavebeen calling conditional probability tables really arecon-
ditional probability tablesaccording tothesemanticsdefinedinEquation(14.1).
Toillustratethis,wecancalculatetheprobabilitythatthealarmhassounded,butneither
aburglary noranearthquake hasoccurred, andbothJohnandMarycall. Wemultiplyentries
514 Chapter 14. Probabilistic Reasoning
fromthejointdistribution (usingsingle-letter namesfor thevariables):
P(j,m,a,¬b,¬e) = P(j|a)P(m|a)P(a|¬b∧¬e)P(¬b)P(¬e)
= 0.90×0.70×0.001×0.999×0.998 = 0.000628 .
Section 13.3 explained that the full joint distribution can be used to answer any query about
thedomain. IfaBayesiannetworkisarepresentation ofthejointdistribution, thenittoocan
beusedtoansweranyquery, bysummingalltherelevantjoint entries. Section14.4explains
howtodothis,butalsodescribes methodsthataremuchmoreefficient.
AmethodforconstructingBayesiannetworks
Equation (14.2) defines what a given Bayesian network means. The next step is to explain
how to construct a Bayesian network in such a way that the resulting joint distribution is a
goodrepresentationofagivendomain. WewillnowshowthatEquation(14.2)impliescertain
conditional independence relationships that can be used to guide the knowledge engineer in
constructing thetopologyofthenetwork. First,werewrite theentriesinthejointdistribution
intermsofconditional probability, usingtheproduct rule (seepage486):
P(x 1 ,...,x n ) = P(x n |x n−1 ,...,x 1 )P(x n−1 ,...,x 1 ).
Thenwerepeattheprocess,reducingeachconjunctiveprobabilitytoaconditionalprobability
andasmallerconjunction. Weendupwithonebigproduct:
P(x 1 ,...,x n ) = P(x n |x n−1 ,...,x 1 )P(x n−1 |x n−2 ,...,x 1 ) ··· P(x 2 |x 1 )P(x 1 )
(cid:25)n
= P(x i |x i−1 ,...,x 1 ).
i=1
Thisidentity iscalled thechainrule. Itholds foranysetofrandom variables. Comparingit
CHAINRULE
withEquation(14.2),weseethatthespecificationofthejointdistribution isequivalenttothe
generalassertion that,foreveryvariable X inthenetwork,
i
P(X i |X i−1 ,...,X 1 )= P(X i |Parents(X i )), (14.3)
providedthatParents(X i )⊆ {X i−1 ,...,X 1 }. Thislastconditionissatisfiedbynumbering
thenodesinawaythatisconsistent withthepartialorderimplicitinthegraphstructure.
What Equation (14.3) says is that the Bayesian network is a correct representation of
the domain only if each node is conditionally independent of its other predecessors in the
nodeordering, givenitsparents. Wecansatisfythiscondition withthismethodology:
1. Nodes: Firstdeterminethesetofvariables thatarerequired tomodelthedomain. Now
orderthem,{X ,...,X }. Anyorderwillwork,buttheresultingnetworkwillbemore
1 n
compactifthevariables areorderedsuchthatcausesprecedeeffects.
2. Links: Fori=1tondo:
• Choose, from X 1 ,...,X i−1 , a minimal set of parents for X i , such that Equa-
tion(14.3)issatisfied.
• Foreachparentinsertalinkfromtheparentto X .
i
• CPTs: Writedowntheconditional probability table, P(X |Parents(X )).
i i
Section14.2. TheSemanticsofBayesianNetworks 515
Intuitively, the parents of node X i should contain all those nodes in X 1 , ..., X i−1 that
directly influence X . For example, suppose we have completed the network in Figure 14.2
i
exceptforthechoiceofparentsforMaryCalls. MaryCalls iscertainlyinfluencedbywhether
thereisaBurglary oranEarthquake,butnotdirectlyinfluenced. Intuitively, ourknowledge
of the domain tells us that these events influence Mary’s calling behavior only through their
effectonthealarm. Also,giventhestateofthealarm,whetherJohncallshasnoinfluenceon
Mary’s calling. Formally speaking, we believe that the following conditional independence
statementholds:
P(MaryCalls|JohnCalls,Alarm,Earthquake,Burglary)=P(MaryCalls|Alarm).
Thus,Alarm willbetheonlyparentnodeforMaryCalls.
Becauseeachnodeisconnectedonlytoearliernodes,thisconstruction methodguaran-
teesthatthenetworkisacyclic. AnotherimportantpropertyofBayesiannetworksisthatthey
contain no redundant probability values. If there is no redundancy, then there is no chance
for inconsistency: it is impossible for the knowledge engineer or domain expert to create a
Bayesiannetworkthatviolatestheaxiomsofprobability.
Compactnessandnodeordering
Aswellasbeingacompleteandnonredundant representation ofthedomain,aBayesiannet-
work can often be far more compact than the full joint distribution. This property is what
makesitfeasible tohandle domains withmanyvariables. Thecompactness ofBayesian net-
LOCALLY worksisanexampleofageneralpropertyoflocallystructured(alsocalledsparse)systems.
STRUCTURED
In a locally structured system, each subcomponent interacts directly with only a bounded
SPARSE
number ofother components, regardless ofthe total number of components. Local structure
isusually associated withlinearratherthanexponential growthincomplexity. Inthecaseof
Bayesian networks, it is reasonable to suppose that in most domains each random variable
is directly influenced by at most k others, for some constant k. If we assume n Boolean
variables for simplicity, then the amount of information needed to specify each conditional
probability table will be at most 2k numbers, and the complete network can be specified by
n2k numbers. Incontrast, thejointdistribution contains 2n numbers. Tomakethisconcrete,
suppose we have n=30 nodes, each with five parents (k=5). Then the Bayesian network
requires 960numbers,butthefulljointdistribution requiresoverabillion.
There are domains in which each variable can be influenced directly by all the others,
sothatthenetwork isfullyconnected. Thenspecifying theconditional probability tables re-
quiresthesameamountofinformationasspecifying thejointdistribution. Insomedomains,
there will be slight dependencies that should strictly be included by adding a new link. But
if these dependencies are tenuous, then it maynot be worth the additional complexity in the
network for the small gain in accuracy. For example, one might object to our burglary net-
work on the grounds that if there is an earthquake, then John and Mary would not call even
if they heard the alarm, because they assume that the earthquake is the cause. Whether to
add the link from Earthquake to JohnCalls and MaryCalls (and thus enlarge the tables)
depends oncomparing theimportance ofgetting moreaccurate probabilities withthecostof
specifying theextrainformation.
516 Chapter 14. Probabilistic Reasoning
MaryCalls MaryCalls
JohnCalls JohnCalls
Alarm Earthquake
Burglary Burglary
Earthquake Alarm
(a) (b)
Figure 14.3 Network structure depends on order of introduction. In each network, we
haveintroducednodesintop-to-bottomorder.
Even in a locally structured domain, we will get a compact Bayesian network only if
we choose the node ordering well. What happens if we happen to choose the wrong or-
der? Consider theburglary example again. Suppose wedecide toadd the nodes inthe order
MaryCalls, JohnCalls, Alarm, Burglary, Earthquake. We then get the somewhat more
complicated networkshowninFigure14.3(a). Theprocess goesasfollows:
• AddingMaryCalls: Noparents.
• Adding JohnCalls: If Mary calls, that probably means the alarm has gone off, which
of course would make it more likely that John calls. Therefore, JohnCalls needs
MaryCalls asaparent.
• AddingAlarm: Clearly,ifbothcall,itismorelikelythatthealarmhasgoneoffthanif
justoneorneithercalls,soweneedboth MaryCalls andJohnCalls asparents.
• Adding Burglary: If we know the alarm state, then the call from John or Mary might
giveusinformation aboutourphoneringing orMary’smusic, butnotaboutburglary:
P(Burglary|Alarm,JohnCalls,MaryCalls) =P(Burglary|Alarm).
HenceweneedjustAlarm asparent.
• Adding Earthquake: If the alarm is on, it is more likely that there has been an earth-
quake. (The alarm is an earthquake detector of sorts.) But if we know that there has
beenaburglary,thenthatexplainsthealarm,andtheprobabilityofanearthquakewould
beonlyslightly abovenormal. Hence,weneedbothAlarm andBurglary asparents.
The resulting network has two more links than the original network in Figure 14.2 and re-
quires three more probabilities to be specified. What’s worse, some of the links represent
tenuous relationships that require difficult and unnatural probability judgments, such as as-
Section14.2. TheSemanticsofBayesianNetworks 517
sessing the probability of Earthquake, given Burglary and Alarm. This phenomenon is
quite general and is related to the distinction between causal and diagnostic models intro-
duced in Section 13.5.1 (see also Exercise 8.13). If we try to build a diagnostic model with
links from symptoms to causes (as from MaryCalls to Alarm or Alarm to Burglary), we
enduphavingtospecifyadditionaldependenciesbetweenotherwiseindependentcauses(and
oftenbetweenseparately occurring symptomsaswell). Ifwesticktoacausalmodel,weend
uphavingtospecifyfewernumbers,andthenumberswilloftenbeeasiertocomeupwith. In
the domain of medicine, for example, it has been shown by Tversky and Kahneman (1982)
that expert physicians prefer to give probability judgments for causal rules rather than for
diagnostic ones.
Figure14.3(b) shows averybad node ordering: MaryCalls, JohnCalls, Earthquake,
Burglary,Alarm. Thisnetworkrequires31distinctprobabilitiestobespecified—exactlythe
samenumberasthefull joint distribution. Itisimportant torealize, however, that anyofthe
threenetworkscanrepresent exactlythesamejointdistribution. Thelasttwoversionssimply
failtorepresentalltheconditionalindependence relationships andhenceendupspecifying a
lotofunnecessary numbersinstead.
14.2.2 Conditionalindependence relationsinBayesiannetworks
We have provided a “numerical” semantics for Bayesian networks in terms of the represen-
tation of the full joint distribution, as in Equation (14.2). Using this semantics to derive a
method for constructing Bayesian networks, we were led to the consequence that a node is
conditionally independent of its other predecessors, given its parents. It turns out that we
canalso gointheotherdirection. Wecanstart from a“topological” semantics that specifies
theconditional independence relationships encoded bythe graphstructure, andfromthiswe
can derive the “numerical” semantics. The topological semantics2 specifies that each vari-
able is conditionally independent of its non-descendants, given its parents. Forexample, in
DESCENDANT
Figure14.2, JohnCalls isindependent ofBurglary,Earthquake,andMaryCalls giventhe
value ofAlarm. Thedefinition isillustrated inFigure 14.4(a). From these conditional inde-
pendence assertions and the interpretation of the network parameters θ(X |Parents(X ))
i i
asspecifications ofconditional probabilities P(X |Parents(X )), thefull jointdistribution
i i
given in Equation (14.2) can be reconstructed. In this sense, the “numerical” semantics and
the“topological” semantics areequivalent.
Another important independence property is implied by the topological semantics: a
nodeisconditionallyindependentofallothernodesinthenetwork,givenitsparents,children,
and children’s parents—that is, given its Markovblanket. (Exercise 14.7asks you toprove
MARKOVBLANKET
this.) Forexample,Burglary isindependentofJohnCalls andMaryCalls,givenAlarm and
Earthquake. Thisproperty isillustrated inFigure14.4(b).
2 There is also a general topological criterion called d-separation for deciding whether a set of nodes X is
conditionally independent of another set Y, givenathirdset Z. Thecriterionisrathercomplicated and isnot
neededforderivingthealgorithmsinthischapter,soweomitit.DetailsmaybefoundinPearl(1988)orDarwiche
(2009).Shachter(1998)givesamoreintuitivemethodofascertainingd-separation.
518 Chapter 14. Probabilistic Reasoning
. . .
U U
1 m U 1 . . . U m
X X
Z Z Z Z
1j nj 1j nj
Y Y
Y Y 1 . . . n
1 . . . n
(a) (b)
Figure14.4 (a) A nodeX is conditionallyindependentofits non-descendants(e.g.,the
Zijs) given its parents (the Uis shown in the gray area). (b) A node X is conditionally
independentofallothernodesinthenetworkgivenitsMarkovblanket(thegrayarea).
14.3 EFFICIENT REPRESENTATION OF CONDITIONAL DISTRIBUTIONS
Evenifthe maximumnumberofparents k issmallish, filling inthe CPTforanode requires
uptoO(2k)numbersandperhapsagreatdealofexperiencewithallthepossibleconditioning
cases. Infact,thisisaworst-case scenario inwhichtherelationship betweentheparents and
the child is completely arbitrary. Usually, such relationships are describable by a canonical
CANONICAL distributionthatfitssomestandardpattern. Insuchcases,thecompletetablecanbespecified
DISTRIBUTION
bynamingthepatternandperhaps supplying afewparameters—much easierthansupplying
anexponential numberofparameters.
DETERMINISTIC The simplest example is provided by deterministic nodes. A deterministic node has
NODES
its value specified exactly by the values of its parents, with no uncertainty. The relationship
canbealogicalone: forexample,therelationship betweentheparentnodes Canadian,US,
Mexican and the child node NorthAmerican is simply that the child is the disjunction of
the parents. The relationship can also be numerical: for example, if the parent nodes are
the prices of a particular model of car at several dealers and the child node is the price that
a bargain hunter ends up paying, then the child node is the minimum of the parent values;
or if the parent nodes are a lake’s inflows (rivers, runoff, precipitation) and outflows (rivers,
evaporation, seepage)andthechildisthechangeinthewaterlevelofthelake,thenthevalue
ofthechildisthesumoftheinflowparents minusthesumoftheoutflowparents.
Uncertain relationships can often be characterized by so-called noisy logical relation-
ships. The standard example is the noisy-OR relation, which is a generalization of the log-
NOISY-OR
ical OR. In propositional logic, we might say that Fever is true if and only if Cold, Flu, or
Malaria is true. The noisy-OR model allows for uncertainty about the ability of each par-
ent to cause the child to be true—the causal relationship between parent and child may be
Section14.3. EfficientRepresentation ofConditional Distributions 519
inhibited, and so a patient could have a cold, but not exhibit a fever. The model makes two
assumptions. First, it assumes that all the possible causes are listed. (If some are missing,
we can always add a so-called leak node that covers “miscellaneous causes.”) Second, it
LEAKNODE
assumes that inhibition of each parent is independent of inhibition of any other parents: for
example,whateverinhibitsMalaria fromcausingafeverisindependentofwhateverinhibits
Flu from causing a fever. Given these assumptions, Fever is false if and only if all its true
parents are inhibited, and the probability of this is the product of the inhibition probabilities
q foreachparent. Letussuppose theseindividual inhibition probabilities areasfollows:
q = P(¬fever|cold,¬flu,¬malaria)= 0.6,
cold
q = P(¬fever|¬cold,flu,¬malaria)= 0.2,
flu
q = P(¬fever|¬cold,¬flu,malaria)= 0.1.
malaria
Then,fromthisinformation andthenoisy-OR assumptions, theentireCPTcanbebuilt. The
generalruleisthat
(cid:25)
P(x |parents(X )) = 1− q ,
i i j
{j:Xj=true}
where the product is taken over the parents that are set to true for that row of the CPT. The
followingtableillustrates thiscalculation:
Cold Flu Malaria P(Fever) P(¬Fever)
F F F 0.0 1.0
F F T 0.9 0.1
F T F 0.8 0.2
F T T 0.98 0.02 = 0.2×0.1
T F F 0.4 0.6
T F T 0.94 0.06 = 0.6×0.1
T T F 0.88 0.12 = 0.6×0.2
T T T 0.988 0.012 = 0.6×0.2×0.1
In general, noisy logical relationships in which a variable depends on k parents can be de-
scribed using O(k) parameters instead of O(2k) for the full conditional probability table.
This makes assessment and learning much easier. For example, the CPCS network (Prad-
han etal., 1994) uses noisy-OR and noisy-MAX distributions tomodel relationships among
diseases and symptoms ininternal medicine. With 448 nodes and 906 links, it requires only
8,254valuesinsteadof133,931,430 foranetworkwithfullCPTs.
Bayesiannetswithcontinuousvariables
Manyreal-world problems involve continuous quantities, such asheight, mass, temperature,
andmoney;infact,muchofstatisticsdealswithrandomvariableswhosedomainsarecontin-
uous. By definition, continuous variables have an infinite number of possible values, so itis
impossibletospecifyconditional probabilities explicitlyforeachvalue. Onepossiblewayto
handlecontinuousvariablesistoavoidthembyusingdiscretization—thatis,dividingupthe
DISCRETIZATION
520 Chapter 14. Probabilistic Reasoning
Subsidy Harvest
Cost
Buys
Figure14.5 Asimplenetworkwithdiscretevariables(SubsidyandBuys)andcontinuous
variables(Harvest andCost).
possible values intoafixedsetofintervals. Forexample, temperatures could bedivided into
(<0oC), (0oC−100oC), and (>100oC). Discretization is sometimes an adequate solution,
but often results in a considerable loss of accuracy and very large CPTs. The most com-
monsolutionistodefinestandardfamiliesofprobability densityfunctions(seeAppendixA)
that are specified by a finite number of parameters. For example, a Gaussian (or normal)
PARAMETER
distribution N(μ,σ2)(x) has the mean μ and the variance σ2 as parameters. Yet another
solution—sometimes called a nonparametric representation—is to define the conditional
NONPARAMETRIC
distribution implicitly with a collection of instances, each containing specific values of the
parentandchildvariables. Weexplorethisapproach furtherinChapter18.
A network with both discrete and continuous variables is called a hybrid Bayesian
HYBRIDBAYESIAN network. To specify a hybrid network, we have to specify two new kinds of distributions:
NETWORK
the conditional distribution for a continuous variable given discrete or continuous parents;
andtheconditionaldistributionforadiscretevariablegivencontinuousparents. Considerthe
simple example in Figure 14.5, in which a customer buys some fruit depending on its cost,
whichdependsinturnonthesizeoftheharvestandwhetherthegovernment’ssubsidyscheme
is operating. The variable Cost is continuous and has continuous and discrete parents; the
variable Buys isdiscreteandhasacontinuous parent.
For the Cost variable, we need to specify P(Cost|Harvest,Subsidy). The discrete
parent is handled by enumeration—that is, by specifying both P(Cost|Harvest,subsidy)
andP(Cost|Harvest,¬subsidy). Tohandle Harvest,wespecify howthedistribution over
the cost c depends on the continuous value h of Harvest. In other words, we specify the
parametersofthecostdistribution asafunctionofh. Themostcommonchoiceisthelinear
Gaussian distribution, in which the child has a Gaussian distribution whose mean μ varies
LINEARGAUSSIAN
linearly with the value of the parent and whose standard deviation σ is fixed. We need two
distributions, oneforsubsidy andonefor¬subsidy,withdifferentparameters:
“ ”
1 −1 c−(ath+bt) 2
P(c|h,subsidy) = N(a
t
h+b
t
,σ
t
2)(c) = √ e 2 σt
σ 2π
t
„ «
2
1 −1 c−(afh+bf)
P(c|h,¬subsidy) = N(a h+b ,σ2)(c) = √ e 2 σf .
f f f
σ 2π
f
Forthisexample,then,theconditional distribution forCost isspecifiedbynamingthelinear
Gaussiandistribution andprovidingtheparameters a ,b ,σ ,a ,b ,andσ . Figures14.6(a)
t t t f f f
Section14.3. EfficientRepresentation ofConditional Distributions 521
P(c | h,subsidy) P(c | h,¬subsidy) P(c | h)
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
12 12 12
0 8 10 0 8 10 0 8 10
0 2
C
4
ost
6
c
810 0 2
H
4
a
6
rvesth
0 2
C
4
ost
6
c
810 0 2
H
4
a
6
rvesth
0 2
C
4
ost
6
c
810 0 2
H
4
a
6
rvesth
(a) (b) (c)
Figure 14.6 The graphs in (a) and (b) show the probability distribution over Cost as a
function of Harvest size, with Subsidy true and false, respectively. Graph (c) shows the
distributionP(Cost|Harvest),obtainedbysummingoverthetwosubsidycases.
and (b) show these tworelationships. Notice that in each case the slope is negative, because
cost decreases as supply increases. (Of course, the assumption of linearity implies that the
costbecomesnegativeatsomepoint;thelinearmodelisreasonableonlyiftheharvestsizeis
limitedtoanarrowrange.) Figure14.6(c)showsthedistribution P(c|h),averagingoverthe
twopossible valuesofSubsidy andassumingthateachhaspriorprobability 0.5. Thisshows
thatevenwithverysimplemodels,quiteinteresting distributions canberepresented.
The linear Gaussian conditional distribution has some special properties. A network
containing only continuous variables with linear Gaussian distributions has a joint distribu-
tionthatisamultivariateGaussiandistribution(seeAppendixA)overallthevariables(Exer-
cise14.9). Furthermore,theposteriordistribution given anyevidencealsohasthisproperty.3
When discrete variables are added as parents (not as children) of continuous variables, the
CONDITIONAL network defines a conditional Gaussian, or CG, distribution: given any assignment to the
GAUSSIAN
discretevariables, thedistribution overthecontinuous variables isamultivariate Gaussian.
Now we turn to the distributions for discrete variables with continuous parents. Con-
sider, for example, the Buys node in Figure 14.5. It seems reasonable to assume that the
customer will buy if the cost is low and will not buy if it is high and that the probability of
buyingvariessmoothlyinsomeintermediateregion. Inotherwords,theconditionaldistribu-
tionislikea“soft”threshold function. Onewaytomakesoftthresholds istousetheintegral
ofthestandard normaldistribution:
(cid:26)
x
Φ(x) = N(0,1)(x)dx .
−∞
Thentheprobability ofBuys givenCost mightbe
P(buys|Cost=c) = Φ((−c+μ)/σ),
whichmeansthatthecostthresholdoccursaround μ,thewidthofthethresholdregionispro-
portional to σ, and the probability ofbuying decreases as cost increases. Thisprobit distri-
3 ItfollowsthatinferenceinlinearGaussiannetworkstakesonlyO(n3)timeintheworstcase,regardlessofthe
networktopology.InSection14.4,weseethatinferencefornetworksofdiscretevariablesisNP-hard.
522 Chapter 14. Probabilistic Reasoning
1
0.8
0.6
0.4
0.2
0
0 2 4 6 8 10 12
)c(P
1
0.8
0.6
0.4
0.2
0
0 2 4 6 8 10 12
Costc
)c
| syub(P
Logit
Probit
Costc
(a) (b)
Figure 14.7 (a) A normal (Gaussian) distribution for the cost threshold, centered on
μ=6.0withstandarddeviationσ=1.0. (b)Logitandprobitdistributionsfortheprobability
ofbuys givencost,fortheparametersμ=6.0andσ=1.0.
PROBIT bution(pronounced“pro-bit”andshortfor“probabilityunit”)is illustratedinFigure14.7(a).
DISTRIBUTION
Theformcanbejustifiedbyproposingthattheunderlyingdecisionprocesshasahardthresh-
old,butthatthepreciselocationofthethreshold issubjecttorandomGaussiannoise.
An alternative to the probit model is the logit distribution (pronounced “low-jit”). It
LOGITDISTRIBUTION
usesthelogistic function1/(1+e
−x)toproduce
asoftthreshold:
LOGISTICFUNCTION
1
P(buys|Cost=c) = .
1+exp(−2 −c+μ)
σ
Thisisillustrated inFigure14.7(b). Thetwodistributions look similar, butthelogitactually
hasmuchlonger“tails.” Theprobitisoftenabetterfittorealsituations,butthelogitissome-
times easier to deal with mathematically. It is used widely in neural networks (Chapter 20).
Both probit and logit can be generalized to handle multiple continuous parents by taking a
linearcombination oftheparentvalues.
14.4 EXACT INFERENCE IN BAYESIAN NETWORKS
The basic task for any probabilistic inference system is to compute the posterior probability
distribution for a set of query variables, given some observed event—that is, some assign-
EVENT
mentofvaluestoasetofevidencevariables. Tosimplifythepresentation, wewillconsider
onlyonequeryvariable atatime;thealgorithms caneasilybeextended toqueries withmul-
tiple variables. We will use the notation from Chapter 13: X denotes the query variable; E
denotesthesetofevidencevariablesE ,...,E ,andeisaparticularobservedevent; Ywill
1 m
denotesthenonevidence, nonqueryvariables Y ,...,Y (calledthehiddenvariables). Thus,
HIDDENVARIABLE 1 l
the complete set of variables is X={X}∪E∪Y. A typical query asks for the posterior
probability distribution P(X|e).
Section14.4. ExactInference inBayesianNetworks 523
In the burglary network, we might observe the event in which JohnCalls=true and
MaryCalls=true. Wecouldthenaskfor,say,theprobability thataburglary hasoccurred:
P(Burglary|JohnCalls=true,MaryCalls=true) = (cid:16)0.284,0.716(cid:17).
In this section we discuss exact algorithms for computing posterior probabilities and will
consider the complexity of this task. It turns out that the general case is intractable, so Sec-
tion14.5coversmethodsforapproximate inference.
14.4.1 Inference by enumeration
Chapter 13 explained that any conditional probability can be computed by summing terms
from the full joint distribution. More specifically, a query P(X|e) can be answered using
Equation(13.9),whichwerepeathereforconvenience:
(cid:12)
P(X|e)= αP(X,e)= α P(X,e,y).
y
Now,aBayesian networkgivesacompleterepresentation ofthefulljointdistribution. More
specifically, Equation (14.2) on page 513 shows that the terms P(x,e,y) in the joint distri-
butioncanbewrittenasproducts ofconditional probabilities fromthenetwork. Therefore, a
query can be answered using a Bayesian network by computing sums of products of condi-
tionalprobabilities fromthenetwork.
Consider the query P(Burglary|JohnCalls=true,MaryCalls=true). The hidden
variables for this query are Earthquake and Alarm. From Equation (13.9), using initial
lettersforthevariables toshortentheexpressions, wehave4
(cid:12)(cid:12)
P(B|j,m) = αP(B,j,m) =α P(B,j,m,e,a,).
e a
The semantics of Bayesian networks (Equation (14.2)) then gives us an expression in terms
ofCPTentries. Forsimplicity, wedothisjustforBurglary=true:
(cid:12)(cid:12)
P(b|j,m) = α P(b)P(e)P(a|b,e)P(j|a)P(m|a).
e a
To compute this expression, we have to add four terms, each computed by multiplying five
numbers. Intheworstcase,wherewehavetosumoutalmostallthevariables,thecomplexity
ofthealgorithm foranetworkwith nBooleanvariables isO(n2n).
An improvement can be obtained from the following simple observations: the P(b)
termisaconstantandcanbemovedoutsidethesummationsoveraande,andtheP(e)term
canbemovedoutsidethesummationovera. Hence,wehave
(cid:12) (cid:12)
P(b|j,m) = αP(b) P(e) P(a|b,e)P(j|a)P(m|a). (14.4)
e a
Thisexpression canbeevaluated bylooping through thevariables inorder, multiplying CPT
entries as we go. For each summation, we also need to loop over the variable’s possible
P
4 Anexpressionsuchas P(a,e)meanstosumP(A = a,E = e)forallpossiblevaluesofe. WhenE is
e
Boolean,thereisanambiguityinthatP(e)isusedtomeanbothP(E =true)andP(E =e),butitshouldbe
clearfromcontextwhichisintended;inparticular,inthecontextofasumthelatterisintended.
524 Chapter 14. Probabilistic Reasoning
values. The structure of this computation is shown in Figure 14.8. Using the numbers from
Figure 14.2, we obtain P(b|j,m) = α×0.00059224. The corresponding computation for
¬byieldsα×0.0014919;hence,
P(B|j,m) = α(cid:16)0.00059224,0.0014919(cid:17) ≈ (cid:16)0.284,0.716(cid:17).
Thatis,thechanceofaburglary, givencallsfrombothneighbors, isabout28%.
Theevaluation processfortheexpression inEquation(14.4)isshownasanexpression
tree in Figure 14.8. The ENUMERATION-ASK algorithm in Figure 14.9 evaluates such trees
using depth-first recursion. Thealgorithm is verysimilar instructure tothe backtracking al-
gorithmforsolvingCSPs(Figure6.5)andtheDPLLalgorithmforsatisfiability(Figure7.17).
ThespacecomplexityofENUMERATION-ASK isonlylinearinthenumberofvariables:
thealgorithm sumsoverthefulljointdistribution without everconstructing itexplicitly. Un-
fortunately, its time complexity for a network with n Boolean variables is always O(2n)—
betterthantheO(n2n)forthesimpleapproach described earlier, butstillrather grim.
Note that the tree in Figure 14.8 makes explicit the repeated subexpressions evalu-
atedbythealgorithm. Theproducts P(j|a)P(m|a)andP(j|¬a)P(m|¬a)arecomputed
twice,onceforeachvalueofe. Thenextsectiondescribesageneralmethodthatavoidssuch
wastedcomputations.
14.4.2 The variableeliminationalgorithm
The enumeration algorithm can be improved substantially by eliminating repeated calcula-
tions of the kind illustrated in Figure 14.8. The idea is simple: do the calculation once and
savetheresultsforlateruse. Thisisaformofdynamicprogramming. Thereareseveralver-
VARIABLE sionsofthisapproach;wepresentthevariableeliminationalgorithm,whichisthesimplest.
ELIMINATION
Variableelimination worksbyevaluating expressions such asEquation(14.4)inright-to-left
order(thatis,bottomupinFigure14.8). Intermediateresultsarestored,andsummationsover
eachvariable aredoneonlyforthoseportions oftheexpression thatdependonthevariable.
Letusillustrate thisprocessfortheburglary network. Weevaluatetheexpression
(cid:12) (cid:12)
P(B|j,m) = α P(B) P(e) P(a|B,e)P(j|a)P(m|a) .
(cid:27)(cid:28)(cid:29)(cid:30) (cid:27)(cid:28)(cid:29)(cid:30) (cid:27) (cid:28)(cid:29) (cid:30)(cid:27) (cid:28)(cid:29) (cid:30)(cid:27) (cid:28)(cid:29) (cid:30)
e a
f1(B) f2(E) f3(A,B,E) f4(A) f5(A)
Noticethatwehaveannotatedeachpartoftheexpressionwiththenameofthecorresponding
factor; each factorisamatrixindexed bythevalues ofitsargument variables. Forexample,
FACTOR
thefactorsf (A)andf (A)corresponding toP(j|a)andP(m|a)dependjustonAbecause
4 5
J andM arefixedbythequery. Theyaretherefore two-elementvectors:
(cid:13) (cid:14) (cid:13) (cid:14) (cid:13) (cid:14) (cid:13) (cid:14)
P(j|a) 0.90 P(m|a) 0.70
f (A) = = f (A) = = .
4 P(j|¬a) 0.05 5 P(m|¬a) 0.01
f (A,B,E)willbea2×2×2matrix,whichishardtoshowontheprintedpage. (The“first”
3
element isgiven byP(a|b,e)=0.95 andthe“last” byP(¬a|¬b,¬e)=0.999.) Intermsof
factors, thequeryexpression iswrittenas
(cid:12) (cid:12)
P(B|j,m) = αf (B)× f (E)× f (A,B,E)×f (A)×f (A)
1 2 3 4 5
e a
Section14.4. ExactInference inBayesianNetworks 525
P(b)
.001
P(e) P(¬e)
.002 .998
P(a|b,e) P(¬a|b,e) P(a|b,¬e) P(¬a|b,¬e)
.95 .05 .94 .06
P(j|a) P( j|¬a) P( j|a) P( j|¬a)
.90 .05 .90 .05
P(m|a) P(m|¬a) P(m|a) P(m|¬a)
.70 .01 .70 .01
Figure 14.8 The structure of the expression shown in Equation (14.4). The evaluation
proceedstopdown,multiplyingvaluesalongeachpathandsummingatthe“+”nodes.Notice
therepetitionofthepathsforjandm.
functionENUMERATION-ASK(X,e,bn)returnsadistributionoverX
inputs:X, thequeryvariable
e,observedvaluesforvariablesE
bn,aBayesnetwithvariables{X} ∪ E ∪ Y /*Y=hiddenvariables*/
Q(X)←adistributionoverX,initiallyempty
foreachvaluexi ofX do
Q(xi)←ENUMERATE-ALL(bn.VARS,exi )
whereexi iseextendedwithX= xi
returnNORMALIZE(Q(X))
functionENUMERATE-ALL(vars,e)returnsarealnumber
ifEMPTY?(vars)thenreturn1.0
Y ←FIRST(vars)
ifY hasvaluey ine
thenreturn(cid:2)P(y|parents(Y)) × ENUMERATE-ALL(REST(vars),e)
elsereturn
y
P(y|parents(Y)) × ENUMERATE-ALL(REST(vars),ey)
whereey iseextendedwithY = y
Figure14.9 TheenumerationalgorithmforansweringqueriesonBayesiannetworks.
526 Chapter 14. Probabilistic Reasoning
wherethe“×”operatorisnotordinarymatrixmultiplication butinsteadthepointwiseprod-
POINTWISE uctoperation, tobedescribed shortly.
PRODUCT
The process of evaluation is a process of summing out variables (right to left) from
pointwise products of factors to produce new factors, eventually yielding a factor that is the
solution, i.e.,theposteriordistribution overthequeryvariable. Thestepsareasfollows:
• First,wesumoutAfromtheproduct off ,f ,andf . Thisgivesusanew2×2factor
3 4 5
f (B,E)whoseindicesrangeoverjust B andE:
6
(cid:12)
f (B,E) = f (A,B,E)×f (A)×f (A)
6 3 4 5
a
= (f (a,B,E)×f (a)×f (a))+(f (¬a,B,E)×f (¬a)×f (¬a)).
3 4 5 3 4 5
Nowweareleftwiththeexpression
(cid:12)
P(B|j,m) = αf (B)× f (E)×f (B,E).
1 2 6
e
• Next,wesumoutE fromtheproductof f andf :
2 6
(cid:12)
f (B) = f (E)×f (B,E)
7 2 6
e
= f (e)×f (B,e)+f (¬e)×f (B,¬e).
2 6 2 6
Thisleavestheexpression
P(B|j,m) = αf (B)×f (B)
1 7
whichcanbeevaluatedbytakingthepointwiseproduct andnormalizing theresult.
Examiningthissequence,weseethattwobasiccomputationaloperationsarerequired: point-
wiseproduct ofapairoffactors, andsumming out avariable from aproduct offactors. The
nextsectiondescribes eachoftheseoperations.
Operationsonfactors
The pointwise product of two factors f and f yields a new factor f whose variables are
1 2
the union of the variables in f and f and whose elements are given by the product of the
1 2
correspondingelementsinthetwofactors. SupposethetwofactorshavevariablesY ,...,Y
1 k
incommon. Thenwehave
f(X ...X ,Y ...Y ,Z ...Z )= f (X ...X ,Y ...Y )f (Y ...Y ,Z ...Z ).
1 j 1 k 1 l 1 1 j 1 k 2 1 k , l
If all the variables are binary, then f and f have 2j+k and 2k+l entries, respectively, and
1 2
the pointwise product has 2j+k+l entries. For example, given two factors f (A,B) and
1
f (B,C), the pointwise product f ×f =f (A,B,C) has 21+1+1=8 entries, as illustrated
2 1 2 3
in Figure 14.10. Notice that the factor resulting from a pointwise product can contain more
variablesthananyofthefactorsbeingmultipliedandthatthesizeofafactorisexponentialin
the number of variables. This is where both space and time complexity arise in the variable
elimination algorithm.
Section14.4. ExactInference inBayesianNetworks 527
A B f (A,B) B C f (B,C) A B C f (A,B,C)
1 2 3
T T .3 T T .2 T T T .3×.2=.06
T F .7 T F .8 T T F .3×.8=.24
F T .9 F T .6 T F T .7×.6=.42
F F .1 F F .4 T F F .7×.4=.28
F T T .9×.2=.18
F T F .9×.8=.72
F F T .1×.6=.06
F F F .1×.4=.04
Figure14.10 Illustratingpointwisemultiplication:f (A,B)×f (B,C)=f (A,B,C).
1 2 3
Summingoutavariablefromaproductoffactorsisdonebyaddingupthesubmatrices
formed by fixing the variable to each of its values in turn. For example, to sum out A from
f (A,B,C),wewrite
3 (cid:12)
f(B,C) = f (A,B,C) = f (a,B,C)+f (¬a,B,C)
3 3 3
(cid:13)a (cid:14) (cid:13) (cid:14) (cid:13) (cid:14)
.06 .24 .18 .72 .24 .96
= + = .
.42 .28 .06 .04 .48 .32
Theonlytrick istonotice thatanyfactorthatdoes notdepend onthevariable tobesummed
out can be moved outside the summation. Forexample, if wewere to sum out E first in the
burglary network, therelevantpartoftheexpression would be
(cid:12) (cid:12)
f (E)×f (A,B,E)×f (A)×f (A) = f (A)×f (A)× f (E)×f (A,B,E).
2 3 4 5 4 5 2 3
e e
Now the pointwise product inside the summation is computed, and the variable is summed
outoftheresulting matrix.
Notice that matrices are not multiplied until we need to sum out a variable from the
accumulated product. Atthatpoint, wemultiply justthose matrices thatinclude thevariable
to be summed out. Given functions for pointwise product and summing out, the variable
elimination algorithm itselfcanbewrittenquitesimply,asshowninFigure14.11.
Variableorderingandvariablerelevance
ThealgorithminFigure14.11includesanunspecifiedORDERfunctiontochooseanordering
for the variables. Every choice of ordering yields a valid algorithm, but different orderings
cause different intermediate factors to be generated during the calculation. For example, in
the calculation shown previously, we eliminated A before E; if we do it the other way, the
calculation becomes
(cid:12) (cid:12)
P(B|j,m) = αf (B)× f (A)×f (A)× f (E)×f (A,B,E),
1 4 5 2 3
a e
duringwhichanewfactor f (A,B)willbegenerated.
6
In general, the time and space requirements of variable elimination are dominated by
the size of the largest factor constructed during the operation of the algorithm. This in turn
528 Chapter 14. Probabilistic Reasoning
functionELIMINATION-ASK(X,e,bn)returnsadistributionoverX
inputs:X,thequeryvariable
e,observedvaluesforvariablesE
bn,aBayesiannetworkspecifyingjointdistributionP(X
1
,...,Xn)
factors←[]
foreachvar inORDER(bn.VARS)do
factors←[MAKE-FACTOR(var,e)|factors]
ifvar isahiddenvariablethenfactors←SUM-OUT(var,factors)
returnNORMALIZE(POINTWISE-PRODUCT(factors))
Figure14.11 ThevariableeliminationalgorithmforinferenceinBayesiannetworks.
is determined by the order of elimination of variables and by the structure of the network.
It turns out to be intractable to determine the optimal ordering, but several good heuristics
are available. One fairly effective method is a greedy one: eliminate whichever variable
minimizesthesizeofthenextfactortobeconstructed.
Let us consider one more query: P(JohnCalls|Burglary=true). As usual, the first
stepistowriteoutthenestedsummation:
(cid:12) (cid:12) (cid:12)
P(J|b) =αP(b) P(e) P(a|b,e)P(J|a) P(m|a).
e a m (cid:2)
Evaluating thisexpression from right toleft, wenotice something interesting: P(m|a)
m
isequal to1by definition! Hence, there wasnoneed toinclude itin thefirstplace; the vari-
able M is irrelevant to this query. Another way of saying this is that the result of the query
P(JohnCalls|Burglary=true) is unchanged if we remove MaryCalls from the network
altogether. Ingeneral,wecanremoveanyleafnodethatisnotaqueryvariableoranevidence
variable. Afteritsremoval, theremaybesomemoreleafnodes, andthesetoomaybeirrele-
vant. Continuing this process, we eventually find that every variable that is not an ancestor
of a query variable or evidence variable is irrelevant to the query. A variable elimination
algorithm cantherefore removeallthesevariablesbeforeevaluating thequery.
14.4.3 The complexityofexact inference
ThecomplexityofexactinferenceinBayesiannetworksdependsstronglyonthestructureof
thenetwork. TheburglarynetworkofFigure14.2belongstothefamilyofnetworksinwhich
thereisatmostoneundirected pathbetweenanytwonodes inthenetwork. Thesearecalled
singlyconnectednetworksorpolytrees,andtheyhaveaparticularlyniceproperty: Thetime
SINGLYCONNECTED
andspacecomplexityofexactinferenceinpolytreesislinearinthesizeofthenetwork. Here,
POLYTREE
the size is defined as the number of CPT entries; if the number of parents of each node is
bounded byaconstant, thenthecomplexity willalsobelinearinthenumberofnodes.
MULTIPLY Formultiplyconnectednetworks,suchasthatofFigure14.12(a),variableelimination
CONNECTED
can have exponential time and space complexity in the worst case, even when the number
of parents per node is bounded. This is not surprising when one considers that because it
Section14.4. ExactInference inBayesianNetworks 529
P(C)=.5
Cloudy
P(C)=.5
C P(S) C P(R)
t .10 Sprinkler Rain t .80 Cloudy
f .50 f .20 P(S+R=x)
C t t t f f t f f
Wet
Grass Spr+Rain t .08 .02.72 .18
f .10 .40.10 .40
S R P(W) S+R P(W)
t t .99 t t .99 Wet
t f .90 t f .90 Grass
f t .90 f t .90
f f .00 f f .00
(a) (b)
Figure14.12 (a)Amultiplyconnectednetworkwithconditionalprobabilitytables. (b)A
clusteredequivalentofthemultiplyconnectednetwork.
includes inferenceinpropositional logicasaspecialcase,inference inBayesiannetworksis
NP-hard. Infact,itcanbeshown(Exercise14.16)thattheproblemisashardasthatofcom-
puting the number of satisfying assignments for a propositional logic formula. This means
thatitis#P-hard(“number-P hard”)—that is,strictlyharderthanNP-completeproblems.
Thereisacloseconnection betweenthecomplexityofBayesiannetworkinferenceand
the complexity of constraint satisfaction problems (CSPs). As we discussed in Chapter 6,
the difficulty of solving a discrete CSP is related to how “treelike” its constraint graph is.
Measures such as tree width, which bound the complexity of solving a CSP, can also be
applied directly to Bayesian networks. Moreover, the variable elimination algorithm can be
generalized tosolveCSPsaswellasBayesiannetworks.
14.4.4 Clustering algorithms
Thevariableeliminationalgorithmissimpleandefficientforansweringindividualqueries. If
wewanttocompute posterior probabilities forallthevariables inanetwork, however, itcan
be less efficient. Forexample, in a polytree network, one would need to issue O(n) queries
costing O(n) each, for a total of O(n2) time. Using clustering algorithms (also known as
CLUSTERING
jointreealgorithms), thetimecanbereduced to O(n). Forthisreason, thesealgorithms are
JOINTREE
widelyusedincommercialBayesiannetworktools.
The basic idea of clustering is to join individual nodes of the network to form clus-
ter nodes in such a way that the resulting network is a polytree. For example, the multiply
connected network shown in Figure 14.12(a) can be converted into a polytree by combin-
ing the Sprinkler and Rain node into a cluster node called Sprinkler+Rain, as shown in
Figure 14.12(b). The two Boolean nodes are replaced by a “meganode” that takes on four
possible values: tt,tf,ft,andff. Themeganodehasonlyoneparent, theBooleanvariable
Cloudy, so there are two conditioning cases. Although this example doesn’t show it, the
processofclustering oftenproduces meganodesthatsharesomevariables.
530 Chapter 14. Probabilistic Reasoning
Oncethenetworkisinpolytreeform,aspecial-purpose inferencealgorithmisrequired,
becauseordinary inference methodscannothandlemeganodesthatsharevariables witheach
other. Essentially,thealgorithmisaformofconstraintpropagation(seeChapter6)wherethe
constraintsensurethatneighboringmeganodesagreeonthe posteriorprobabilityofanyvari-
ablesthattheyhaveincommon. Withcarefulbookkeeping, thisalgorithmisabletocompute
posterior probabilities forallthe nonevidence nodes inthenetwork intime linear inthesize
oftheclustered network. However,theNP-hardnessoftheproblem hasnotdisappeared: ifa
network requires exponential timeand space withvariable elimination, then theCPTsinthe
clustered networkwillnecessarily beexponentially large.
14.5 APPROXIMATE INFERENCE IN BAYESIAN NETWORKS
Giventhe intractability ofexact inference in large, multiply connected networks, itis essen-
tialtoconsiderapproximate inferencemethods. Thissectiondescribesrandomized sampling
algorithms, also called Monte Carlo algorithms, that provide approximate answers whose
MONTECARLO
accuracy depends on the number of samples generated. Monte Carlo algorithms, of which
simulated annealing (page 126) is an example, are used in many branches of science to es-
timate quantities that are difficult to calculate exactly. In this section, we are interested in
sampling applied to the computation of posterior probabilities. We describe two families of
algorithms: directsamplingandMarkovchainsampling. Twootherapproaches—variational
methodsandloopypropagation—are mentioned inthenotesattheendofthechapter.
14.5.1 Directsampling methods
Theprimitive elementinanysampling algorithm isthegeneration ofsamples from aknown
probabilitydistribution. Forexample,anunbiasedcoincanbethoughtofasarandomvariable
Coin with values (cid:16)heads,tails(cid:17) and a prior distribution P(Coin) = (cid:16)0.5,0.5(cid:17). Sampling
fromthisdistributionisexactlylikeflippingthecoin: withprobability0.5itwillreturnheads,
and with probability 0.5 it will return tails. Given a source of random numbers uniformly
distributed in the range [0,1], it is a simple matter to sample any distribution on a single
variable, whetherdiscreteorcontinuous. (SeeExercise14.17.)
Thesimplestkindofrandomsamplingprocess forBayesiannetworksgenerates events
from a network that has no evidence associated with it. The idea is to sample each variable
inturn, intopological order. Theprobability distribution from which thevalue issampled is
conditioned onthevaluesalreadyassignedtothevariable’sparents. Thisalgorithm isshown
in Figure 14.13. Wecan illustrate its operation on the network in Figure 14.12(a), assuming
anordering [Cloudy,Sprinkler,Rain,WetGrass]:
1. SamplefromP(Cloudy)= (cid:16)0.5,0.5(cid:17),valueistrue.
2. SamplefromP(Sprinkler |Cloudy=true) = (cid:16)0.1,0.9(cid:17),valueisfalse.
3. SamplefromP(Rain|Cloudy=true) = (cid:16)0.8,0.2(cid:17),valueistrue.
4. SamplefromP(WetGrass|Sprinkler =false,Rain=true)= (cid:16)0.9,0.1(cid:17),valueistrue.
Inthiscase, PRIOR-SAMPLE returnstheevent [true,false,true,true].
Section14.5. Approximate InferenceinBayesianNetworks 531
functionPRIOR-SAMPLE(bn)returnsaneventsampledfromthepriorspecifiedbybn
inputs:bn,aBayesiannetworkspecifyingjointdistributionP(X
1
,...,Xn)
x←aneventwithnelements
foreachvariableXiinX
1
,...,Xndo
x[i]←arandomsamplefromP(Xi |parents(Xi))
returnx
Figure14.13 AsamplingalgorithmthatgenerateseventsfromaBayesiannetwork.Each
variableissampledaccordingtotheconditionaldistributiongiventhevaluesalreadysampled
forthevariable’sparents.
ItiseasytoseethatPRIOR-SAMPLE generatessamplesfromthepriorjointdistribution
specifiedbythenetwork. First,let S (x ,...,x )betheprobability thataspecificeventis
PS 1 n
generated bythe PRIOR-SAMPLE algorithm. Justlooking atthesamplingprocess, wehave
(cid:25)n
S (x ...x ) = P(x |parents(X ))
PS 1 n i i
i=1
because each sampling step depends only on the parent values. This expression should look
familiar, because itisalsothe probability ofthe eventaccording tothe Bayesian net’s repre-
sentation ofthejointdistribution, asstatedinEquation(14.2). Thatis,wehave
S (x ...x ) = P(x ...x ).
PS 1 n 1 n
Thissimplefactmakesiteasytoanswerquestions byusingsamples.
In any sampling algorithm, the answers are computed by counting the actual samples
generated. Suppose there are N total samples, and let N (x ,...,x ) be the number of
PS 1 n
timesthespecificeventx ,...,x occurs inthesetofsamples. Weexpect thisnumber, asa
1 n
fraction of the total, to converge in the limit to its expected value according to the sampling
probability:
N (x ,...,x )
PS 1 n
lim = S (x ,...,x )= P(x ,...,x ). (14.5)
PS 1 n 1 n
N→∞ N
For example, consider the event produced earlier: [true,false,true,true]. The sampling
probability forthiseventis
S (true,false,true,true) = 0.5×0.9×0.8×0.9 = 0.324.
PS
Hence,inthelimitoflargeN,weexpect32.4%ofthesamplestobeofthisevent.
Wheneverweuseanapproximateequality(“≈”)inwhatfollows,wemeanitinexactly
this sense—that the estimated probability becomes exact in the large-sample limit. Such an
estimate is called consistent. For example, one can produce a consistent estimate of the
CONSISTENT
probability ofanypartially specifiedevent x ,...,x ,wherem ≤ n,asfollows:
1 m
P(x ,...,x )≈ N (x ,...,x )/N . (14.6)
1 m PS 1 m
That is, the probability of the event can be estimated as the fraction of all complete events
generated by the sampling process that match the partially specified event. For example, if
532 Chapter 14. Probabilistic Reasoning
we generate 1000 samples from the sprinkler network, and 511 of them have Rain=true,
thentheestimatedprobability ofrain,writtenas
Pˆ(Rain=true),is0.511.
RejectionsamplinginBayesian networks
REJECTION Rejectionsamplingisageneralmethodforproducingsamplesfromahard-to-sampledistri-
SAMPLING
bution given an easy-to-sample distribution. In its simplest form, it can be used to compute
conditional probabilities—that is,todetermine P(X|e). The REJECTION-SAMPLING algo-
rithmisshowninFigure14.14. First,itgeneratessamplesfromthepriordistributionspecified
bythenetwork. Then,itrejectsallthosethatdonotmatchtheevidence. Finally,theestimate
Pˆ(X=x|e)isobtained bycountinghowoftenX=xoccursintheremaining samples.
LetPˆ(X|e)betheestimateddistributionthatthealgorithmreturns.
Fromthedefinition
ofthealgorithm, wehave
N (X,e)
Pˆ(X|e)= αN (X,e) = PS .
PS
N (e)
PS
FromEquation(14.6),thisbecomes
P(X,e)
Pˆ(X|e)≈ = P(X|e).
P(e)
Thatis,rejection samplingproduces aconsistent estimate ofthetrueprobability.
Continuing withourexamplefrom Figure14.12(a), letusassumethat wewishtoesti-
mate P(Rain|Sprinkler =true), using 100 samples. Ofthe 100 that we generate, suppose
that 73 have Sprinkler =false and are rejected, while 27 have Sprinkler =true; of the 27,
8haveRain=true and19haveRain=false. Hence,
P(Rain|Sprinkler =true)≈ NORMALIZE((cid:16)8,19(cid:17))= (cid:16)0.296,0.704(cid:17).
The true answer is (cid:16)0.3,0.7(cid:17). As more samples are collected, the estimate will converge to
the true answer. The standard deviation of the error in each probability will be proportional
√
to1/ n,wherenisthenumberofsamplesusedintheestimate.
The biggest problem with rejection sampling is that it rejects so many samples! The
fraction ofsamples consistent withtheevidence edrops exponentially asthe numberofevi-
dencevariablesgrows,sotheprocedure issimplyunusable forcomplexproblems.
Noticethatrejectionsamplingisverysimilartotheestimationofconditionalprobabili-
tiesdirectlyfromtherealworld. Forexample,toestimate P(Rain|RedSkyAtNight=true),
one can simply count how often it rains after a red sky is observed the previous evening—
ignoring those evenings when the sky is not red. (Here, the world itself plays the role of
the sample-generation algorithm.) Obviously, this could take a long time if the sky is very
seldomred,andthatistheweaknessofrejection sampling.
Likelihoodweighting
LIKELIHOOD Likelihoodweightingavoidstheinefficiencyofrejectionsamplingbygeneratingonlyevents
WEIGHTING
that are consistent with the evidence e. It is a particular instance of the general statistical
IMPORTANCE techniqueofimportancesampling,tailoredforinferenceinBayesiannetworks. Webeginby
SAMPLING
Section14.5. Approximate InferenceinBayesianNetworks 533
functionREJECTION-SAMPLING(X,e,bn,N)returnsanestimateofP(X|e)
inputs:X,thequeryvariable
e,observedvaluesforvariablesE
bn,aBayesiannetwork
N,thetotalnumberofsamplestobegenerated
localvariables: N,avectorofcountsforeachvalueofX,initiallyzero
forj =1toN do
x←PRIOR-SAMPLE(bn)
ifxisconsistentwithethen
N[x]←N[x]+1wherex isthevalueofX inx
returnNORMALIZE(N)
Figure14.14 Therejection-samplingalgorithmforansweringqueriesgivenevidenceina
Bayesiannetwork.
describing howthealgorithmworks;thenweshowthatitworkscorrectly—that is,generates
consistent probability estimates.
LIKELIHOOD-WEIGHTING (see Figure 14.15) fixes the values for the evidence vari-
ables E and samples only the nonevidence variables. This guarantees that each event gener-
ated is consistent with the evidence. Not all events are equal, however. Before tallying the
countsinthedistribution forthequeryvariable, eacheventisweightedbythelikelihood that
theeventaccordstotheevidence, asmeasuredbytheproduct oftheconditionalprobabilities
foreach evidence variable, given itsparents. Intuitively, events inwhich the actual evidence
appearsunlikely shouldbegivenlessweight.
Let us apply the algorithm to the network shown in Figure 14.12(a), with the query
P(Rain|Cloudy=true,WetGrass=true) and the ordering Cloudy, Sprinkler, Rain, Wet-
Grass. (Anytopological ordering willdo.) Theprocess goes asfollows: First, theweight w
issetto1.0. Thenaneventisgenerated:
1. Cloudy isanevidence variablewithvaluetrue. Therefore, weset
w ← w×P(Cloudy=true)= 0.5.
2. Sprinkler isnotanevidencevariable,sosamplefromP(Sprinkler |Cloudy=true)=
(cid:16)0.1,0.9(cid:17);suppose thisreturns false.
3. Similarly, sample from P(Rain|Cloudy=true) = (cid:16)0.8,0.2(cid:17); suppose this returns
true.
4. WetGrass isanevidence variablewithvalue true. Therefore, weset
w ← w×P(WetGrass=true|Sprinkler =false,Rain=true) = 0.45.
Here WEIGHTED-SAMPLE returns the event [true,false,true,true] with weight 0.45, and
thisistalliedunderRain=true.
To understand why likelihood weighting works, we start by examining the sampling
probabilityS
WS
forWEIGHTED-SAMPLE. RememberthattheevidencevariablesEarefixed
534 Chapter 14. Probabilistic Reasoning
functionLIKELIHOOD-WEIGHTING(X,e,bn,N)returnsanestimateofP(X|e)
inputs:X,thequeryvariable
e,observedvaluesforvariablesE
bn,aBayesiannetworkspecifyingjointdistributionP(X
1
,...,Xn)
N,thetotalnumberofsamplestobegenerated
localvariables: W,avectorofweightedcountsforeachvalueofX,initiallyzero
forj =1toN do
x,w←WEIGHTED-SAMPLE(bn,e)
W[x]←W[x]+w wherex isthevalueofX inx
returnNORMALIZE(W)
functionWEIGHTED-SAMPLE(bn,e)returnsaneventandaweight
w←1;x←aneventwithnelementsinitializedfrome
foreachvariableXiinX
1
,...,Xndo
ifXiisanevidencevariablewithvaluexi ine
thenw←w × P(Xi= xi |parents(Xi))
elsex[i]←arandomsamplefromP(Xi |parents(Xi))
returnx,w
Figure14.15 Thelikelihood-weightingalgorithmforinferenceinBayesiannetworks. In
WEIGHTED-SAMPLE, each nonevidence variable is sampled according to the conditional
distribution given the values already sampled for the variable’s parents, while a weight is
accumulatedbasedonthelikelihoodforeachevidencevariable.
with values e. We call the nonevidence variables Z (including the query variable X). The
algorithm sampleseachvariablein Zgivenitsparentvalues:
(cid:25)l
S (z,e)= P(z |parents(Z )). (14.7)
WS i i
i=1
NoticethatParents(Z )canincludebothnonevidencevariablesandevidencevariables. Un-
i
likethepriordistribution P(z),thedistribution S payssomeattentiontotheevidence: the
WS
sampled values foreach Z willbeinfluenced byevidence among Z ’sancestors. Forexam-
i i
ple,whensamplingSprinkler thealgorithmpaysattentiontotheevidenceCloudy=true in
itsparent variable. Ontheotherhand, S pays lessattention tothe evidence than does the
WS
true posterior distribution P(z|e), because the sampled values for each Z ignore evidence
i
amongZ ’snon-ancestors.5 Forexample,whensampling Sprinkler andRain thealgorithm
i
ignorestheevidenceinthechildvariableWetGrass=true;thismeansitwillgeneratemany
samples with Sprinkler =false and Rain=false despite the fact that the evidence actually
rulesoutthiscase.
5 Ideally,wewouldliketouseasamplingdistributionequaltothetrueposteriorP(z|e),totakealltheevidence
into account. This cannot be done efficiently, however. If it could, then we could approximate the desired
probabilitytoarbitraryaccuracywithapolynomialnumberofsamples.Itcanbeshownthatnosuchpolynomial-
timeapproximationschemecanexist.
Section14.5. Approximate InferenceinBayesianNetworks 535
The likelihood weight w makes up for the difference between the actual and desired
sampling distributions. The weight for a given sample x, composed from z and e, is the
product of the likelihoods for each evidence variable given its parents (some orall of which
maybeamongtheZ s):
i
(cid:25)m
w(z,e) = P(e |parents(E )). (14.8)
i i
i=1
MultiplyingEquations(14.7)and(14.8),weseethattheweightedprobabilityofasamplehas
theparticularly convenient form
(cid:25)l (cid:25)m
S (z,e)w(z,e) = P(z |parents(Z )) P(e |parents(E ))
WS i i i i
i=1 i=1
= P(z,e) (14.9)
because the two products cover all the variables in the network, allowing us to use Equa-
tion(14.2)forthejointprobability.
Now it is easy to show that likelihood weighting estimates are consistent. For any
particularvalue xofX,theestimatedposteriorprobability canbecalculated asfollows:
(cid:12)
Pˆ(x|e) = α N
WS
(x,y,e)w(x,y,e) from LIKELIHOOD-WEIGHTING
y
(cid:12)
≈ α (cid:2) S (x,y,e)w(x,y,e) forlarge N
WS
y
(cid:12)
(cid:2)
= α P(x,y,e) byEquation(14.9)
y
= α (cid:2) P(x,e) = P(x|e).
Hence,likelihood weighting returnsconsistent estimates.
Because likelihood weighting uses all the samples generated, it can be much more ef-
ficient than rejection sampling. It will, however, suffer a degradation in performance as the
number of evidence variables increases. This is because most samples will have very low
weights and hence the weighted estimate will be dominated by the tiny fraction of samples
thataccordmorethananinfinitesimallikelihoodtotheevidence. Theproblemisexacerbated
if the evidence variables occur late in the variable ordering, because then the nonevidence
variableswillhavenoevidenceintheirparentsandancestorstoguidethegeneration ofsam-
ples. This means the samples will be simulations that bear little resemblance to the reality
suggested bytheevidence.
14.5.2 Inference by Markovchain simulation
MARKOVCHAIN MarkovchainMonteCarlo(MCMC)algorithmsworkquitedifferentlyfromrejectionsam-
MONTECARLO
pling and likelihood weighting. Instead of generating each sample from scratch, MCMCal-
gorithms generate each sample by making a random change to the preceding sample. It is
thereforehelpfultothinkofanMCMCalgorithm asbeinginaparticular currentstatespeci-
fyingavalueforeveryvariableandgeneratinga nextstatebymakingrandomchangestothe
536 Chapter 14. Probabilistic Reasoning
currentstate. (IfthisremindsyouofsimulatedannealingfromChapter4orWALKSAT from
Chapter7, thatisbecause both aremembersofthe MCMCfamily.) Herewedescribe apar-
ticularform ofMCMCcalled Gibbssampling, whichisespecially wellsuited forBayesian
GIBBSSAMPLING
networks. (Otherforms,someofthemsignificantlymorepowerful,arediscussedinthenotes
attheendofthechapter.) Wewillfirstdescribewhatthealgorithmdoes,thenwewillexplain
whyitworks.
GibbssamplinginBayesian networks
The Gibbs sampling algorithm forBayesian networks starts with an arbitrary state (with the
evidence variables fixed at their observed values) and generates a next state by randomly
sampling a value for one of the nonevidence variables X . The sampling for X is done
i i
conditioned onthecurrent valuesofthevariables intheMarkovblanketofX . (Recallfrom
i
page517thattheMarkovblanketofavariableconsistsofitsparents,children,andchildren’s
parents.) The algorithm therefore wanders randomly around the state space—the space of
possible complete assignments—flipping one variable at a time, but keeping the evidence
variables fixed.
Consider the query P(Rain|Sprinkler =true,WetGrass=true) applied to the net-
work inFigure 14.12(a). Theevidence variables Sprinkler and WetGrass are fixedtotheir
observed valuesand thenonevidence variables Cloudy andRain areinitialized randomly—
let us say to true and false respectively. Thus, the initial state is [true,true,false,true].
Nowthenonevidence variables aresampledrepeatedly inanarbitrary order. Forexample:
1. Cloudy is sampled, given the current values of its Markov blanket variables: in this
case, we sample from P(Cloudy|Sprinkler =true,Rain=false). (Shortly, we will
show how to calculate this distribution.) Suppose the result is Cloudy=false. Then
thenewcurrentstateis[false,true,false,true].
2. Rain issampled, giventhecurrent valuesofitsMarkovblanket variables: inthiscase,
wesamplefrom P(Rain|Cloudy=false,Sprinkler =true,WetGrass=true). Sup-
posethisyieldsRain=true. Thenewcurrentstateis [false,true,true,true].
Eachstatevisitedduringthisprocessisasamplethatcontributestotheestimateforthequery
variable Rain. Iftheprocess visits20stateswhere Rain istrueand60states where Rain is
false, then the answer to the query is NORMALIZE((cid:16)20,60(cid:17)) = (cid:16)0.25,0.75(cid:17). The complete
algorithm isshowninFigure14.16.
WhyGibbssamplingworks
We will now show that Gibbs sampling returns consistent estimates for posterior probabil-
ities. The material in this section is quite technical, but the basic claim is straightforward:
the sampling process settles into a“dynamic equilibrium” in which the long-run fraction of
time spent in each state is exactly proportional to its posterior probability. This remarkable
TRANSITION propertyfollowsfromthespecifictransitionprobabilitywithwhichtheprocessmovesfrom
PROBABILITY
one state to another, as defined by the conditional distribution given the Markov blanket of
thevariablebeingsampled.
Section14.5. Approximate InferenceinBayesianNetworks 537
functionGIBBS-ASK(X,e,bn,N)returnsanestimateofP(X|e)
localvariables: N,avectorofcountsforeachvalueofX,initiallyzero
Z,thenonevidencevariablesinbn
x,thecurrentstateofthenetwork,initiallycopiedfrome
initializexwithrandomvaluesforthevariablesinZ
forj =1toN do
foreachZiinZdo
setthevalueofZiinxbysamplingfromP(Zi |mb(Zi))
N[x]←N[x]+1wherex isthevalueofX inx
returnNORMALIZE(N)
Figure14.16 TheGibbssamplingalgorithmforapproximateinferenceinBayesian net-
works;thisversioncyclesthroughthevariables,butchoosingvariablesatrandomalsoworks.
Let q(x → x (cid:2) ) be the probability that the process makes a transition from state x to
(cid:2)
statex. Thistransitionprobability defineswhatiscalledaMarkovchainonthestatespace.
MARKOVCHAIN
(Markov chains also figure prominently in Chapters 15 and 17.) Now suppose that we run
the Markov chain for t steps, and let π (x) be the probability that the system is in state x at
t
(cid:2) (cid:2)
time t. Similarly, let π (x) be the probability of being in state x at time t + 1. Given
t+1
(cid:2)
π (x), wecan calculate π (x)by summing, forallstates the system could be inat time t,
t t+1
(cid:2)
theprobability ofbeinginthatstatetimestheprobability ofmakingthetransition tox:
(cid:12)
π (x (cid:2) ) = π (x)q(x → x (cid:2) ).
t+1 t
x
STATIONARY We say that the chain has reached its stationary distribution if π =π . Let us call this
DISTRIBUTION t t+1
stationary distribution π;itsdefiningequation istherefore
(cid:12)
π(x (cid:2) ) = π(x)q(x → x (cid:2) ) forallx (cid:2) . (14.10)
x
Provided the transition probability distribution q isergodic—that is, every state isreachable
ERGODIC
fromeveryotherandtherearenostrictlyperiodiccycles—there isexactlyonedistribution π
satisfying thisequation foranygivenq.
Equation(14.10)canbereadassayingthattheexpected“outflow”fromeachstate(i.e.,
its current “population”) is equal to the expected “inflow” from all the states. One obvious
way to satisfy this relationship is if the expected flowbetween any pair of states is the same
inbothdirections; thatis,
π(x)q(x → x (cid:2) ) = π(x (cid:2) )q(x (cid:2) → x) forallx, x (cid:2) . (14.11)
Whentheseequations hold,wesaythatq(x → x (cid:2) )isindetailedbalancewithπ(x).
DETAILEDBALANCE
We can show that detailed balance implies stationarity simply by summing over x in
Equation(14.11). Wehave
(cid:12) (cid:12) (cid:12)
π(x)q(x → x (cid:2) )= π(x (cid:2) )q(x (cid:2) → x) = π(x (cid:2) ) q(x (cid:2) → x)= π(x (cid:2) )
x x x
538 Chapter 14. Probabilistic Reasoning
(cid:2)
wherethelaststepfollowsbecauseatransition from x isguaranteed tooccur.
The transition probability q(x → x (cid:2) ) defined by the sampling step in GIBBS-ASK is
actually aspecial case ofthemoregeneral definition ofGibbs sampling, according towhich
each variable is sampled conditionally on the current values of all the other variables. We
start by showing that this general definition of Gibbs sampling satisfies the detailed balance
equation with a stationary distribution equal to P(x|e), (the true posterior distribution on
the nonevidence variables). Then, wesimply observe that, for Bayesian networks, sampling
conditionallyonallvariablesisequivalenttosamplingconditionallyonthevariable’sMarkov
blanket(seepage517).
Toanalyze thegeneral Gibbssampler, whichsamples each X inturnwithatransition
i
probability q that conditions on all the other variables, we define X to be these other vari-
i i
ables (except the evidence variables); their values in the current state are x . If we sample a
i
(cid:2)
newvaluex forX conditionally onalltheothervariables, including theevidence, wehave
i i
q (x → x (cid:2) ) = q ((x ,x ) → (x (cid:2) ,x )) = P(x (cid:2)|x ,e).
i i i i i i i i
Nowweshow thatthetransition probability foreachstepoftheGibbssamplerisindetailed
balancewiththetrueposterior:
π(x)q (x → x (cid:2) ) = P(x|e)P(x (cid:2)|x ,e) = P(x ,x |e)P(x (cid:2)|x ,e)
i i i i i i i
= P(x |x ,e)P(x |e)P(x (cid:2)|x ,e) (usingthechainruleonthefirstterm)
i i i i i
= P(x |x ,e)P(x (cid:2) ,x |e) (usingthechainrulebackward)
i i i i
= π(x (cid:2) )q (x (cid:2) → x).
i
Wecanthinkoftheloop“foreachZ inZdo”inFigure14.16asdefiningonelargetransition
i
probability qthatisthesequentialcompositionq ◦q ◦···◦q ofthetransitionprobabilities
1 2 n
for the individual variables. It is easy to show (Exercise 14.19) that if each of q and q has
i j
π as its stationary distribution, then the sequential composition q ◦ q does too; hence the
i j
transition probability q for the whole loop has P(x|e)as its stationary distribution. Finally,
unless the CPTscontain probabilities of 0 or 1—which can cause the state space to become
disconnected—it is easy to see that q is ergodic. Hence, the samples generated by Gibbs
samplingwilleventually bedrawnfromthetrueposteriordistribution.
The final step is to show how to perform the general Gibbs sampling step—sampling
X from P(X |x ,e)—in a Bayesian network. Recall from page 517 that avariable isinde-
i i i
pendent ofallothervariables givenitsMarkovblanket; hence,
P(x
(cid:2)|x
,e)= P(x
(cid:2)|mb(X
)),
i i i i
where mb(X ) denotes the values of the variables in X ’s Markov blanket, MB(X ). As
i i i
showninExercise14.7,theprobabilityofavariablegivenitsMarkovblanketisproportional
totheprobability ofthevariablegivenitsparentstimestheprobabilityofeachchildgivenits
respective parents:
(cid:25)
P(x (cid:2)|mb(X )) = αP(x (cid:2)|parents(X ))× P(y |parents(Y )). (14.12)
i i i i j j
Yj ∈Children(Xi)
Hence,toflipeachvariableX conditioned onitsMarkovblanket, thenumberofmultiplica-
i
tionsrequiredisequaltothenumberof X ’schildren.
i
Section14.6. Relational andFirst-OrderProbability Models 539
Quality(B ) Quality(B )
1 2
Honesty(C ) Kindness(C ) Honesty(C ) Kindness(C )
Quality(B ) 1 1 2 2
1
Honesty(C ) Kindness(C )
1 1
Recommendation(C , B ) Recommendation(C , B )
1 1 2 1
Recommendation(C , B ) Recommendation(C , B ) Recommendation(C , B )
1 1 1 2 2 2
(a) (b)
Figure 14.17 (a) Bayes net for a single customer C recommending a single book B .
1 1
Honest(C )isBoolean,whiletheothervariableshaveintegervaluesfrom1to5. (b)Bayes
1
netwithtwocustomersandtwobooks.
14.6 RELATIONAL AND FIRST-ORDER PROBABILITY MODELS
In Chapter 8, we explained the representational advantages possessed by first-order logic in
comparison to propositional logic. First-order logic commits to the existence of objects and
relationsamongthemandcanexpressfactsaboutsomeoralloftheobjectsinadomain. This
oftenresultsinrepresentations thatarevastlymoreconcise thantheequivalent propositional
descriptions. Now, Bayesian networks are essentially propositional: the set of random vari-
ables is fixed and finite, and each has a fixed domain of possible values. This fact limits the
applicability of Bayesian networks. If wecan find a wayto combine probability theory with
theexpressive poweroffirst-order representations, weexpecttobeabletoincreasedramati-
callytherangeofproblemsthatcanbehandled.
Forexample, suppose that an online book retailer would like to provide overall evalu-
ations of products based on recommendations received from its customers. The evaluation
will take the form of a posterior distribution over the quality of the book, given the avail-
able evidence. Thesimplest solution tobase theevaluation ontheaverage recommendation,
perhaps withavariance determinedbythenumberofrecommendations, butthisfailstotake
intoaccountthefactthatsomecustomersarekinderthanothersandsomearelesshonestthan
others. Kind customers tend to give high recommendations even to fairly mediocre books,
while dishonest customers give very high or very low recommendations for reasons other
thanquality—forexample, theymightworkforapublisher.6
For a single customer C , recommending a single book B , the Bayes net might look
1 1
like the one shown in Figure 14.17(a). (Just as in Section 9.1, expressions with parentheses
suchasHonest(C )arejustfancysymbols—inthiscase,fancynamesforrandomvariables.)
1
6 Agametheoristwouldadviseadishonestcustomertoavoiddetectionbyoccasionallyrecommendingagood
bookfromacompetitor.SeeChapter17.
540 Chapter 14. Probabilistic Reasoning
Withtwocustomers andtwobooks, theBayesnetlooks like the oneinFigure14.17(b). For
larger numbers of books and customers, it becomes completely impractical to specify the
networkbyhand.
Fortunately, the network has a lot of repeated structure. Each Recommendation(c,b)
variablehasasitsparentsthevariables Honest(c),Kindness(c),andQuality(b). Moreover,
the CPTs for all the Recommendation(c,b) variables are identical, as are those for all the
Honest(c) variables, and so on. The situation seems tailor-made for a first-order language.
Wewouldliketosaysomething like
Recommendation(c,b) ∼ RecCPT(Honest(c),Kindness(c),Quality(b))
with the intended meaning that a customer’s recommendation for a book depends on the
customer’s honesty and kindness and the book’s quality according to some fixed CPT. This
sectiondevelops alanguage thatletsussayexactlythis,andalotmorebesides.
14.6.1 Possibleworlds
Recall from Chapter 13 that a probability model defines a set Ω of possible worlds with
a probability P(ω) for each world ω. For Bayesian networks, the possible worlds are as-
signments of values to variables; for the Boolean case in particular, the possible worlds are
identical to those of propositional logic. For a first-order probability model, then, it seems
we need the possible worlds to be those of first-order logic—that is, a set of objects with
relations among them and aninterpretation that maps constant symbols toobjects, predicate
symbols to relations, and function symbols to functions on those objects. (See Section 8.2.)
Themodelalsoneedstodefineaprobability foreachsuchpossibleworld,justasaBayesian
networkdefinesaprobability foreachassignment ofvaluestovariables.
Letus suppose, fora moment, that wehave figured out how to do this. Then, as usual
(see page 485), we can obtain the probability of any first-order logical sentence φ as a sum
overthepossible worldswhereitistrue:
(cid:12)
P(φ) = P(ω). (14.13)
ω:φistrueinω
Conditional probabilities P(φ|e)canbe obtained similarly, sowecan, inprinciple, ask any
question we want of our model—e.g., “Which books are most likely to be recommended
highlybydishonest customers?”—and getananswer. Sofar,sogood.
There is, however, a problem: the set of first-order models is infinite. We saw this
explicitlyinFigure8.4onpage293,whichweshowagaininFigure14.18(top). Thismeans
that(1)thesummationinEquation(14.13)couldbeinfeasible,and(2)specifyingacomplete,
consistent distribution overaninfinitesetofworldscould beverydifficult.
Section 14.6.2 explores one approach to dealing with this problem. The idea is to
borrow not from the standard semantics of first-order logic but from the database seman-
tics defined in Section 8.2.8 (page 299). The database semantics makes the unique names
assumption—here, weadoptitfortheconstant symbols. Italsoassumes domainclosure—
there are no more objects than those that are named. We can then guarantee a finite set of
possible worlds by making the set of objects in each world be exactly the set of constant
Section14.6. Relational andFirst-OrderProbability Models 541
R J R J R J R J R J R J
. . . . . .
. . .
R J R J R J R J R J
R R R R . . . R
J J J J J
Figure14.18 Top:Somemembersofthesetofallpossibleworldsforalanguagewithtwo
constantsymbols,RandJ,andonebinaryrelationsymbol,underthestandardsemanticsfor
first-orderlogic. Bottom: thepossibleworldsunderdatabasesemantics. Theinterpretation
oftheconstantsymbolsisfixed,andthereisadistinctobjectforeachconstantsymbol.
symbols that are used; as shown in Figure 14.18 (bottom), there is no uncertainty about the
mappingfromsymbolstoobjectsorabouttheobjectsthatexist. Wewillcallmodelsdefined
RELATIONAL in this way relational probability models, or RPMs.7 The most significant difference be-
PROBABILITYMODEL
tween the semantics of RPMsand the database semantics introduced in Section 8.2.8 is that
RPMsdo not make the closed-world assumption—obviously, assuming that every unknown
factisfalsedoesn’t makesenseinaprobabilistic reasoning system!
Whentheunderlyingassumptionsofdatabasesemanticsfailtohold,RPMswon’twork
well. Forexample,abookretailermightuseanISBN(International StandardBookNumber)
as a constant symbol to name each book, even though a given “logical” book (e.g., “Gone
With the Wind”) may have several ISBNs. It would make sense to aggregate recommenda-
tions across multiple ISBNs, but the retailer may not know for sure which ISBNs are really
thesamebook. (Notethatwearenotreifying theindividual copiesofthebook,whichmight
be necessary for used-book sales, car sales, and so on.) Worse still, each customer is iden-
tified by a login ID, but a dishonest customer may have thousands of IDs! In the computer
securityfield,thesemultipleIDsarecalledsibylsandtheirusetoconfound areputation sys-
SIBYL
tem is called a sibyl attack. Thus, even a simple application in a relatively well-defined,
SIBYLATTACK
EXISTENCE online domain involves both existence uncertainty (what are the real books and customers
UNCERTAINTY
IDENTITY underlying the observed data) and identity uncertainty (which symbol really refer to the
UNCERTAINTY
sameobject). Weneedtobitethebullet anddefineprobability modelsbasedonthestandard
semantics of first-order logic, for which the possible worlds vary in the objects they contain
andinthemappings fromsymbolstoobjects. Section14.6.3showshowtodothis.
7 ThenamerelationalprobabilitymodelwasgivenbyPfeffer(2000)toaslightlydifferentrepresentation,but
theunderlyingideasarethesame.
542 Chapter 14. Probabilistic Reasoning
14.6.2 Relationalprobability models
Like first-order logic, RPMshave constant, function, and predicate symbols. (It turns out to
be easier to view predicates as functions that return true or false.) We will also assume a
typesignatureforeachfunction,thatis,aspecificationofthetypeofeachargumentandthe
TYPESIGNATURE
function’svalue. Ifthetypeofeachobjectisknown,manyspuriouspossibleworldsareelim-
inated by this mechanism. For the book-recommendation domain, the types are Customer
andBook,andthetypesignatures forthefunctions andpredicates areasfollows:
Honest : Customer → {true,false}Kindness : Customer → {1,2,3,4,5}
Quality :Book → {1,2,3,4,5}
Recommendation : Customer ×Book → {1,2,3,4,5}
Theconstantsymbolswillbewhatevercustomerandbooknamesappearintheretailer’sdata
set. Intheexamplegivenearlier(Figure14.17(b)), thesewereC , C andB , B .
1 2 1 2
Given the constants and their types, together with the functions and their type signa-
tures,therandomvariablesoftheRPMareobtainedbyinstantiating eachfunction witheach
possible combination of objects: Honest(C ), Quality(B ), Recommendation(C ,B ),
1 2 1 2
and so on. These are exactly the variables appearing in Figure 14.17(b). Because each type
hasonlyfinitelymanyinstances, thenumberofbasicrandomvariables isalsofinite.
To complete the RPM, we have to write the dependencies that govern these random
variables. Thereisonedependency statement foreachfunction, whereeachargument ofthe
function isalogicalvariable (i.e.,avariablethatranges overobjects, asinfirst-orderlogic):
Honest(c) ∼ (cid:16)0.99,0.01(cid:17)
Kindness(c) ∼ (cid:16)0.1,0.1,0.2,0.3,0.3(cid:17)
Quality(b) ∼ (cid:16)0.05,0.2,0.4,0.2,0.15(cid:17)
Recommendation(c,b) ∼ RecCPT(Honest(c),Kindness(c),Quality(b))
where RecCPT is a separately defined conditional distribution with 2×5×5=50 rows,
each with 5 entries. The semantics of the RPM can be obtained by instantiating these de-
pendencies for all known constants, giving a Bayesian network (as in Figure 14.17(b)) that
definesajointdistribution overtheRPM’srandomvariables.8
CONTEXT-SPECIFIC Wecanrefinethemodelbyintroducing a context-specific independencetoreflectthe
INDEPENDENCE
factthatdishonestcustomersignorequalitywhengivingarecommendation; moreover,kind-
nessplaysnoroleintheirdecisions. Acontext-specific independence allowsavariable tobe
independentofsomeofitsparentsgivencertainvaluesofothers;thus,Recommendation(c,b)
isindependent ofKindness(c)andQuality(b)whenHonest(c)=false:
Recommendation(c,b) ∼ ifHonest(c) then
HonestRecCPT(Kindness(c),Quality(b))
else(cid:16)0.4,0.1,0.0,0.1,0.4(cid:17) .
8 Sometechnicalconditions mustbeobserved toguaranteethattheRPMdefinesaproperdistribution. First,
thedependenciesmustbeacyclic,otherwisetheresultingBayesiannetworkwillhavecyclesandwillnotdefine
aproperdistribution. Second,thedependenciesmustbe well-founded,thatis,therecanbenoinfiniteancestor
chains,suchasmightarisefromrecursivedependencies. Undersomecircumstances(seeExercise14.6),afixed-
pointcalculationyieldsawell-definedprobabilitymodelforarecursiveRPM.
Section14.6. Relational andFirst-OrderProbability Models 543
Fan(C , A ) Fan(C , A ) Author(B )
1 1 1 2 2
Quality(B ) Honesty(C ) Kindness(C ) Quality(B )
1 1 1 2
Recommendation(C , B ) Recommendation(C , B )
1 1 2 1
Figure14.19 FragmentoftheequivalentBayesnetwhenAuthor(B )isunknown.
2
Thiskindofdependencymaylooklikeanordinaryif–then–elsestatementonaprogramming
language, but there is a key difference: the inference engine doesn’t necessarily know the
valueoftheconditional test!
We can elaborate this model in endless ways to make it more realistic. For example,
suppose that an honest customer who is a fan of a book’s author always gives the book a 5,
regardlessofquality:
Recommendation(c,b) ∼ ifHonest(c)then
ifFan(c,Author(b))thenExactly(5)
elseHonestRecCPT(Kindness(c),Quality(b))
else(cid:16)0.4,0.1,0.0,0.1,0.4(cid:17)
Again,theconditional testFan(c,Author(b))isunknown,butifacustomergivesonly5sto
aparticularauthor’sbooksandisnototherwiseespecially kind,thentheposteriorprobability
that the customer is afan of that author willbe high. Furthermore, the posterior distribution
willtendtodiscount thecustomer’s5sinevaluating thequalityofthatauthor’sbooks.
Inthepreceding example,weimplicitlyassumedthatthevalueofAuthor(b)isknown
forevery b,but this maynotbethe case. Howcanthesystem reason about whether, say, C
1
is a fan of Author(B ) when Author(B ) is unknown? The answer is that the system may
2 2
havetoreason about allpossible authors. Suppose (tokeepthings simple)thattherearejust
two authors, A and A . Then Author(B ) is a random variable with two possible values,
1 2 2
A andA ,anditisaparentofRecommendation(C ,B ). Thevariables Fan(C ,A )and
1 2 1 2 1 1
Fan(C ,A )are parents too. Theconditional distribution for Recommendation(C ,B )is
1 2 1 2
then essentially a multiplexer in which the Author(B ) parent acts as a selector to choose
MULTIPLEXER 2
which of Fan(C ,A ) and Fan(C ,A ) actually gets to influence the recommendation. A
1 1 1 2
fragment of the equivalent Bayes net is shown in Figure 14.19. Uncertainty in the value
of Author(B ), which affects the dependency structure of the network, is an instance of
2
RELATIONAL relational uncertainty.
UNCERTAINTY
In case you are wondering how the system can possibly work out who the author of
B is: consider the possibility that three other customers are fans of A (and have no other
2 1
favorite authors in common) and all three have given B a 5, even though most other cus-
2
tomers find it quite dismal. In that case, it is extremely likely that A is the author of B .
1 2
544 Chapter 14. Probabilistic Reasoning
The emergence of sophisticated reasoning like this from an RPM model of just a few lines
isanintriguing exampleofhow probabilistic influences spread through thewebofintercon-
nectionsamongobjectsinthemodel. Asmoredependencies andmoreobjectsareadded,the
pictureconveyed bytheposteriordistribution oftenbecomesclearerandclearer.
The next question is how to do inference in RPMs. One approach is to collect the
evidence and query and the constant symbols therein, construct the equivalent Bayes net,
and apply any of the inference methods discussed in this chapter. This technique is called
unrolling. Theobvious drawbackisthattheresulting Bayesnetmaybeverylarge. Further-
UNROLLING
more,iftherearemanycandidate objectsforanunknownrelationorfunction—for example,
theunknownauthorofB —thensomevariables inthenetworkmayhavemanyparents.
2
Fortunately, much can be done to improve on generic inference algorithms. First, the
presence of repeated substructure in the unrolled Bayes net means that many of the factors
constructed during variable elimination (and similar kinds of tables constructed by cluster-
ing algorithms) will be identical; effective caching schemes have yielded speedups of three
ordersofmagnitudeforlargenetworks. Second,inferencemethodsdevelopedtotakeadvan-
tage ofcontext-specific independence inBayesnets findmanyapplications inRPMs. Third,
MCMC inference algorithms have some interesting properties when applied to RPMs with
relational uncertainty. MCMCworksbysamplingcomplete possible worlds,soineachstate
therelational structure iscompletely known. Intheexamplegivenearlier, eachMCMCstate
wouldspecifythevalueofAuthor(B ),andsotheotherpotential authorsarenolongerpar-
2
entsoftherecommendationnodesforB . ForMCMC,then,relationaluncertainty causesno
2
increaseinnetworkcomplexity; instead,theMCMCprocessincludestransitions thatchange
therelational structure, andhencethedependency structure, oftheunrolled network.
AllofthemethodsjustdescribedassumethattheRPMhastobepartiallyorcompletely
unrolled into a Bayesian network. This is exactly analogous to the method of proposition-
alization for first-order logical inference. (See page 322.) Resolution theorem-provers and
logic programming systems avoid propositionalizing by instantiating the logical variables
onlyasneededtomaketheinferencegothrough;thatis,theylifttheinferenceprocessabove
the level of ground propositional sentences and make each lifted step do the work of many
ground steps. Thesame idea applied inprobabilistic inference. Forexample, inthe variable
elimination algorithm, alifted factorcanrepresent anentire setofground factors that assign
probabilitiestorandomvariablesintheRPM,wherethoserandomvariablesdifferonlyinthe
constant symbols usedtoconstruct them. Thedetailsofthis methodarebeyond thescopeof
thisbook,butreferences aregivenattheendofthechapter.
14.6.3 Open-universe probability models
We argued earlier that database semantics was appropriate for situations in which we know
exactlythesetofrelevantobjectsthatexistandcanidentifythemunambiguously. (Inpartic-
ular, all observations about an object are correctly associated with the constant symbol that
namesit.) Inmanyreal-worldsettings,however,theseassumptionsaresimplyuntenable. We
gavetheexamplesofmultiple ISBNsandsibyl attacks inthebook-recommendation domain
(towhichwewillreturninamoment),butthephenomenon isfarmorepervasive:
Section14.6. Relational andFirst-OrderProbability Models 545
• Avisionsystemdoesn’tknowwhatexists,ifanything,aroundthenextcorner,andmay
notknowiftheobjectitseesnowisthesameoneitsawafewminutesago.
• Atext-understanding systemdoesnotknowinadvancetheentitiesthatwillbefeatured
in a text, and must reason about whether phrases such as “Mary,” “Dr. Smith,” “she,”
“hiscardiologist,” “hismother,”andsoonrefertothesame object.
• Anintelligence analyst hunting forspies never knows how many spies there really are
andcanonlyguesswhethervariouspseudonyms, phonenumbers,andsightingsbelong
tothesameindividual.
In fact, a major part of human cognition seems to require learning what objects exist and
beingabletoconnectobservations—which almostnevercomewithuniqueIDsattached—to
hypothesized objectsintheworld.
For these reasons, we need to be able to write so-called open-universe probability
OPENUNIVERSE
models or OUPMs based on the standard semantics of first-order logic, as illustrated at the
top of Figure 14.18. A language for OUPMs provides a way of writing such models easily
while guaranteeing a unique, consistent probability distribution over the infinite space of
possible worlds.
The basic idea is to understand how ordinary Bayesian networks and RPMs manage
to define a unique probability model and to transfer that insight to the first-order setting. In
essence, aBayes net generates each possible world, event by event, in the topological order
defined bythenetwork structure, whereeach eventisanassignment ofavaluetoavariable.
An RPM extends this to entire sets of events, defined by the possible instantiations of the
logical variables inagiven predicate orfunction. OUPMsgo further byallowing generative
steps that add objects to the possible world under construction, where the number and type
of objects may depend on the objects that are already in that world. That is, the event being
generated isnottheassignment ofavaluetoavariable, buttheveryexistence ofobjects.
OnewaytodothisinOUPMsistoaddstatements thatdefineconditional distributions
over the numbers of objects of various kinds. For example, in the book-recommendation
domain, we might want to distinguish between customers (real people) and their login IDs.
Supposeweexpectsomewherebetween100and10,000distinctcustomers(whomwecannot
observedirectly). Wecanexpressthisasapriorlog-normal distribution9 asfollows:
#Customer ∼ LogNormal[6.9,2.32]().
We expect honest customers to have just one ID, whereas dishonest customers might have
anywherebetween10and1000IDs:
#LoginID(Owner =c) ∼ ifHonest(c)thenExactly(1)
elseLogNormal[6.9,2.32]().
This statement defines the number of login IDs for a given owner, who is a customer. The
Owner function is called an origin function because it says where each generated object
ORIGINFUNCTION
camefrom. Intheformal semantics of BLOG (asdistinct from first-order logic), thedomain
elementsineachpossibleworldareactuallygenerationhistories(e.g.,“thefourthloginIDof
theseventhcustomer”)ratherthansimpletokens.
9 AdistributionLogNormal[μ,σ2](x)isequivalenttoadistributionN[μ,σ2](x)overlog (x).
e
546 Chapter 14. Probabilistic Reasoning
Subject to technical conditions of acyclicity and well-foundedness similar to those for
RPMs, open-universe models of this kind define a unique distribution over possible worlds.
Furthermore, there exist inference algorithms such that, for every such well-defined model
and every first-order query, the answer returned approaches the true posterior arbitrarily
closely in the limit. There are some tricky issues involved in designing these algorithms.
For example, an MCMC algorithm cannot sample directly in the space of possible worlds
when the size of those worlds is unbounded; instead, it samples finite, partial worlds, rely-
ing on the fact that only finitely many objects can be relevant to the query in distinct ways.
Moreover, transitions must allow formerging two objects into one orsplitting one into two.
(Details are given in the references at the end of the chapter.) Despite these complications,
thebasicprincipleestablished inEquation(14.13)stillholds: theprobability ofanysentence
iswelldefinedandcanbecalculated.
Researchinthisareaisstillatanearlystage, butalready itisbecoming clearthatfirst-
orderprobabilistic reasoning yieldsatremendous increase intheeffectiveness ofAIsystems
at handling uncertain information. Potential applications include those mentioned above—
computer vision, text understanding, and intelligence analysis—as well asmanyother kinds
ofsensorinterpretation.
14.7 OTHER APPROACHES TO UNCERTAIN REASONING
Other sciences (e.g., physics, genetics, and economics) have long favored probability as a
model foruncertainty. In 1819, Pierre Laplace said, “Probability theory is nothing but com-
mon sense reduced to calculation.” In 1850, James Maxwell said, “The true logic for this
world isthe calculus of Probabilities, which takes account ofthe magnitude ofthe probabil-
itywhichis,oroughttobe,inareasonable man’smind.”
Given this long tradition, it is perhaps surprising that AI has considered many alterna-
tives to probability. The earliest expert systems of the 1970s ignored uncertainty and used
strictlogicalreasoning,butitsoonbecameclearthatthis wasimpracticalformostreal-world
domains. The next generation ofexpert systems (especially in medical domains) used prob-
abilistic techniques. Initial results were promising, but they did not scale up because of the
exponentialnumberofprobabilities requiredinthefulljointdistribution. (EfficientBayesian
network algorithms were unknown then.) As a result, probabilistic approaches fell out of
favor from roughly 1975 to1988, and avariety ofalternatives toprobability weretried fora
varietyofreasons:
• One common view is that probability theory is essentially numerical, whereas human
judgmental reasoning is more “qualitative.” Certainly, we are not consciously aware
of doing numerical calculations of degrees of belief. (Neither are we aware of doing
unification, yet we seem to be capable of some kind of logical reasoning.) It might be
that we have some kind of numerical degrees of belief encoded directly in strengths
of connections and activations in our neurons. In that case, the difficulty of conscious
accesstothosestrengthsisnotsurprising. Oneshouldalso notethatqualitativereason-
Section14.7. OtherApproaches toUncertainReasoning 547
ingmechanisms canbebuiltdirectly ontopofprobability theory, sothe“nonumbers”
argument against probability has little force. Nonetheless, some qualitative schemes
have a good deal of appeal in their own right. One of the best studied is default rea-
soning,whichtreats conclusions notas“believed toacertaindegree,” butas“believed
until a better reason is found to believe something else.” Default reasoning is covered
inChapter12.
• Rule-based approaches to uncertainty have also been tried. Such approaches hope to
build on the success of logical rule-based systems, but add a sort of “fudge factor” to
eachruletoaccommodateuncertainty. Thesemethodsweredevelopedinthemid-1970s
andformedthebasisforalargenumberofexpertsystemsinmedicineandotherareas.
• One area that we have not addressed so far is the question of ignorance, as opposed
to uncertainty. Consider the flipping of a coin. If we know that the coin is fair, then
a probability of 0.5 for heads is reasonable. If we know that the coin is biased, but
we do not know which way, then 0.5 for heads is again reasonable. Obviously, the
twocasesaredifferent, yettheoutcomeprobability seemsnottodistinguish them. The
Dempster–Shafertheoryusesinterval-valueddegreesofbelieftorepresentanagent’s
knowledgeoftheprobability ofaproposition.
• Probabilitymakesthesameontologicalcommitmentaslogic: thatpropositions aretrue
orfalseintheworld, eveniftheagentisuncertain astowhichisthecase. Researchers
infuzzylogichaveproposedanontologythatallowsvagueness: thataproposition can
be“sortof”true. Vaguenessanduncertainty areinfactorthogonal issues.
Thenextthreesubsectionstreatsomeoftheseapproachesinslightlymoredepth. Wewillnot
providedetailed technical material,butwecitereferences forfurtherstudy.
14.7.1 Rule-based methods foruncertain reasoning
Rule-based systems emerged from early work on practical and intuitive systems for logical
inference. Logicalsystemsingeneral,andlogicalrule-basedsystemsinparticular,havethree
desirable properties:
• Locality: In logical systems, whenever we have a rule of the form A ⇒ B, we can
LOCALITY
concludeB,givenevidenceA,withoutworryingaboutanyotherrules. Inprobabilistic
systems,weneedtoconsider alltheevidence.
• Detachment: Oncealogicalproofisfoundforaproposition B,theproposition canbe
DETACHMENT
usedregardlessofhowitwasderived. Thatis,itcanbe detachedfromitsjustification.
Indealing with probabilities, on the other hand, the source of the evidence fora belief
isimportantforsubsequent reasoning.
TRUTH- • Truth-functionality: In logic, the truth of complex sentences can be computed from
FUNCTIONALITY
the truth of the components. Probability combination does not work this way, except
understrongglobalindependence assumptions.
There have been several attempts to devise uncertain reasoning schemes that retain these
advantages. The idea is to attach degrees of belief to propositions and rules and to devise
purely local schemes for combining and propagating those degrees of belief. The schemes
548 Chapter 14. Probabilistic Reasoning
arealsotruth-functional; forexample,thedegreeofbeliefinA∨B isafunctionofthebelief
inAandthebeliefinB.
Thebadnewsforrule-based systems isthattheproperties of locality, detachment, and
truth-functionality are simply not appropriate for uncertain reasoning. Let us look at truth-
functionality first. LetH betheeventthatafaircoinflipcomesupheads,letT betheevent
1 1
that the coin comes up tails on that same flip, and let H be the event that the coin comes
2
up heads on a second flip. Clearly, all three events have the same probability, 0.5, and so a
truth-functional system must assign the same belief to the disjunction of any two of them.
But we can see that the probability of the disjunction depends on the events themselves and
notjustontheirprobabilities:
P(A) P(B) P(A∨B)
P(H ) = 0.5 P(H ∨H )= 0.50
1 1 1
P(H ) = 0.5 P(T ) = 0.5 P(H ∨T )= 1.00
1 1 1 1
P(H ) = 0.5 P(H ∨H )= 0.75
2 1 2
It gets worse when we chain evidence together. Truth-functional systems have rules of the
form A %→ B that allow us to compute the belief in B as a function of the belief in the rule
andthebeliefinA. Bothforward-andbackward-chaining systemscanbedevised. Thebelief
intheruleisassumedtobeconstantandisusuallyspecifiedbytheknowledgeengineer—for
example,asA%→ B.
0.9
Consider the wet-grass situation from Figure 14.12(a) (page 529). If we wanted to be
abletodobothcausalanddiagnostic reasoning, wewouldneedthetworules
Rain %→ WetGrass and WetGrass %→ Rain .
Thesetworulesform afeedback loop: evidence for Rain increases thebelief in WetGrass,
which in turn increases the belief in Rain even more. Clearly, uncertain reasoning systems
needtokeeptrackofthepathsalongwhichevidence ispropagated.
Intercausal reasoning (orexplaining away)isalsotricky. Considerwhathappens when
wehavethetworules
Sprinkler %→ WetGrass and WetGrass %→ Rain .
Supposeweseethatthesprinklerison. Chainingforwardthroughourrules,thisincreasesthe
belief that the grass willbe wet, which in turn increases the belief that it is raining. But this
is ridiculous: the fact that the sprinkler is on explains awaythe wetgrass and should reduce
thebeliefinrain. Atruth-functional systemactsasifitalsobelievesSprinkler %→ Rain.
Given these difficulties, how can truth-functional systems be made useful in practice?
The answer lies in restricting the task and in carefully engineering the rule base so that un-
desirable interactions do not occur. The most famous example of a truth-functional system
CERTAINTYFACTOR
foruncertain reasoning isthe certaintyfactors model,whichwasdeveloped forthe MYCIN
medical diagnosis program and was widely used in expert systems of the late 1970s and
1980s. Almostallusesofcertainty factorsinvolved rulesetsthatwereeitherpurelydiagnos-
tic (as in MYCIN) or purely causal. Furthermore, evidence was entered only at the “roots”
oftheruleset, andmostrulesetsweresingly connected. Heckerman(1986) hasshownthat,
Section14.7. OtherApproaches toUncertainReasoning 549
underthesecircumstances, aminorvariationoncertainty-factor inferencewasexactlyequiv-
alenttoBayesianinferenceonpolytrees. Inothercircumstances, certaintyfactorscouldyield
disastrously incorrect degrees of belief through overcounting of evidence. As rule sets be-
camelarger, undesirable interactions betweenrulesbecamemorecommon,andpractitioners
foundthatthecertaintyfactorsofmanyotherruleshadtobe “tweaked”whennewruleswere
added. Forthesereasons, Bayesiannetworkshavelargelysupplanted rule-based methodsfor
uncertain reasoning.
14.7.2 Representing ignorance: Dempster–Shafer theory
DEMPSTER–SHAFER The Dempster–Shafer theory is designed to deal with the distinction between uncertainty
THEORY
and ignorance. Rather than computing the probability of a proposition, it computes the
probability that the evidence supports the proposition. This measure of belief is called a
belieffunction,writtenBel(X).
BELIEFFUNCTION
We return to coin flipping for an example of belief functions. Suppose you pick a
coin from a magician’s pocket. Given that the coin might or might not be fair, what belief
should you ascribe to the event that it comes up heads? Dempster–Shafer theory says that
because you have no evidence either way, you have to say that the belief Bel(Heads) = 0
and also that Bel(¬Heads) = 0. This makes Dempster–Shafer reasoning systems skeptical
in a way that has some intuitive appeal. Now suppose you have an expert at your disposal
who testifies with 90% certainty that the coin is fair (i.e., he is 90% sure that P(Heads) =
0.5). Then Dempster–Shafer theory gives Bel(Heads) = 0.9 × 0.5 = 0.45 and likewise
Bel(¬Heads) = 0.45. Thereisstilla10percentage point“gap”thatisnotaccounted forby
theevidence.
The mathematical underpinnings of Dempster–Shafer theory have a similar flavor to
those of probability theory; the main difference is that, instead of assigning probabilities
to possible worlds, the theory assigns masses to sets of possible world, that is, to events.
MASS
The masses still must add to 1 over all possible events. Bel(A) is defined to be the sum of
masses for all events that are subsets of (i.e., that entail) A, including A itself. With this
definition,Bel(A)andBel(¬A)sumtoatmost1,andthegap—theintervalbetweenBel(A)
and1−Bel(¬A)—isofteninterpreted asbounding theprobability of A.
Aswithdefaultreasoning,thereisaprobleminconnectingbeliefstoactions. Whenever
there is a gap in the beliefs, then a decision problem can be defined such that a Dempster–
Shafer system is unable to make a decision. In fact, the notion of utility in the Dempster–
Shafer model is not yet well understood because the meanings of masses and beliefs them-
selves have yet tobeunderstood. Pearl(1988) hasargued thatBel(A)should beinterpreted
not as adegree of belief in Abut as the probability assigned to all the possible worlds (now
interpreted as logical theories) in which A is provable. While there are cases in which this
quantity mightbeofinterest, itisnotthesameastheprobability thatAistrue.
ABayesiananalysisofthecoin-flippingexamplewouldsuggestthatnonewformalism
isnecessary tohandlesuchcases. Themodelwouldhavetwovariables: theBias ofthecoin
(anumberbetween0and1,where0isacointhatalwaysshowstailsand1acointhatalways
shows heads) and the outcome of the next Flip. The prior probability distribution for Bias
550 Chapter 14. Probabilistic Reasoning
wouldreflectourbeliefsbasedonthesourceofthecoin(themagician’spocket): somesmall
probability that it is fair and some probability that it is heavily biased toward heads or tails.
Theconditional distribution P(Flip|Bias)simplydefineshowthebiasoperates. IfP(Bias)
issymmetricabout0.5,thenourpriorprobability fortheflipis
(cid:26)
1
P(Flip=heads) = P(Bias=x)P(Flip=heads|Bias=x)dx = 0.5.
0
This is the same prediction as if we believe strongly that the coin is fair, but that does not
mean that probability theory treats the two situations identically. The difference arises after
theflipsincomputing theposteriordistribution for Bias. Ifthecoincamefrom abank, then
seeing itcomeupheads threetimesrunning wouldhavealmost noeffectonourstrong prior
belief in its fairness; but if the coin comes from the magician’s pocket, the same evidence
willleadtoastrongerposterior beliefthatthecoinisbiased towardheads. Thus,aBayesian
approach expresses our“ignorance” in termsofhow ourbeliefs would change in theface of
futureinformation gathering.
14.7.3 Representing vagueness: Fuzzy sets andfuzzy logic
Fuzzy set theory is a means of specifying how well an object satisfies a vague description.
FUZZYSETTHEORY
(cid:2) (cid:2)(cid:2)
For example, consider the proposition “Nate is tall.” Is this true if Nate is 5 10 ? Most
people would hesitate to answer “true” or “false,” preferring to say, “sort of.” Note that this
isnot a question of uncertainty about theexternal world—we aresure ofNate’s height. The
issueisthatthelinguisticterm“tall”doesnotrefertoasharpdemarcationofobjectsintotwo
classes—there are degrees of tallness. For this reason, fuzzy set theory is not a method for
uncertain reasoning atall. Rather, fuzzy settheory treats Tall asafuzzy predicate andsays
that the truth value of Tall(Nate) is a number between 0 and 1, rather than being just true
or false. The name “fuzzy set” derives from the interpretation of the predicate as implicitly
definingasetofitsmembers—asetthatdoesnothavesharpboundaries.
Fuzzylogicisamethodforreasoning withlogicalexpressions describing membership
FUZZYLOGIC
in fuzzy sets. For example, the complex sentence Tall(Nate)∧Heavy(Nate) has a fuzzy
truth value that is a function of the truth values of its components. The standard rules for
evaluating thefuzzytruth, T,ofacomplexsentenceare
T(A∧B)= min(T(A),T(B))
T(A∨B)= max(T(A),T(B))
T(¬A) = 1−T(A).
Fuzzy logic is therefore a truth-functional system—a fact that causes serious difficulties.
Forexample, suppose that T(Tall(Nate))=0.6andT(Heavy(Nate))=0.4. Thenwehave
T(Tall(Nate) ∧ Heavy(Nate))=0.4, which seems reasonable, but we also get the result
T(Tall(Nate)∧¬Tall(Nate))=0.4, which does not. Clearly, the problem arises from the
inabilityofatruth-functionalapproachtotakeintoaccountthecorrelationsoranticorrelations
amongthecomponent propositions.
Fuzzycontrolisamethodologyforconstructingcontrolsystemsinwhichthemapping
FUZZYCONTROL
between real-valued input and output parameters is represented by fuzzy rules. Fuzzy con-
trolhasbeenverysuccessful incommercial products suchas automatic transmissions, video
Section14.8. Summary 551
cameras, and electric shavers. Critics (see, e.g., Elkan, 1993) argue that these applications
are successful because they have small rule bases, no chaining of inferences, and tunable
parameters that can be adjusted to improve the system’s performance. Thefact that they are
implemented with fuzzy operators might be incidental to their success; the key is simply to
provideaconcise andintuitivewaytospecify asmoothlyinterpolated, real-valued function.
Therehavebeenattemptstoprovideanexplanation offuzzylogicintermsofprobabil-
itytheory. Oneideaistoviewassertionssuchas“NateisTall”asdiscreteobservations made
concerningacontinuoushiddenvariable,Nate’sactual Height. Theprobabilitymodelspeci-
fiesP(ObserversaysNateistall | Height),perhapsusingaprobitdistributionasdescribed
on page 522. A posterior distribution over Nate’s height can then be calculated in the usual
way,forexample,ifthemodelispartofahybridBayesiannetwork. Suchanapproach isnot
truth-functional, ofcourse. Forexample,theconditional distribution
P(ObserversaysNateistallandheavy |Height,Weight)
allows for interactions between height and weight in the causing of the observation. Thus,
someone who iseight feet tall and weighs 190 pounds is very unlikely tobe called “tall and
heavy,”eventhough“eightfeet”countsas“tall”and“190pounds”counts as“heavy.”
Fuzzy predicates can also be given a probabilistic interpretation in terms of random
sets—that is, random variables whosepossible values are sets of objects. Forexample, Tall
RANDOMSET
is a random set whose possible values are sets of people. The probability P(Tall =S ),
1
where S is some particular set of people, is the probability that exactly that set would be
1
identified as “tall” by an observer. Then the probability that “Nate is tall” is the sum of the
probabilities ofallthesetsofwhichNateisamember.
Both the hybrid Bayesian network approach and the random sets approach appear to
capture aspects offuzziness without introducing degrees oftruth. Nonetheless, there remain
manyopenissuesconcerning theproperrepresentation oflinguisticobservations andcontin-
uousquantities—issues thathavebeenneglected bymostoutside thefuzzycommunity.
14.8 SUMMARY
ThischapterhasdescribedBayesiannetworks,awell-developedrepresentationforuncertain
knowledge. Bayesian networks play a role roughly analogous to that of propositional logic
fordefiniteknowledge.
• A Bayesian network is a directed acyclic graph whose nodes correspond to random
variables;eachnodehasaconditional distribution forthe node,givenitsparents.
• Bayesiannetworksprovideaconcisewaytorepresent conditionalindependencerela-
tionships inthedomain.
• ABayesian network specifies afull joint distribution; each joint entry isdefined asthe
product of the corresponding entries in the local conditional distributions. A Bayesian
networkisoftenexponentially smallerthananexplicitlyenumerated jointdistribution.
• Many conditional distributions can be represented compactly by canonical families of
552 Chapter 14. Probabilistic Reasoning
distributions. HybridBayesian networks,whichincludebothdiscreteandcontinuous
variables, useavarietyofcanonical distributions.
• Inference in Bayesian networks means computing the probability distribution of a set
of query variables, given a set of evidence variables. Exact inference algorithms, such
asvariable elimination, evaluate sumsofproducts ofconditional probabilities as effi-
cientlyaspossible.
• In polytrees (singly connected networks), exact inference takes time linear in the size
ofthenetwork. Inthegeneralcase,theproblem isintractable.
• Stochasticapproximation techniques suchaslikelihoodweightingandMarkovchain
MonteCarlocan give reasonable estimates ofthe true posterior probabilities in anet-
workandcancopewithmuchlargernetworksthancanexactalgorithms.
• Probabilitytheorycanbecombinedwithrepresentational ideasfromfirst-orderlogicto
produce very powerful systems forreasoning under uncertainty. Relational probabil-
ity models (RPMs) include representational restrictions that guarantee a well-defined
probabilitydistributionthatcanbeexpressedasanequivalentBayesiannetwork. Open-
universeprobabilitymodelshandleexistenceandidentityuncertainty,definingprob-
abiltydistributions overtheinfinitespaceoffirst-orderpossible worlds.
• Variousalternative systemsforreasoning underuncertainty havebeensuggested. Gen-
erallyspeaking, truth-functionalsystemsarenotwellsuitedforsuchreasoning.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The use of networks to represent probabilistic information began early in the 20th century,
with the work of Sewall Wright on the probabilistic analysis of genetic inheritance and an-
imal growth factors (Wright, 1921, 1934). I. J. Good (1961), in collaboration with Alan
Turing, developed probabilistic representations and Bayesian inference methods that could
be regarded as a forerunner of modern Bayesian networks—although the paper is not often
citedinthiscontext.10 Thesamepaperistheoriginal sourceforthenoisy-OR model.
The influence diagram representation for decision problems, which incorporated a
DAG representation for random variables, was used in decision analysis in the late 1970s
(see Chapter 16), but only enumeration was used for evaluation. Judea Pearl developed the
message-passing methodforcarryingoutinferenceintreenetworks(Pearl,1982a)andpoly-
tree networks (Kim and Pearl, 1983) and explained the importance of causal rather than di-
agnostic probability models, incontrast tothecertainty-factor systemstheninvogue.
The first expert system using Bayesian networks was CONVINCE (Kim, 1983). Early
applicationsinmedicineincludedtheMUNINsystemfordiagnosingneuromusculardisorders
(Andersen etal.,1989) andthe PATHFINDER system forpathology (Heckerman, 1991). The
CPCS system (Pradhan et al., 1994) is a Bayesian network for internal medicine consisting
10 I.J.GoodwaschiefstatisticianforTuring’scode-breakingteaminWorldWarII.In2001: ASpaceOdyssey
(Clarke,1968a),GoodandMinskyarecreditedwithmakingthebreakthroughthatledtothedevelopmentofthe
HAL9000computer.
Bibliographical andHistorical Notes 553
of 448 nodes, 906 links and 8,254 conditional probability values. (The front cover shows a
portionofthenetwork.)
Applications in engineering include the Electric Power Research Institute’s work on
monitoring power generators (Morjaria et al., 1995), NASA’s work on displaying time-
criticalinformationatMissionControlinHouston(HorvitzandBarry,1995),andthegeneral
fieldofnetworktomography,whichaimstoinferunobserved localproperties ofnodesand
links in the Internet from observations of end-to-end message performance (Castro et al.,
2004). Perhaps the most widely used Bayesian network systems have been the diagnosis-
and-repairmodules(e.g.,thePrinterWizard)inMicrosoft Windows(BreeseandHeckerman,
1996) and the Office Assistant in Microsoft Office (Horvitz et al., 1998). Another impor-
tant application area is biology: Bayesian networks have been used for identifying human
genesbyreferencetomousegenes(Zhangetal.,2003),inferringcellularnetworksFriedman
(2004), and manyother tasks in bioinformatics. Wecould go on, but instead we’ll refer you
toPourret etal.(2008), a400-page guidetoapplications ofBayesiannetworks.
RossShachter(1986),workingintheinfluencediagramcommunity,developedthefirst
complete algorithm for general Bayesian networks. His method was based on goal-directed
reduction ofthenetworkusing posterior-preserving transformations. Pearl(1986) developed
aclusteringalgorithmforexactinferenceingeneralBayesiannetworks,utilizingaconversion
to a directed polytree of clusters in which message passing was used to achieve consistency
over variables shared between clusters. A similar approach, developed by the statisticians
David Spiegelhalter and Steffen Lauritzen (Lauritzen and Spiegelhalter, 1988), is based on
conversion to an undirected form of graphical model called a Markov network. This ap-
MARKOVNETWORK
proach isimplemented in the HUGIN system, anefficient and widely used tool foruncertain
reasoning(Andersenetal.,1989). Boutilieretal.(1996)showhowtoexploitcontext-specific
independence inclustering algorithms.
The basic idea of variable elimination—that repeated computations within the overall
sum-of-productsexpressioncanbeavoidedbycaching—appearedinthesymbolicprobabilis-
tic inference (SPI)algorithm (Shachter et al., 1990). Theelimination algorithm wedescribe
is closest to that developed by Zhang and Poole (1994). Criteria for pruning irrelevant vari-
ablesweredeveloped byGeigeretal.(1990)andbyLauritzen etal.(1990); thecriterion we
giveisasimplespecialcaseofthese. Dechter(1999)showshowthevariableeliminationidea
NONSERIALDYNAMIC isessentiallyidenticaltononserialdynamicprogramming(BerteleandBrioschi,1972),an
PROGRAMMING
algorithmic approach thatcanbeapplied tosolve arange ofinference problems inBayesian
networks—for example, finding the most likely explanation for a set of observations. This
connectsBayesiannetworkalgorithmstorelatedmethodsforsolvingCSPsandgivesadirect
measureofthecomplexityofexactinferenceintermsofthetreewidthofthenetwork. Wexler
and Meek (2009) describe a method of preventing exponential growth in the size of factors
computed invariable elimination; theiralgorithm breaks downlarge factors into products of
smallerfactorsandsimultaneously computesanerrorbound fortheresultingapproximation.
The inclusion of continuous random variables in Bayesian networks was considered
by Pearl (1988) and Shachter and Kenley (1989); these papers discussed networks contain-
ing only continuous variables with linear Gaussian distributions. The inclusion of discrete
variables has been investigated by Lauritzen and Wermuth (1989) and implemented in the
554 Chapter 14. Probabilistic Reasoning
cHUGIN system (Olesen, 1993). Further analysis of linear Gaussian models, with connec-
tionstomanyothermodelsusedinstatistics, appearsinRoweisandGhahramani(1999)The
probit distribution is usually attributed to Gaddum (1933) and Bliss (1934), although it had
been discovered several times in the 19th century. Bliss’s work was expanded considerably
byFinney(1947). Theprobit hasbeenusedwidelyformodeling discrete choicephenomena
andcanbeextended tohandlemorethantwochoices(Daganzo,1979). Thelogitmodelwas
introduced by Berkson (1944); initially much derided, it eventually became more popular
thantheprobitmodel. Bishop(1995)givesasimplejustification foritsuse.
Cooper(1990)showedthatthegeneralproblemofinferenceinunconstrained Bayesian
networks is NP-hard, and Paul Dagum and Mike Luby (1993) showed the corresponding
approximation problem to be NP-hard. Space complexity is also a serious problem in both
clustering andvariableelimination methods. Themethodofcutsetconditioning,whichwas
developed forCSPsin Chapter 6, avoids the construction of exponentially large tables. In a
Bayesian network, a cutset is a set of nodes that, when instantiated, reduces the remaining
nodes to a polytree that can be solved in linear time and space. The query is answered by
summingoveralltheinstantiations ofthecutset, sotheoverallspace requirement isstilllin-
ear(Pearl, 1988). Darwiche (2001) describes a recursive conditioning algorithm that allows
acompleterangeofspace/time tradeoffs.
The development of fast approximation algorithms for Bayesian network inference is
a very active area, with contributions from statistics, computer science, and physics. The
rejection sampling method is a general technique that is long known to statisticians; it was
first applied to Bayesian networks by Max Henrion (1988), who called it logic sampling.
Likelihood weighting, which was developed by Fung and Chang (1989) and Shachter and
Peot (1989), is an example of the well-known statistical method of importance sampling.
Cheng and Druzdzel (2000) describe an adaptive version oflikelihood weighting that works
wellevenwhentheevidence hasverylowpriorlikelihood.
Markovchain Monte Carlo(MCMC)algorithms began withthe Metropolis algorithm,
due to Metropolis et al. (1953), which was also the source of the simulated annealing algo-
rithm described inChapter4. TheGibbssamplerwasdevised byGemanandGeman(1984)
for inference in undirected Markov networks. The application of MCMC to Bayesian net-
worksisduetoPearl(1987). ThepaperscollectedbyGilks etal.(1996)coverawidevariety
of applications of MCMC, several of which were developed in the well-known BUGS pack-
age(Gilksetal.,1994).
Therearetwoveryimportant familiesofapproximation methods thatwedidnotcover
VARIATIONAL in the chapter. The first is the family of variational approximation methods, which can be
APPROXIMATION
used to simplify complex calculations of all kinds. The basic idea is to propose a reduced
version of the original problem that is simple to work with, but that resembles the original
problem as closely as possible. The reduced problem is described by some variational pa-
VARIATIONAL rameters λ that are adjusted to minimize a distance function D between the original and
PARAMETER
the reduced problem, often by solving the system of equations ∂D/∂λ=0. In many cases,
strict upper and lower bounds can be obtained. Variational methods have long been used in
statistics (Rustagi, 1976). In statistical physics, the mean-field method is a particular vari-
MEANFIELD
ational approximation in which the individual variables making up the model are assumed
Bibliographical andHistorical Notes 555
to be completely independent. This idea was applied to solve large undirected Markov net-
works (Peterson and Anderson, 1987; Parisi, 1988). Saul et al. (1996) developed the math-
ematical foundations for applying variational methods to Bayesian networks and obtained
accuratelower-boundapproximations forsigmoidnetworks withtheuseofmean-fieldmeth-
ods. Jaakkola and Jordan (1996) extended the methodology to obtain both lower and upper
bounds. Since these early papers, variational methods have been applied to many specific
families ofmodels. Theremarkable paper byWainwright andJordan (2008) provides auni-
fyingtheoretical analysis oftheliterature onvariational methods.
A second important family of approximation algorithms is based on Pearl’s polytree
message-passing algorithm (1982a). This algorithm can be applied to general networks, as
suggested byPearl(1988). Theresults mightbeincorrect, orthealgorithm might failto ter-
minate, but in many cases, the values obtained are close to the true values. Little attention
BELIEF was paid to this so-called belief propagation (orBP)approach until McEliece et al. (1998)
PROPAGATION
observed that message passing in a multiply connected Bayesian network was exactly the
computation performed by the turbo decoding algorithm (Berrou et al., 1993), which pro-
TURBODECODING
videdamajorbreakthrough inthedesignofefficienterror-correcting codes. Theimplication
isthatBPisbothfastandaccurateontheverylargeandveryhighlyconnectednetworksused
fordecoding andmighttherefore beusefulmoregenerally. Murphyetal.(1999)presented a
promising empirical study of BP’s performance, and Weiss and Freeman (2001) established
strongconvergence resultsforBPonlinearGaussiannetworks. Weiss(2000b)showshowan
approximationcalledloopybeliefpropagationworks,andwhentheapproximationiscorrect.
Yedidia et al. (2005) made further connections between loopy propagation and ideas from
statistical physics.
Theconnection betweenprobability andfirst-order languages wasfirststudied byCar-
nap(1950). Gaifman(1964)andScottandKrauss(1966)definedalanguageinwhichproba-
bilities could beassociated withfirst-ordersentences and forwhichmodelswereprobability
measures on possible worlds. Within AI, this idea was developed for propositional logic
by Nilsson (1986) and for first-order logic by Halpern (1990). The first extensive inves-
tigation of knowledge representation issues in such languages was carried out by Bacchus
(1990). Thebasicideaisthateachsentence intheknowledgebaseexpressed aconstraint on
the distribution over possible worlds; one sentence entails another if it expresses a stronger
constraint. For example, the sentence ∀x P(Hungry(x)) > 0.2 rules out distributions
in which any object is hungry with probability less than 0.2; thus, it entails the sentence
∀x P(Hungry(x)) > 0.1. It turns out that writing a consistent set of sentences in these
languages is quite difficult and constructing a unique probability model nearly impossible
unless oneadopts therepresentation approach ofBayesian networks bywritingsuitable sen-
tencesaboutconditional probabilities.
Beginning in the early 1990s, researchers working on complex applications noticed
theexpressivelimitationsofBayesiannetworksanddevelopedvariouslanguages forwriting
“templates”withlogicalvariables,fromwhichlargenetworkscouldbeconstructed automat-
ically for each problem instance (Breese, 1992; Wellman et al., 1992). The most important
such language was BUGS (Bayesian inference Using Gibbs Sampling) (Gilks et al., 1994),
INDEXEDRANDOM whichcombinedBayesiannetworkswiththeindexedrandomvariablenotationcommonin
VARIABLE
556 Chapter 14. Probabilistic Reasoning
statistics. (InBUGS,anindexedrandomvariablelookslikeX[i],whereihasadefinedinteger
range.) TheselanguagesinheritedthekeypropertyofBayesiannetworks: everywell-formed
knowledgebasedefinesaunique,consistentprobabilitymodel. Languageswithwell-defined
semantics based on unique names and domain closure drew on the representational capa-
bilities of logic programming (Poole, 1993; Sato and Kameya, 1997; Kersting et al., 2000)
and semantic networks (Koller and Pfeffer, 1998; Pfeffer, 2000). Pfeffer (2007) went on to
develop IBAL,whichrepresents first-order probability models asprobabilistic programs ina
programming language extended with a randomization primitive. Another important thread
was the combination of relational and first-order notations with (undirected) Markov net-
works (Taskar et al., 2002; Domingos and Richardson, 2004), where the emphasis has been
lessonknowledgerepresentation andmoreonlearning fromlargedatasets.
Initially,inferenceinthesemodelswasperformedbygeneratinganequivalentBayesian
network. Pfeffer et al. (1999) introduced a variable elimination algorithm that cached each
computed factor for reuse by later computations involving the same relations but different
objects, thereby realizing some of the computational gains of lifting. The first truly lifted
inference algorithm wasalifted form of variable elimination described by Poole (2003) and
subsequently improved by de Salvo Braz et al. (2007). Further advances, including cases
wherecertainaggregateprobabilitiescanbecomputedinclosedform,aredescribedbyMilch
etal.(2008)andKisynskiandPoole(2009). PasulaandRussell(2001)studiedtheapplication
of MCMC to avoid building the complete equivalent Bayes net in cases of relational and
identity uncertainty. Getoor and Taskar (2007) collect many important papers on first-order
probability modelsandtheiruseinmachinelearning.
Probabilistic reasoning about identity uncertainty has two distinct origins. In statis-
tics, theproblem of record linkagearises whendata records donotcontain standard unique
RECORDLINKAGE
identifiers—for example, various citations of this book might name its first author “Stuart
Russell” or“S. J. Russell” oreven “Stewart Russle,” and other authors may use the some of
the same names. Literally hundreds of companies exist solely to solve record linkage prob-
lems in financial, medical, census, and other data. Probabilistic analysis goes back to work
by Dunn (1946); the Fellegi–Sunter model (1969), which is essentially naive Bayes applied
to matching, still dominates current practice. The second origin for work on identity uncer-
tainty is multitarget tracking (Sittler, 1964), which we cover in Chapter 15. For most of its
history, work in symbolic AI assumed erroneously that sensors could supply sentences with
uniqueidentifiersforobjects. Theissuewasstudiedinthecontextoflanguageunderstanding
by Charniak and Goldman (1992) and in the context of surveillance by (Huang and Russell,
1998) and Pasula et al. (1999). Pasula et al. (2003) developed a complex generative model
for authors, papers, and citation strings, involving both relational and identity uncertainty,
and demonstrated high accuracy for citation information extraction. The first formally de-
fined language for open-universe probability models was BLOG (Milch et al., 2005), which
camewith acomplete (albeit slow) MCMCinference algorithm forallwell-defined mdoels.
(The program code faintly visible on the front cover of this book is part of a BLOG model
fordetecting nuclear explosions from seismic signals aspart ofthe UNComprehensive Test
Ban Treaty verification regime.) Laskey (2008) describes another open-universe modeling
language calledmulti-entityBayesiannetworks.
Bibliographical andHistorical Notes 557
As explained in Chapter 13, early probabilistic systems fell out of favor in the early
1970s, leaving a partial vacuum to be filled by alternative methods. Certainty factors were
invented foruseinthemedicalexpertsystem MYCIN (Shortliffe, 1976),whichwasintended
both as an engineering solution and as a model of human judgment under uncertainty. The
collection Rule-Based Expert Systems (Buchanan and Shortliffe, 1984) provides a complete
overview of MYCIN and its descendants (see also Stefik, 1995). David Heckerman (1986)
showed that a slightly modified version of certainty factor calculations gives correct proba-
bilistic results in some cases, but results in serious overcounting of evidence in other cases.
ThePROSPECTOR expertsystem(Dudaetal.,1979)usedarule-basedapproachinwhichthe
ruleswerejustifiedbya(seldom tenable)globalindependence assumption.
Dempster–Shafer theoryoriginates withapaperbyArthurDempster(1968) proposing
ageneralizationofprobabilitytointervalvaluesandacombinationruleforusingthem. Later
workbyGlennShafer(1976)ledtotheDempster-Shafertheory’sbeingviewedasacompet-
ing approach to probability. Pearl (1988) and Ruspini et al. (1992) analyze the relationship
betweentheDempster–Shafer theoryandstandardprobability theory.
FuzzysetsweredevelopedbyLotfiZadeh(1965)inresponsetotheperceiveddifficulty
of providing exact inputs to intelligent systems. The text by Zimmermann (2001) provides
a thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in
Zimmermann (1999). As we mentioned in the text, fuzzy logic has often been perceived
incorrectlyasadirectcompetitortoprobabilitytheory,whereasinfactitaddressesadifferent
setofissues. Possibilitytheory(Zadeh,1978)wasintroducedtohandleuncertaintyinfuzzy
POSSIBILITYTHEORY
systems and has much in common with probability. Dubois and Prade (1994) survey the
connections betweenpossibility theoryandprobability theory.
The resurgence of probability depended mainly on Pearl’s development of Bayesian
networksasamethodforrepresentingandusingconditional independence information. This
resurgence did not comewithout afight; PeterCheeseman’s (1985) pugnacious “In Defense
ofProbability” andhislaterarticle“AnInquiry intoComputerUnderstanding” (Cheeseman,
1988, with commentaries) give something of the flavor of the debate. Eugene Charniak
helped present the ideas to AI researchers with a popular article, “Bayesian networks with-
out tears”11 (1991), and book (1993). The book by Dean and Wellman (1991) also helped
introduceBayesiannetworkstoAIresearchers. Oneoftheprincipalphilosophicalobjections
ofthelogicists wasthat thenumerical calculations thatprobability theory wasthought tore-
quirewerenotapparenttointrospection andpresumedanunrealistic levelofprecisioninour
uncertain knowledge. The development of qualitative probabilistic networks (Wellman,
1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion of
positive and negative influences between variables. Wellman shows that in manycases such
information is sufficient for optimal decision making without the need for the precise spec-
ification of probability values. Goldszmidt and Pearl (1996) take a similar approach. Work
by Adnan Darwiche and Matt Ginsberg (1992) extracts the basic properties of conditioning
andevidencecombinationfromprobability theoryandshows thattheycanalsobeappliedin
logicalanddefaultreasoning. Often,programsspeaklouderthanwords,andthereadyavail-
11 Thetitleoftheoriginalversionofthearticlewas“Pearlforswine.”
558 Chapter 14. Probabilistic Reasoning
abilityofhigh-quality softwaresuchastheBayesNettoolkit(Murphy,2001)accelerated the
adoption ofthetechnology.
ThemostimportantsinglepublicationinthegrowthofBayesiannetworkswasundoubt-
edly the text Probabilistic Reasoning in Intelligent Systems (Pearl, 1988). Several excellent
texts (Lauritzen, 1996; Jensen, 2001; Korb and Nicholson, 2003; Jensen, 2007; Darwiche,
2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have cov-
ered in this chapter. New research on probabilistic reasoning appears both in mainstream
AI journals, such as Artificial Intelligence and the Journal of AI Research, and in more spe-
cialized journals, suchastheInternational JournalofApproximate Reasoning. Manypapers
on graphical models, which include Bayesian networks, appear in statistical journals. The
proceedings oftheconferences onUncertainty inArtificial Intelligence (UAI),NeuralInfor-
mation Processing Systems (NIPS), and Artificial Intelligence and Statistics (AISTATS)are
excellent sourcesforcurrentresearch.
EXERCISES
14.1 Wehaveabagofthree biased coins a,b,and cwithprobabilities ofcoming upheads
of 20%, 60%, and 80%, respectively. Onecoin isdrawn randomly from the bag (with equal
likelihood of drawing each of the three coins), and then the coin is flipped three times to
generate theoutcomes X ,X ,andX .
1 2 3
a. DrawtheBayesiannetworkcorresponding tothissetupanddefinethenecessaryCPTs.
b. Calculate which coinwasmostlikely tohavebeen drawnfrom thebag iftheobserved
flipscomeoutheadstwiceandtailsonce.
14.2 Equation (14.1) on page 513 defines the joint distribution represented by a Bayesian
networkintermsoftheparametersθ(X |Parents(X )). Thisexerciseasksyoutoderivethe
i i
equivalence between the parameters and the conditional probabilities P(X |Parents(X ))
i i
fromthisdefinition.
a. Consider a simple network X → Y → Z with three Boolean variables. Use Equa-
tions (13.3) and (13.6) (pages 485 and 492) to express the conditional probability
P(z|y)astheratiooftwosums,eachoverentriesinthejointdistribution P(X,Y,Z).
b. Now use Equation (14.1) to write this expression in terms of the network parameters
θ(X),θ(Y |X),andθ(Z|Y).
c. Next,expandoutthesummationsinyourexpressionfrompart(b),writingoutexplicitly
the terms for the true and false values of each summed variable. Assuming that all
(cid:2)
network parameters satisfy the constraint θ(x |parents(X ))=1, show that the
xi i i
resultingexpression reducesto θ(x|y).
d. Generalize this derivation to show that θ(X |Parents(X )) = P(X |Parents(X ))
i i i i
foranyBayesiannetwork.
Exercises 559
14.3 TheoperationofarcreversalinaBayesiannetworkallowsustochangethedirection
ARCREVERSAL
of an arc X → Y while preserving the joint probability distribution that the network repre-
sents (Shachter, 1986). Arc reversal may require introducing new arcs: all the parents of X
alsobecomeparentsofY,andallparents ofY alsobecomeparents ofX.
a. Assume that X and Y start with m and n parents, respectively, and that all variables
havekvalues. BycalculatingthechangeinsizefortheCPTsofX andY,showthatthe
total number of parameters in the network cannot decrease during arc reversal. (Hint:
theparents ofX andY neednotbedisjoint.)
b. Underwhatcircumstances canthetotalnumberremainconstant?
c. Let the parents of X be U∪ V and the parents of Y be V ∪ W, where U and W are
disjoint. TheformulasforthenewCPTsafterarcreversalareasfollows:
(cid:12)
P(Y |U,V,W) = P(Y |V,W,x)P(x|U,V)
x
P(X|U,V,W,Y) = P(Y |X,V,W)P(X|U,V)/P(Y |U,V,W).
Prove that the new network expresses the same joint distribution over all variables as
theoriginal network.
14.4 ConsidertheBayesiannetworkinFigure14.2.
a. Ifnoevidenceisobserved,areBurglary andEarthquake independent? Provethisfrom
thenumerical semanticsandfromthetopological semantics.
b. Ifweobserve Alarm=true,areBurglary andEarthquake independent? Justifyyour
answerbycalculatingwhethertheprobabilities involvedsatisfythedefinitionofcondi-
tionalindependence.
14.5 SupposethatinaBayesiannetworkcontaining anunobserved variable Y,allthevari-
ablesintheMarkovblanket MB(Y)havebeenobserved.
a. Provethatremovingthenode Y fromthenetworkwillnotaffecttheposteriordistribu-
tionforanyotherunobserved variableinthenetwork.
b. Discusswhetherwecanremove Y ifweareplanning touse(i)rejection sampling and
(ii)likelihood weighting.
14.6 LetH bearandomvariabledenotingthehandednessofanindividualx,withpossible
x
values l orr. Acommonhypothesis isthatleft-orright-handedness isinherited byasimple
mechanism; that is, perhaps there is a gene G , also with values l or r, and perhaps actual
x
handedness turns out mostly the same (with some probability s) as the gene an individual
possesses. Furthermore, perhaps the gene itself is equally likely to be inherited from either
ofanindividual’s parents, withasmallnonzeroprobability mofarandom mutationflipping
thehandedness.
a. Which of the three networks in Figure 14.20 claim that P(G ,G ,G ) =
father mother child
P(G )P(G )P(G )?
father mother child
b. Which of the three networks make independence claims that are consistent with the
hypothesis abouttheinheritance ofhandedness?
560 Chapter 14. Probabilistic Reasoning
G G G G G G
mother father mother father mother father
H H H H H H
mother father mother father mother father
G G G
child child child
H H H
child child child
(a) (b) (c)
Figure14.20 ThreepossiblestructuresforaBayesiannetworkdescribinggeneticinheri-
tanceofhandedness.
c. Whichofthethreenetworksisthebestdescription ofthehypothesis?
d. WritedowntheCPTforthe G nodeinnetwork(a),intermsof sandm.
child
e. Suppose that P(G =l) = P(G =l) = q. In network (a), derive an expres-
father mother
sionforP(G =l)intermsofmandq only,byconditioning onitsparentnodes.
child
f. Under conditions of genetic equilibrium, we expect the distribution of genes to be the
sameacrossgenerations. Usethistocalculatethevalueofq,and,givenwhatyouknow
abouthandedness inhumans, explainwhythehypothesis described atthebeginning of
thisquestionmustbewrong.
14.7 The Markov blanket of a variable is defined on page 517. Prove that a variable
is independent of all other variables in the network, given its Markov blanket and derive
Equation(14.12)(page538).
Battery
Radio Ignition Gas
Starts
Moves
Figure 14.21 A Bayesian network describing some features of a car’s electrical system
and engine. Each variable is Boolean, and the true value indicates that the corresponding
aspectofthevehicleisinworkingorder.
Exercises 561
14.8 Considerthenetworkforcardiagnosis showninFigure14.21.
a. ExtendthenetworkwiththeBooleanvariables IcyWeather andStarterMotor.
b. Givereasonable conditional probability tablesforallthenodes.
c. How many independent values are contained in the joint probability distribution for
eight Boolean nodes, assuming that no conditional independence relations are known
toholdamongthem?
d. Howmanyindependent probability valuesdoyournetworktablescontain?
e. TheconditionaldistributionforStarts couldbedescribedasanoisy-ANDdistribution.
Definethisfamilyingeneral andrelateittothenoisy-ORdistribution.
14.9 ConsiderthefamilyoflinearGaussiannetworks, asdefinedonpage520.
a. In a two-variable network, let X be the parent of X , let X have a Gaussian prior,
1 2 1
and let P(X |X ) be a linear Gaussian distribution. Show that the joint distribution
2 1
P(X ,X )isamultivariate Gaussian, andcalculateitscovariance matrix.
1 2
b. Prove by induction that the joint distribution for a general linear Gaussian network on
X ,...,X isalsoamultivariate Gaussian.
1 n
14.10 Theprobit distribution definedonpage522describes theprobability distribution for
aBooleanchild,givenasinglecontinuous parent.
a. Howmightthedefinition beextended tocovermultiplecontinuous parents?
b. Howmight itbe extended to handle amultivalued child variable? Consider both cases
where the child’s values are ordered (as in selecting a gear while driving, depending
on speed, slope, desired acceleration, etc.) and cases where they are unordered (as in
selecting bus, train, orcarto gettowork). (Hint: Considerways todivide the possible
valuesintotwosets,tomimicaBooleanvariable.)
14.11 Inyourlocalnuclearpowerstation, thereisanalarmthatsenseswhenatemperature
gauge exceeds agiven threshold. Thegaugemeasures thetemperature ofthecore. Consider
the Boolean variables A (alarm sounds), F (alarm is faulty), and F (gauge is faulty) and
A G
themultivaluednodesG(gaugereading) andT (actualcoretemperature).
a. Draw a Bayesian network for this domain, given that the gauge is more likely to fail
whenthecoretemperature getstoohigh.
b. Isyournetworkapolytree? Whyorwhynot?
c. Supposetherearejusttwopossibleactualandmeasuredtemperatures,normalandhigh;
theprobability thatthegaugegivesthecorrecttemperature isxwhenitisworking, but
y whenitisfaulty. Givetheconditional probability tableassociated withG.
d. Suppose the alarm works correctly unless it is faulty, in which case it never sounds.
Givetheconditional probability tableassociated withA.
e. Suppose the alarm and gauge are working and the alarm sounds. Calculate an expres-
sion for the probability that the temperature of the core is too high, in terms of the
variousconditional probabilities inthenetwork.
562 Chapter 14. Probabilistic Reasoning
F F F N F M M
1 2 1 2 1 2
M M N
1 2
M M
1 2
N F F
1 2
(i) (ii) (iii)
Figure14.22 Threepossiblenetworksforthetelescopeproblem.
14.12 Two astronomers in different parts of the world make measurements M and M of
1 2
thenumberofstarsN insomesmallregionofthesky,usingtheirtelescopes. Normally,there
is a small possibility e of error by up to one star in each direction. Each telescope can also
(withamuch smallerprobability f)bebadly outoffocus (events F and F ), inwhichcase
1 2
the scientist will undercount by three or more stars (or if N is less than 3, fail to detect any
starsatall). ConsiderthethreenetworksshowninFigure14.22.
a. Which of these Bayesian networks are correct (but not necessarily efficient) represen-
tationsofthepreceding information?
b. Whichisthebestnetwork? Explain.
c. WriteoutaconditionaldistributionforP(M |N),forthecasewhereN ∈{1,2,3}and
1
M ∈{0,1,2,3,4}. Eachentryintheconditional distribution should beexpressed asa
1
functionoftheparameters eand/orf.
d. SupposeM =1andM =3. Whatarethepossible numbersofstarsifyouassumeno
1 2
priorconstraint onthevaluesof N?
e. What is the most likely number of stars, given these observations? Explain how to
computethis, orifitisnotpossible tocompute, explain whatadditional information is
neededandhowitwouldaffecttheresult.
14.13 Consider the network shown inFigure 14.22(ii), and assume that the twotelescopes
work identically. N ∈{1,2,3} and M ,M ∈{0,1,2,3,4}, with the symbolic CPTs as de-
1 2
scribed in Exercise 14.12. Using the enumeration algorithm (Figure 14.9 on page 525), cal-
culatetheprobability distribution P(N |M =2,M =2).
1 2
14.14 ConsidertheBayesnetshowninFigure14.23.
a. Whichofthefollowingareassertedbythenetwork structure?
(i) P(B,I,M) = P(B)P(I)P(M).
(ii) P(J|G) = P(J|G,I).
(iii) P(M|G,B,I) = P(M|G,B,I,J).
Exercises 563
B M P(I)
t t .9
t f .5
f t .5
P(B) f f .1 P(M)
.9 .1
B I M
B I M P(G)
G
t t t .9
t t f .8
t f t .0
t f f .0 G P(J)
f t t .2 J t .9
f t f .1 f .0
f f t .0
f f f .0
Figure 14.23 A simple Bayes net with Boolean variables B=BrokeElectionLaw,
I=Indicted,M=PoliticallyMotivatedProsecutor,G=FoundGuilty,J=Jailed.
b. CalculatethevalueofP(b,i,¬m,g,j).
c. Calculate the probability that someone goes to jail given that they broke the law, have
beenindicted, andfaceapolitically motivatedprosecutor.
d. A context-specific independence (see page 542) allows a variable to be independent
ofsomeofitsparentsgivencertainvaluesofothers. Inadditiontotheusualconditional
independences given bythegraph structure, whatcontext-specific independences exist
intheBayesnetinFigure14.23?
e. SupposewewanttoaddthevariableP =PresidentialPardon tothenetwork;drawthe
newnetworkandbrieflyexplainanylinksyouadd.
14.15 Considerthevariable eliminationalgorithm inFigure14.11(page528).
a. Section14.4appliesvariableelimination tothequery
P(Burglary|JohnCalls=true,MaryCalls=true).
Performthecalculations indicated andcheckthattheansweriscorrect.
b. Countthenumberofarithmetic operations performed, andcompare itwiththenumber
performedbytheenumeration algorithm.
c. Supposeanetworkhastheformofachain: asequenceofBooleanvariablesX ,...,X
1 n
where Parents(X i )={X i−1 } for i=2,...,n. What is the complexity of computing
P(X |X =true)usingenumeration? Usingvariableelimination?
1 n
d. Provethatthecomplexityofrunningvariableeliminationonapolytreenetworkislinear
inthesizeofthetreeforanyvariableordering consistent withthenetworkstructure.
14.16 Investigate thecomplexity ofexactinferenceingeneralBayesiannetworks:
a. Provethatany3-SATproblemcanbereducedtoexactinferenceinaBayesiannetwork
constructed to represent the particular problem and hence that exact inference is NP-
564 Chapter 14. Probabilistic Reasoning
hard. (Hint: Consideranetworkwithonevariableforeachproposition symbol,onefor
eachclause,andonefortheconjunction ofclauses.)
b. Theproblem ofcounting thenumberofsatisfying assignments fora3-SATproblem is
#P-complete. Showthatexactinference isatleastashardasthis.
14.17 Consider the problem of generating a random sample from a specified distribution
on a single variable. Assume you have a random number generator that returns a random
numberuniformly distributed between0and1.
a. Let X be a discrete variable with P(X=x )=p for i∈{1,...,k}. The cumulative
i i
CUMULATIVE distributionofX givestheprobability thatX∈{x ,...,x }foreachpossiblej. (See
DISTRIBUTION 1 j
also Appendix A.) Explain how to calculate the cumulative distribution in O(k) time
and how to generate a single sample of X from it. Can the latter be done in less than
O(k)time?
b. Nowsuppose wewanttogenerate N samplesofX,whereN & k. Explainhowtodo
thiswithanexpectedruntimepersamplethatis constant (i.e.,independent ofk).
c. Now consider a continuous-valued variable with a parameterized distribution (e.g.,
Gaussian). Howcansamplesbegenerated fromsuchadistribution?
d. Suppose youwanttoquery acontinuous-valued variable and youareusing asampling
algorithmsuchas LIKELIHOODWEIGHTING todotheinference. Howwouldyouhave
tomodifythequery-answering process?
14.18 ConsiderthequeryP(Rain|Sprinkler =true,WetGrass=true)inFigure14.12(a)
(page529)andhowGibbssamplingcananswerit.
a. HowmanystatesdoestheMarkovchainhave?
b. Calculatethetransition matrixQcontaining q(y → y (cid:2) )forally,y (cid:2) .
c. WhatdoesQ2,thesquareofthetransition matrix,represent?
d. WhataboutQn asn → ∞?
e. Explain how to do probabilistic inference in Bayesian networks, assuming that Qn is
available. Isthisapractical waytodoinference?
14.19 Thisexerciseexploresthestationary distribution forGibbssamplingmethods.
a. Theconvexcomposition [α,q ;1−α,q ]ofq andq isatransition probability distri-
1 2 1 2
bution that first chooses one of q and q with probabilities α and 1−α, respectively,
1 2
and then applies whichever is chosen. Prove that if q and q are in detailed balance
1 2
with π, then their convex composition is also in detailed balance with π. (Note: this
result justifies avariant of GIBBS-ASK inwhichvariables arechosen atrandom rather
thansampledinafixedsequence.)
b. Prove that if each of q and q has π as its stationary distribution, then the sequential
1 2
composition q=q ◦q alsohasπ asitsstationary distribution.
1 2
METROPOLIS– 14.20 TheMetropolis–HastingsalgorithmisamemberoftheMCMCfamily;assuch,itis
HASTINGS
designedtogeneratesamplesx(eventually)accordingtotargetprobabilities π(x). (Typically
Exercises 565
weare interested in sampling from π(x)=P(x|e).) Like simulated annealing, Metropolis–
(cid:2)
PROPOSAL Hastingsoperatesintwostages. First,itsamplesanewstatex fromaproposaldistribution
DISTRIBUTION
q(x
(cid:2)|x),giventhecurrent
state x. Then,itprobabilistically acceptsorrejects x
(cid:2)
according to
ACCEPTANCE theacceptanceprobability
PROBABILITY (cid:13) (cid:14)
π(x (cid:2) )q(x|x (cid:2) )
α(x
(cid:2)|x)=
min 1, .
π(x)q(x(cid:2)|x)
Iftheproposal isrejected, thestateremainsat x.
a. Consider an ordinary Gibbs sampling step for a specific variable X . Show that this
i
step, considered as a proposal, is guaranteed to be accepted by Metropolis–Hastings.
(Hence,GibbssamplingisaspecialcaseofMetropolis–Hastings.)
b. Showthatthetwo-stepprocessabove,viewedasatransition probability distribution, is
indetailed balancewithπ.
14.21 Three soccer teams A, B, and C, play each other once. Each match is between two
teams, and can be won, drawn, orlost. Each team has afixed, unknown degree ofquality—
anintegerrangingfrom0to3—andtheoutcomeofamatchdependsprobabilistically onthe
difference inquality betweenthetwoteams.
a. Constructarelationalprobabilitymodeltodescribethis domain,andsuggestnumerical
valuesforallthenecessary probability distributions.
b. Constructtheequivalent Bayesiannetworkforthethreematches.
c. Suppose that in the first two matches A beats B and draws with C. Using an exact
inference algorithm ofyourchoice, compute theposterior distribution forthe outcome
ofthethirdmatch.
d. Suppose there are n teams in the league and we have the results for all but the last
match. Howdoesthecomplexityofpredicting thelastgamevarywithn?
e. Investigate theapplication ofMCMCtothisproblem. Howquicklydoesitconverge in
practiceandhowwelldoesitscale?
15
PROBABILISTIC
REASONING OVER TIME
Inwhichwetrytointerpret thepresent, understand thepast,andperhaps predict
thefuture, evenwhenverylittleiscrystalclear.
Agentsinpartiallyobservableenvironmentsmustbeabletokeeptrackofthecurrentstate,to
theextentthattheirsensorsallow. InSection4.4weshowedamethodologyfordoingthat: an
agentmaintainsabeliefstatethatrepresentswhichstatesoftheworldarecurrentlypossible.
From the belief state and a transition model, the agent can predict how the world might
evolve in the next timestep. From the percepts observed and a sensor model, the agent can
updatethebeliefstate. Thisisapervasiveidea: inChapter4beliefstateswererepresentedby
explicitly enumerated sets of states, whereas in Chapters 7 and 11 they were represented by
logicalformulas. Thoseapproaches definedbeliefstatesintermsofwhichworldstateswere
possible, butcouldsaynothing aboutwhichstateswerelikelyorunlikely. Inthischapter, we
useprobability theorytoquantify thedegreeofbeliefinelementsofthebeliefstate.
As we show in Section 15.1, time itself is handled in the same way as in Chapter 7: a
changingworldismodeledusingavariableforeachaspectoftheworldstateateachpointin
time. The transition and sensor models maybe uncertain: the transition model describes the
probability distribution of the variables at time t, given the state of the world at past times,
while the sensor model describes the probability of each percept at time t, given the current
state of the world. Section 15.2 defines the basic inference tasks and describes the gen-
eral structure of inference algorithms for temporal models. Then we describe three specific
kinds of models: hidden Markov models, Kalman filters, and dynamic Bayesian net-
works (which include hidden Markov models and Kalman filters as special cases). Finally,
Section15.6examinestheproblemsfacedwhenkeeping track ofmorethanonething.
15.1 TIME AND UNCERTAINTY
Wehavedevelopedourtechniques forprobabilistic reasoning inthecontextofstaticworlds,
in which each random variable has a single fixed value. Forexample, when repairing a car,
we assume that whatever is broken remains broken during the process of diagnosis; our job
istoinferthestateofthecarfromobservedevidence, which alsoremainsfixed.
566
Section15.1. TimeandUncertainty 567
Nowconsideraslightly different problem: treating adiabetic patient. Asinthecaseof
carrepair, wehave evidence such asrecent insulin doses, food intake, blood sugarmeasure-
ments,andotherphysicalsigns. Thetaskistoassessthecurrentstateofthepatient,including
the actual blood sugar level and insulin level. Given this information, we can make a deci-
sion about the patient’s food intake and insulin dose. Unlike the case of car repair, here the
dynamic aspects of the problem are essential. Blood sugar levels and measurements thereof
can change rapidly over time, depending on recent food intake and insulin doses, metabolic
activity, the time of day, and so on. To assess the current state from the history of evidence
andtopredict theoutcomes oftreatmentactions, wemustmodelthesechanges.
Thesame considerations arise in many other contexts, such as tracking the location of
a robot, tracking the economic activity of a nation, and making sense of a spoken orwritten
sequence ofwords. Howcandynamicsituations likethesebemodeled?
15.1.1 States and observations
We view the world as a series of snapshots, or time slices, each of which contains a set of
TIMESLICE
random variables, some observable and some not.1 For simplicity, we will assume that the
samesubsetofvariablesisobservableineachtimeslice(althoughthisisnotstrictlynecessary
inanything thatfollows). WewilluseX todenote thesetofstate variables attime t,which
t
are assumed to be unobservable, and E to denote the set of observable evidence variables.
t
Theobservation attimetisE =e forsomesetofvalues e .
t t t
Considerthefollowingexample: Youarethesecurityguardstationedatasecretunder-
ground installation. Youwanttoknowwhetherit’srainingtoday, butyouronlyaccesstothe
outside worldoccurseachmorningwhenyouseethedirectorcominginwith,orwithout, an
umbrella. Foreachdayt,thesetE thuscontainsasingleevidencevariableUmbrella orU
t t t
forshort(whethertheumbrellaappears),andtheset X containsasinglestatevariableRain
t t
orR forshort(whetheritisraining). Otherproblemscaninvolve largersetsofvariables. In
t
thediabetes example,wemighthaveevidencevariables, suchasMeasuredBloodSugar and
t
PulseRate , andstate variables, such as BloodSugar andStomachContents . (Notice that
t t t
BloodSugar andMeasuredBloodSugar arenotthesamevariable; thisishowwedealwith
t t
noisymeasurements ofactualquantities.)
Theintervalbetweentimeslicesalsodependsontheproblem. Fordiabetesmonitoring,
asuitable interval might beanhourrather than aday. Inthis chapter weassume theinterval
between slices is fixed, so we can label times by integers. We will assume that the state
sequencestartsatt=0;forvariousuninterestingreasons,wewillassumethatevidencestarts
arrivingatt=1ratherthant=0. Hence,ourumbrellaworldisrepresentedbystatevariables
R , R , R ,... and evidence variables U , U ,.... We will use the notation a:b to denote
0 1 2 1 2
the sequence of integers from a to b (inclusive), and the notation X to denote the set of
a:b
variables from X toX . Forexample, U corresponds tothevariables U ,U ,U .
a b 1:3 1 2 3
1 Uncertaintyovercontinuoustimecanbemodeledbystochasticdifferentialequations(SDEs). Themodels
studiedinthischaptercanbeviewedasdiscrete-timeapproximationstoSDEs.
568 Chapter 15. Probabilistic Reasoning overTime
(a) X X X X X
t–2 t–1 t t+1 t+2
(b) X X X X X
t–2 t–1 t t+1 t+2
Figure15.1 (a)Bayesiannetworkstructurecorrespondingtoafirst-orderMarkovprocess
withstatedefinedbythevariablesXt. (b)Asecond-orderMarkovprocess.
15.1.2 Transitionandsensormodels
With the set of state and evidence variables for a given problem decided on, the next step is
to specify how the world evolves (the transition model) and how the evidence variables get
theirvalues(thesensormodel).
Thetransitionmodelspecifiestheprobabilitydistribution overthelateststatevariables,
given the previous values, that is, P(X t |X 0:t−1 ). Now we face a problem: the set X 0:t−1 is
MARKOV unbounded insizeastincreases. WesolvetheproblembymakingaMarkovassumption—
ASSUMPTION
that thecurrent state depends ononly a finite fixed number ofprevious states. Processes sat-
isfyingthisassumption werefirststudiedindepthbytheRussianstatistician AndreiMarkov
(1856–1922)andarecalledMarkovprocessesorMarkovchains. Theycomeinvariousfla-
MARKOVPROCESS
FIRST-ORDER vors;thesimplestisthefirst-orderMarkovprocess,inwhichthecurrentstatedependsonly
MARKOVPROCESS
on the previous state and not on any earlier states. In other words, a state provides enough
information tomakethefutureconditionally independent ofthepast,andwehave
P(X t |X 0:t−1 ) = P(X t |X t−1 ). (15.1)
Hence, in a first-order Markov process, the transition model is the conditional distribution
P(X t |X t−1 ). The transition model for a second-order Markov process is the conditional
distribution P(X t |X t−2 ,X t−1 ). Figure 15.1 shows the Bayesian network structures corre-
sponding tofirst-orderandsecond-order Markovprocesses.
Even with the Markov assumption there is still a problem: there are infinitely many
possible values of t. Do we need to specify a different distribution for each time step? We
avoid this problem by assuming that changes in the world state are caused by a stationary
STATIONARY process—thatis,aprocessofchangethatisgovernedbylawsthatdonotthemselveschange
PROCESS
overtime. (Don’t confuse stationary with static: in a static process, the state itself does not
change.) Intheumbrellaworld,then, theconditional probability ofrain, P(R t |R t−1 ),isthe
sameforallt,andweonlyhavetospecifyoneconditional probability table.
Now for the sensor model. The evidence variables E could depend on previous vari-
t
ablesaswellasthecurrent state variables, butanystatethat’s worthitssaltshould sufficeto
SENSORMARKOV generatethecurrentsensorvalues. Thus,wemakea sensorMarkovassumptionasfollows:
ASSUMPTION
P(E t |X 0:t ,E 0:t−1 )= P(E t |X t ). (15.2)
Thus,P(E |X )isoursensormodel(sometimescalledtheobservation model). Figure15.2
t t
shows both the transition model and the sensor model for the umbrella example. Notice the
Section15.1. TimeandUncertainty 569
R P(R )
t-1 t
t 0.7
f 0.3
Rain Rain Rain
t–1 t t+1
R P(U )
t t
t 0.9
f 0.2
Umbrella Umbrella Umbrella
t–1 t t+1
Figure 15.2 Bayesian network structure and conditional distributions describing the
umbrella world. The transition model is P(Raint |Raint−1 ) and the sensor model is
P(Umbrellat |Raint).
direction of the dependence between state and sensors: the arrows go from the actual state
of the world to sensor values because the state of the world causes the sensors to take on
particular values: the rain causes the umbrella to appear. (The inference process, of course,
goes in the other direction; the distinction between the direction of modeled dependencies
andthedirection ofinference isoneoftheprincipal advantages ofBayesiannetworks.)
In addition to specifying the transition and sensor models, we need to say how every-
thing gets started—the prior probability distribution at time 0, P(X ). With that, we have a
0
specification of the complete joint distribution over all the variables, using Equation (14.2).
Foranyt,
(cid:25)t
P(X 0:t ,E 1:t )= P(X 0 ) P(X i |X i−1 )P(E i |X i ). (15.3)
i=1
Thethreetermsontheright-hand sidearetheinitial statemodelP(X ),thetransition model
0
P(X i |X i−1 ),andthesensormodelP(E i |X i ).
Thestructure in Figure 15.2 is a first-order Markov process—the probability of rain is
assumed todepend only onwhetheritrained theprevious day. Whethersuch anassumption
isreasonable depends on the domain itself. Thefirst-order Markov assumption says that the
state variables contain all the information needed tocharacterize the probability distribution
forthenext time slice. Sometimes the assumption is exactly true—forexample, ifaparticle
is executing a random walk along the x-axis, changing its position by ±1 at each time step,
then using the x-coordinate as the state gives a first-order Markov process. Sometimes the
assumptionisonlyapproximate, asinthecaseofpredicting rainonlyonthebasisofwhether
itrainedthepreviousday. Therearetwowaystoimprovetheaccuracyoftheapproximation:
1. Increasing the order of the Markov process model. For example, we could make a
second-ordermodelbyaddingRain t−2 asaparentofRain t ,whichmightgiveslightly
more accurate predictions. For example, in Palo Alto, California, it very rarely rains
morethantwodaysinarow.
2. Increasing the set of state variables. For example, we could add Season to allow
t
570 Chapter 15. Probabilistic Reasoning overTime
us to incorporate historical records of rainy seasons, or we could add Temperature ,
t
Humidity andPressure (perhapsatarangeoflocations)toallowustouseaphysical
t t
modelofrainyconditions.
Exercise 15.1 asks you to show that the first solution—increasing the order—can always be
reformulated as an increase in the set of state variables, keeping the order fixed. Notice that
adding state variables might improve the system’s predictive power but also increases the
prediction requirements: we now have to predict the new variables as well. Thus, we are
lookingfora“self-sufficient”setofvariables,whichreallymeansthatwehavetounderstand
the “physics” of the process being modeled. The requirement for accurate modeling of the
process isobviously lessened ifwecan add newsensors (e.g., measurements oftemperature
andpressure) thatprovideinformation directly aboutthenewstatevariables.
Consider,forexample,theproblemoftrackingarobotwanderingrandomlyontheX–Y
plane. Onemightpropose thattheposition andvelocityarea sufficientsetofstatevariables:
onecansimplyuseNewton’slawstocalculatethenewposition,andthevelocitymaychange
unpredictably. Iftherobotisbattery-powered,however,thenbatteryexhaustionwouldtendto
haveasystematiceffectonthechangeinvelocity. Becausethisinturndependsonhowmuch
powerwasused byallprevious maneuvers, the Markov property isviolated. Wecan restore
theMarkovproperty byincluding thechargelevel Battery asoneofthestatevariables that
t
make up X . This helps in predicting the motion of the robot, but in turn requires a model
t
for predicting Battery t from Battery t−1 and the velocity. In some cases, that can be done
reliably, but more often we findthat error accumulates overtime. In that case, accuracy can
beimprovedbyaddinganewsensorforthebatterylevel.
15.2 INFERENCE IN TEMPORAL MODELS
Havingsetupthestructureofagenerictemporalmodel,wecanformulatethebasicinference
tasksthatmustbesolved:
• Filtering: This is the task of computing the belief state—the posterior distribution
FILTERING
over the most recent state—given all evidence to date. Filtering2 is also called state
BELIEFSTATE
estimation. Inourexample,wewishtocompute P(X |e ). Intheumbrellaexample,
STATEESTIMATION t 1:t
this would mean computing the probability of rain today, given all the observations of
the umbrella carrier made so far. Filtering is what a rational agent does to keep track
of the current state so that rational decisions can be made. It turns out that an almost
identicalcalculation providesthelikelihood oftheevidence sequence, P(e ).
1:t
• Prediction: Thisisthetaskofcomputingtheposteriordistributionoverthefuturestate,
PREDICTION
givenall evidence todate. Thatis, wewishtocompute P(X |e )forsome k > 0.
t+k 1:t
Inthe umbrella example, thismight meancomputing theprobability ofrainthree days
fromnow,givenalltheobservationstodate. Predictionisusefulforevaluatingpossible
coursesofactionbasedontheirexpectedoutcomes.
2 Theterm“filtering”referstotherootsofthisprobleminearlyworkonsignalprocessing,wheretheproblem
istofilteroutthenoiseinasignalbyestimatingitsunderlyingproperties.
Section15.2. Inference inTemporalModels 571
• Smoothing: This is the task of computing the posterior distribution over a past state,
SMOOTHING
givenallevidenceuptothepresent. Thatis,wewishtocomputeP(X |e )forsomek
k 1:t
suchthat0 ≤ k <t. Intheumbrellaexample,itmightmeancomputingtheprobability
that it rained last Wednesday, given all the observations of the umbrella carrier made
uptotoday. Smoothing provides abetterestimate ofthestate thanwasavailable atthe
time,becauseitincorporates moreevidence.3
• Mostlikelyexplanation: Givenasequence ofobservations, wemightwishtofindthe
sequence ofstates thatismostlikely tohavegenerated those observations. Thatis, we
wishtocomputeargmax P(x |e ). Forexample,iftheumbrellaappearsoneach
x1:t 1:t 1:t
ofthefirstthreedaysandisabsentonthefourth,thenthemostlikelyexplanationisthat
itrained on the first three days and did not rain on the fourth. Algorithms forthis task
areusefulinmanyapplications, includingspeechrecognition—where theaimistofind
themostlikely sequence ofwords, givenaseries ofsounds—and thereconstruction of
bitstringstransmitted overanoisychannel.
Inaddition totheseinference tasks,wealsohave
• Learning: The transition and sensor models, if not yet known, can be learned from
observations. JustaswithstaticBayesiannetworks,dynamicBayesnetlearningcanbe
done as a by-product of inference. Inference provides an estimate of what transitions
actually occurred andofwhatstatesgenerated thesensorreadings, andthese estimates
canbeused toupdate themodels. Theupdated models provide newestimates, and the
process iterates to convergence. The overall process is an instance of the expectation-
maximizationorEMalgorithm. (SeeSection20.3.)
Notethatlearning requiressmoothing, ratherthanfiltering, becausesmoothing provides bet-
terestimatesofthestatesoftheprocess. Learningwithfilteringcanfailtoconvergecorrectly;
consider, for example, the problem of learning to solve murders: unless you are an eyewit-
ness, smoothing is always required to infer what happened at the murder scene from the
observable variables.
Theremainderofthissection describes generic algorithms forthefourinference tasks,
independent oftheparticularkindofmodelemployed. Improvementsspecifictoeachmodel
aredescribed insubsequent sections.
15.2.1 Filteringand prediction
As we pointed out in Section 7.7.3, a useful filtering algorithm needs to maintain a current
stateestimateandupdateit,ratherthangoingbackovertheentirehistoryofperceptsforeach
update. (Otherwise, thecostofeachupdateincreases astimegoesby.) Inotherwords,given
the result of filtering up to time t, the agent needs to compute the result for t+1 from the
newevidencee ,
t+1
P(X |e ) = f(e ,P(X |e )),
t+1 1:t+1 t+1 t 1:t
RECURSIVE forsomefunctionf. Thisprocessiscalledrecursiveestimation. Wecanviewthecalculation
ESTIMATION
3 Inparticular,whentrackingamovingobjectwithinaccuratepositionobservations,smoothinggivesasmoother
estimatedtrajectorythanfiltering—hencethename.
572 Chapter 15. Probabilistic Reasoning overTime
asbeingcomposedoftwoparts: first,thecurrent statedistribution isprojected forwardfrom
ttot+1;thenitisupdatedusingthenewevidencee . Thistwo-partprocessemergesquite
t+1
simplywhentheformulaisrearranged:
P(X |e ) = P(X |e ,e ) (dividing uptheevidence)
t+1 1:t+1 t+1 1:t t+1
= αP(e |X ,e )P(X |e ) (usingBayes’rule)
t+1 t+1 1:t t+1 1:t
= αP(e |X )P(X |e ) (bythesensorMarkovassumption). (15.4)
t+1 t+1 t+1 1:t
Hereandthroughoutthischapter, αisanormalizingconstantusedtomakeprobabilities sum
up to 1. The second term, P(X |e ) represents a one-step prediction of the next state,
t+1 1:t
andthefirsttermupdatesthiswiththenewevidence;noticethatP(e |X )isobtainable
t+1 t+1
directly from the sensor model. Now weobtain the one-step prediction for the next state by
conditioning onthecurrentstate X :
t (cid:12)
P(X |e ) = αP(e |X ) P(X |x ,e )P(x |e )
t+1 1:t+1 t+1 t+1 t+1 t 1:t t 1:t
(cid:12) xt
= αP(e |X ) P(X |x )P(x |e ) (Markovassumption). (15.5)
t+1 t+1 t+1 t t 1:t
xt
Withinthesummation,thefirstfactorcomesfromthetransitionmodelandthesecondcomes
fromthecurrentstatedistribution. Hence,wehavethedesiredrecursiveformulation. Wecan
thinkofthefilteredestimate P(X |e )asa“message”f thatispropagated forwardalong
t 1:t 1:t
thesequence, modifiedbyeachtransition andupdatedbyeachnewobservation. Theprocess
isgivenby
f
1:t+1
= αFORWARD(f
1:t
,e
t+1
),
whereFORWARDimplementstheupdatedescribedinEquation(15.5)andtheprocessbegins
with f = P(X ). When all the state variables are discrete, the time for each update is
1:0 0
constant (i.e., independent of t), and the space required is also constant. (The constants
depend, of course, on the size of the state space and the specific type of the temporal model
inquestion.) Thetimeandspacerequirementsforupdatingmustbeconstantifanagentwith
limitedmemoryistokeeptrackofthecurrentstatedistribution overanunbounded sequence
ofobservations.
Let us illustrate the filtering process for two steps in the basic umbrella example (Fig-
ure15.2.) Thatis,wewillcompute P(R |u )asfollows:
2 1:2
• Onday0,wehavenoobservations, onlythesecurityguard’spriorbeliefs;let’sassume
thatconsistsofP(R ) = (cid:16)0.5,0.5(cid:17).
0
• Onday1,theumbrellaappears, so U =true. Theprediction from t=0tot=1is
(cid:12) 1
P(R ) = P(R |r )P(r )
1 1 0 0
r0
= (cid:16)0.7,0.3(cid:17)×0.5+(cid:16)0.3,0.7(cid:17)×0.5 = (cid:16)0.5,0.5(cid:17).
Thentheupdate stepsimply multiplies bytheprobability of theevidence fort=1and
normalizes, asshowninEquation(15.4):
P(R |u ) = αP(u |R )P(R )= α(cid:16)0.9,0.2(cid:17)(cid:16)0.5,0.5(cid:17)
1 1 1 1 1
= α(cid:16)0.45,0.1(cid:17) ≈ (cid:16)0.818,0.182(cid:17).
Section15.2. Inference inTemporalModels 573
• Onday2,theumbrellaappears, so U =true. Theprediction from t=1tot=2is
(cid:12) 2
P(R |u ) = P(R |r )P(r |u )
2 1 2 1 1 1
r1
= (cid:16)0.7,0.3(cid:17)×0.818+(cid:16)0.3,0.7(cid:17)×0.182 ≈ (cid:16)0.627,0.373(cid:17),
andupdatingitwiththeevidencefort=2gives
P(R |u ,u ) = αP(u |R )P(R |u )= α(cid:16)0.9,0.2(cid:17)(cid:16)0.627,0.373(cid:17)
2 1 2 2 2 2 1
= α(cid:16)0.565,0.075(cid:17) ≈ (cid:16)0.883,0.117(cid:17).
Intuitively, the probability of rain increases from day 1to day 2 because rain persists. Exer-
cise15.2(a)asksyoutoinvestigate thistendencyfurther.
The task of prediction can be seen simply as filtering without the addition of new
evidence. In fact, the filtering process already incorporates a one-step prediction, and it is
easy toderivethe following recursive computation forpredicting the stateatt+k+1from
aprediction fort+k:
(cid:12)
P(X |e ) = P(X |x )P(x |e ). (15.6)
t+k+1 1:t t+k+1 t+k t+k 1:t
xt+k
Naturally, thiscomputation involves onlythetransition modelandnotthesensormodel.
It is interesting to consider what happens as we try to predict further and further into
the future. As Exercise 15.2(b) shows, the predicted distribution for rain converges to a
fixed point (cid:16)0.5,0.5(cid:17), after which it remains constant for all time. This is the stationary
distribution ofthe Markov process defined by thetransition model. (Seealso page 537.) A
great deal is known about the properties of such distributions and about the mixing time—
MIXINGTIME
roughly, thetime taken to reach the fixedpoint. Inpractical terms, this dooms tofailure any
attempt topredict the actual state foranumber ofsteps that is morethan asmall fraction of
themixing time, unless the stationary distribution itself isstrongly peaked inasmallarea of
the state space. Themore uncertainty there is inthe transition model, the shorter willbe the
mixingtimeandthemorethefutureisobscured.
In addition to filtering and prediction, we can use a forward recursion to compute the
likelihoodoftheevidencesequence,P(e ). Thisisausefulquantityifwewanttocompare
1:t
different temporal models that might have produced the same evidence sequence (e.g., two
different modelsforthepersistence ofrain). Forthisrecursion, weusealikelihood message
(cid:3) (X )=P(X ,e ). Itisasimpleexercisetoshowthatthemessagecalculation isidentical
1:t t t 1:t
tothatforfiltering:
(cid:3)
1:t+1
= FORWARD((cid:3)
1:t
,e
t+1
).
Havingcomputed(cid:3) ,weobtaintheactuallikelihood bysummingoutX :
1:t (cid:12) t
L = P(e ) = (cid:3) (x ). (15.7)
1:t 1:t 1:t t
xt
Noticethatthelikelihood messagerepresents theprobabilities oflongerandlongerevidence
sequencesastimegoesbyandsobecomesnumericallysmallerandsmaller,leadingtounder-
flow problems with floating-point arithmetic. This is an important problem in practice, but
weshallnotgointosolutions here.
574 Chapter 15. Probabilistic Reasoning overTime
X X X X
0 1 k t
E E E
1 k t
Figure 15.3 Smoothing computes P(Xk |e 1:t), the posterior distribution of the state at
somepasttimekgivenacompletesequenceofobservationsfrom1tot.
15.2.2 Smoothing
As we said earlier, smoothing is the process of computing the distribution over past states
given evidence up to the present; that is, P(X |e ) for 0 ≤ k < t. (See Figure 15.3.)
k 1:t
Inanticipation ofanotherrecursive message-passing approach, wecansplit thecomputation
intotwoparts—the evidenceuptok andtheevidence fromk+1tot,
P(X |e ) = P(X |e ,e )
k 1:t k 1:k k+1:t
= αP(X |e )P(e |X ,e ) (usingBayes’rule)
k 1:k k+1:t k 1:k
= αP(X |e )P(e |X ) (usingconditional independence)
k 1:k k+1:t k
= αf ×b . (15.8)
1:k k+1:t
where “×” represents pointwise multiplication of vectors. Here we have defined a “back-
ward”message b =P(e |X ), analogous tothe forward message f . Theforward
k+1:t k+1:t k 1:k
message f can becomputed byfiltering forward from 1to k, as givenby Equation (15.5).
1:k
It turns out that the backward message b can be computed by a recursive process that
k+1:t
runsbackwardfrom t:
(cid:12)
P(e |X ) = P(e |X ,x )P(x |X ) (conditioning onX )
k+1:t k k+1:t k k+1 k+1 k k+1
x (cid:12)k+1
= P(e |x )P(x |X ) (byconditional independence)
k+1:t k+1 k+1 k
x (cid:12)k+1
= P(e ,e |x )P(x |X )
k+1 k+2:t k+1 k+1 k
x (cid:12)k+1
= P(e |x )P(e |x )P(x |X ), (15.9)
k+1 k+1 k+2:t k+1 k+1 k
xk+1
wherethelast stepfollows bytheconditional independence ofe and e ,given X .
k+1 k+2:t k+1
Ofthethreefactorsinthissummation,thefirstandthirdareobtaineddirectlyfromthemodel,
andthesecondisthe“recursive call.” Usingthemessagenotation, wehave
b
k+1:t
= BACKWARD(b
k+2:t
,e
k+1
),
whereBACKWARDimplementstheupdatedescribedinEquation(15.9). Aswiththeforward
recursion, thetimeandspaceneededforeachupdateareconstantandthusindependent oft.
Wecan now see that the twoterms in Equation (15.8) can both be computed by recur-
sions through time, one running forward from 1 to k and using the filtering equation (15.5)
Section15.2. Inference inTemporalModels 575
and the other running backward from t to k + 1 and using Equation (15.9). Note that the
backward phase is initialized with b =P(e |X )=P( |X )1, where 1 is a vector of
t+1:t t+1:t t t
1s. (Becausee isanemptysequence, theprobability ofobserving itis1.)
t+1:t
Let us now apply this algorithm to the umbrella example, computing the smoothed
estimate for the probability of rain at time k=1, given the umbrella observations on days 1
and2. FromEquation(15.8),thisisgivenby
P(R |u ,u )= αP(R |u )P(u |R ). (15.10)
1 1 2 1 1 2 1
The first term we already know to be (cid:16).818,.182(cid:17), from the forward filtering process de-
scribed earlier. The second term can be computed by applying the backward recursion in
Equation(15.9):
(cid:12)
P(u |R ) = P(u |r )P( |r )P(r |R )
2 1 2 2 2 2 1
r2
= (0.9×1×(cid:16)0.7,0.3(cid:17))+(0.2×1×(cid:16)0.3,0.7(cid:17)) = (cid:16)0.69,0.41(cid:17).
PluggingthisintoEquation(15.10), wefindthatthesmoothedestimateforrainonday1is
P(R |u ,u )= α(cid:16)0.818,0.182(cid:17)×(cid:16)0.69,0.41(cid:17) ≈ (cid:16)0.883,0.117(cid:17).
1 1 2
Thus, the smoothed estimate for rain on day 1 is higher than the filtered estimate (0.818) in
this case. This is because the umbrella on day 2 makes it more likely to have rained on day
2;inturn,becauseraintendstopersist, thatmakesitmorelikelytohaverainedonday1.
Both the forward and backward recursions take a constant amount of time per step;
hence, the time complexity of smoothing with respect to evidence e is O(t). This is the
1:t
complexity for smoothing at a particular time step k. If we want to smooth the whole se-
quence, one obvious method is simply to run the whole smoothing process once for each
time step to be smoothed. This results in a time complexity of O(t2). A better approach
usesasimpleapplication ofdynamicprogramming toreducethecomplexitytoO(t). Aclue
appears in the preceding analysis of the umbrella example, where wewere able to reuse the
results of the forward-filtering phase. The key to the linear-time algorithm is to record the
results of forward filtering over the whole sequence. Then we run the backward recursion
from tdownto1,computing thesmoothed estimate ateachstep k fromthecomputed back-
ward message b and the stored forward message f . The algorithm, aptly called the
k+1:t 1:k
FORWARD–
forward–backwardalgorithm,isshowninFigure15.4.
BACKWARD
ALGORITHM
The alert reader will have spotted that the Bayesian network structure shown in Fig-
ure 15.3 is a polytree as defined on page 528. This means that a straightforward application
of the clustering algorithm also yields a linear-time algorithm that computes smoothed es-
timates for the entire sequence. It is now understood that the forward–backward algorithm
is in fact a special case of the polytree propagation algorithm used with clustering methods
(although thetwoweredevelopedindependently).
Theforward–backwardalgorithmformsthecomputationalbackboneformanyapplica-
tionsthatdealwithsequences ofnoisyobservations. Asdescribed sofar,ithastwopractical
drawbacks. Thefirstisthatitsspacecomplexitycanbetoohighwhenthestatespaceislarge
andthesequences arelong. Ituses O(|f|t)spacewhere|f|isthesizeoftherepresentation of
the forward message. Thespace requirement can be reduced to O(|f|logt) witha concomi-
576 Chapter 15. Probabilistic Reasoning overTime
tant increase inthe time complexity by afactor of logt, asshown in Exercise 15.3. In some
cases(seeSection15.3),aconstant-space algorithm canbeused.
The second drawback of the basic algorithm is that it needs to be modified to work
in an online setting where smoothed estimates must be computed for earlier time slices as
new observations are continuously added to the end of the sequence. The most common
FIXED-LAG requirement is for fixed-lag smoothing, which requires computing the smoothed estimate
SMOOTHING
P(X t−d |e 1:t ) for fixed d. That is, smoothing is done for the time slice d steps behind the
current time t; as t increases, the smoothing has to keep up. Obviously, we can run the
forward–backward algorithm over the d-step “window” as each new observation is added,
butthisseemsinefficient. InSection15.3, wewillseethatfixed-lagsmoothing can, insome
cases,bedoneinconstanttimeperupdate,independent ofthelagd.
15.2.3 Finding the mostlikely sequence
Suppose that [true,true,false,true,true] is the umbrella sequence for the security guard’s
first five days on the job. What is the weather sequence most likely to explain this? Does
the absence of the umbrella on day 3 mean that it wasn’t raining, or did the director forget
to bring it? If it didn’t rain on day 3, perhaps (because weather tends to persist) it didn’t
rain on day 4 either, but the director brought the umbrella just in case. In all, there are 25
possibleweathersequenceswecouldpick. Isthereawaytofindthemostlikelyone,shortof
enumerating allofthem?
Wecouldtrythislinear-timeprocedure: usesmoothingtofindtheposteriordistribution
fortheweatherateachtimestep;thenconstruct thesequence, usingateachsteptheweather
that is most likely according to the posterior. Such an approach should set off alarm bells
in the reader’s head, because the posterior distributions computed by smoothing are distri-
functionFORWARD-BACKWARD(ev,prior)returnsavectorofprobabilitydistributions
inputs:ev,avectorofevidencevaluesforsteps1,...,t
prior,thepriordistributionontheinitialstate,P(X )
0
localvariables: fv,avectorofforwardmessagesforsteps0,...,t
b,arepresentationofthebackwardmessage,initiallyall1s
sv,avectorofsmoothedestimatesforsteps1,...,t
fv[0]←prior
fori= 1totdo
fv[i]←FORWARD(fv[i−1],ev[i])
fori= tdownto1do
sv[i]←NORMALIZE(fv[i]×b)
b←BACKWARD(b,ev[i])
returnsv
Figure15.4 Theforward–backwardalgorithmforsmoothing: computingposteriorprob-
abilities of a sequence of states given a sequence of observations. The FORWARD and
BACKWARDoperatorsaredefinedbyEquations(15.5)and(15.9),respectively.
Section15.2. Inference inTemporalModels 577
Rain Rain Rain Rain Rain
1 2 3 4 5
true true true true true
(a)
false false false false false
Umbrellat true true false true true
.8182 .5155 .0361 .0334 .0210
(b)
.1818 .0491 .1237 .0173 .0024
m m m m m
1:1 1:2 1:3 1:4 1:5
Figure15.5 (a)PossiblestatesequencesforRaintcanbeviewedaspathsthroughagraph
of the possible states ateach time step. (States are shownas rectanglesto avoid confusion
with nodes in a Bayes net.) (b) Operationof the Viterbi algorithm for the umbrella obser-
vationsequence[true,true,false,true,true]. Foreacht, wehaveshownthevaluesofthe
messagem 1:t,whichgivestheprobabilityofthebestsequencereachingeachstateattimet.
Also,foreachstate,theboldarrowleadingintoitindicatesitsbestpredecessorasmeasured
bytheproductoftheprecedingsequenceprobabilityandthetransitionprobability.Following
theboldarrowsbackfromthemostlikelystateinm givesthemostlikelysequence.
1:5
butions over single time steps, whereas to find the most likely sequence we must consider
joint probabilities over all the time steps. The results can in fact be quite different. (See
Exercise15.4.)
There is a linear-time algorithm for finding the most likely sequence, but it requires a
littlemorethought. ItreliesonthesameMarkovpropertythatyieldedefficientalgorithmsfor
filteringandsmoothing. Theeasiestwaytothinkabouttheproblemistovieweachsequence
as a path through a graph whose nodes are the possible states at each time step. Such a
graph is shown for the umbrella world in Figure 15.5(a). Now consider the task of finding
the most likely path through this graph, where the likelihood of any path is the product of
the transition probabilities along the path and the probabilities of the given observations at
each state. Let’s focus in particular on paths that reach the state Rain =true. Because of
5
theMarkovproperty,itfollowsthatthemostlikelypathtothestateRain =true consistsof
5
themostlikelypathtosomestateattime4followedbyatransition toRain =true;andthe
5
stateattime4thatwillbecomepartofthepathtoRain =true iswhichevermaximizesthe
5
likelihood of that path. In other words, there is a recursive relationship between most likely
pathstoeachstatex andmostlikelypathstoeachstatex . Wecanwritethisrelationship
t+1 t
asanequation connecting theprobabilities ofthepaths:
maxP(x ,...,x ,X |e )
1 t t+1 1:t+1
x1...xt (cid:13) (cid:14)
= αP(e t+1 |X t+1 )max P(X t+1 |x t ) max P(x 1 ,...,x t−1 ,x t |e 1:t ) . (15.11)
xt x1...xt−1
Equation(15.11)isidentical tothefilteringequation (15.5)exceptthat
578 Chapter 15. Probabilistic Reasoning overTime
1. Theforwardmessage f =P(X |e )isreplacedbythemessage
1:t t 1:t
m 1:t = max P(x 1 ,...,x t−1 ,X t |e 1:t ),
x1...xt−1
thatis,theprobabilities ofthemostlikelypathtoeachstatex ;and
t
2. the summation over x in Equation (15.5) is replaced by the maximization over x in
t t
Equation(15.11).
Thus,thealgorithmforcomputingthemostlikelysequenceissimilartofiltering: itrunsfor-
wardalongthesequence,computingthemmessageateachtimestep,usingEquation(15.11).
The progress of this computation is shown in Figure 15.5(b). At the end, it will have the
probability forthemostlikelysequencereaching eachofthefinalstates. Onecanthuseasily
select the most likely sequence overall (the states outlined in bold). In order to identify the
actualsequence, asopposed tojustcomputing itsprobability, thealgorithm willalsoneedto
record, foreach state, the best state that leads toit; these areindicated by thebold arrowsin
Figure15.5(b). Theoptimalsequenceisidentifiedbyfollowingtheseboldarrowsbackwards
fromthebestfinalstate.
ThealgorithmwehavejustdescribediscalledtheViterbialgorithm,afteritsinventor.
VITERBIALGORITHM
Like the filtering algorithm, its time complexity is linear in t, the length of the sequence.
Unlike filtering, which uses constant space, its space requirement is also linear in t. This
is because the Viterbi algorithm needs to keep the pointers that identify the best sequence
leadingtoeachstate.
15.3 HIDDEN MARKOV MODELS
Theprecedingsectiondevelopedalgorithmsfortemporalprobabilisticreasoningusingagen-
eralframeworkthatwasindependent ofthespecificformofthetransitionandsensormodels.
In this and the next two sections, we discuss more concrete models and applications that
illustrate thepowerofthebasicalgorithms andinsomecasesallowfurtherimprovements.
HIDDENMARKOV We begin with the hidden Markov model, or HMM. An HMM is a temporal proba-
MODEL
bilistic modelinwhichthestateoftheprocess isdescribed byasingle discrete random vari-
able. The possible values of the variable are the possible states of the world. The umbrella
example described in the preceding section is therefore an HMM, since it has just one state
variable: Rain . Whathappensifyouhaveamodelwithtwoormorestatevariables? Youcan
t
still fititinto theHMMframework by combining the variables into asingle “megavariable”
whose values are all possible tuples of values of the individual state variables. We will see
thattherestricted structure ofHMMsallowsforasimpleand elegantmatriximplementation
ofallthebasicalgorithms.4
4 ThereaderunfamiliarwithbasicoperationsonvectorsandmatricesmightwishtoconsultAppendixAbefore
proceedingwiththissection.
Section15.3. HiddenMarkovModels 579
15.3.1 Simplifiedmatrix algorithms
With a single, discrete state variable X , we can give concrete form to the representations
t
of the transition model, the sensor model, and the forward and backward messages. Let the
statevariableX havevaluesdenotedbyintegers1,...,S,whereS isthenumberofpossible
t
states. Thetransition modelP(X t |X t−1 )becomesanS×S matrixT,where
T ij = P(X t =j|X t−1 =i).
Thatis,T istheprobabilityofatransitionfromstate itostatej. Forexample,thetransition
ij
matrixfortheumbrellaworldis
(cid:13) (cid:14)
0.7 0.3
T = P(X t |X t−1 )= 0.3 0.7 .
Wealso putthesensor modelinmatrixform. Inthis case, because thevalue oftheevidence
variable E is known at time t (call it e ), weneed only specify, for each state, how likely it
t t
isthatthestatecausese toappear: weneedP(e |X =i)foreachstatei. Formathematical
t t t
convenience we place these values into an S × S diagonal matrix, O whose ith diagonal
t
entry is P(e |X =i)and whose other entries are 0. Forexample, on day 1 in the umbrella
t t
worldofFigure15.5, U =true,andonday3,U =false,so,fromFigure15.2,wehave
(cid:13) (cid:14)1 (cid:13) 3(cid:14)
0.9 0 0.1 0
O = ; O = .
1 0 0.2 3 0 0.8
Now,ifweusecolumnvectorstorepresenttheforwardandbackwardmessages,allthecom-
putations becomesimplematrix–vectoroperations. Theforwardequation (15.5)becomes
(cid:12)
f = αO T f (15.12)
1:t+1 t+1 1:t
andthebackwardequation (15.9)becomes
b = TO b . (15.13)
k+1:t k+1 k+2:t
From these equations, we can see that the time complexity of the forward–backward algo-
rithm (Figure 15.4) applied to a sequence of length t is O(S2t), because each step requires
multiplying an S-element vector by an S×S matrix. The space requirement is O(St), be-
causetheforwardpassstores tvectorsofsizeS.
Besides providing an elegant description of the filtering and smoothing algorithms for
HMMs, the matrix formulation reveals opportunities for improved algorithms. The first is
a simple variation on the forward–backward algorithm that allows smoothing to be carried
out in constant space, independently ofthe length of the sequence. Theidea isthat smooth-
ingforanyparticulartimeslice krequiresthesimultaneouspresenceofboththeforwardand
backwardmessages,f andb ,accordingtoEquation(15.8). Theforward–backwardal-
1:k k+1:t
gorithmachievesthisbystoringthefscomputedontheforwardpasssothattheyareavailable
during the backward pass. Another way to achieve this is with a single pass that propagates
both fandbinthesamedirection. Forexample, the“forward” message fcanbepropagated
backwardifwemanipulate Equation(15.12)toworkintheotherdirection:
f = α
(cid:2)
(T
(cid:12)
)
−1O −1
f .
1:t t+1 1:t+1
Themodifiedsmoothing algorithm worksbyfirstrunning thestandard forward passtocom-
putef (forgetting alltheintermediate results) andthen running thebackward passforboth
t:t
580 Chapter 15. Probabilistic Reasoning overTime
functionFIXED-LAG-SMOOTHING(et,hmm,d)returnsadistributionoverXt−d
inputs:et,thecurrentevidencefortimestept
hmm,ahiddenMarkovmodelwithS× S transitionmatrixT
d,thelengthofthelagforsmoothing
persistent: t,thecurrenttime,initially1
f,theforwardmessageP(Xt |e 1:t),initiallyhmm.PRIOR
B,thed-stepbackwardtransformationmatrix,initiallytheidentitymatrix
et−d:t,double-endedlistofevidencefromt−dtot,initiallyempty
localvariables: Ot−d,Ot,diagonalmatricescontainingthesensormodelinformation
addettotheendofet−d:t
Ot ←diagonalmatrixcontainingP(et |Xt)
ift>dthen
f←FORWARD(f,et)
removeet−d−1 fromthebeginningofet−d:t
Ot−d ←diagonalmatrixcontainingP(et−d |Xt−d)
B←O −
t−
1
d
T −1BTOt
elseB←BTOt
t←t +1
ift>dthenreturnNORMALIZE(f × B1)elsereturnnull
Figure 15.6 An algorithm for smoothing with a fixed time lag of d steps, implemented
as an online algorithm that outputs the new smoothed estimate given the observation for a
new time step. Notice that the final output NORMALIZE(f×B1) is just αf×b, by Equa-
tion(15.14).
b and ftogether, using them to compute the smoothed estimate ateach step. Since only one
copy of each message is needed, the storage requirements are constant (i.e., independent of
t, the length of the sequence). There are two significant restrictions on this algorithm: it re-
quires thatthetransition matrixbeinvertible andthatthe sensormodelhavenozeroes—that
is,thateveryobservation bepossible ineverystate.
A second area in which the matrix formulation reveals an improvement is in online
smoothing with a fixed lag. The fact that smoothing can be done in constant space suggests
that there should exist an efficient recursive algorithm for online smoothing—that is, an al-
gorithm whose time complexity is independent of the length of the lag. Let us suppose that
the lag is d; that is, we are smoothing at time slice t−d, where the current time is t. By
Equation(15.8),weneedtocompute
αf 1:t−d ×b t−d+1:t
forslicet−d. Then,whenanewobservation arrives,weneedtocompute
αf 1:t−d+1 ×b t−d+2:t+1
forslicet−d+1. Howcanthisbedoneincrementally? First,wecancomputef 1:t−d+1 from
f 1:t−d ,usingthestandard filteringprocess, Equation(15.5).
Section15.3. HiddenMarkovModels 581
Computing the backward message incrementally istrickier, because there isnosimple
relationship between the old backward message b t−d+1:t and the new backward message
b t−d+2:t+1 . Instead, we will examine the relationship between the old backward message
b t−d+1:t andthebackward messageatthefrontofthesequence, b t+1:t . Todothis,weapply
Equation(15.13)dtimestoget
(cid:31)
(cid:25)t
b t−d+1:t = TO i b t+1:t = B t−d+1:t 1, (15.14)
i=t−d+1
where the matrix B t−d+1:t is the product of the sequence of T and O matrices. B can be
thought of as a “transformation operator” that transforms a later backward message into an
earlierone. Asimilarequation holds forthenewbackward messages afterthenextobserva-
tionarrives:
(cid:31)
t(cid:25)+1
b t−d+2:t+1 = TO i b t+2:t+1 = B t−d+2:t+1 1. (15.15)
i=t−d+2
Examiningtheproductexpressions inEquations(15.14)and (15.15),weseethattheyhavea
simple relationship: to get the second product, “divide” the first product by the firstelement
TO t−d+1 , and multiply by the new last element TO t+1 . In matrix language, then, there is a
simplerelationship betweentheoldandnewBmatrices:
B t−d+2:t+1 = O − t− 1 d+1 T −1B t−d+1:t TO t+1 . (15.16)
Thisequation providesanincremental updateforthe Bmatrix,whichinturn(through Equa-
tion (15.15)) allows us to compute the new backward message b t−d+2:t+1 . The complete
algorithm, whichrequiresstoring andupdating fandB,isshowninFigure15.6.
15.3.2 HiddenMarkov model example: Localization
Onpage145,weintroducedasimpleformofthelocalizationproblemforthevacuumworld.
In that version, the robot had a single nondeterministic Move action and its sensors reported
perfectly whether or not obstacles lay immediately to the north, south, east, and west; the
robot’sbeliefstatewasthesetofpossiblelocations itcouldbein.
Here we make the problem slightly more realistic by including a simple probability
model forthe robot’s motion andby allowing fornoise inthesensors. Thestate variable X
t
represents the location of the robot on the discrete grid; the domain of this variable is the
set of empty squares {s
1
,...,s
n
}. Let NEIGHBORS(s) be the set of empty squares that are
adjacent to sand let N(s)be the size ofthat set. Then the transition model for Move action
saysthattherobotisequally likelytoendupatanyneighboring square:
P(X
t+1
=j|X
t
=i) = T
ij
= (1/N(i)ifj ∈ NEIGHBORS(i)else0).
Wedon’t know where the robot starts, so wewill assume auniform distribution overall the
squares; thatis,P(X =i)=1/n. Fortheparticularenvironment weconsider(Figure15.7),
0
n=42andthetransition matrix Thas42×42=1764entries.
Thesensorvariable E has16possiblevalues,eachafour-bitsequencegivingthepres-
t
ence or absence of an obstacle in a particular compass direction. We will use the notation
582 Chapter 15. Probabilistic Reasoning overTime
(a)Posteriordistribution overrobotlocation after E = NSW
1
(b)Posteriordistribution overrobotlocation after E = NSW,E = NS
1 2
Figure15.7 Posteriordistributionoverrobotlocation: (a)one observationE =NSW;
1
(b)afterasecondobservationE =NS.Thesizeofeachdiskcorrespondstotheprobability
2
thattherobotisatthatlocation.Thesensorerrorrateis (cid:2)=0.2.
NS,forexample,tomeanthatthenorthandsouthsensorsreport anobstacleandtheeastand
westdonot. Supposethateachsensor’serrorrateis (cid:2)andthaterrorsoccurindependently for
thefoursensordirections. Inthatcase,theprobability of gettingallfourbitsrightis(1−(cid:2))4
andtheprobabilityofgettingthemallwrongis(cid:2)4. Furthermore,ifd isthediscrepancy—the
it
numberofbitsthataredifferent—between thetruevaluesforsquare iandtheactualreading
e ,thentheprobability thatarobotinsquare iwouldreceiveasensorreading e is
t t
P(E =e |X =i) = O = (1−(cid:2))4−dit(cid:2)dit .
t t t tii
Forexample,theprobabilitythatasquarewithobstaclestothenorthandsouthwouldproduce
asensorreading NSE is(1−(cid:2))3(cid:2)1.
Given the matrices T and O , the robot can use Equation (15.12) to compute the pos-
t
terior distribution over locations—that is, to work out where it is. Figure 15.7 shows the
distributions P(X |E =NSW)andP(X |E =NSW,E =NS). Thisisthesamemaze
1 1 2 1 2
wesawbefore in Figure 4.18(page 146), but there weused logical filtering tofindthe loca-
tions that were possible, assuming perfect sensing. Those same locations are still the most
likelywithnoisysensing, butnoweverylocation hassomenonzero probability.
In addition to filtering to estimate its current location, the robot can use smoothing
(Equation (15.13)) to work out where it was at any given past time—for example, where it
began attime 0—and it can use theViterbi algorithm towork out the mostlikely path it has
Section15.3. HiddenMarkovModels 583
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0 5 10 15 20 25 30 35 40
rorre
noitazilacoL
1
ε = 0.20 0.9
ε = 0.10
ε = 0.05 0.8
ε = 0.02 0.7
ε = 0.00
0.6
0.5
0.4
0.3
0.2
0.1
0 5 10 15 20 25 30 35 40
Number of observations
ycarucca
htaP
ε = 0.00
ε = 0.02
ε = 0.05
ε = 0.10
ε = 0.20
Number of observations
(a) (b)
Figure15.8 PerformanceofHMMlocalizationasafunctionofthelengthoftheobserva-
tionsequenceforvariousdifferentvaluesofthesensorerrorprobability(cid:2);dataaveragedover
400runs.(a)Thelocalizationerror,definedastheManhattandistancefromthetruelocation.
(b)TheViterbipathaccuracy,definedasthefractionofcorrectstatesontheViterbipath.
takentogetwhereitisnow. Figure15.8showsthelocalizationerrorandViterbipathaccuracy
forvarious values of the per-bit sensor errorrate (cid:2). Evenwhen(cid:2) is20%—which means that
theoverallsensorreadingiswrong59%ofthetime—therobot isusuallyabletoworkoutits
location within two squares after 25 observations. This is because of the algorithm’s ability
tointegrate evidenceovertimeandtotakeintoaccount theprobabilistic constraints imposed
on the location sequence by the transition model. When (cid:2) is 10%, the performance after
a half-dozen observations is hard to distinguish from the performance with perfect sensing.
Exercise15.7asksyoutoexplore howrobust theHMMlocalization algorithm istoerrorsin
thepriordistribution P(X )andinthetransition modelitself. Broadly speaking, highlevels
0
of localization and path accuracy are maintained even in the face of substantial errors in the
modelsused.
The state variable for the example we have considered in this section is a physical
location in the world. Other problems can, of course, include other aspects of the world.
Exercise15.8asksyoutoconsideraversionofthevacuumrobotthathasthepolicyofgoing
straight for as long as it can; only when it encounters an obstacle does it change to a new
(randomly selected) heading. To model this robot, each state in the model consists of a
(location, heading) pair. For the environment in Figure 15.7, which has 42 empty squares,
thisleadsto168statesandatransitionmatrixwith1682=28,224entries—stillamanageable
number. Ifweaddthepossibility ofdirtinthesquares, thenumberofstates ismultiplied by
242 and the transition matrix ends up with more than 1029 entries—no longer a manageable
number; Section 15.5showshowtousedynamic Bayesian networks tomodeldomains with
many state variables. If we allow the robot to move continuously rather than in a discrete
grid,thenumberofstatesbecomesinfinite;thenextsection showshowtohandlethiscase.
584 Chapter 15. Probabilistic Reasoning overTime
15.4 KALMAN FILTERS
Imagine watching a small bird flying through dense jungle foliage at dusk: you glimpse
brief,intermittent flashesofmotion;youtryhardtoguesswherethebirdisandwhereitwill
appear next so that you don’t lose it. Orimagine that you are a World WarII radar operator
peeringatafaint,wanderingblipthatappearsonceevery10secondsonthescreen. Or,going
back further still, imagine you are Kepler trying to reconstruct the motions of the planets
fromacollectionofhighlyinaccurateangularobservationstakenatirregularandimprecisely
measuredintervals. Inallthesecases,youaredoingfiltering: estimatingstatevariables(here,
position and velocity) from noisy observations over time. If the variables were discrete, we
could model the system with a hidden Markov model. This section examines methods for
handling continuous variables, using an algorithm called Kalman filtering, after one of its
KALMANFILTERING
inventors, RudolfE.Kalman.
Thebird’sflightmightbespecifiedbysixcontinuousvariablesateachtimepoint;three
forposition(X ,Y ,Z )andthreeforvelocity(X˙ ,Y˙ ,Z˙ ). Wewillneedsuitableconditional
t t t t t t
densities to represent the transition and sensor models; as in Chapter 14, we will use linear
Gaussian distributions. Thismeans that the next state X mustbe alinear function of the
t+1
currentstateX ,plussomeGaussiannoise,aconditionthatturnsouttobequitereasonablein
t
practice. Consider, forexample,the X-coordinate ofthebird,ignoring theothercoordinates
for now. Let the time interval between observations be Δ, and assume constant velocity
duringtheinterval;thenthepositionupdateisgivenbyX = X +X˙ Δ. AddingGaussian
t+Δ t
noise(toaccount forwindvariation, etc.),weobtainalinearGaussiantransition model:
P(X =x |X =x ,X˙ =x˙ )= N(x +x˙ Δ,σ2)(x ).
t+Δ t+Δ t t t t t t t+Δ
TheBayesiannetworkstructureforasystemwithpositionvectorX andvelocityX˙ isshown
t t
in Figure 15.9. Note that this is a very specific form of linear Gaussian model; the general
formwillbedescribed laterinthissectionandcoversavast arrayofapplications beyondthe
simplemotionexamplesofthefirstparagraph. ThereadermightwishtoconsultAppendixA
for some of the mathematical properties of Gaussian distributions; for our immediate pur-
MULTIVARIATE poses, the most important is that a multivariate Gaussian distribution for d variables is
GAUSSIAN
specifiedbyad-elementmeanμandad×dcovariance matrix Σ.
15.4.1 Updating Gaussiandistributions
InChapter14onpage521,wealludedtoakeypropertyofthelinearGaussianfamilyofdis-
tributions: itremainsclosedunderthestandardBayesiannetworkoperations. Here,wemake
this claim precise in the context of filtering in a temporal probability model. The required
properties correspond tothetwo-stepfilteringcalculation inEquation(15.5):
1. Ifthecurrent distribution P(X |e )isGaussian andthetransition model P(X |x )
t 1:t t+1 t
islinearGaussian,thentheone-step predicted distribution givenby
(cid:26)
P(X |e )= P(X |x )P(x |e )dx (15.17)
t+1 1:t t+1 t t 1:t t
xt
isalsoaGaussiandistribution.
Section15.4. KalmanFilters 585
X t X t+1
X t X t+1
Z t Z t+1
Figure15.9 BayesiannetworkstructureforalineardynamicalsystemwithpositionXt,
velocityX˙ t,andpositionmeasurementZt.
2. IfthepredictionP(X |e )isGaussianandthesensormodelP(e |X )islinear
t+1 1:t t+1 t+1
Gaussian,then,afterconditioning onthenewevidence, theupdated distribution
P(X |e )= αP(e |X )P(X |e ) (15.18)
t+1 1:t+1 t+1 t+1 t+1 1:t
isalsoaGaussiandistribution.
Thus, the FORWARD operator for Kalman filtering takes a Gaussian forward message f
1:t
,
specifiedbyameanμ andcovariance matrix Σ ,andproducesanewmultivariate Gaussian
t t
forward message f , specified by a mean μ and covariance matrix Σ . So, if we
1:t+1 t+1 t+1
startwithaGaussianprior f =P(X )=N(μ ,Σ ),filteringwithalinearGaussianmodel
1:0 0 0 0
produces aGaussianstatedistribution foralltime.
This seems to be a nice, elegant result, but why is it so important? The reason is that,
except for a few special cases such as this, filtering with continuous or hybrid (discrete and
continuous)networksgeneratesstatedistributionswhoserepresentationgrowswithoutbound
over time. This statement is not easy to prove in general, but Exercise 15.10 shows what
happens forasimpleexample.
15.4.2 A simpleone-dimensional example
We have said that the FORWARD operator for the Kalman filtermaps a Gaussian into a new
Gaussian. Thistranslates into computing anew meanand covariance matrixfrom theprevi-
ous mean and covariance matrix. Deriving the update rule in the general (multivariate) case
requiresratheralotoflinearalgebra,sowewillsticktoaverysimpleunivariatecasefornow;
and later give the results for the general case. Even for the univariate case, the calculations
are somewhat tedious, but we feel that they are worth seeing because the usefulness of the
Kalmanfilteristiedsointimatelytothemathematicalproperties ofGaussiandistributions.
Thetemporalmodelweconsiderdescribesarandomwalkofasinglecontinuousstate
variableX withanoisyobservationZ . Anexamplemightbethe“consumerconfidence”in-
t t
dex,whichcanbemodeledasundergoing arandomGaussian-distributed changeeachmonth
andismeasuredbyarandomconsumersurveythatalsointroducesGaussiansamplingnoise.
586 Chapter 15. Probabilistic Reasoning overTime
Thepriordistribution isassumedtobeGaussianwithvariance σ2:
0
„ «
−1
(x0−μ0)2
P(x
0
) = αe 2 σ0 2 .
(Forsimplicity, weusethesamesymbolαforallnormalizing constants inthissection.) The
transition modeladdsaGaussianperturbation ofconstant variance σ2 tothecurrentstate:
x
„ «
−1
(xt+1−xt)2
P(x |x )= αe 2 σx 2 .
t+1 t
ThesensormodelassumesGaussiannoisewithvariance σ2:
z
„ «
−1
(zt−xt)2
P(z |x )= αe 2 σz 2 .
t t
Now,giventhepriorP(X ),theone-steppredicteddistributioncomesfromEquation(15.17):
0
(cid:26) (cid:26) „ « „ «
∞ ∞ −1 (x1−x0)2 −1 (x0−μ0)2
P(x
1
) = P(x
1
|x
0
)P(x
0
)dx
0
= α e 2 σx 2 e 2 σ0 2 dx
0
−∞ −∞
(cid:26) „ «
∞ −1 σ0 2(x1−x0)2+σx 2(x0−μ0)2
= α e 2 σ0 2σx 2 dx
0
.
−∞
Thisintegrallooksrathercomplicated. Thekeytoprogress istonoticethattheexponentisthe
sumoftwoexpressionsthatarequadraticinx andhenceisitselfaquadraticinx . Asimple
0 0
COMPLETINGTHE trick known as completing the square allows the rewriting of any quadratic ax2 +bx +c
SQUARE 0 0
as the sum of asquared term a(x − −b)2 and a residual term c− b2 that is independent of
0 2a 4a
x . Theresidual termcanbetakenoutside theintegral,giving us
0
“ ”(cid:26)
P(x 1 ) = αe
−
2
1 c−
4
b2
a
∞
e
−1
2
(a(x0 −−
2a
b)2)
dx 0 .
−∞
NowtheintegralisjusttheintegralofaGaussianoveritsfullrange,whichissimply1. Thus,
weare left with only the residual term from the quadratic. Then, wenotice that the residual
termisaquadratic inx ;infact,aftersimplification, weobtain
1
„ «
−1
(x1−μ0)2
P(x
1
) = αe 2 σ0 2+σx 2 .
Thatis,theone-steppredicteddistributionisaGaussianwiththesamemeanμ andavariance
0
equaltothesumoftheoriginal variance σ2 andthetransition variance σ2.
0 x
To complete the update step, we need to condition on the observation at the first time
step,namely,z . FromEquation(15.18), thisisgivenby
1
P(x |z ) = αP(z |x )P(x )
1 1 1 1 1
„ « „ «
−1
(z1−x1)2
−1
(x1−μ0)2
= αe 2 σz 2 e 2 σ0 2+σx 2 .
Onceagain,wecombinetheexponents andcompletethesquare (Exercise15.11), obtaining
0 1
−1 B B (x1− (σ0 2 σ + 0 2 σ + x 2 σ )z x 2 1 + + σ σ z 2 z 2μ0)2 C C
2@ (σ0 2+σx 2)σz 2/(σ0 2+σx 2+σz 2) A
P(x |z ) = αe . (15.19)
1 1
Section15.4. KalmanFilters 587
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
-10 -5 0 5 10
)x(P
P(x | z = 2.5)
1 1
P(x) 0
P(x)
1
z*
1
x position
Figure 15.10 Stages in the Kalman filter update cycle for a random walk with a prior
given by μ
0
=0.0 and σ
0
=1.0, transition noise given by σx=2.0, sensor noise given by
σz=1.0,andafirstobservationz
1
=2.5(markedonthex-axis). Noticehowtheprediction
P(x )isflattenedout,relativeto P(x ), bythetransitionnoise. Noticealso thatthemean
1 0
oftheposteriordistributionP(x |z )isslightlytotheleftoftheobservationz becausethe
1 1 1
meanisaweightedaverageofthepredictionandtheobservation.
Thus,afteroneupdatecycle,wehaveanewGaussiandistribution forthestatevariable.
FromtheGaussianformulainEquation(15.19),weseethatthenewmeanandstandard
deviation canbecalculated fromtheoldmeanandstandard deviation asfollows:
(σ2+σ2)z +σ2μ (σ2+σ2)σ2
μ = t x t+1 z t and σ2 = t x z . (15.20)
t+1 σ2+σ2 +σ2 t+1 σ2+σ2 +σ2
t x z t x z
Figure15.10showsoneupdatecycleforparticularvaluesofthetransitionandsensormodels.
Equation (15.20) plays exactly the samerole asthe general filtering equation (15.5) or
theHMMfilteringequation (15.12). Becauseofthespecial natureofGaussiandistributions,
however, the equations have some interesting additional properties. First, we can interpret
the calculation for the new mean μ as simply a weighted mean of the new observation
t+1
z and theold mean μ . Ifthe observation is unreliable, then σ2 islarge and wepay more
t+1 t z
attention to the old mean; if the old mean is unreliable (σ2 is large) or the process is highly
t
unpredictable (σ2 is large), then we pay more attention to the observation. Second, notice
x
that the update for the variance σ2 is independent of the observation. We can therefore
t+1
compute in advance what the sequence of variance values will be. Third, the sequence of
variance values converges quickly to a fixed value that depends only on σ2 and σ2, thereby
x z
substantially simplifying thesubsequent calculations. (SeeExercise15.12.)
15.4.3 The general case
The preceding derivation illustrates the key property of Gaussian distributions that allows
Kalmanfiltering towork: thefact that the exponent isaquadratic form. Thisistrue not just
fortheunivariatecase;thefullmultivariateGaussiandistribution hastheform
“ ”
N(μ,Σ)(x) = αe − 2 1 (x−μ )(cid:4)Σ−1 (x−μ ) .
588 Chapter 15. Probabilistic Reasoning overTime
Multiplying outthetermsintheexponent makesitclearthat theexponent isalsoaquadratic
function of the values x in x. As in the univariate case, the filtering update preserves the
i
Gaussiannatureofthestatedistribution.
LetusfirstdefinethegeneraltemporalmodelusedwithKalmanfiltering. Boththetran-
sition model and the sensor model allow for a linear transformation with additive Gaussian
noise. Thus,wehave
P(x |x ) = N(Fx ,Σ )(x )
t+1 t t x t+1
(15.21)
P(z |x ) = N(Hx ,Σ )(z ),
t t t z t
where F and Σ are matrices describing the linear transition model and transition noise co-
x
variance,andHandΣ arethecorresponding matricesforthesensormodel. Nowthe update
z
equations forthemeanandcovariance, intheirfull,hairyhorribleness, are
μ = Fμ +K (z −HFμ )
t+1 t t+1 t+1 t (15.22)
Σ = (I−K H)(FΣ F (cid:12) +Σ ),
t+1 t+1 t x
whereK =(FΣ F (cid:12) +Σ )H (cid:12) (H(FΣ F (cid:12) +Σ )H (cid:12) +Σ ) −1 iscalledtheKalmangain
t+1 t x t x z
KALMANGAIN matrix. Believeitornot, these equations make someintuitive sense. Forexample, consider
MATRIX
the update for the mean state estimate μ. The term Fμ is the predicted state at t + 1, so
t
HFμ is the predicted observation. Therefore, the term z −HFμ represents the errorin
t t+1 t
the predicted observation. This is multiplied by K to correct the predicted state; hence,
t+1
K isameasureofhowseriously totakethenewobservation relativetotheprediction. As
t+1
inEquation (15.20), wealsohavetheproperty thatthevariance update isindependent ofthe
observations. The sequence of values for Σ and K can therefore be computed offline, and
t t
theactualcalculations requiredduringonlinetracking arequitemodest.
Toillustrate these equations at work, we have applied them to the problem of tracking
anobject moving on theX–Y plane. Thestate variables are X = (X,Y,X˙,Y˙) (cid:12) , soF, Σ ,
x
H, and Σ are 4×4 matrices. Figure 15.11(a) shows the true trajectory, a series of noisy
z
observations, and the trajectory estimated by Kalman filtering, along with the covariances
indicated by the one-standard-deviation contours. The filtering process does a good job of
tracking theactualmotion,and,asexpected, thevariancequicklyreaches afixedpoint.
We can also derive equations for smoothing as well as filtering with linear Gaussian
models. Thesmoothing resultsareshowninFigure15.11(b). Noticehowthevariance inthe
position estimate issharply reduced, except attheends ofthetrajectory (why?),and thatthe
estimatedtrajectory ismuchsmoother.
15.4.4 ApplicabilityofKalmanfiltering
TheKalmanfilteranditselaborationsareusedinavastarrayofapplications. The“classical”
application isinradartracking ofaircraftandmissiles. Relatedapplications includeacoustic
tracking of submarines and ground vehicles and visual tracking of vehicles and people. In a
slightly more esoteric vein, Kalman filters are used to reconstruct particle trajectories from
bubble-chamber photographs and ocean currents from satellite surface measurements. The
rangeofapplicationismuchlargerthanjustthetrackingofmotion: anysystemcharacterized
by continuous state variables and noisy measurements will do. Such systems include pulp
mills,chemicalplants, nuclearreactors, plantecosystems, andnational economies.
Section15.4. KalmanFilters 589
2D filtering 2D smoothing
12 12
true true
observed observed
smoothed smoothed
11 11
10 10
Y 9 Y 9
8 8
7 7
6 6
8 10 12 14 16 X18 20 22 24 26 8 10 12 14 16 X18 20 22 24 26
(a) (b)
Figure 15.11 (a) Results of Kalman filtering for an object moving on the X–Y plane,
showing the true trajectory (left to right), a series of noisy observations, and the trajectory
estimatedbyKalmanfiltering.Varianceinthepositionestimateisindicatedbytheovals.(b)
TheresultsofKalmansmoothingforthesameobservationsequence.
The fact that Kalman filtering can be applied to a system does not mean that the re-
sultswillbevalidoruseful. Theassumptions made—alinear Gaussiantransition andsensor
EXTENDEDKALMAN models—areverystrong. The extendedKalmanfilter(EKF)attemptstoovercomenonlin-
FILTER(EKF)
earities in the system being modeled. A system is nonlinear if the transition model cannot
NONLINEAR
be described as a matrix multiplication of the state vector, as in Equation (15.21). The EKF
worksbymodelingthesystemaslocallylinearinx intheregionofx =μ ,themeanofthe
t t t
current state distribution. This works well forsmooth, well-behaved systems and allows the
trackertomaintainandupdateaGaussianstatedistributionthatisareasonableapproximation
tothetrueposterior. AdetailedexampleisgiveninChapter 25.
What does it mean for a system to be “unsmooth” or “poorly behaved”? Technically,
it means that there is significant nonlinearity in system response within the region that is
“close” (according to the covariance Σ ) to the current mean μ . To understand this idea
t t
in nontechnical terms, consider the example of trying to track a bird as it flies through the
jungle. The bird appears to be heading at high speed straight for a tree trunk. The Kalman
filter,whetherregularorextended,canmakeonlyaGaussian predictionofthelocationofthe
bird,andthemeanofthisGaussianwillbecenteredonthetrunk,asshowninFigure15.12(a).
Areasonablemodelofthebird,ontheotherhand,wouldpredictevasiveactiontoonesideor
the other, asshown in Figure 15.12(b). Sucha model is highly nonlinear, because the bird’s
decision variessharply depending onitsprecise locationrelativetothetrunk.
To handle examples like these, we clearly need a more expressive language for repre-
senting thebehaviorofthesystem being modeled. Withinthe control theorycommunity, for
which problems such asevasive maneuvering by aircraft raise the same kinds of difficulties,
SWITCHINGKALMAN the standard solution isthe switching Kalmanfilter. Inthis approach, multiple Kalman fil-
FILTER
590 Chapter 15. Probabilistic Reasoning overTime
(a) (b)
Figure15.12 Abirdflyingtowardatree(topviews). (a)AKalmanfilterwillpredictthe
location of the bird using a single Gaussian centered on the obstacle. (b) A more realistic
modelallowsforthebird’sevasiveaction,predictingthatitwillflytoonesideortheother.
tersruninparallel, eachusingadifferentmodelofthesystem—forexample,oneforstraight
flight, one forsharp left turns, and one for sharp right turns. A weighted sum of predictions
is used, where the weight depends on how well each filter fits the current data. We will see
in the next section that this is simply a special case of the general dynamic Bayesian net-
work model, obtained by adding a discrete “maneuver” state variable to the network shown
inFigure15.9. SwitchingKalmanfiltersarediscussed furtherinExercise15.10.
15.5 DYNAMIC BAYESIAN NETWORKS
DYNAMICBAYESIAN A dynamic Bayesian network, or DBN, is a Bayesian network that represents a temporal
NETWORK
probability model of the kind described in Section 15.1. We have already seen examples of
DBNs: theumbrellanetworkinFigure15.2andtheKalmanfilternetworkinFigure15.9. In
general,eachsliceofaDBNcanhaveanynumberofstatevariablesX andevidencevariables
t
E . For simplicity, we assume that the variables and their links are exactly replicated from
t
slice toslice andthat theDBNrepresents afirst-orderMarkov process, sothat eachvariable
canhaveparentsonlyinitsownsliceortheimmediately preceding slice.
It should be clear that every hidden Markov model can be represented as a DBN with
a single state variable and a single evidence variable. It is also the case that every discrete-
variableDBNcanberepresented asanHMM;asexplained inSection15.3,wecancombine
all the state variables in the DBN into a single state variable whose values are all possible
tuples of values of the individual state variables. Now, if every HMM is a DBN and every
DBN can be translated into an HMM, what’s the difference? The difference is that, by de-
Section15.5. DynamicBayesianNetworks 591
composingthestateofacomplexsystemintoitsconstituentvariables,thecantakeadvantage
of sparseness in the temporal probability model. Suppose, for example, that a DBN has 20
Boolean state variables, each of which has three parents in the preceding slice. Then the
DBNtransitionmodelhas20×23=160probabilities, whereasthecorresponding HMMhas
220 states and therefore 240, orroughly a trillion, probabilities in the transition matrix. This
is bad for at least three reasons: first, the HMM itself requires much more space; second,
thehugetransition matrixmakesHMMinference muchmoreexpensive;andthird,theprob-
lem of learning such a huge number of parameters makes the pure HMM model unsuitable
forlarge problems. Therelationship between DBNsand HMMsis roughly analogous to the
relationship betweenordinaryBayesiannetworksandfulltabulated jointdistributions.
We have already explained that every Kalman filter model can be represented in a
DBN with continuous variables and linear Gaussian conditional distributions (Figure 15.9).
Itshouldbeclearfromthediscussion attheendofthepreceding sectionthatnoteveryDBN
canberepresented byaKalmanfiltermodel. InaKalmanfilter,thecurrentstatedistribution
isalwaysasinglemultivariate Gaussiandistribution—that is,asingle“bump”inaparticular
location. DBNs, on the other hand, can model arbitrary distributions. For many real-world
applications, this flexibility is essential. Consider, for example, the current location of my
keys. They might be in my pocket, on the bedside table, on the kitchen counter, dangling
from the front door, or locked in the car. A single Gaussian bump that included all these
places would have toallocate significant probability tothe keysbeing inmid-airin thefront
hall. Aspects of the real world such as purposive agents, obstacles, and pockets introduce
“nonlinearities” thatrequirecombinationsofdiscreteandcontinuousvariablesinordertoget
reasonable models.
15.5.1 Constructing DBNs
Toconstruct aDBN,onemustspecify three kinds ofinformation: thepriordistribution over
thestatevariables,P(X );thetransitionmodelP(X |X );andthesensormodelP(E |X ).
0 t+1 t t t
To specify the transition and sensor models, one must also specify the topology of the con-
nections between successive slices and between the state and evidence variables. Because
thetransition andsensormodelsareassumedtobestationary—the sameforallt—itismost
convenient simply tospecify them forthe firstslice. Forexample, thecomplete DBNspeci-
ficationfortheumbrellaworldisgivenbythethree-node networkshowninFigure15.13(a).
Fromthis specification, the complete DBNwithanunbounded numberoftimeslices canbe
constructed asneededbycopying thefirstslice.
Let us now consider a more interesting example: monitoring a battery-powered robot
moving in the X–Y plane, as introduced at the end of Section 15.1. First, we need state
variables, which will include both X =(X ,Y ) for position and X˙ =(X˙ ,Y˙ ) for velocity.
t t t t t t
We assume some method of measuring position—perhaps a fixed camera or onboard GPS
(Global Positioning System)—yielding measurements Z . Theposition atthenext timestep
t
depends on the current position and velocity, as in the standard Kalman filter model. The
velocity at the next step depends on the current velocity and the state of the battery. We
add Battery to represent the actual battery charge level, which has as parents the previous
t
592 Chapter 15. Probabilistic Reasoning overTime
BMeter
1
R P(R )
P(R ) 0 1
0 t 0.7 Battery Battery
0.7 f 0.3 0 1
Rain Rain
0 1 X X
0 1
R P(U )
1 1
t 0.9
XXt
0
X
1
f 0.2
Umbrella 1 Z 1
(a) (b)
Figure 15.13 (a) Specification of the prior, transition model, and sensor model for the
umbrellaDBN.Allsubsequentslicesareassumedtobecopiesofslice1. (b)AsimpleDBN
forrobotmotionintheX–Yplane.
batterylevelandthevelocity,andweaddBMeter ,whichmeasuresthebatterychargelevel.
t
ThisgivesusthebasicmodelshowninFigure15.13(b).
It is worth looking in more depth at the nature of the sensor model for BMeter . Let
t
us suppose, for simplicity, that both Battery and BMeter can take on discrete values 0
t t
through 5. Ifthemeterisalwaysaccurate, thentheCPTP(BMeter |Battery )shouldhave
t t
probabilities of 1.0 “along the diagonal” and probabilities of 0.0elsewhere. In reality, noise
always creeps into measurements. For continuous measurements, a Gaussian distribution
with a small variance might be used.5 For our discrete variables, we can approximate a
Gaussian using a distribution in which the probability of error drops off in the appropriate
way, so that the probability of a large error is very small. We use the term Gaussian error
GAUSSIANERROR modeltocoverboththecontinuous anddiscrete versions.
MODEL
Anyone with hands-on experience of robotics, computerized process control, or other
formsofautomaticsensingwillreadilytestifytothefactthatsmallamountsofmeasurement
noise are often the least of one’s problems. Real sensors fail. When a sensor fails, it does
not necessarily send a signal saying, “Oh, by the way, the data I’m about to send you is a
load of nonsense.” Instead, it simply sends the nonsense. The simplest kind of failure is
calledatransientfailure,wherethesensoroccasionallydecidestosendsomenonsense. For
TRANSIENTFAILURE
example, thebattery levelsensormighthaveahabitofsending azerowhensomeonebumps
therobot,evenifthebatteryisfullycharged.
Let’sseewhathappenswhenatransientfailureoccurswithaGaussianerrormodelthat
doesn’taccommodatesuchfailures. Suppose,forexample,thattherobotissittingquietlyand
observes20consecutivebatteryreadingsof5. Thenthebatterymeterhasatemporaryseizure
5 Strictlyspeaking,aGaussiandistributionisproblematicbecauseitassignsnonzeroprobabilitytolargenega-
tivechargelevels.Thebetadistributionissometimesabetterchoiceforavariablewhoserangeisrestricted.
Section15.5. DynamicBayesianNetworks 593
andthenextreadingisBMeter =0. WhatwillthesimpleGaussianerrormodelleadusto
21
believe about Battery ? According to Bayes’ rule, the answer depends on both the sensor
21
model P(BMeter =0|Battery ) and the prediction P(Battery |BMeter ). If the
21 21 21 1:20
probabilityofalargesensorerrorissignificantlylesslikelythantheprobabilityofatransition
toBattery =0,evenifthelatterisveryunlikely, thentheposteriordistribution willassign
21
a high probability to the battery’s being empty. A second reading of 0 at t=22 will make
thisconclusion almostcertain. Ifthetransientfailurethendisappears andthereadingreturns
to 5 from t=23 onwards, the estimate for the battery level will quickly return to 5, as if by
magic. ThiscourseofeventsisillustratedintheuppercurveofFigure15.14(a),whichshows
theexpectedvalueofBattery overtime,usingadiscrete Gaussianerrormodel.
t
Despitetherecovery,thereisatime(t=22)whentherobotisconvincedthatitsbattery
is empty; presumably, then, it should send out a mayday signal and shut down. Alas, its
oversimplified sensor model has led it astray. How can this be fixed? Consider a familiar
examplefromeverydayhumandriving: onsharpcurvesorsteephills,one’s“fueltankempty”
warninglightsometimesturnson. Ratherthanlookingfortheemergency phone, onesimply
recallsthatthefuelgaugesometimesgivesaverylargeerrorwhenthefuelissloshingaround
in the tank. The moral of the story is the following: for the system to handle sensor failure
properly, thesensormodelmustincludethepossibility offailure.
The simplest kind of failure model for a sensor allows a certain probability that the
sensor will return some completely incorrect value, regardless of the true state of the world.
Forexample,ifthebatterymeterfailsbyreturning 0,wemightsaythat
P(BMeter =0|Battery =5)=0.03,
t t
whichispresumably muchlarger thantheprobability assigned bythesimple Gaussian error
TRANSIENTFAILURE model. Let’s call this the transient failure model. How does it help when we are faced
MODEL
with a reading of 0? Provided that the predicted probability of an empty battery, according
to the readings so far, is much less than 0.03, then the best explanation of the observation
BMeter =0isthatthesensorhastemporarilyfailed. Intuitively, wecanthinkofthebelief
21
aboutthebatterylevelashaving acertainamountof“inertia” thathelpstoovercometempo-
rary blips in the meter reading. The upper curve in Figure 15.14(b) shows that the transient
failuremodelcanhandletransient failureswithoutacatastrophic changeinbeliefs.
Somuchfortemporary blips. Whataboutapersistent sensorfailure? Sadly,failures of
this kind are alltoo common. Ifthesensor returns 20 readings of5followed by20 readings
of 0, then the transient sensor failure model described in the preceding paragraph willresult
in the robot gradually coming to believe that its battery is empty when in fact it may be that
the meter has failed. The lower curve in Figure 15.14(b) shows the belief “trajectory” for
this case. By t=25—five readings of 0—the robot is convinced that its battery is empty.
Obviously, we would prefer the robot to believe that its battery meter is broken—if indeed
thisisthemorelikelyevent.
PERSISTENT Unsurprisingly, to handle persistent failure, we need a persistent failure model that
FAILUREMODEL
describes how the sensor behaves under normal conditions and after failure. To do this, we
need to augment the state of the system with an additional variable, say, BMBroken, that
describes the status of the battery meter. The persistence of failure must be modeled by an
594 Chapter 15. Probabilistic Reasoning overTime
5
4
3
2
1
0
-1
15 20 25 30
)yrettaB(E t
E(Battery |...5555005555...)
t
5
4
3
2
1
0
E(Battery |...5555000000...)
t
-1
15 20 25 30
Time step t
)yrettaB(E t
E(Battery |...5555005555...)
t
E(Battery |...5555000000...)
t
Time step
(a) (b)
Figure15.14 (a)Uppercurve:trajectoryoftheexpectedvalueofBatterytforanobserva-
tionsequenceconsistingofall5sexceptfor0satt=21andt=22,usingasimpleGaussian
errormodel.Lowercurve:trajectorywhentheobservationremainsat0fromt=21onwards.
(b)Thesameexperimentrunwiththetransientfailuremodel. Noticethatthetransientfail-
ureishandledwell,butthepersistentfailureresultsinexcessivepessimismaboutthebattery
charge.
B
0
P(B
1
)
5
t 1.000
f 0.001 4
3
BMBroken BMBroken
0 1
2
1
BMeter
1
0
-1
Battery Battery 15 20 25 30
0 1
)yrettaB(E t
E(Battery |...5555005555...)
t
E(Battery |...5555000000...)
t
P(BMBroken |...5555000000...)
t
P(BMBroken |...5555005555...)
t
Time step
(a) (b)
Figure15.15 (a)ADBN fragmentshowingthesensorstatusvariablerequiredformod-
eling persistentfailureof the battery sensor. (b)Uppercurves: trajectoriesof the expected
valueofBatterytforthe“transientfailure”and“permanentfailure”observationssequences.
Lowercurves:probabilitytrajectoriesfor BMBroken giventhetwoobservationsequences.
arc linking BMBroken toBMBroken . This persistence arc has aCPTthat gives asmall
PERSISTENCEARC 0 1
probability of failure in any given time step, say, 0.001, but specifies that the sensor stays
broken once it breaks. When the sensor is OK, the sensor model for BMeter is identical to
thetransientfailuremodel;whenthesensorisbroken,itsaysBMeter isalways0,regardless
oftheactualbatterycharge.
Section15.5. DynamicBayesianNetworks 595
P(R 0) R t 0 P 0 (R .7 1 ) P(R 0) R t 0 P 0 (R .7 1 ) R t 1 P 0 (R .7 2 ) R t 2 P 0 (R .7 3 ) R t 3 P 0 (R .7 4 )
0.7 f 0.3 0.7 f 0.3 f 0.3 f 0.3 f 0.3
Rain Rain Rain Rain Rain Rain Rain
0 1 0 1 2 3 4
Umbrella 1 Umbrella 1 Umbrella 2 Umbrella 3 Umbrella 4
R 1 P(U 1 ) R 1 P(U 1 ) R 2 P(U 2 ) R 3 P(U 3 ) R 4 P(U 4 )
t 0.9 t 0.9 t 0.9 t 0.9 t 0.9
f 0.2 f 0.2 f 0.2 f 0.2 f 0.2
Figure15.16 UnrollingadynamicBayesiannetwork: slicesarereplicatedtoaccommo-
datetheobservationsequenceUmbrella .Furthersliceshavenoeffectoninferenceswithin
1:3
theobservationperiod.
The persistent failure model for the battery sensor is shown in Figure 15.15(a). Its
performance on the two data sequences (temporary blip and persistent failure) is shown in
Figure 15.15(b). There are several things to notice about these curves. First, in the case
of the temporary blip, the probability that the sensor is broken rises significantly after the
second 0 reading, but immediately drops back to zero once a 5 is observed. Second, in the
case of persistent failure, the probability that the sensor is broken rises quickly to almost 1
and stays there. Finally, once the sensor is known to be broken, the robot can only assume
thatitsbatterydischargesatthe“normal”rate,asshownby thegraduallydescending levelof
E(Battery |...).
t
So far, we have merely scratched the surface of the problem of representing complex
processes. The variety of transition models is huge, encompassing topics as disparate as
modeling thehumanendocrine system andmodelingmultiple vehicles driving onafreeway.
Sensor modeling is also a vast subfield in itself, but even subtle phenomena, such as sensor
drift, sudden decalibration, and the effects of exogenous conditions (such as weather) on
sensorreadings,canbehandledbyexplicitrepresentation withindynamicBayesiannetworks.
15.5.2 Exactinference inDBNs
Having sketched some ideas for representing complex processes as DBNs, we now turn to
the question of inference. In a sense, this question has already been answered: dynamic
Bayesian networks are Bayesian networks, and we already have algorithms for inference in
Bayesian networks. Given a sequence of observations, one can construct the full Bayesian
network representation of a DBN by replicating slices until the network is large enough to
accommodate theobservations, asinFigure15.16. Thistechnique, mentioned inChapter14
inthecontext ofrelational probability models, iscalled unrolling. (Technically, theDBNis
equivalent to the semi-infinite network obtained by unrolling forever. Slices added beyond
the last observation have no effect on inferences within the observation period and can be
omitted.) Once the DBN is unrolled, one can use any of the inference algorithms—variable
elimination, clustering methods,andsoon—described inChapter14.
Unfortunately, a naive application of unrolling would not be particularly efficient. If
we want to perform filtering or smoothing with a long sequence of observations e , the
1:t
596 Chapter 15. Probabilistic Reasoning overTime
unrolled network would require O(t) space and would thus grow without bound as more
observations were added. Moreover, if we simply run the inference algorithm anew each
timeanobservation isadded,theinference timeperupdatewillalsoincreaseasO(t).
LookingbacktoSection15.2.1,weseethatconstanttimeandspaceperfilteringupdate
can be achieved if the computation can be done recursively. Essentially, the filtering update
in Equation (15.5) works by summing out the state variables ofthe previous time step to get
the distribution for the new time step. Summing out variables is exactly what the variable
elimination (Figure14.11)algorithm does,anditturnsoutthatrunning variable elimination
with the variables in temporal order exactly mimics the operation of the recursive filtering
update in Equation (15.5). The modified algorithm keeps at most two slices in memory at
anyonetime: startingwithslice0,weaddslice1,thensumoutslice0,thenaddslice2,then
sum out slice 1, and soon. In this way, wecan achieve constant space and timeperfiltering
update. (The same performance can be achieved by suitable modifications to the clustering
algorithm.) Exercise15.17asksyoutoverifythisfactfortheumbrellanetwork.
Somuch for the good news; now forthe bad news: It turns out that the “constant” for
theper-updatetimeandspacecomplexityis,inalmostallcases,exponentialinthenumberof
state variables. What happens is that, as the variable elimination proceeds, the factors grow
toincludeallthestatevariables(or,moreprecisely, allthosestatevariablesthathaveparents
intheprevioustimeslice). ThemaximumfactorsizeisO(dn+k)andthetotalupdatecostper
stepisO(ndn+k),wheredisthedomainsizeofthevariablesandk isthemaximumnumber
ofparents ofanystatevariable.
Of course, this is much less than the cost of HMM updating, which is O(d2n), but it
is still infeasible for large numbers of variables. This grim fact is somewhat hard to accept.
What it means is that even though we can use DBNs to represent very complex temporal
processes with many sparsely connected variables, we cannot reason efficiently and exactly
about those processes. The DBN model itself, which represents the prior joint distribution
overall the variables, is factorable into its constituent CPTs, but the posterior joint distribu-
tionconditioned onanobservation sequence—that is,theforwardmessage—isgenerally not
factorable. So far, no one has found a way around this problem, despite the fact that many
importantareasofscienceandengineeringwouldbenefitenormouslyfromitssolution. Thus,
wemustfallbackonapproximate methods.
15.5.3 Approximateinference inDBNs
Section 14.5 described two approximation algorithms: likelihood weighting (Figure 14.15)
andMarkovchainMonteCarlo(MCMC,Figure14.16). Ofthetwo,theformerismosteasily
adapted totheDBNcontext. (AnMCMCfiltering algorithm isdescribed brieflyinthenotes
attheendofthechapter.) Wewillsee,however,thatseveral improvementsarerequired over
thestandard likelihood weightingalgorithm beforeapracticalmethodemerges.
Recall that likelihood weighting works by sampling the nonevidence nodes of the net-
workintopologicalorder,weightingeachsamplebythelikelihooditaccordstotheobserved
evidence variables. As with the exact algorithms, we could apply likelihood weighting di-
rectly to an unrolled DBN,but this would suffer from the same problems of increasing time
Section15.5. DynamicBayesianNetworks 597
and space requirements per update as the observation sequence grows. The problem is that
the standard algorithm runs each sample in turn, all the way through the network. Instead,
we can simply run all N samples together through the DBN, one slice at a time. The mod-
ified algorithm fits the general pattern of filtering algorithms, with the set of N samples as
the forward message. The first key innovation, then, is to use the samples themselves as an
approximaterepresentation ofthecurrentstatedistribution. Thismeetstherequirementofa
“constant”timeperupdate,althoughtheconstantdependsonthenumberofsamplesrequired
tomaintainanaccurateapproximation. Thereisalsononeed tounrolltheDBN,because we
needtohaveinmemoryonlythecurrent sliceandthenextslice.
In our discussion of likelihood weighting in Chapter 14, we pointed out that the al-
gorithm’s accuracy suffers if the evidence variables are “downstream” from the variables
being sampled, because in that case the samples are generated without any influence from
the evidence. Looking at the typical structure of a DBN—say, the umbrella DBN in Fig-
ure15.16—weseethatindeedtheearlystatevariableswillbesampledwithoutthebenefitof
thelaterevidence. Infact, looking morecarefully, weseethatnoneofthestatevariables has
anyevidence variables amongitsancestors! Hence, although theweightofeachsamplewill
depend on the evidence, the actual set of samples generated will be completely independent
of the evidence. For example, even if the boss brings in the umbrella every day, the sam-
pling process could stillhallucinate endless days ofsunshine. Whatthismeansinpractice is
that the fraction of samples that remain reasonably close to the actual series of events (and
therefore have nonnegligible weights) drops exponentially with t, the length of the observa-
tionsequence. Inotherwords, tomaintain agivenlevelofaccuracy, weneed toincrease the
number of samples exponentially with t. Given that a filtering algorithm that works in real
timecanuseonlyafixednumberofsamples,whathappensinpracticeisthattheerrorblows
upafteraverysmallnumberofupdatesteps.
Clearly, we need a better solution. The second key innovation is to focus the set of
samples on the high-probability regions of the state space. This can be done by throwing
away samples that have very low weight, according to the observations, while replicating
thosethathavehighweight. Inthatway,thepopulationofsampleswillstayreasonablyclose
toreality. Ifwethinkofsamplesasaresource formodelingtheposteriordistribution, thenit
makessensetousemoresamplesinregionsofthestatespacewheretheposteriorishigher.
A family of algorithms called particle filtering is designed to do just that. Particle
PARTICLEFILTERING
filteringworksasfollows: First,apopulationofN initial-statesamplesiscreatedbysampling
fromthepriordistribution P(X ). Thentheupdate cycleisrepeated foreachtimestep:
0
1. Each sample is propagated forward by sampling the next state value x given the
t+1
currentvalue x forthesample,basedonthetransition model P(X |x ).
t t+1 t
2. Eachsampleisweightedbythelikelihooditassignstothenewevidence,P(e |x ).
t+1 t+1
3. The population is resampled to generate a new population of N samples. Each new
sample isselected from the current population; theprobability that aparticular sample
isselectedisproportional toitsweight. Thenewsamplesareunweighted.
The algorithm is shown in detail in Figure 15.17, and its operation for the umbrella DBNis
illustrated inFigure15.18.
598 Chapter 15. Probabilistic Reasoning overTime
functionPARTICLE-FILTERING(e,N,dbn)returnsasetofsamplesforthenexttimestep
inputs:e,thenewincomingevidence
N,thenumberofsamplestobemaintained
dbn,aDBNwithpriorP(X ),transitionmodelP(X |X ),sensormodelP(E |X )
0 1 0 1 1
persistent: S,avectorofsamplesofsizeN,initiallygeneratedfromP(X )
0
localvariables: W,avectorofweightsofsizeN
fori =1toN do
S[i]←samplefromP(X | X = S[i]) /*step1*/
1 0
W[i]←P(e| X = S[i]) /*step2*/
1
S←WEIGHTED-SAMPLE-WITH-REPLACEMENT(N,S,W) /*step3*/
returnS
Figure 15.17 The particle filtering algorithm implemented as a recursive update op-
eration with state (the set of samples). Each of the sampling operations involves sam-
pling the relevant slice variables in topological order, much as in PRIOR-SAMPLE. The
WEIGHTED-SAMPLE-WITH-REPLACEMENToperationcanbeimplementedtoruninO(N)
expectedtime. Thestepnumbersrefertothedescriptioninthetext.
Rain Rain Rain Rain
t t+1 t+1 t+1
true
false
(a) Propagate (b) Weight (c) Resample
Figure15.18 TheparticlefilteringupdatecyclefortheumbrellaDBNwithN=10,show-
ingthesamplepopulationsofeachstate. (a)Attimet,8samplesindicaterain and2indicate
¬rain. Eachispropagatedforwardbysamplingthenextstatethroughthetransitionmodel.
Attimet+1, 6samplesindicaterain and4indicate¬rain. (b)¬umbrella isobservedat
t+1. Eachsampleisweightedbyitslikelihoodfortheobservation,asindicatedbythesize
ofthecircles. (c)Anewsetof10samplesisgeneratedbyweightedrandomselectionfrom
thecurrentset,resultingin2samplesthatindicaterain and8thatindicate¬rain.
Wecanshowthatthisalgorithmisconsistent—givesthecorrectprobabilitiesasN tends
toinfinity—byconsideringwhathappensduringoneupdatecycle. Weassumethatthesample
population starts with a correct representation of the forward message f =P(X |e ) at
1:t t 1:t
time t. Writing N(x |e ) for the number of samples occupying state x after observations
t 1:t t
e havebeenprocessed, wetherefore have
1:t
N(x |e )/N = P(x |e ) (15.23)
t 1:t t 1:t
forlarge N. Nowwepropagateeachsampleforwardbysamplingthestatevariablesatt+1,
given the values for the sample at t. The number of samples reaching state x from each
t+1
Section15.6. KeepingTrackofManyObjects 599
x isthetransition probability timesthepopulation of x ;hence, thetotalnumberofsamples
t t
reaching x is
t+1 (cid:12)
N(x |e ) = P(x |x )N(x |e ).
t+1 1:t t+1 t t 1:t
xt
Nowweweighteachsamplebyitslikelihoodfortheevidenceatt+1. Asampleinstatex
t+1
receives weight P(e |x ). The total weight of the samples in x after seeing e is
t+1 t+1 t+1 t+1
therefore
W(x |e ) = P(e |x )N(x |e ).
t+1 1:t+1 t+1 t+1 t+1 1:t
Now for the resampling step. Since each sample is replicated with probability proportional
toitsweight,thenumberofsamplesinstatex afterresampling isproportional tothetotal
t+1
weightinx beforeresampling:
t+1
N(x |e )/N = αW(x |e )
t+1 1:t+1 t+1 1:t+1
= αP(e |x )N(x |e )
t+1 t+1 (cid:12)t+1 1:t
= αP(e |x ) P(x |x )N(x |e )
t+1 t+1 t+1 t t 1:t
xt(cid:12)
= αNP(e |x ) P(x |x )P(x |e ) (by15.23)
t+1 t+1 t+1 t t 1:t
(cid:12)xt
= α (cid:2) P(e |x ) P(x |x )P(x |e )
t+1 t+1 t+1 t t 1:t
xt
= P(x |e ) (by15.5).
t+1 1:t+1
Thereforethesamplepopulationafteroneupdatecyclecorrectlyrepresentstheforwardmes-
sageattimet+1.
Particlefiltering is consistent, therefore, butisit efficient? Inpractice, itseemsthatthe
answerisyes: particle filteringseemstomaintain agoodapproximation tothetrueposterior
usingaconstantnumberofsamples. Undercertainassumptions—inparticular, thattheprob-
abilities in the transition and sensor models are strictly greater than 0 and less than 1—it is
possible to prove that the approximation maintains bounded error with high probability. On
the practical side, the range of applications has grown to include many fields of science and
engineering; somereferences aregivenattheendofthechapter.
15.6 KEEPING TRACK OF MANY OBJECTS
The preceding sections have considered—without mentioning it—state estimation problems
involving a single object. In this section, we see what happens when two or more objects
generate the observations. What makes this case different from plain old state estimation is
that there is now thepossibility of uncertainty about which object generated which observa-
tion. ThisistheidentityuncertaintyproblemofSection14.6.3(page544),nowviewedina
temporal context. Inthecontrol theory literature, this is the dataassociation problem—that
DATAASSOCIATION
is,theproblem ofassociating observation datawiththeobjectsthatgenerated them.
600 Chapter 15. Probabilistic Reasoning overTime
1 5 1 5
2 4 2 4
3 3
2 2
4 4
1 3 1 3
5 5
(a) (b)
track termination
1 5 1 5
2 4 2 4
3 3
detection
failure
2 2
4 4
1 3 1 3
track
5 false alarm initiation 5
(c) (d)
Figure15.19 (a)Observationsmadeofobjectlocationsin2Dspaceoverfivetimesteps.
Eachobservationislabeledwiththetimestepbutdoesnotidentifytheobjectthatproduced
it. (b–c) Possible hypothesesabout the underlyingobject tracks. (d) A hypothesisfor the
caseinwhichfalsealarms,detectionfailures,andtrackinitiation/terminationarepossible.
The data association problem was studied originally in the context of radar tracking,
wherereflectedpulsesaredetectedatfixedtimeintervalsbyarotatingradarantenna. Ateach
timestep,multipleblipsmayappearonthescreen,butthereisnodirectobservationofwhich
blips at timet belong to which blips at timet−1. Figure 15.19(a) shows a simple example
with two blips pertime step forfive steps. Let the two blip locations at time t be e1 and e2.
t t
(Thelabelingofblipswithinatimestepas“1”and“2”iscompletelyarbitraryandcarriesno
information.) Letusassume,forthetimebeing,thatexactlytwoaircraft, AandB,generated
theblips; theirtrue positions are XA andXB. Just tokeep things simple, we’ll alsoassume
t t
that the each aircraft moves independently according to a known transition model—e.g., a
linearGaussianmodelasusedintheKalmanfilter(Section15.4).
Suppose we try to write down the overall probability model for this scenario, just as
we did for general temporal processes in Equation (15.3) on page 569. As usual, the joint
distribution factorsintocontributions foreachtimestep asfollows:
P(xA ,xB ,e1 ,e2 ) =
0:t 0:t 1:t 1:t
(cid:25)t
P(xA)P(xB) P(xA|xA )P(xB|xB )P(e1,e2|xA,xB). (15.24)
0 0 i i−1 i i−1 i i i i
i=1
Wewould like tofactor the observation term P(e1,e2|xA,xB)into aproduct oftwoterms,
i i i i
one for each object, but this would require knowing which observation was generated by
whichobject. Instead, wehavetosumoverallpossible waysofassociating theobservations
Section15.6. KeepingTrackofManyObjects 601
with the objects. Some of those ways are shown in Figure 15.19(b–c); in general, for n
objectsandT timesteps,thereare (n!)T waysofdoingit—anawfullylargenumber.
Mathematically speaking, the “way of associating the observations with the objects”
is a collection of unobserved random variable that identify the source of each observation.
We’llwriteω todenote theone-to-one mappingfromobjects toobservations attimet,with
t
ω (A) and ω (B) denoting the specific observations (1 or 2) that ω assigns to A and B.
t t t
(For n objects, ω will have n! possible values; here, n!=2.) Because the labels “1” ad
t
“2” on the observations are assigned arbitrarily, the prior on ω is uniform and ω is inde-
t t
pendent of the states of the objects, xA and xB). So we can condition the observation term
t t
P(e1,e2|xA,xB)onω andthensimplify:
i i i i t (cid:12)
P(e1,e2|xA,xB) = P(e1,e2|xA,xB,ω )P(ω |xA,xB)
i i i i i i i i i i i i
(cid:12)
ωi
= P(e ωi(A)|xA)P(e ωi(B)|xB)P(ω |xA,xB)
i i i i i i i
ωi
(cid:12)
1
= P(e
ωi(A)|xA)P(e ωi(B)|xB).
2 i i i i
ωi
Plugging this into Equation (15.24), we get an expression that is only in terms of transition
andsensormodelsforindividual objects andobservations.
As for all probability models, inference means summing out the variables other than
the query and the evidence. Forfiltering in HMMs and DBNs, wewere able to sum out the
statevariablesfrom1tot−1byasimpledynamicprogrammingtrick;forKalmanfilters,we
tookadvantageofspecialpropertiesofGaussians. Fordataassociation, wearelessfortunate.
There is no (known) efficient exact algorithm, for the same reason that there is none for the
switching Kalman filter (page 589): the filtering distribution P(xA|e1 ,e2 ) for object A
t 1:t 1:t
ends up as a mixture of exponentially many distributions, one for each way of picking a
sequence ofobservations toassigntoA.
As a result of the complexity of exact inference, many different approximate methods
have been used. The simplest approach is to choose a single “best” assignment at each time
step, given the predicted positions of the objects at the current time step. This assignment
associates observations with objects and enables the track of each object to be updated and
a prediction made for the next time step. Forchoosing the “best” assignment, it is common
NEAREST-NEIGHBOR to use the so-called nearest-neighbor filter, which repeatedly chooses the closest pairing
FILTER
of predicted position and observation and adds that pairing to the assignment. The nearest-
neighborfilterworkswellwhentheobjectsarewellseparatedinstatespaceandtheprediction
uncertainty and observation error are small—in other words, when there is no possibility of
confusion. When there is more uncertainty as to the correct assignment, a better approach
is to choose the assignment that maximizes the joint probability of the current observations
given the predicted positions. This can be done very efficiently using the Hungarian algo-
HUNGARIAN rithm(Kuhn,1955), eventhoughtherearen!assignments tochoosefrom.
ALGORITHM
Anymethod that commits toasingle best assignment ateach timestep fails miserably
under more difficult conditions. In particular, if the algorithm commits to an incorrect as-
signment, the prediction at the next time step may be significantly wrong, leading to more
602 Chapter 15. Probabilistic Reasoning overTime
(a) (b)
Figure15.20 Imagesfrom(a)upstreamand(b)downstreamsurveillancecamerasroughly
two miles apart on Highway 99 in Sacramento, California. The boxed vehicle has been
identifiedatbothcameras.
incorrect assignments, and so on. Two modern approaches turn out to be much more effec-
tive. Aparticlefilteringalgorithm (seepage598)fordataassociation worksbymaintaining
alarge collection ofpossible current assignments. An MCMCalgorithm explores thespace
ofassignment histories—for example, Figure15.19(b–c) mightbestatesintheMCMCstate
space—and can change its mind about previous assignment decisions. Current MCMCdata
association methods can handle many hundreds of objects in real time while giving a good
approximation tothetrueposteriordistributions.
The scenario described so far involved n known objects generating n observations at
each time step. Real application of data association are typically much more complicated.
Often,thereported observations include falsealarms(alsoknownasclutter),whicharenot
FALSEALARM
causedbyrealobjects. Detectionfailurescanoccur,meaningthatnoobservationisreported
CLUTTER
forarealobject. Finally,newobjectsarriveandoldonesdisappear. Thesephenomena,which
DETECTIONFAILURE
createevenmorepossible worldstoworryabout, areillustrated inFigure15.19(d).
Figure15.20showstwoimagesfromwidelyseparatedcamerasonaCaliforniafreeway.
Inthis application, weareinterested intwogoals: estimating thetimeittakes, undercurrent
traffic conditions, to go from one place to another in the freeway system; and measuring
demand, i.e., how many vehicles travel between any two points in the system at particular
times of the day and on particular days of the week. Both goals require solving the data
association problem over a wide area with many cameras and tens of thousands of vehicles
per hour. With visual surveillance, false alarms are caused by moving shadows, articulated
vehicles,reflectionsinpuddles,etc.;detectionfailuresarecausedbyocclusion,fog,darkness,
and lack of visual contrast; and vehicles are constantly entering and leaving the freeway
system. Furthermore, the appearance of any given vehicle can change dramatically between
cameras depending on lighting conditions and vehicle pose in the image, and the transition
modelchangesastrafficjamscomeandgo. Despitetheseproblems,moderndataassociation
algorithms havebeensuccessful inestimating trafficparameters inreal-world settings.
Section15.7. Summary 603
Data association is an essential foundation for keeping track of a complex world, be-
causewithoutitthereisnowaytocombinemultipleobservations ofanygivenobject. When
objects in the world interact with each other in complex activities, understanding the world
requirescombiningdataassociationwiththerelationalandopen-universeprobabilitymodels
ofSection14.6.3. Thisiscurrently anactiveareaofresearch.
15.7 SUMMARY
This chapter has addressed the general problem of representing and reasoning about proba-
bilistictemporalprocesses. Themainpointsareasfollows:
• Thechanging stateoftheworldishandled byusingasetofrandom variables torepre-
sentthestateateachpointintime.
• Representations can be designed to satisfy the Markov property, so that the future
is independent of the past given the present. Combined with the assumption that the
process is stationary—that is, the dynamics do not change over time—this greatly
simplifiestherepresentation.
• A temporal probability model can be thought of as containing a transition model de-
scribingthestateevolution andasensormodeldescribing theobservation process.
• The principal inference tasks in temporal models are filtering, prediction, smooth-
ing, and computing the most likely explanation. Each of these can be achieved using
simple,recursive algorithmswhoseruntimeislinearinthe lengthofthesequence.
• Threefamilies oftemporalmodels werestudied inmoredepth: hiddenMarkovmod-
els,Kalmanfilters,anddynamicBayesiannetworks(whichinclude theothertwoas
specialcases).
• Unless special assumptions are made, as in Kalman filters, exact inference with many
statevariablesisintractable. Inpractice,theparticlefilteringalgorithmseemstobean
effectiveapproximation algorithm.
• Whentryingtokeeptrackofmanyobjects, uncertainty arisesastowhichobservations
belong to which objects—the data association problem. The number of association
hypotheses is typically intractably large, but MCMC and particle filtering algorithms
fordataassociation workwellinpractice.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Manyofthebasic ideasforestimating thestateofdynamical systemscamefrom themathe-
matician C. F. Gauss (1809), who formulated adeterministic least-squares algorithm for the
problem of estimating orbits from astronomical observations. A. A. Markov (1913) devel-
oped what was later called the Markov assumption in his analysis of stochastic processes;
604 Chapter 15. Probabilistic Reasoning overTime
heestimated afirst-order Markov chain onletters from thetextofEugene Onegin. Thegen-
eraltheoryofMarkovchainsandtheirmixingtimesiscoveredbyLevinetal.(2008).
SignificantclassifiedworkonfilteringwasdoneduringWorld WarIIbyWiener(1942)
for continuous-time processes and by Kolmogorov (1941) for discrete-time processes. Al-
though this work led to important technological developments over the next 20 years, its
use of a frequency-domain representation made many calculations quite cumbersome. Di-
rect state-space modeling of the stochastic process turned out to be simpler, as shown by
Peter Swerling (1959) and Rudolf Kalman (1960). The latter paper described what is now
known as the Kalman filter for forward inference in linear systems with Gaussian noise;
Kalman’sresultshad,however,beenobtainedpreviously bytheDanishstatistician Thorvold
Thiele(1880)andbytheRussianmathematicianRuslanStratonovich(1959),whomKalman
met in Moscow in 1960. After a visit to NASA Ames Research Center in 1960, Kalman
saw the applicability of the method to the tracking of rocket trajectories, and the filter was
later implemented forthe Apollo missions. Important results on smoothing were derived by
Rauch et al. (1965), and the impressively named Rauch–Tung–Striebel smoother is still a
standard technique today. Many early results are gathered in Gelb (1974). Bar-Shalom and
Fortmann (1988) give amoremodern treatment withaBayesian flavor, aswellas manyref-
erences tothevastliterature onthesubject. Chatfield(1989) andBoxetal.(1994) coverthe
controltheoryapproach totimeseriesanalysis.
The hidden Markov model and associated algorithms for inference and learning, in-
cluding the forward–backward algorithm, were developed by Baum and Petrie (1966). The
Viterbialgorithmfirstappearedin(Viterbi,1967). Similarideasalsoappearedindependently
in the Kalman filtering community (Rauch et al., 1965). The forward–backward algorithm
was one of the main precursors of the general formulation of the EM algorithm (Dempster
etal.,1977);seealsoChapter20. Constant-spacesmoothingappearsinBinderetal.(1997b),
as does the divide-and-conquer algorithm developed in Exercise 15.3. Constant-time fixed-
lag smoothing for HMMs first appeared in Russell and Norvig (2003). HMMs have found
manyapplicationsinlanguageprocessing(Charniak,1993),speechrecognition(Rabinerand
Juang,1993),machinetranslation(OchandNey,2003),computationalbiology(Kroghetal.,
1994;Baldietal.,1994),financialeconomicsBharandHamori(2004)andotherfields. There
have been several extensions to the basic HMM model, for example the Hierarchical HMM
(Fine et al., 1998) and Layered HMM (Oliver et al., 2004) introduce structure back into the
model,replacing thesinglestatevariableofHMMs.
Dynamic Bayesian networks (DBNs)can be viewedas asparse encoding ofaMarkov
process and were first used in AI by Dean and Kanazawa (1989b), Nicholson and Brady
(1992), and Kjaerulff (1992). The last work extends the HUGIN Bayes net system to ac-
commodate dynamic Bayesian networks. The book by Dean and Wellman (1991) helped
popularize DBNsand the probabilistic approach to planning and control within AI.Murphy
(2002)provides athorough analysisofDBNs.
Dynamic Bayesian networks have become popular for modeling a variety of com-
plex motion processes in computer vision (Huang et al., 1994; Intille and Bobick, 1999).
Like HMMs, they have found applications in speech recognition (Zweig and Russell, 1998;
Richardsonetal.,2000;Stephensonetal.,2000;Nefianetal.,2002;Livescuetal.,2003),ge-
Bibliographical andHistorical Notes 605
nomics(MurphyandMian,1999;Perrinetal.,2003;Husmeier,2003)androbotlocalization
(Theocharous et al., 2004). Thelink between HMMsand DBNs, and between the forward–
backward algorithm and Bayesian network propagation, was made explicitly by Smyth et
al.(1997). Afurtherunification withKalmanfilters(andotherstatistical models) appears in
Roweisand Ghahramani (1999). Procedures exist forlearning theparameters (Binder etal.,
1997a;Ghahramani, 1998)andstructures (Friedman etal.,1998)ofDBNs.
The particle filtering algorithm described in Section 15.5 has a particularly interesting
history. Thefirstsamplingalgorithmsforparticlefiltering(alsocalledsequentialMonteCarlo
methods)weredeveloped inthecontroltheorycommunitybyHandschinandMayne(1969),
and the resampling idea that is the core of particle filtering appeared in a Russian control
journal(Zaritskii etal.,1975). Itwaslaterreinvented instatisticsassequentialimportance-
samplingresampling,orSIR(Rubin,1988;LiuandChen,1998),incontroltheoryasparti-
clefiltering (Gordon etal., 1993; Gordon, 1994), inAI as survival of thefittest(Kanazawa
etal.,1995),andincomputervisionascondensation(IsardandBlake,1996). Thepaperby
EVIDENCE Kanazawaetal.(1995)includes animprovementcalled evidencereversalwherebythestate
REVERSAL
attimet+1issampledconditional onboththestateattimetandtheevidenceattimet+1.
This allows the evidence to influence sample generation directly and was proved by Doucet
(1997)andLiuandChen(1998)toreducetheapproximationerror. Particlefilteringhasbeen
appliedinmanyareas,includingtrackingcomplexmotionpatternsinvideo(IsardandBlake,
1996), predicting the stock market (de Freitas et al., 2000), and diagnosing faults on plane-
RAO-
tary rovers (Verma et al., 2004). A variant called the Rao-Blackwellized particle filter or
BLACKWELLIZED
PARTICLEFILTER
RBPF (Doucet et al., 2000; Murphy and Russell, 2001) applies particle filtering to a subset
ofstate variables and, foreach particle, performs exact inference on the remaining variables
conditionedonthevaluesequenceintheparticle. InsomecasesRBPFworkswellwiththou-
sands of state variables. An application of RBPF to localization and mapping in robotics is
describedinChapter25. ThebookbyDoucetetal.(2001)collectsmanyimportantpaperson
SEQUENTIALMONTE sequentialMonteCarlo(SMC)algorithms, ofwhichparticlefilteringisthemostimportant
CARLO
instance. Pierre Del Moral and colleagues have performed extensive theoretical analyses of
SMCalgorithms(DelMoral,2004;DelMoraletal.,2006).
MCMC methods (see Section 14.5.2) can be applied to the filtering problem; for ex-
ample, Gibbssampling canbeapplied directly toanunrolled DBN.Toavoid theproblem of
increasing update times as the unrolled network grows, the decayed MCMC filter (Marthi
DECAYEDMCMC
et al., 2002) prefers to sample more recent state variables, with a probability that decays as
1/k2 for a variable k steps into the past. Decayed MCMC is a provably nondivergent filter.
ASSUMED-DENSITY Nondivergence theorems can also be obtained for certain types of assumed-density filter.
FILTER
Anassumed-density filterassumesthattheposteriordistribution overstatesattimetbelongs
toaparticular finitelyparameterized family;iftheprojection andupdatestepstakeitoutside
this family, the distribution is projected back to give the best approximation within the fam-
FACTORED ily. For DBNs, the Boyen–Koller algorithm (Boyen et al., 1999) and the factored frontier
FRONTIER
algorithm (Murphy and Weiss, 2001) assume that the posterior distribution can be approxi-
mated well by a product of small factors. Variational techniques (see Chapter 14) have also
been developed fortemporal models. Ghahramani andJordan (1997) discuss anapproxima-
tionalgorithm forthefactorialHMM,aDBNinwhichtwoormoreindependently evolving
FACTORIALHMM
606 Chapter 15. Probabilistic Reasoning overTime
Markovchainsarelinkedbyasharedobservationstream. Jordanetal.(1998)coveranumber
ofotherapplications.
Data association for multitarget tracking was first described in a probabilistic setting
by Sittler (1964). The first practical algorithm for large-scale problems was the “multiple
hypothesistracker”orMHTalgorithm(Reid,1979). Manyimportantpapersarecollectedby
Bar-Shalom and Fortmann (1988) and Bar-Shalom (1992). The development of an MCMC
algorithm fordata association isdue toPasula etal. (1999), whoapplied ittotraffic surveil-
lance problems. Ohetal. (2009) provide aformal analysis and extensive experimental com-
parisons toother methods. Schulz et al. (2003) describe adata association method based on
particle filtering. IngemarCoxanalyzed thecomplexity ofdataassociation (Cox,1993; Cox
andHingorani,1994)andbroughtthetopictotheattentionofthevisioncommunity. Healso
noted the applicability of the polynomial-time Hungarian algorithm to the problem of find-
ing most-likely assignments, which had long been considered an intractable problem in the
tracking community. The algorithm itself was published by Kuhn (1955), based on transla-
tions ofpapers published in1931 bytwoHungarian mathematicians, De´nesKo¨nig andJeno¨
Egerva´ry. Thebasic theorem hadbeen derived previously, however, inanunpublished Latin
manuscript bythefamousPrussianmathematician CarlGustavJacobi(1804–1851).
EXERCISES
15.1 Show that any second-order Markov process can be rewritten as a first-order Markov
process with an augmented set of state variables. Can this always be done parsimoniously,
i.e.,withoutincreasingthenumberofparametersneededtospecifythetransitionmodel?
15.2 In this exercise, we examine what happens to the probabilities in the umbrella world
inthelimitoflongtimesequences.
a. Suppose we observe an unending sequence of days on which the umbrella appears.
Showthat,asthedaysgoby,theprobability ofrainonthecurrentdayincreases mono-
tonicallytowardafixedpoint. Calculatethisfixedpoint.
b. Now consider forecasting further and further into the future, given just the first two
umbrella observations. First, compute the probability P(r |u ,u ) for k=1...20
2+k 1 2
andplottheresults. Youshouldseethattheprobabilityconvergestowardsafixedpoint.
Provethattheexactvalueofthisfixedpointis0.5.
15.3 This exercise develops a space-efficient variant of the forward–backward algorithm
described in Figure 15.4 (page 576). We wish to compute P(X |e ) for k=1,...,t. This
k 1:t
willbedonewithadivide-and-conquer approach.
a. Suppose,forsimplicity, thattisodd,andletthehalfwaypointbeh=(t+1)/2. Show
thatP(X |e )canbecomputedfork=1,...,hgivenjusttheinitialforwardmessage
k 1:t
f ,thebackward messageb ,andtheevidence e .
1:0 h+1:t 1:h
b. Showasimilarresultforthesecondhalfofthesequence.
Exercises 607
c. Given the results of (a) and (b), a recursive divide-and-conquer algorithm can be con-
structed by first running forward along the sequence and then backward from the end,
storing just the required messages at the middle and the ends. Then the algorithm is
calledoneachhalf. Writeoutthealgorithm indetail.
d. Computethetimeandspacecomplexityofthealgorithmasafunctionoft,thelengthof
thesequence. Howdoesthischangeifwedividetheinputintomorethantwopieces?
15.4 Onpage577,weoutlinedaflawedprocedureforfindingthemostlikelystatesequence,
given an observation sequence. The procedure involves finding the most likely state at each
timestep, usingsmoothing, andreturning thesequence composed ofthesestates. Showthat,
for some temporal probability models and observation sequences, this procedure returns an
impossible statesequence (i.e.,theposteriorprobability ofthesequenceiszero).
15.5 Equation (15.12) describes thefiltering process forthematrix formulation of HMMs.
Giveasimilarequationforthecalculation oflikelihoods, whichwasdescribedgenerically in
Equation(15.7).
15.6 Consider the vacuum worlds of Figure 4.18 (perfect sensing) and Figure 15.7 (noisy
sensing). Suppose that the robot receives an observation sequence such that, with perfect
sensing, there is exactly one possible location it could be in. Is this location necessarily the
most probable location under noisy sensing for sufficiently small noise probability (cid:2)? Prove
yourclaimorfindacounterexample.
15.7 In Section 15.3.2, the prior distribution over locations is uniform and the transition
model assumes an equal probability of moving to any neighboring square. What if those
assumptions are wrong? Suppose that the initial location is actually chosen uniformly from
the northwest quadrant of the room and the Move action actually tends to move southeast.
Keeping the HMM model fixed, explore the effect on localization and path accuracy as the
southeasterly tendency increases, fordifferent valuesof (cid:2).
15.8 Consideraversionofthevacuumrobot(page582)thathasthepolicyofgoingstraight
foraslong as itcan; only when itencounters an obstacle does itchange toanew (randomly
selected) heading. Tomodelthisrobot, eachstate inthemodelconsists ofa(location, head-
ing)pair. ImplementthismodelandseehowwelltheViterbialgorithmcantrackarobotwith
this model. The robot’s policy is more constrained than the random-walk robot; does that
meanthatpredictions ofthemostlikelypatharemoreaccurate?
15.9 This exercise is concerned with filtering in an environment with no landmarks. Con-
sideravacuumrobotinanemptyroom,representedbyann×mrectangulargrid. Therobot’s
location is hidden; theonly evidence available totheobserver isanoisy location sensor that
gives an approximation to the robot’s location. If the robot is at location (x,y) then with
probability .1 the sensor gives the correct location, with probability .05 each it reports one
of the 8 locations immediately surrounding (x,y), with probability .025 each it reports one
of the 16 locations that surround those 8, and with the remaining probability of .1 it reports
“no reading.” The robot’s policy is to pick a direction and follow it with probability .8 on
eachstep;therobotswitchestoarandomlyselectednewheadingwithprobability .2(orwith
608 Chapter 15. Probabilistic Reasoning overTime
S S
t t+1
X X
t t+1
Z Z
t t+1
Figure 15.21 A Bayesian network representation of a switching Kalman filter. The
switching variable St is a discrete state variable whose value determines the transition
model for the continuous state variables Xt. For any discrete state i, the transition model
P(Xt+1 |Xt,St=i) isa linearGaussian model,just asin a regularKalmanfilter. Thetran-
sitionmodelforthediscretestate,P(St+1 |St),canbethoughtofasamatrix,asinahidden
Markovmodel.
probability 1ifitencounters awall). ImplementthisasanHMManddofilteringtotrackthe
robot. Howaccurately canwetracktherobot’spath?
15.10 Often,wewishtomonitoracontinuous-state systemwhosebehaviorswitchesunpre-
dictablyamongasetofkdistinct“modes.” Forexample,anaircrafttryingtoevadea missile
canexecute aseries ofdistinct maneuvers that the missilemayattempt totrack. ABayesian
networkrepresentation ofsucha switchingKalmanfiltermodelisshowninFigure15.21.
a. Suppose that the discrete state S has k possible values and that the prior continuous
t
state estimate P(X ) is a multivariate Gaussian distribution. Show that the prediction
0
P(X )is a mixture of Gaussians—that is, a weighted sum of Gaussians such that the
1
weightssumto1.
b. Show that if the current continuous state estimate P(X |e ) is a mixture of m Gaus-
t 1:t
sians, theninthegeneral casetheupdated stateestimate P(X |e )willbeamix-
t+1 1:t+1
tureofkmGaussians.
c. Whataspectofthetemporalprocess dotheweightsintheGaussianmixturerepresent?
Theresultsin(a)and(b)showthattherepresentationoftheposteriorgrowswithoutlimiteven
forswitchingKalmanfilters,whichareamongthesimplesthybriddynamicmodels.
15.11 CompletethemissingstepinthederivationofEquation(15.19)onpage586,thefirst
updatestepfortheone-dimensional Kalmanfilter.
15.12 Letusexaminethebehaviorofthevariance updateinEquation(15.20)(page587).
a. Plotthevalueofσ2 asafunction oft,givenvariousvaluesforσ2 andσ2.
t x z
b. Showthat the update has afixedpoint σ2 such that σ2 → σ2 as t → ∞, and calculate
t
thevalueofσ2.
c. Giveaqualitative explanation forwhathappens asσ2 → 0andasσ2 → 0.
x z
Exercises 609
15.13 A professor wants to know if students are getting enough sleep. Each day, the pro-
fessor observes whether the students sleep in class, and whether they have red eyes. The
professorhasthefollowingdomaintheory:
• Thepriorprobability ofgetting enoughsleep,withnoobservations, is0.7.
• The probability of getting enough sleep on night t is 0.8 given that the student got
enoughsleeptheprevious night,and0.3ifnot.
• Theprobability ofhaving redeyesis0.2ifthestudent gotenough sleep,and0.7ifnot.
• Theprobabilityofsleepinginclassis0.1ifthestudentgotenoughsleep,and0.3ifnot.
Formulate this information as a dynamic Bayesian network that the professor could use to
filter or predict from a sequence of observations. Then reformulate it as a hidden Markov
model that has only a single observation variable. Give the complete probability tables for
themodel.
15.14 FortheDBNspecifiedinExercise15.13andfortheevidence values
e = notredeyes,notsleepinginclass
1
e = redeyes,notsleepinginclass
2
e = redeyes,sleepinginclass
3
perform thefollowingcomputations:
a. Stateestimation: ComputeP(EnoughSleep |e )foreachoft = 1,2,3.
t 1:t
b. Smoothing: ComputeP(EnoughSleep |e )foreachoft = 1,2,3.
t 1:3
c. Comparethefilteredandsmoothedprobabilities for t = 1andt = 2.
15.15 Supposethataparticularstudentshowsupwithredeyesandsleepsinclasseveryday.
GiventhemodeldescribedinExercise15.13,explainwhytheprobabilitythatthestudenthad
enough sleeptheprevious nightconverges toafixedpointratherthancontinuing togodown
as wegather more days of evidence. What is the fixed point? Answerthis both numerically
(bycomputation) andanalytically.
15.16 Thisexerciseanalyzesinmoredetailthepersistent-failure modelforthebatterysen-
sorinFigure15.15(a) (page594).
a. Figure 15.15(b) stops at t=32. Describe qualitatively what should happen as t → ∞
ifthesensorcontinues toread0.
b. Supposethattheexternaltemperatureaffectsthebattery sensorinsuchawaythattran-
sient failures become more likely as temperature increases. Show how to augment the
DBNstructure inFigure15.15(a),andexplainanyrequired changestotheCPTs.
c. Giventhenewnetwork structure, canbattery readings beusedbytherobottoinferthe
currenttemperature?
15.17 Consider applying the variable elimination algorithm to the umbrella DBNunrolled
forthree slices, where the query is P(R |u ,u ,u ). Show that the space complexity of the
3 1 2 3
algorithm—thesizeofthelargestfactor—isthesame,regardlessofwhethertherainvariables
areeliminatedinforwardorbackwardorder.
16
MAKING SIMPLE
DECISIONS
Inwhichweseehowanagentshouldmakedecisionssothatitgetswhatitwants—
onaverage, atleast.
Inthischapter, wefillinthedetailsofhowutilitytheorycombineswithprobability theoryto
yield adecision-theoretic agent—an agent that can make rational decisions based on whatit
believesandwhatitwants. Suchanagentcanmakedecisionsincontextsinwhichuncertainty
and conflicting goals leave a logical agent with no way to decide: a goal-based agent has a
binary distinction between good (goal) and bad (non-goal) states, while a decision-theoretic
agenthasacontinuous measureofoutcomequality.
Section 16.1 introduces the basic principle of decision theory: the maximization of
expected utility. Section 16.2 shows that the behavior of any rational agent can be captured
bysupposing autility function thatisbeingmaximized. Section 16.3discusses thenatureof
utilityfunctionsinmoredetail,andinparticulartheirrelationtoindividualquantitiessuchas
money. Section16.4showshowtohandleutilityfunctions thatdependonseveralquantities.
In Section 16.5, we describe the implementation of decision-making systems. In particular,
weintroduce aformalism called a decision network (also known asaninfluencediagram)
that extends Bayesian networks by incorporating actions and utilities. The remainder of the
chapterdiscusses issuesthatariseinapplications ofdecisiontheorytoexpertsystems.
16.1 COMBINING BELIEFS AND DESIRES UNDER UNCERTAINTY
Decision theory, initssimplest form, deals withchoosing among actions based on thedesir-
abilityoftheirimmediateoutcomes; thatis,theenvironment isassumedtobeepisodic inthe
sense defined on page 43. (Thisassumption isrelaxed inChapter 17.) InChapter3weused
the notation RESULT(s
0
,a) for the state that is the deterministic outcome of taking action a
in state s . In this chapter wedeal withnondeterministic partially observable environments.
0
Sincetheagentmaynotknowthecurrentstate,weomititanddefineRESULT(a)asarandom
(cid:2)
variable whose values arethepossible outcome states. Theprobability of outcome s, given
evidence observations e,iswritten
P(RESULT(a)=s
(cid:2)|a,e),
610
Section16.2. TheBasisofUtilityTheory 611
wheretheaontheright-handsideoftheconditioning barstandsforthe eventthatactionais
executed.1
Theagent’spreferencesarecapturedbyautilityfunction,U(s),whichassignsasingle
UTILITYFUNCTION
numbertoexpressthedesirability ofastate. The expected utilityofanactiongiventheevi-
EXPECTEDUTILITY
dence,EU(a|e),isjusttheaverageutilityvalueoftheoutcomes,weightedbytheprobability
thattheoutcomeoccurs:
(cid:12)
EU(a|e) = P(RESULT(a)=s (cid:2)|a,e)U(s (cid:2) ). (16.1)
s(cid:3)
MAXIMUMEXPECTED Theprinciple ofmaximumexpectedutility(MEU)saysthatarational agentshouldchoose
UTILITY
theactionthatmaximizestheagent’s expectedutility:
action = argmaxEU(a|e)
a
Inasense,theMEUprinciplecouldbeseenasdefiningallofAI.Allanintelligent agenthas
to do is calculate the various quantities, maximize utility over its actions, and away it goes.
ButthisdoesnotmeanthattheAIproblemissolvedbythedefinition!
The MEU principle formalizes the general notion that the agent should “do the right
thing,” but goes only a small distance toward a full operationalization of that advice. Es-
timating the state of the world requires perception, learning, knowledge representation, and
inference. Computing P(RESULT(a)|a,e) requires a complete causal model of the world
and,aswesawinChapter14,NP-hardinferencein(verylarge)Bayesiannetworks. Comput-
(cid:2)
ing the outcome utilities U(s) often requires searching or planning, because an agent may
not know how good astate is until itknows where it can get to from that state. So, decision
theoryisnotapanacea thatsolvestheAIproblem—but itdoes provideausefulframework.
TheMEUprinciplehasaclearrelationtotheideaofperformancemeasuresintroduced
in Chapter 2. The basic idea is simple. Consider the environments that could lead to an
agent having a given percept history, and consider the different agents that wecould design.
If an agent acts so as to maximize a utility function that correctly reflects the performance
measure, then the agent will achieve the highest possible performance score (averaged over
all the possible environments). This is the central justification for the MEU principle itself.
While the claim may seem tautological, it does in fact embody a very important transition
from a global, external criterion of rationality—the performance measure over environment
histories—toalocal,internalcriterioninvolvingthemaximizationofautilityfunctionapplied
tothenextstate.
16.2 THE BASIS OF UTILITY THEORY
Intuitively, the principle of Maximum Expected Utility (MEU) seems like a reasonable way
to make decisions, but it is by no means obvious that it is the only rational way. After all,
why should maximizing the average utility be so special? What’s wrong with an agent that
1 ClassicaldecisiontheorylPeavesthecurrentstateS0implicit,butwecouldmakeitexplicitbywriting
P(RESULT(a)=s(cid:3)|a,e)=
s
P(RESULT(s,a)=s(cid:3)|a)P(S0=s|e).
612 Chapter 16. MakingSimpleDecisions
maximizes the weighted sum of the cubes of the possible utilities, or tries to minimize the
worst possible loss? Could an agent act rationally just by expressing preferences between
states, without giving them numeric values? Finally, why should a utility function with the
required properties existatall? Weshallsee.
16.2.1 Constraints onrationalpreferences
Thesequestions canbeansweredbywritingdownsomeconstraints onthepreferences thata
rational agentshouldhaveandthenshowingthattheMEUprinciple canbederivedfromthe
constraints. Weusethefollowingnotation todescribe anagent’spreferences:
A ’ B theagentprefers AoverB.
A ∼ B theagentisindifferent betweenAandB.
’
A ∼ B theagentprefers AoverB orisindifferent betweenthem.
Now the obvious question is, what sorts of things are A and B? They could be states of the
world, but more often than not there is uncertainty about what is really being offered. For
example, an airline passenger who is offered “the pasta dish or the chicken” does not know
whatlurksbeneath thetinfoilcover.2 Thepastacouldbedelicious orcongealed, thechicken
juicyorovercooked beyondrecognition. Wecanthinkofthesetofoutcomes foreachaction
asalottery—think ofeachactionasaticket. Alottery LwithpossibleoutcomesS ,...,S
LOTTERY 1 n
thatoccurwithprobabilities p ,...,p iswritten
1 n
L = [p ,S ; p ,S ; ... p ,S ].
1 1 2 2 n n
Ingeneral, each outcome S ofalottery canbeeither anatomicstate oranother lottery. The
i
primary issue for utility theory is to understand how preferences between complex lotteries
are related to preferences between the underlying states in those lotteries. To address this
issuewelistsixconstraints thatwerequire anyreasonable preference relationtoobey:
• Orderability: Given any two lotteries, a rational agent must either prefer one to the
ORDERABILITY
otherorelseratethetwoasequallypreferable. Thatis,the agentcannotavoiddeciding.
Aswesaidonpage490,refusing tobetislikerefusing toallowtimetopass.
Exactlyoneof(A ’B), (B ’ A), or(A ∼ B)holds.
• Transitivity: Givenany three lotteries, ifan agent prefers Ato B and prefers B to C,
TRANSITIVITY
thentheagentmustprefer AtoC.
(A ’ B)∧(B ’ C) ⇒ (A ’ C).
• Continuity: If some lottery B is between A and C in preference, then there is some
CONTINUITY
probability pforwhichtherationalagentwillbeindifferentbetweengettingB forsure
andthelotterythatyieldsAwithprobability pandC withprobability 1−p.
A’ B ’ C ⇒ ∃p [p,A; 1−p,C] ∼ B .
• Substitutability: If an agent is indifferent between two lotteries A and B, then the
SUBSTITUTABILITY
agentisindifferentbetweentwomorecomplexlotteriesthatarethesameexceptthatB
2 Weapologizetoreaderswhoselocalairlinesnolongerofferfoodonlongflights.
Section16.2. TheBasisofUtilityTheory 613
is substituted for A in one of them. This holds regardless of the probabilities and the
otheroutcome(s) inthelotteries.
A∼ B ⇒ [p,A; 1−p,C]∼ [p,B;1−p,C].
Thisalsoholdsifwesubstitute ’for∼inthisaxiom.
• Monotonicity: Suppose two lotteries have the sametwo possible outcomes, Aand B.
MONOTONICITY
If an agent prefers A to B, then the agent must prefer the lottery that has a higher
probability forA(andviceversa).
A’ B ⇒ (p > q ⇔ [p,A; 1−p,B]’ [q,A; 1−q,B]).
• Decomposability: Compound lotteries can be reduced to simpler ones using the laws
DECOMPOSABILITY
of probability. This has been called the “no fun in gambling” rule because it says that
two consecutive lotteries can be compressed into a single equivalent lottery, as shown
inFigure16.1(b).3
[p,A; 1−p,[q,B; 1−q,C]]∼ [p,A; (1−p)q,B; (1−p)(1−q),C].
These constraints are known as the axioms of utility theory. Each axiom can be motivated
by showing that an agent that violates it will exhibit patently irrational behavior in some
situations. Forexample, we can motivate transitivity by making an agent with nontransitive
preferences give us all its money. Suppose that the agent has the nontransitive preferences
A ’ B ’ C ’ A, where A, B, and C are goods that can be freely exchanged. If the agent
currently has A, then we could offer to trade C for A plus one cent. The agent prefers C,
andsowould bewilling tomakethistrade. Wecould then offer totrade B forC,extracting
another cent, and finally trade A for B. This brings us back where we started from, except
that theagent hasgiven usthree cents (Figure16.1(a)). Wecankeep going around thecycle
untiltheagenthasnomoneyatall. Clearly, theagenthasactedirrationally inthiscase.
16.2.2 Preferences leadto utility
Noticethattheaxiomsofutilitytheoryarereallyaxiomsaboutpreferences—theysaynothing
about a utility function. But in fact from the axioms of utility we can derive the following
consequences (fortheproof, seevonNeumannandMorgenstern, 1944):
• ExistenceofUtilityFunction: Ifanagent’spreferencesobeytheaxiomsofutility,then
there exists a function U such that U(A) > U(B) if and only if A is preferred to B,
andU(A) = U(B)ifandonlyiftheagentisindifferent between AandB.
U(A) > U(B) ⇔ A’ B
U(A) = U(B) ⇔ A∼ B
• Expected Utility of a Lottery: Theutility of a lottery is the sum of the probability of
eachoutcometimestheutilityofthatoutcome.
(cid:12)
U([p ,S ;...;p ,S ]) = p U(S ).
1 1 n n i i
i
3 Wecan account forthe enjoyment of gambling by encoding gambling events intothestatedescription; for
example,“Have$10andgambled”couldbepreferredto“Have$10anddidn’tgamble.”
614 Chapter 16. MakingSimpleDecisions
A
p
A B
q
1¢ 1¢
(1–p)
(1–q)
C
is equivalent to
A
B C p
(1–p)q
B
1¢
(1–p)(1–q) C
(a) (b)
Figure 16.1 (a) A cycle of exchanges showing that the nontransitive preferences A ’
B ’C ’Aresultinirrationalbehavior.(b)Thedecomposabilityaxiom.
Inotherwords,oncetheprobabilitiesandutilitiesofthepossibleoutcomestatesarespecified,
theutilityofacompoundlotteryinvolvingthosestatesiscompletelydetermined. Becausethe
outcomeofanondeterministic actionisalottery,itfollowsthatanagentcanactrationally—
thatis,consistentlywithitspreferences—onlybychoosinganactionthatmaximizesexpected
utilityaccording toEquation(16.1).
Theprecedingtheoremsestablishthatautilityfunctionexistsforanyrationalagent,but
theydonotestablish thatitisunique. Itiseasytosee,infact,thatanagent’sbehaviorwould
notchangeifitsutilityfunction U(S)weretransformed according to
(cid:2)
U (S)= aU(S)+b, (16.2)
where a and b are constants and a > 0; an affine transformation.4 This fact was noted in
Chapter5fortwo-playergamesofchance;here,weseethatit iscompletelygeneral.
As in game-playing, in a deterministic environment an agent just needs a preference
ranking on states—the numbers don’t matter. This is called a value function or ordinal
VALUEFUNCTION
ORDINALUTILITY utilityfunction.
FUNCTION
It is important to remember that the existence of a utility function that describes an
agent’spreferencebehaviordoesnotnecessarilymeanthat theagentisexplicitlymaximizing
thatutilityfunctioninitsowndeliberations. AsweshowedinChapter2,rationalbehaviorcan
be generated in any number of ways. By observing a rational agent’s preferences, however,
anobservercanconstruct theutilityfunction thatrepresents whattheagentisactually trying
toachieve(eveniftheagentdoesn’t knowit).
4 Inthissense,utilitiesresembletemperatures:atemperatureinFahrenheitis1.8timestheCelsiustemperature
plus32.Yougetthesameresultsineithermeasurementsystem.
Section16.3. UtilityFunctions 615
16.3 UTILITY FUNCTIONS
Utilityisafunctionthatmapsfromlotteriestorealnumbers. Weknowtherearesomeaxioms
on utilities that all rational agents must obey. Is that all we can say about utility functions?
Strictlyspeaking, thatisit: anagentcanhaveanypreferences itlikes. Forexample,anagent
mightprefertohaveaprimenumberofdollarsinitsbankaccount;inwhichcase,ifithad$16
itwouldgiveaway$3. Thismightbeunusual, butwecan’t callitirrational. Anagentmight
prefer adented 1973 FordPinto toa shiny new Mercedes. Preferences can also interact: for
example, the agent might prefer prime numbers of dollars only when it owns the Pinto, but
whenitownstheMercedes,itmightprefermoredollarstofewer. Fortunately,thepreferences
ofrealagentsareusually moresystematic, andthuseasiertodealwith.
16.3.1 Utilityassessmentandutility scales
If we want to build a decision-theoretic system that helps the agent make decisions or acts
onhisorherbehalf, wemustfirstworkoutwhattheagent’sutilityfunction is. Thisprocess,
PREFERENCE often called preference elicitation, involves presenting choices to the agent and using the
ELICITATION
observed preferences topindowntheunderlying utilityfunction.
Equation(16.2)saysthatthereisnoabsolutescaleforutilities,butitishelpful,nonethe-
less,toestablishsomescaleonwhichutilitiescanberecordedandcomparedforany particu-
larproblem. Ascalecanbeestablished byfixingtheutilitiesofanytwoparticularoutcomes,
just as we fix a temperature scale by fixing the freezing point and boiling point of water.
Typically, we fix the utility of a “best possible prize” at U(S) = u(cid:12) and a “worst possible
NORMALIZED catastrophe” atU(S) = u⊥. Normalizedutilitiesuseascalewithu⊥ = 0andu(cid:12) = 1.
UTILITIES
Given a utility scale between u(cid:12) and u⊥, we can assess the utility of any particular
STANDARDLOTTERY
prizeS byaskingtheagenttochoosebetweenS andastandardlottery[p,u(cid:12); (1−p),u⊥].
Theprobability pisadjusteduntiltheagentisindifferentbetweenS andthestandardlottery.
Assumingnormalizedutilities,theutilityofS isgivenbyp. Oncethisisdoneforeachprize,
theutilitiesforalllotteriesinvolving thoseprizesaredetermined.
In medical, transportation, and environmental decision problems, among others, peo-
ple’slivesareatstake. Insuchcases,u⊥ isthevalueassignedtoimmediatedeath(orperhaps
many deaths). Although nobody feels comfortable withputting avalue on human life, itisa
fact that tradeoffs are made all the time. Aircraft are given a complete overhaul at intervals
determined by trips and miles flown, rather than after every trip. Cars are manufactured in
a way that trades off costs against accident survival rates. Paradoxically, a refusal to “put a
monetary value on life” means that life is often undervalued. Ross Shachter relates an ex-
perience with a government agency that commissioned a study on removing asbestos from
schools. Thedecisionanalystsperformingthestudyassumedaparticulardollarvalueforthe
life of a school-age child, and argued that the rational choice under that assumption was to
remove the asbestos. The agency, morally outraged at the idea of setting the value of a life,
rejectedthereportoutofhand. Itthendecidedagainstasbestosremoval—implicitlyasserting
alowervalueforthelifeofachildthanthatassigned bytheanalysts.
616 Chapter 16. MakingSimpleDecisions
Some attempts have been made to find out the value that people place on their own
lives. One common “currency” used in medical and safety analysis is the micromort, a
MICROMORT
one in a million chance of death. If you ask people how much they would pay to avoid a
risk—for example, to avoid playing Russian roulette with a million-barreled revolver—they
will respond with very large numbers, perhaps tens of thousands of dollars, but their actual
behaviorreflectsamuchlowermonetaryvalueforamicromort. Forexample,drivinginacar
for230 miles incurs arisk of one micromort; overthe life of yourcar—say, 92,000 miles—
that’s 400 micromorts. People appear to be willing to pay about $10,000 (at 2009 prices)
more for a safer car that halves the risk of death, or about $50 per micromort. A number
of studies have confirmed a figure in this range across many individuals and risk types. Of
course, this argument holds only forsmallrisks. Mostpeople won’t agree tokillthemselves
for$50million.
Another measure is the QALY,or quality-adjusted life year. Patients with a disability
QALY
are willing to accept a shorter life expectancy to be restored to full health. For example,
kidneypatientsonaverageareindifferentbetweenlivingtwoyearsonadialysismachineand
oneyearatfullhealth.
16.3.2 The utilityofmoney
Utility theory has its roots in economics, and economics provides one obvious candidate
for a utility measure: money (or more specifically, an agent’s total net assets). The almost
universal exchangeability of money for all kinds of goods and services suggests that money
playsasignificantroleinhumanutilityfunctions.
Itwillusuallybethecasethatanagentprefersmoremoneytoless,allotherthingsbeing
MONOTONIC equal. We say that the agent exhibits a monotonic preference for more money. This does
PREFERENCE
notmeanthatmoneybehaves asautility function, because itsaysnothing aboutpreferences
betweenlotteriesinvolving money.
Supposeyouhavetriumphedovertheothercompetitorsinatelevisiongameshow. The
host now offers you achoice: either you can take the $1,000,000 prize oryou can gamble it
on the flip of a coin. If the coin comes up heads, you end up with nothing, but if it comes
up tails, you get $2,500,000. If you’re like most people, you would decline the gamble and
pocketthemillion. Areyoubeingirrational?
EXPECTED Assumingthecoinisfair,theexpectedmonetaryvalue(EMV)ofthegambleis 1($0)
MONETARYVALUE 2
+ 1($2,500,000) = $1,250,000, which is more than the original $1,000,000. But that does
2
not necessarily mean that accepting the gamble is a better decision. Suppose we use S to
n
denote the state of possessing total wealth $n, and that your current wealth is $k. Then the
expectedutilities ofthetwoactions ofaccepting anddeclining thegambleare
EU(Accept) = 1U(S )+ 1U(S ),
2 k 2 k+2,500,000
EU(Decline) = U(S ).
k+1,000,000
To determine what to do, we need to assign utilities to the outcome states. Utility is not
directly proportional tomonetary value, because theutilityforyourfirstmillionisveryhigh
(orso they say), whereas the utility foran additional million is smaller. Suppose you assign
autilityof5toyourcurrentfinancialstatus(S ),a9tothestateS ,andan8tothe
k k+2,500,000
Section16.3. UtilityFunctions 617
U U
o o o o o o
o o
o
o $ $
o
o
-150,000 o 800,000
o
o
(a) (b)
Figure16.2 Theutilityofmoney. (a)EmpiricaldataforMr.Beardoveralimitedrange.
(b)Atypicalcurveforthefullrange.
state S . Then the rational action would beto decline, because the expected utility
k+1,000,000
of accepting is only 7 (less than the 8 for declining). On the other hand, a billionaire would
mostlikely haveautility function that islocally linearoverthe range ofafewmillionmore,
andthuswouldacceptthegamble.
Inapioneeringstudyofactualutilityfunctions,Grayson(1960)foundthattheutilityof
money was almost exactly proportional to the logarithm of the amount. (This idea was first
suggested by Bernoulli (1738); see Exercise 16.3.) Oneparticular utility curve, foracertain
Mr. Beard, is shown in Figure 16.2(a). The data obtained for Mr. Beard’s preferences are
consistent withautilityfunction
U(S )= −263.31+22.09log(n+150,000)
k+n
fortherangebetween n = −$150,000andn= $800,000.
Weshould notassume thatthisisthedefinitive utility function formonetary value, but
itislikely thatmostpeople haveautility function thatisconcave forpositive wealth. Going
into debt is bad, but preferences between different levels of debt can display a reversal of
theconcavity associatedwithpositivewealth. Forexample,someonealready$10,000,000in
debt might well accept a gamble on a fair coin with a gain of $10,000,000 for heads and a
lossof$20,000,000 fortails.5 ThisyieldstheS-shapedcurveshowninFigure16.2(b).
Ifwerestrictourattention tothepositivepartofthecurves,wheretheslopeisdecreas-
ing,thenforanylottery L,theutilityofbeingfacedwiththatlotteryislessthantheutilityof
beinghandedtheexpectedmonetaryvalueofthelotteryasasurething:
U(L) < U(S ).
EMV(L)
That is, agents with curves of this shape are risk-averse: they prefer a sure thing with a
RISK-AVERSE
payoff that is less than the expected monetary value of a gamble. On the other hand, in the
“desperate” region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking.
RISK-SEEKING
5 Suchbehaviormightbecalleddesperate,butitisrationalifoneisalreadyinadesperatesituation.
618 Chapter 16. MakingSimpleDecisions
CERTAINTY The value an agent will accept in lieu of a lottery is called the certainty equivalent of the
EQUIVALENT
lottery. Studies have shownthat most people willaccept about $400 inlieu ofagamble that
gives$1000halfthetimeand$0theotherhalf—thatis,thecertaintyequivalentofthelottery
is$400,whiletheEMVis$500. ThedifferencebetweentheEMVofalotteryanditscertainty
INSURANCE equivalent is called the insurance premium. Risk aversion is the basis for the insurance
PREMIUM
industry, because it means that insurance premiums are positive. People would rather pay a
small insurance premium than gamble the price of their house against the chance of a fire.
From the insurance company’s point of view, the price of the house is very small compared
with the firm’s total reserves. This means that the insurer’s utility curve is approximately
linearoversuchasmallregion, andthegamblecoststhecompanyalmostnothing.
Noticethatforsmallchanges inwealthrelative tothecurrentwealth, almostany curve
willbeapproximately linear. Anagent thathasalinearcurveissaidtoberisk-neutral. For
RISK-NEUTRAL
gambles with small sums, therefore, we expect risk neutrality. In a sense, this justifies the
simplified procedure that proposed small gambles to assess probabilities and to justify the
axiomsofprobability inSection13.2.3.
16.3.3 Expected utilityand post-decisiondisappointment
∗
Therational waytochoosethebestaction, a ,istomaximizeexpected utility:
a ∗ = argmaxEU(a|e).
a
Ifwehavecalculatedtheexpectedutilitycorrectlyaccordingtoourprobability model,andif
the probability model correctly reflects the underlying stochastic processes that generate the
outcomes, then,onaverage,wewillgettheutilityweexpectifthewholeprocess isrepeated
manytimes.
In reality, however, our model usually oversimplifies the real situation, either because
we don’t know enough (e.g., when making a complex investment decision) or because the
computation of the true expected utility is too difficult (e.g., when estimating the utility of
successor states of the root node in backgammon). In that case, we are really working with
estimates E ! U(a|e) of the true expected utility. We will assume, kindly perhaps, that the
estimates are unbiased,thatis,theexpected valueoftheerror, E(E ! U(a|e)−EU(a|e))),is
UNBIASED
zero. In that case, it still seems reasonable to choose the action with the highest estimated
utilityandtoexpecttoreceivethatutility, onaverage, whentheactionisexecuted.
Unfortunately, the real outcome will usually be significantly worse than we estimated,
even though the estimate was unbiased! To see why, consider a decision problem in which
there are k choices, each of which has true estimated utility of 0. Suppose that the error in
each utility estimate has zero mean and standard deviation of 1, shown as the bold curve in
Figure 16.3. Now, as we actually start to generate the estimates, some of the errors will be
negative (pessimistic) and some will be positive (optimistic). Because we select the action
with the highest utility estimate, we are obviously favoring the overly optimistic estimates,
and that is the source of the bias. It is a straightforward matter to calculate the distribution
of the maximum of the k estimates (see Exercise 16.11) and hence quantify the extent of
our disappointment. The curve in Figure 16.3 for k=3 has a mean around 0.85, so the
average disappointment will be about 85% of the standard deviation in the utility estimates.
Section16.3. UtilityFunctions 619
0.9
k=30
0.8
0.7
k=10
0.6
k=3
0.5
0.4
0.3
0.2
0.1
0
-5 -4 -3 -2 -1 0 1 2 3 4 5
Error in utility estimate
Figure16.3 Plot of the errorin each of k utility estimates andof the distributionof the
maximumofkestimatesfork=3,10,and30.
With more choices, extremely optimistic estimates are more likely to arise: for k=30, the
disappointment willbearoundtwicethestandarddeviation intheestimates.
This tendency for the estimated expected utility of the best choice to be too high is
called the optimizer’s curse (Smith and Winkler, 2006). It afflicts even the most seasoned
OPTIMIZER’SCURSE
decision analysts and statisticians. Serious manifestations include believing that an exciting
new drug that has cured 80% patients in a trial will cure 80% of patients (it’s been chosen
from k= thousands of candidate drugs) or that a mutual fund advertised as having above-
average returns will continue to have them (it’s been chosen to appear in the advertisement
out of k= dozens of funds in the company’s overall portfolio). It can even be the case that
what appears to be the best choice may not be, if the variance in the utility estimate is high:
adrug, selected from thousands tried, that has cured 9of 10 patients is probably worse than
onethathascured800of1000.
Theoptimizer’scursecropsupeverywherebecauseoftheubiquityofutility-maximizing
selectionprocesses,sotakingtheutilityestimatesatfacevalueisabadidea. Wecanavoidthe
cursebyusinganexplicitprobability model P(E ! U |EU)oftheerrorintheutilityestimates.
Giventhis model andaprior P(EU)onwhat wemight reasonably expect the utilities tobe,
wetreattheutilityestimate,onceobtained,asevidenceandcomputetheposteriordistribution
forthetrueutilityusingBayes’rule.
16.3.4 Humanjudgment and irrationality
Decision theory is a normative theory: it describes how a rational agent should act. A
NORMATIVETHEORY
DESCRIPTIVE descriptivetheory,ontheotherhand,describeshowactualagents—forexample,humans—
THEORY
really do act. The application of economic theory would be greatly enhanced if the two
coincided, butthereappears tobesomeexperimental evidence tothecontrary. Theevidence
suggests thathumansare“predictably irrational” (Ariely,2009).
620 Chapter 16. MakingSimpleDecisions
Thebest-knownproblemistheAllaisparadox(Allais,1953). Peoplearegivenachoice
betweenlotteries AandB andthenbetweenC andD,whichhavethefollowingprizes:
A: 80%chanceof$4000 C : 20%chanceof$4000
B : 100%chanceof$3000 D : 25%chanceof$3000
Most people consistently prefer B over A (taking the sure thing), and C over D (taking the
higher EMV). The normative analysis disagrees! We can see this most easily if we use the
freedom implied by Equation (16.2) to set U($0) = 0. In that case, then B ’ A implies
that U($3000) > 0.8U($4000), whereas C ’ D implies exactly the reverse. In other
words, there is no utility function that is consistent with these choices. One explanation for
the apparently irrational preferences is the certainty effect (Kahneman and Tversky, 1979):
CERTAINTYEFFECT
peoplearestronglyattractedtogainsthatarecertain. Thereareseveralreasonswhythismay
be so. First, people may prefer to reduce their computational burden; by choosing certain
outcomes, they don’t have to compute with probabilities. But the effect persists even when
thecomputations involved areveryeasyones. Second, peoplemaydistrust thelegitimacyof
thestatedprobabilities. Itrustthatacoinflipisroughly 50/50 ifIhavecontrol overthecoin
andthe flip,butI maydistrust theresult iftheflipisdone bysomeone withavested interest
intheoutcome.6 Inthepresenceofdistrust,itmightbebettertogoforthesurething.7 Third,
people may be accounting for their emotional state as well as their financial state. People
knowtheywouldexperienceregretiftheygaveupacertainreward(B)foran80%chanceat
REGRET
ahigherrewardandthenlost. Inotherwords,if Aischosen,thereisa20%chanceofgetting
no money and feeling like a complete idiot, which is worse than just getting no money. So
perhaps people who choose B over A and C over D are not being irrational; they are just
sayingthattheyarewillingtogiveup$200ofEMVtoavoida20%chanceoffeelinglikean
idiot.
ArelatedproblemistheEllsbergparadox. Heretheprizesarefixed,buttheprobabilities
areunderconstrained. Yourpayoffwilldependonthecolorofaballchosenfromanurn. You
aretoldthattheurncontains 1/3redballs, and2/3eitherblackoryellowballs,butyoudon’t
knowhowmanyblackandhowmanyyellow. Again,youareaskedwhetheryoupreferlottery
AorB;andthenC orD:
A: $100foraredball C : $100foraredoryellowball
B : $100forablackball D : $100forablackoryellowball .
Itshouldbeclearthatifyouthinktherearemoreredthanblackballsthenyoushould prefer
A over B and C over D; if you think there are fewer red than black you should prefer the
opposite. But it turns out that most people prefer A over B and also prefer D over C, even
though there is no state of the world for which this is rational. It seems that people have
AMBIGUITY ambiguity aversion: A gives you a 1/3 chance of winning, while B could be anywhere
AVERSION
between0and2/3. Similarly,Dgivesyoua2/3chance,whileC couldbeanywherebetween
1/3and3/3. Mostpeopleelecttheknownprobability ratherthantheunknownunknowns.
6 Forexample, the mathematician/magician Persi Diaconis can make a coin flip come out the way he wants
everytime(Landhuis,2004).
7 Eventhesurethingmaynotbecertain.Despitecast-ironpromises,wehavenotyetreceivedthat$27,000,000
fromtheNigerianbankaccountofapreviouslyunknowndeceasedrelative.
Section16.3. UtilityFunctions 621
Yet another problem is that the exact wording of a decision problem can have a big
impactontheagent’schoices;thisiscalledtheframingeffect. Experimentsshowthatpeople
FRAMINGEFFECT
like a medical procedure that it is described as having a “90% survival rate” about twice as
muchasonedescribedashavinga“10%deathrate,”eventhoughthesetwostatementsmean
exactlythesamething. Thisdiscrepancyinjudgmenthasbeenfoundinmultipleexperiments
andisaboutthesamewhetherthesubjectswerepatientsinaclinic,statisticallysophisticated
business schoolstudents, orexperienced doctors.
People feel more comfortable making relative utility judgments rather than absolute
ones. ImayhavelittleideahowmuchImightenjoythevariouswinesofferedbyarestaurant.
Therestauranttakesadvantageofthisbyofferinga$200bottlethatitknowsnobodywillbuy,
butwhichserves toskewupward thecustomer’s estimate ofthevalueofallwinesandmake
the$55bottleseemlikeabargain. Thisiscalledtheanchoringeffect.
ANCHORINGEFFECT
Ifhumaninformants insistoncontradictory preference judgments, thereisnothing that
automatedagentscandotobeconsistentwiththem. Fortunately,preferencejudgmentsmade
by humans are often open to revision in the light of further consideration. Paradoxes like
the Allais paradox are greatly reduced (but not eliminated) if the choices are explained bet-
ter. In work at the Harvard Business School on assessing the utility of money, Keeney and
Raiffa(1976,p.210)foundthefollowing:
Subjectstendtobetoorisk-averseinthesmallandtherefore...thefittedutilityfunctions
exhibitunacceptablylargeriskpremiumsforlotterieswithalargespread. ...Mostofthe
subjects, however,canreconciletheirinconsistenciesandfeelthattheyhavelearnedan
importantlessonabouthowtheywanttobehave.Asaconsequence,somesubjectscancel
theirautomobilecollisioninsuranceandtakeoutmoreterminsuranceontheirlives.
The evidence for human irrationality is also questioned by researchers in the field of evo-
EVOLUTIONARY lutionary psychology, who point to the fact that our brain’s decision-making mechanisms
PSYCHOLOGY
did not evolve to solve word problems with probabilities and prizes stated as decimal num-
bers. Let us grant, for the sake of argument, that the brain has built-in neural mechanism
forcomputingwithprobabilities andutilities, orsomethingfunctionally equivalent; ifso,the
requiredinputswouldbeobtainedthroughaccumulatedexperienceofoutcomesandrewards
ratherthanthroughlinguisticpresentationsofnumerical values. Itisfarfromobviousthatwe
candirectlyaccessthebrain’sbuilt-inneuralmechanisms bypresentingdecisionproblemsin
linguistic/numerical form. The very fact that different wordings of the same decision prob-
lem elicit different choices suggests that the decision problem itself is not getting through.
Spurred by this observation, psychologists have tried presenting problems in uncertain rea-
soning and decision making in “evolutionarily appropriate” forms; for example, instead of
saying “90% survival rate,” the experimenter might show 100 stick-figure animations of the
operation, wherethepatient diesin10ofthemandsurvives in90. (Boredom isacomplicat-
ing factor in these experiments!) With decision problems posed in this way, people seem to
bemuchclosertorationalbehaviorthanpreviously suspected.
622 Chapter 16. MakingSimpleDecisions
16.4 MULTIATTRIBUTE UTILITY FUNCTIONS
Decision making in the field of public policy involves high stakes, in both money and lives.
Forexample, indeciding whatlevelsofharmfulemissions to allow fromapowerplant, pol-
icymakersmustweighthepreventionofdeathanddisability againstthebenefitofthepower
and the economic burden of mitigating the emissions. Siting a new airport requires consid-
eration of the disruption caused by construction; the cost of land; the distance from centers
ofpopulation; the noise offlight operations; safety issues arising from local topography and
weatherconditions; and so on. Problems like these, inwhich outcomes are characterized by
MULTIATTRIBUTE twoormoreattributes, arehandledby multiattributeutilitytheory.
UTILITYTHEORY
We will call the attributes X=X ,...,X ; a complete vector of assignments will be
1 n
x=(cid:16)x ,...,x (cid:17),whereeachx iseitheranumericvalueoradiscretevaluewithanassumed
1 n i
ordering on values. We will assume that higher values of an attribute correspond to higher
utilities, all other things being equal. For example, if we choose AbsenceOfNoise as an
attributeintheairportproblem,thenthegreateritsvalue,thebetterthesolution.8 Webeginby
examining casesinwhichdecisions canbemadewithoutcombining theattribute valuesinto
a single utility value. Then we look at cases in which the utilities of attribute combinations
canbespecifiedveryconcisely.
16.4.1 Dominance
SupposethatairportsiteS costsless,generateslessnoisepollution,andissaferthansiteS .
1 2
One would not hesitate to reject S . We then say that there is strict dominance of S over
STRICTDOMINANCE 2 1
S . Ingeneral, ifanoption isoflowervalueonallattributes thansomeotheroption, itneed
2
not be considered further. Strict dominance is often very useful in narrowing down the field
of choices to the real contenders, although it seldom yields a unique choice. Figure 16.4(a)
showsaschematicdiagram forthetwo-attribute case.
Thatisfineforthedeterministic case, inwhichtheattribute valuesareknownforsure.
What about the general case, where the outcomes are uncertain? A direct analog of strict
dominancecanbeconstructed, where,despitetheuncertainty, allpossibleconcreteoutcomes
for S strictly dominate all possible outcomes for S . (See Figure 16.4(b).) Of course, this
1 2
willprobably occurevenlessoftenthaninthedeterministic case.
STOCHASTIC Fortunately, there isa more useful generalization called stochastic dominance, which
DOMINANCE
occurs very frequently in real problems. Stochastic dominance is easiest to understand in
thecontextofasingleattribute. Supposewebelievethatthecostofsitingtheairportat S is
1
uniformlydistributedbetween$2.8billionand$4.8billionandthatthecostatS isuniformly
2
distributedbetween$3billionand$5.2billion. Figure16.5(a)showsthesedistributions, with
cost plotted asanegative value. Then, given only the information that utility decreases with
8 Insomecases,itmaybenecessarytosubdividetherangeofvaluessothatutilityvariesmonotonicallywithin
eachrange.Forexample,iftheRoomTemperatureattributehasautilitypeakat70◦F,wewouldsplititintotwo
attributesmeasuringthedifferencefromtheideal,onecolderandonehotter.Utilitywouldthenbemonotonically
increasingineachattribute.
Section16.4. Multiattribute UtilityFunctions 623
X X
2 2
This region
dominates A
B
C B
C
A A
D
X X
1 1
(a) (b)
Figure16.4 Strictdominance.(a)Deterministic:OptionAisstrictlydominatedbyBbut
notbyCorD.(b)Uncertain:AisstrictlydominatedbyBbutnotbyC.
0.6
0.5
0.4
0.3
0.2
0.1
0
-6 -5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
ytilibaborP
1.2
1
0.8
S S 0.6
2 1
0.4
0.2
0
-6 -5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
Negative cost
ytilibaborP
S
2
S
1
Negative cost
(a) (b)
Figure16.5 Stochastic dominance. (a) S stochastically dominatesS on cost. (b) Cu-
1 2
mulativedistributionsforthenegativecostofS andS .
1 2
cost,wecansaythatS stochasticallydominatesS (i.e.,S canbediscarded). Itisimportant
1 2 2
tonotethatthisdoesnotfollowfromcomparingtheexpectedcosts. Forexample,ifweknew
thecostofS tobeexactly$3.8billion,thenwewouldbeunabletomakeadecisionwithout
1
additional information on theutility of money. (It might seem odd that moreinformation on
the cost of S could make the agent less able to decide. The paradox is resolved by noting
1
thatintheabsenceofexactcostinformation, thedecisioniseasiertomakebutismorelikely
tobewrong.)
Theexactrelationship betweentheattributedistributions neededtoestablishstochastic
dominanceisbestseenbyexaminingthecumulativedistributions,showninFigure16.5(b).
(Seealso Appendix A.) Thecumulative distribution measures theprobability thatthecostis
less than or equal to any given amount—that is, it integrates the original distribution. If the
cumulative distribution for S is always to the right of the cumulative distribution for S ,
1 2
624 Chapter 16. MakingSimpleDecisions
then,stochastically speaking, S ischeaperthanS . Formally,iftwoactions A andA lead
1 2 1 2
toprobability distributions p (x)andp (x)onattribute X,thenA stochastically dominates
1 2 1
A onX if
2
(cid:26)x (cid:26)x
∀x p (x (cid:2) ) dx (cid:2) ≤ p (x (cid:2) )dx (cid:2) .
1 2
−∞ −∞
Therelevanceofthisdefinitiontotheselectionofoptimaldecisionscomesfromthefollowing
property: ifA stochasticallydominatesA ,thenforanymonotonicallynondecreasingutility
1 2
function U(x), the expected utility of A is at least as high as the expected utility of A .
1 2
Hence,ifanactionisstochastically dominated byanotheraction onallattributes, thenitcan
bediscarded.
The stochastic dominance condition might seem rather technical and perhaps not so
easy to evaluate without extensive probability calculations. In fact, it can be decided very
easilyinmanycases. Suppose,forexample,thattheconstructiontransportation costdepends
on the distance to the supplier. The cost itself is uncertain, but the greater the distance, the
greater the cost. If S is closer than S , then S will dominate S on cost. Although we
1 2 1 2
will not present them here, there exist algorithms for propagating this kind of qualitative
QUALITATIVE
information among uncertain variables in qualitative probabilistic networks, enabling a
PROBABILISTIC
NETWORKS
systemtomakerationaldecisionsbasedonstochasticdominance,withoutusinganynumeric
values.
16.4.2 Preference structure and multiattribute utility
Suppose we have n attributes, each of which has d distinct possible values. To specify the
completeutilityfunctionU(x ,...,x ),weneeddnvaluesintheworstcase. Now,theworst
1 n
casecorrespondstoasituationinwhichtheagent’spreferenceshavenoregularityatall. Mul-
tiattributeutilitytheoryisbasedonthesuppositionthatthepreferencesoftypicalagentshave
muchmorestructurethanthat. Thebasicapproachistoidentifyregularitiesinthepreference
REPRESENTATION behaviorwewouldexpecttoseeandtousewhatarecalledrepresentationtheoremstoshow
THEOREM
thatanagentwithacertainkindofpreference structure has autilityfunction
U(x ,...,x ) = F[f (x ),...,f (x )],
1 n 1 1 n n
where F is, we hope, a simple function such as addition. Notice the similarity to the use of
Bayesiannetworkstodecomposethejointprobability ofseveralrandom variables.
Preferences withoutuncertainty
Let us begin with the deterministic case. Rememberthat for deterministic environments the
agent has a value function V(x ,...,x ); the aim is to represent this function concisely.
1 n
The basic regularity that arises in deterministic preference structures is called preference
PREFERENCE independence. Twoattributes X and X are preferentially independent of a third attribute
INDEPENDENCE 1 2
X ifthe preference between outcomes (cid:16)x ,x ,x (cid:17)and (cid:16)x (cid:2) ,x (cid:2) ,x (cid:17)does not depend on the
3 1 2 3 1 2 3
particularvalue x forattribute X .
3 3
Going back to the airport example, where we have (among other attributes) Noise,
Cost,andDeaths toconsider, onemayproposethatNoise andCost arepreferentially inde-
Section16.4. Multiattribute UtilityFunctions 625
pendentofDeaths. Forexample,ifwepreferastatewith20,000peopleresidingintheflight
pathandaconstructioncostof$4billionoverastatewith70,000peopleresidingintheflight
pathandacostof$3.7billionwhenthesafetylevelis0.06deathspermillionpassengermiles
inboth cases, thenwewouldhavethesamepreference whenthe safetylevelis0.12or0.03;
and the sameindependence would hold forpreferences between any other pairof values for
Noise and Cost. It is also apparent that Cost and Deaths are preferentially independent of
Noise and that Noise and Deaths are preferentially independent of Cost. We say that the
MUTUAL set ofattributes {Noise,Cost,Deaths}exhibits mutualpreferential independence(MPI).
PREFERENTIAL
INDEPENDENCE
MPI says that, whereas each attribute may be important, it does not affect the way in which
onetradesofftheotherattributes againsteachother.
Mutual preferential independence is something of a mouthful, but thanks to a remark-
abletheoremduetotheeconomistGe´rardDebreu(1960),wecanderivefromitaverysimple
form fortheagent’s value function: Ifattributes X , ..., X aremutually preferentially in-
1 n
dependent, thentheagent’spreferencebehaviorcanbedescribedasmaximizingthefunction
(cid:12)
V(x ,...,x ) = V (x ),
1 n i i
i
where each V is a value function referring only to the attribute X . For example, it might
i i
wellbethecasethattheairportdecision canbemadeusingavaluefunction
V(noise,cost,deaths) = −noise ×104−cost −deaths ×1012 .
ADDITIVEVALUE Avalue function ofthistypeiscalled anadditivevaluefunction. Additivefunctions arean
FUNCTION
extremely natural way to describe an agent’s preferences and are valid in many real-world
situations. Fornattributes, assessinganadditivevaluefunctionrequires assessingnseparate
one-dimensionalvaluefunctionsratherthanonen-dimensionalfunction;typically,thisrepre-
sentsanexponentialreductioninthenumberofpreferenceexperimentsthatareneeded. Even
when MPI does not strictly hold, as might be the case at extreme values of the attributes, an
additive value function might still provide a good approximation to the agent’s preferences.
This is especially true when the violations of MPI occur in portions of the attribute ranges
thatareunlikely tooccurinpractice.
Tounderstand MPI better, ithelps tolook at cases where it doesn’t hold. Suppose you
are at a medieval market, considering the purchase of some hunting dogs, some chickens,
and some wicker cages for the chickens. The hunting dogs are very valuable, but if you
don’t haveenough cages forthechickens, thedogs willeatthechickens; hence, thetradeoff
between dogs and chickens depends strongly on the number of cages, and MPI is violated.
Theexistenceofthesekindsofinteractionsamongvariousattributesmakesitmuchharderto
assesstheoverallvaluefunction.
Preferences withuncertainty
When uncertainty is present in the domain, we also need to consider the structure of prefer-
ences between lotteries and to understand the resulting properties of utility functions, rather
than just value functions. The mathematics of this problem can become quite complicated,
sowepresent justoneofthemainresults togiveaflavorofwhatcanbedone. Thereaderis
referredtoKeeneyandRaiffa(1976)forathorough surveyof thefield.
626 Chapter 16. MakingSimpleDecisions
UTILITY The basic notion of utility independence extends preference independence to cover
INDEPENDENCE
lotteries: aset ofattributes Xisutility independent ofaset ofattributes Yifpreferences be-
tweenlotteriesontheattributes inXareindependent oftheparticularvaluesoftheattributes
MUTUALLYUTILITY in Y. A set of attributes is mutually utility independent (MUI) if each of its subsets is
INDEPENDENT
utility-independent of the remaining attributes. Again, it seems reasonable to propose that
theairportattributes areMUI.
MUI implies that the agent’s behavior can be described using a multiplicative utility
MULTIPLICATIVE function(Keeney,1974). Thegeneralformofamultiplicativeutilityfunctionisbestseenby
UTILITYFUNCTION
looking atthecaseforthreeattributes. Forconciseness, weuseU tomeanU (x ):
i i i
U =k U +k U +k U +k k U U +k k U U +k k U U
1 1 2 2 3 3 1 2 1 2 2 3 2 3 3 1 3 1
+k k k U U U .
1 2 3 1 2 3
Althoughthisdoesnotlookverysimple,itcontainsjustthreesingle-attributeutilityfunctions
andthreeconstants. Ingeneral,ann-attributeproblemexhibitingMUIcanbemodeledusing
n single-attribute utilities and n constants. Each of the single-attribute utility functions can
be developed independently of the other attributes, and this combination will be guaranteed
to generate the correct overall preferences. Additional assumptions are required to obtain a
purelyadditiveutilityfunction.
16.5 DECISION NETWORKS
In this section, welook at a general mechanism formaking rational decisions. The notation
is often called an influence diagram (Howard and Matheson, 1984), but we will use the
INFLUENCEDIAGRAM
more descriptive term decision network. Decision networks combine Bayesian networks
DECISIONNETWORK
withadditional nodetypesforactionsandutilities. Weuseairportsitingasanexample.
16.5.1 Representing a decisionproblem witha decisionnetwork
Initsmostgeneral form,adecisionnetworkrepresents information abouttheagent’scurrent
state, its possible actions, the state that will result from the agent’s action, and the utility of
that state. It therefore provides a substrate forimplementing utility-based agents of the type
first introduced in Section 2.4.5. Figure 16.6 shows a decision network forthe airport siting
problem. Itillustrates thethreetypesofnodesused:
• Chancenodes(ovals)representrandomvariables,justastheydoinBayesiannetworks.
CHANCENODES
Theagentcould beuncertain about theconstruction cost,thelevelofairtrafficandthe
potential forlitigation, andtheDeaths,Noise,andtotalCost variables, eachofwhich
alsodepends onthesitechosen. Eachchance node hasassociated withitaconditional
distribution that is indexed by the state of the parent nodes. In decision networks, the
parent nodes can include decision nodes as well as chance nodes. Note that each of
the current-state chance nodes could be part of alarge Bayesian network for assessing
construction costs,airtrafficlevels,orlitigationpotentials.
• Decision nodes(rectangles) represent points wherethedecision makerhas achoice of
DECISIONNODES
Section16.5. DecisionNetworks 627
Airport Site
Air Traffic Deaths
Litigation Noise U
Construction Cost
Figure16.6 Asimpledecisionnetworkfortheairport-sitingproblem.
actions. In this case, the AirportSite action can take on a different value for each site
under consideration. The choice influences the cost, safety, and noise that will result.
Inthis chapter, weassume thatwearedealing withasingle decision node. Chapter17
dealswithcasesinwhichmorethanonedecision mustbemade.
• Utility nodes (diamonds) represent the agent’s utility function.9 The utility node has
UTILITYNODES
as parents all variables describing the outcome that directly affect utility. Associated
with the utility node is a description of the agent’s utility as a function of the parent
attributes. The description could be just a tabulation of the function, or it might be a
parameterized additiveorlinearfunction oftheattribute values.
A simplified form is also used in many cases. The notation remains identical, but the
chancenodesdescribing theoutcomestateareomitted. Instead, theutilitynodeisconnected
directlytothecurrent-statenodesandthedecisionnode. Inthiscase,ratherthanrepresenting
autilityfunctiononoutcomestates,theutilitynoderepresentstheexpectedutilityassociated
with each action, as defined in Equation (16.1) on page 611; that is, the node is associated
ACTION-UTILITY with an action-utility function (also known as a Q-function in reinforcement learning, as
FUNCTION
described in Chapter 21). Figure 16.7 shows the action-utility representation of the airport
sitingproblem.
Noticethat,becausetheNoise,Deaths,andCost chance nodesinFigure16.6referto
future states, they can neverhave theirvalues set asevidence variables. Thus, thesimplified
version that omits these nodes can be used whenever the more general form can be used.
Although the simplified form contains fewer nodes, the omission of an explicit description
of the outcome of the siting decision means that it is less flexible with respect to changes in
circumstances. Forexample,inFigure16.6,achange inaircraft noiselevelscanbereflected
by a change in the conditional probability table associated with the Noise node, whereas a
change in the weight accorded to noise pollution in the utility function can be reflected by
9 Thesenodesarealsocalledvaluenodesintheliterature.
628 Chapter 16. MakingSimpleDecisions
Airport Site
Air Traffic
Litigation U
Construction
Figure16.7 Asimplifiedrepresentationoftheairport-sitingproblem. Chancenodescor-
respondingtooutcomestateshavebeenfactoredout.
a change in the utility table. In the action-utility diagram, Figure 16.7, on the other hand,
all such changes have to be reflected by changes to the action-utility table. Essentially, the
action-utility formulation isacompiledversionoftheoriginal formulation.
16.5.2 Evaluating decisionnetworks
Actionsareselected byevaluating thedecision networkfor eachpossible settingofthedeci-
sionnode. Oncethedecision nodeisset, itbehaves exactly likeachance nodethathasbeen
setasanevidencevariable. Thealgorithm forevaluating decisionnetworks isthefollowing:
1. Settheevidence variablesforthecurrentstate.
2. Foreachpossible valueofthedecision node:
(a) Setthedecision nodetothatvalue.
(b) Calculate theposterior probabilities fortheparent nodes oftheutility node, using
astandard probabilistic inference algorithm.
(c) Calculatetheresulting utilityfortheaction.
3. Returntheactionwiththehighestutility.
This is a straightforward extension of the Bayesian network algorithm and can be incorpo-
rated directly into the agent design given in Figure 13.1 on page 484. We will see in Chap-
ter 17 that the possibility of executing several actions in sequence makes the problem much
moreinteresting.
16.6 THE VALUE OF INFORMATION
Inthepreceding analysis, wehaveassumedthatallrelevant information, oratleastallavail-
able information, is provided to the agent before it makes its decision. In practice, this is
Section16.6. TheValueofInformation 629
hardly ever the case. One of the most important parts of decision making is knowing what
questions to ask. Forexample, a doctor cannot expect to be provided with the results of all
possiblediagnostictestsandquestionsatthetimeapatientfirstenterstheconsultingroom.10
Testsare often expensive and sometimes hazardous (both directly and because ofassociated
delays). Their importance depends on two factors: whether the test results would lead to a
significantly bettertreatmentplan,andhowlikelythevarioustestresultsare.
INFORMATIONVALUE This section describes information value theory, which enables an agent to choose
THEORY
what information to acquire. We assume that, prior to selecting a “real” action represented
by the decision node, the agent can acquire the value of any of the potentially observable
chance variables in the model. Thus, information value theory involves a simplified form
of sequential decision making—simplified because the observation actions affect only the
agent’s belief state, not the external physical state. The value of any particular observation
mustderivefromthepotentialtoaffecttheagent’seventualphysicalaction;andthispotential
canbeestimated directlyfromthedecision modelitself.
16.6.1 A simpleexample
Supposeanoilcompanyishopingtobuyoneofnindistinguishable blocksofocean-drilling
rights. LetusassumefurtherthatexactlyoneoftheblockscontainsoilworthC dollars,while
the others are worthless. The asking price of each block is C/n dollars. If the company is
risk-neutral, thenitwillbeindifferent betweenbuyingablockandnotbuyingone.
Now suppose that a seismologist offers the company the results of a survey of block
number 3, which indicates definitively whether the block contains oil. How much should
the company be willing to pay for the information? The way to answer this question is to
examinewhatthecompanywoulddoifithadtheinformation:
• Withprobability 1/n,thesurveywillindicate oilinblock 3. Inthis case, thecompany
willbuyblock3forC/ndollarsandmakeaprofitofC−C/n = (n−1)C/ndollars.
• Withprobability(n−1)/n,thesurveywillshowthattheblockcontainsnooil,inwhich
case the company will buy a different block. Now the probability of finding oil in one
oftheotherblockschangesfrom1/nto1/(n−1),sothecompanymakesanexpected
profitofC/(n−1)−C/n = C/n(n−1)dollars.
Nowwecancalculate theexpected profit,giventhesurveyinformation:
1 (n−1)C n−1 C
× + × = C/n.
n n n n(n−1)
Therefore, the company should be willing to pay the seismologist up to C/n dollars for the
information: theinformation isworthasmuchastheblockitself.
The value of information derives from the fact that with the information, one’s course
of action can be changed to suit the actual situation. One can discriminate according to the
situation, whereas without the information, one has to do what’s best on average over the
possible situations. In general, the value of a given piece of information is defined to be the
difference inexpected valuebetweenbestactionsbeforeandafterinformation isobtained.
10 IntheUnitedStates,theonlyquestionthatisalwaysaskedbeforehandiswhetherthepatienthasinsurance.
630 Chapter 16. MakingSimpleDecisions
16.6.2 A general formulaforperfect information
Itissimpletoderiveageneralmathematicalformulaforthevalueofinformation. Weassume
that exact evidence canbeobtained about the value ofsomerandom variable E (that is, we
j
VALUEOFPERFECT learnE = e ),sothephrase valueofperfectinformation(VPI)isused.11
INFORMATION j j
Let the agent’s initial evidence be e. Then the value of the current best action α is
definedby
(cid:12)
EU(α|e)= max P(RESULT(a)=s (cid:2)|a,e)U(s (cid:2) ),
a
s(cid:3)
andthevalueofthenewbestaction(afterthenewevidence E = e isobtained) willbe
j j
(cid:12)
EU(α
ej
|e,e
j
) = max P(RESULT(a)=s (cid:2)|a,e,e
j
)U(s (cid:2) ).
a
s(cid:3)
ButE isarandom variable whosevalue is currently unknown, sotodetermine thevalue of
j
discovering E ,givencurrentinformation ewemustaverageoverallpossiblevaluese that
j jk
wemightdiscoverforE ,usingourcurrentbeliefsaboutitsvalue:
j
(cid:31)
(cid:12)
VPI (E )= P(E =e |e)EU(α |e,E =e ) −EU(α|e).
e j j jk ejk j jk
k
To get some intuition for this formula, consider the simple case where there are only two
actions,a anda ,fromwhichtochoose. Theircurrentexpectedutilitiesare U andU . The
1 2 1 2
(cid:2) (cid:2)
information E = e will yield some new expected utilities U and U for the actions, but
j jk 1 2
before weobtain E ,wewillhavesome probability distributions overthe possible values of
j
(cid:2) (cid:2)
U andU (whichweassumeareindependent).
1 2
Suppose that a and a represent two different routes through a mountain range in
1 2
winter. a is anice, straight highway through a low pass, and a is awinding dirt road over
1 2
the top. Just given this information, a is clearly preferable, because it is quite possible that
1
a is blocked by avalanches, whereas it is unlikely that anything blocks a . U is therefore
2 1 1
clearly higher than U . Itispossible toobtain satellite reports E on theactual state ofeach
2 j
(cid:2) (cid:2)
road that would give new expectations, U and U , for the two crossings. The distributions
1 2
fortheseexpectations areshowninFigure16.8(a). Obviously,inthiscase,itisnotworththe
expenseofobtainingsatellitereports,becauseitisunlikelythattheinformationderivedfrom
themwillchangetheplan. Withnochange, information hasnovalue.
Nowsuppose thatwearechoosing betweentwodifferentwindingdirtroadsofslightly
different lengths and we are carrying a seriously injured passenger. Then, even when U
1
(cid:2) (cid:2)
and U are quite close, the distributions of U and U are very broad. There is a significant
2 1 2
possibility thatthesecondroutewillturnouttobeclearwhilethefirstisblocked, andinthis
11 There isno loss of expressiveness in requiring perfect information. Suppose we wanted to model the case
inwhichwebecomesomewhat morecertainabout avariable. We candothatbyintroducing anothervariable
aboutwhichwelearnperfectinformation. Forexample, supposeweinitiallyhavebroaduncertaintyabout the
variable Temperature. Then we gain the perfect knowledge Thermometer = 37; this gives us imperfect
informationaboutthetrueTemperature,andtheuncertaintyduetomeasurementerrorisencodedinthesensor
modelP(Thermometer|Temperature).SeeExercise16.17foranotherexample.
Section16.6. TheValueofInformation 631
P(U | E) P(U | E) P(U | E)
j j j
U U U
U U U U U U
2 1 2 1 2 1
(a) (b) (c)
Figure16.8 Threegenericcasesforthevalueofinformation. In(a), a willalmostcer-
1
tainlyremainsuperiortoa ,sotheinformationisnotneeded.In(b),thechoiceisunclearand
2
theinformationiscrucial. In(c),thechoiceisunclear,butbecauseitmakeslittledifference,
theinformationislessvaluable.(Note: ThefactthatU hasahighpeakin(c)meansthatits
2
expectedvalueisknownwithhighercertaintythanU .)
1
case the difference in utilities will be very high. The VPI formula indicates that it might be
worthwhilegettingthesatellite reports. Suchasituation isshowninFigure16.8(b).
Finally,supposethatwearechoosingbetweenthetwodirtroadsinsummertime,when
blockage byavalanches isunlikely. Inthis case, satellite reports might show one route tobe
more scenic than the other because of flowering alpine meadows, orperhaps wetterbecause
of errant streams. It is therefore quite likely that we would change our plan if we had the
information. In this case, however, the difference in value between the two routes is still
likelytobeverysmall,sowewillnotbothertoobtainthereports. Thissituation isshownin
Figure16.8(c).
In sum, information has value to the extent that it is likely to cause a change of plan
andtotheextentthatthenewplanwillbesignificantly betterthantheoldplan.
16.6.3 Properties ofthe valueofinformation
One might ask whether it is possible for information to be deleterious: can it actually have
negative expected value? Intuitively, one should expect this to be impossible. After all, one
couldintheworstcasejustignoretheinformationandpretendthatonehasneverreceivedit.
Thisisconfirmedbythefollowingtheorem,whichappliestoanydecision-theoretic agent:
Theexpected valueofinformation isnonnegative:
∀e,E VPI (E ) ≥ 0.
j e j
ThetheoremfollowsdirectlyfromthedefinitionofVPI,andweleavetheproofasanexercise
(Exercise16.18). Itis,ofcourse,atheoremaboutexpectedvalue,notactualvalue. Additional
information can easily lead to a plan that turns out to be worse than the original plan if the
information happens tobemisleading. Forexample, amedical testthatgivesafalsepositive
resultmayleadtounnecessarysurgery;butthatdoesnotmeanthatthetestshouldn’tbedone.
632 Chapter 16. MakingSimpleDecisions
ItisimportanttorememberthatVPIdependsonthecurrentstateofinformation, which
is why it is subscripted. It can change as more information is acquired. Forany given piece
of evidence E , the value of acquiring it can go down (e.g., if another variable strongly
j
constrains the posterior for E ) or up (e.g., if another variable provides a clue on which E
j j
builds, enabling anewandbetterplantobedevised). Thus,VPIisnotadditive. Thatis,
VPI (E ,E )(cid:7)= VPI (E )+VPI (E ) (ingeneral) .
e j k e j e k
VPIis,however,orderindependent. Thatis,
VPI (E ,E )= VPI (E )+VPI (E ) = VPI (E )+VPI (E ).
e j k e j e,ej k e k e,ek j
Order independence distinguishes sensing actions from ordinary actions and simplifies the
problem ofcalculating thevalueofasequenceofsensingactions.
16.6.4 Implementationofaninformation-gathering agent
A sensible agent should ask questions in a reasonable order, should avoid asking questions
that are irrelevant, should take into account the importance of each piece of information in
relation to its cost, and should stop asking questions when that is appropriate. All of these
capabilities canbeachievedbyusingthevalueofinformation asaguide.
Figure 16.9 shows the overall design of an agent that can gather information intel-
ligently before acting. For now, we assume that with each observable evidence variable
E , there is an associated cost, Cost(E ), which reflects the cost of obtaining the evidence
j j
through tests, consultants, questions, orwhatever. Theagentrequests whatappears tobethe
most efficient observation interms of utility gain perunit cost. Weassume that the result of
theactionRequest(E )isthatthenextperceptprovides thevalueof E . Ifnoobservation is
j j
worthitscost,theagentselectsa“real”action.
The agent algorithm we have described implements a form of information gathering
thatiscalled myopic. Thisisbecause itusestheVPIformula shortsightedly, calculating the
MYOPIC
value of information as if only a single evidence variable will be acquired. Myopic control
is based on the same heuristic idea as greedy search and often works well in practice. (For
example, it has been shown to outperform expert physicians in selecting diagnostic tests.)
functionINFORMATION-GATHERING-AGENT(percept)returnsanaction
persistent: D,adecisionnetwork
integratepercept intoD
j ←thevaluethatmaximizesVPI(Ej)/Cost(Ej)
ifVPI(Ej) > Cost(Ej)
returnREQUEST(Ej)
elsereturnthebestactionfromD
Figure16.9 Designofasimpleinformation-gatheringagent. Theagentworksbyrepeat-
edlyselecting the observationwith the highestinformationvalue, untilthe cost of the next
observationisgreaterthanitsexpectedbenefit.
Section16.7. Decision-Theoretic ExpertSystems 633
However, if there is no single evidence variable that will help a lot, a myopic agent might
hastily take an action when it would have been better to request two or more variables first
and then take action. A better approach in this situation would be to construct a conditional
plan (as described in Section 11.3.2) that asks for variable values and takes different next
stepsdepending ontheanswer.
Onefinalconsideration istheeffectaseries ofquestions willhaveonahumanrespon-
dent. Peoplemayrespondbettertoaseriesofquestionsifthey“makesense,”sosomeexpert
systems are built to take this into account, asking questions in an order that maximizes the
totalutilityofthesystemandhumanratherthananorderthatmaximizesvalueofinformation.
16.7 DECISION-THEORETIC EXPERT SYSTEMS
Thefieldofdecisionanalysis,whichevolvedinthe1950sand1960s,studiestheapplication
DECISIONANALYSIS
of decision theory to actual decision problems. It is used to help make rational decisions in
important domains where the stakes are high, such as business, government, law, military
strategy,medicaldiagnosisandpublichealth,engineering design,andresourcemanagement.
The process involves a careful study of the possible actions and outcomes, as well as the
preferences placed on each outcome. It is traditional in decision analysis to talk about two
roles: the decision maker states preferences between outcomes, and the decision analyst
DECISIONMAKER
enumeratesthepossibleactionsandoutcomesandelicitspreferencesfromthedecisionmaker
DECISIONANALYST
to determine the best course of action. Until the early 1980s, the main purpose of decision
analysis was to help humans make decisions that actually reflect their own preferences. As
moreandmoredecision processes become automated, decision analysis isincreasingly used
toensurethattheautomated processes arebehaving asdesired.
Earlyexpertsystemresearchconcentrated onansweringquestions, ratherthanonmak-
ing decisions. Those systems that did recommend actions rather than providing opinions on
matters of fact generally did so using condition-action rules, rather than with explicit rep-
resentations of outcomes and preferences. The emergence of Bayesian networks in the late
1980s made it possible to build large-scale systems that generated sound probabilistic infer-
ences from evidence. The addition of decision networks means that expert systems can be
developed that recommend optimal decisions, reflecting the preferences of the agent aswell
astheavailable evidence.
A system that incorporates utilities can avoid one of the most common pitfalls associ-
atedwiththeconsultationprocess: confusinglikelihoodandimportance. Acommonstrategy
inearlymedicalexpertsystems,forexample,wastorankpossiblediagnosesinorderoflike-
lihood and report the most likely. Unfortunately, this can be disastrous! Forthe majority of
patientsingeneralpractice,thetwomostlikelydiagnosesareusually“There’snothingwrong
withyou”and“Youhaveabadcold,”butifthethirdmostlikelydiagnosisforagivenpatient
is lung cancer, that’s a serious matter. Obviously, a testing or treatment plan should depend
both on probabilities and utilities. Current medical expert systems can take into account the
valueofinformation torecommendtests,andthendescribea differential diagnosis.
634 Chapter 16. MakingSimpleDecisions
Wenowdescribe theknowledge engineering process fordecision-theoretic expert sys-
tems. Asanexample weconsider theproblem ofselecting amedical treatment forakind of
congenital heartdiseaseinchildren(seeLucas,1996).
About0.8% ofchildren are born withaheart anomaly, the most commonbeing aortic
AORTIC coarctation (aconstriction oftheaorta). Itcanbetreatedwithsurgery, angioplasty (expand-
COARCTATION
ingtheaortawithaballoonplacedinsidetheartery),ormedication. Theproblemistodecide
whattreatmenttouseandwhentodoit: theyoungertheinfant,thegreatertherisksofcertain
treatments,butonemustn’twaittoolong. Adecision-theoreticexpertsystemforthisproblem
can be created by a team consisting of at least one domain expert (a pediatric cardiologist)
andoneknowledgeengineer. Theprocess canbebrokendownintothefollowingsteps:
Create a causal model. Determine the possible symptoms, disorders, treatments, and
outcomes. Then draw arcs between them, indicating what disorders cause what symptoms,
andwhattreatmentsalleviatewhatdisorders. Someofthiswillbewellknowntothedomain
expert, and some will come from the literature. Often the model will match well with the
informalgraphical descriptions giveninmedicaltextbooks.
Simplify to a qualitative decision model. Since we are using the model to make
treatment decisions and not for other purposes (such as determining the joint probability of
certain symptom/disorder combinations), we can often simplify by removing variables that
are not involved in treatment decisions. Sometimes variables will have to be split or joined
to match the expert’s intuitions. For example, the original aortic coarctation model had a
Treatmentvariable withvalues surgery, angioplasty, andmedication, andaseparate variable
for Timing of the treatment. But the expert had a hard time thinking of these separately, so
theywerecombined, withTreatmenttakingonvaluessuchassurgery in1month. Thisgives
usthemodelofFigure16.10.
Assignprobabilities. Probabilities cancomefrompatientdatabases, literature studies,
ortheexpert’ssubjective assessments. Notethatadiagnostic system willreasonfromsymp-
tomsandotherobservations tothedisease orothercause oftheproblems. Thus, intheearly
years of building these systems, experts were asked for the probability of a cause given an
effect. Ingeneraltheyfoundthisdifficulttodo,andwerebetterabletoassesstheprobability
ofaneffectgivenacause. Somodernsystemsusuallyassesscausalknowledgeandencodeit
directly in the Bayesian network structure of the model, leaving the diagnostic reasoning to
theBayesiannetworkinferencealgorithms (ShachterandHeckerman, 1987).
Assign utilities. When there are a small number of possible outcomes, they can be
enumeratedandevaluatedindividuallyusingthemethodsofSection16.3.1. Wewouldcreate
a scale from best to worst outcome and give each a numeric value, for example 0 for death
and 1 for complete recovery. We would then place the other outcomes on this scale. This
canbedonebytheexpert, butitisbetterifthepatient (orin thecaseofinfants, thepatient’s
parents)canbeinvolved, becausedifferentpeoplehavedifferentpreferences. Ifthereareex-
ponentially manyoutcomes, weneed some waytocombine them using multiattribute utility
functions. Forexample,wemaysaythatthecostsofvariouscomplications areadditive.
Verify and refine the model. To evaluate the system we need a set of correct (input,
output) pairs; a so-called gold standard to compare against. For medical expert systems
GOLDSTANDARD
this usually means assembling the best available doctors, presenting them with a few cases,
Section16.7. Decision-Theoretic ExpertSystems 635
Sex
Postcoarctectomy
Syndrome
Tachypnea Tachycardia
Paradoxical
Failure Hypertension
To Thrive
Aortic
Intercostal Aneurysm
Dyspnea
Recession
Paraplegia
Heart Intermediate Late
Age Treatment
Failure Result Result
Hepato-
CVA
megaly
Pulmonary Aortic
Crepitations Dissection
Myocardial
Cardiomegaly
Infarction
U
Figure16.10 Influencediagramforaorticcoarctation(courtesyofPeterLucas).
and asking them for their diagnosis and recommended treatment plan. We then see how
wellthe system matches theirrecommendations. Ifitdoes poorly, wetryto isolate theparts
that are going wrong and fix them. It can be useful to run the system “backward.” Instead
of presenting the system with symptoms and asking for a diagnosis, we can present it with
a diagnosis such as “heart failure,” examine the predicted probability of symptoms such as
tachycardia, andcomparewiththemedicalliterature.
SENSITIVITY Perform sensitivity analysis. This important step checks whether the best decision is
ANALYSIS
sensitivetosmallchangesintheassignedprobabilities andutilitiesbysystematicallyvarying
those parameters and running the evaluation again. If small changes lead to significantly
different decisions, then it could be worthwhile to spend more resources to collect better
data. Ifallvariationsleadtothesamedecision, thentheagentwillhavemoreconfidencethat
itistherightdecision. Sensitivityanalysis isparticularly important, becauseoneofthemain
636 Chapter 16. MakingSimpleDecisions
criticisms ofprobabilistic approaches toexpert systems is that it istoo difficult to assess the
numerical probabilities required. Sensitivity analysis oftenrevealsthatmanyofthenumbers
need be specified only very approximately. For example, we might be uncertain about the
conditional probability P(tachycardia|dyspnea), but if the optimal decision is reasonably
robusttosmallvariations intheprobability, thenourignorance islessofaconcern.
16.8 SUMMARY
Thischaptershowshowtocombineutilitytheorywithprobabilitytoenableanagenttoselect
actionsthatwillmaximizeitsexpected performance.
• Probability theory describes what an agent should believe on the basis of evidence,
utilitytheorydescribeswhatanagentwants,anddecisiontheoryputsthetwotogether
todescribe whatanagentshoulddo.
• We can use decision theory to build a system that makes decisions by considering all
possible actions and choosing the one that leads to the best expected outcome. Such a
systemisknownasarationalagent.
• Utility theory shows that an agent whose preferences between lotteries are consistent
with a set of simple axioms can be described as possessing a utility function; further-
more,theagentselectsactions asifmaximizingitsexpected utility.
• Multiattribute utility theory deals with utilities that depend on several distinct at-
tributes of states. Stochastic dominance is a particularly useful technique for making
unambiguous decisions, evenwithoutpreciseutilityvaluesforattributes.
• Decision networks provide a simple formalism for expressing and solving decision
problems. Theyare anatural extension ofBayesian networks, containing decision and
utilitynodesinaddition tochancenodes.
• Sometimes, solving a problem involves finding more information before making a de-
cision. The value of information is defined as the expected improvement in utility
comparedwithmakingadecisionwithouttheinformation.
• Expert systems that incorporate utility information have additional capabilities com-
pared with pure inference systems. In addition to being able to make decisions, they
canusethevalueofinformation todecide whichquestions toask, ifany;theycanrec-
ommend contingency plans; and they can calculate the sensitivity of their decisions to
smallchanges inprobability andutilityassessments.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
ThebookL’artdePenser,alsoknownasthePort-RoyalLogic(Arnauld,1662)states:
Tojudgewhatonemustdotoobtainagoodoravoidanevil,itisnecessarytoconsider
notonlythegoodandtheevilinitself,butalsotheprobabilitythatithappensordoesnot
happen;andtoviewgeometricallytheproportionthatallthesethingshavetogether.
Bibliographical andHistorical Notes 637
Modern texts talk of utility rather than good and evil, but this statement correctly notes that
one should multiply utility by probability (“view geometrically”) to give expected utility,
and maximize that over all outcomes (“all these things”) to “judge what one must do.” It
is remarkable how much this got right, 350 years ago, and only 8 years after Pascal and
Fermatshowed howtouse probability correctly. ThePort-Royal Logicalso marked thefirst
publication ofPascal’swager.
Daniel Bernoulli (1738), investigating the St. Petersburg paradox (see Exercise 16.3),
was the first to realize the importance of preference measurement for lotteries, writing “the
value of an item must not be based on its price, but rather on the utility that it yields” (ital-
ics his). Utilitarian philosopher Jeremy Bentham (1823) proposed the hedonic calculus for
weighing “pleasures” and “pains,” arguing that all decisions (not just monetary ones) could
bereducedtoutilitycomparisons.
The derivation of numerical utilities from preferences was first carried out by Ram-
sey(1931); theaxiomsforpreference inthepresenttextare closerinformtothoserediscov-
ered in Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944).
Agoodpresentationoftheseaxioms,inthecourseofadiscussiononriskpreference,isgiven
by Howard (1977). Ramsey had derived subjective probabilities (not just utilities) from an
agent’s preferences; Savage (1954) and Jeffrey (1983) carry out more recent constructions
ofthis kind. VonWinterfeldt andEdwards (1986) provide amodern perspective ondecision
analysis and its relationship to human preference structures. The micromort utility measure
is discussed by Howard (1989). A 1994 survey by the Economist set the value of a life at
between $750,000 and $2.6 million. However, Richard Thaler (1992) found irrational fram-
ing effects on the price one is willing to pay to avoid a risk of death versus the price one is
willing to be paid to accept a risk. For a 1/1000 chance, a respondent wouldn’t pay more
than$200toremovetherisk,butwouldn’taccept$50,000totakeontherisk. Howmuchare
people willing topay foraQALY?When itcomes downtoaspecific case ofsaving oneself
or a family member, the number is approximately “whatever I’ve got.” But we can ask at a
societallevel: supposethereisavaccinethatwouldyieldX QALYsbutcostsY dollars;isit
worthit? Inthiscasepeople reportawiderangeofvaluesfromaround $10,000 to$150,000
per QALY (Prades et al., 2008). QALYs are much more widely used in medical and social
policy decision making thanare micromorts; see (Russell, 1990) foratypical example ofan
argument foramajorchangeinpublichealthpolicyongrounds ofincreased expected utility
measuredinQALYs.
The optimizer’s curse was brought to the attention of decision analysts in a forceful
way by Smith and Winkler (2006), who pointed out that the financial benefits to the client
projected by analysts for their proposed course of action almost never materialized. They
tracethisdirectly tothebiasintroduced byselecting anoptimal action andshowthatamore
complete Bayesian analysis eliminates the problem. The same underlying concept has been
POST-DECISION called post-decision disappointment by Harrison and March (1984) and was noted in the
DISAPPOINTMENT
context of analyzing capital investment projects by Brown (1974). The optimizer’s curse is
also closely related to the winner’s curse (Capen et al., 1971; Thaler, 1992), which applies
WINNER’SCURSE
to competitive bidding in auctions: whoever wins the auction is very likely to have overes-
timated the value of the object in question. Capen et al. quote a petroleum engineer on the
638 Chapter 16. MakingSimpleDecisions
topicofbiddingforoil-drilling rights: “Ifonewinsatractagainsttwoorthreeothershemay
feel fine about his good fortune. But how should he feel if he won against 50 others? Ill.”
REGRESSIONTOTHE Finally, behind both curses is the general phenomenon of regression to themean, whereby
MEAN
individualsselectedonthebasisofexceptionalcharacteristics previouslyexhibitedwill,with
highprobability, becomelessexceptional infuture.
TheAllais paradox, due toNobelPrize-winning economist Maurice Allais (1953) was
tested experimentally (Tversky and Kahneman, 1982; Conlisk, 1989) to show that people
are consistently inconsistent in their judgments. The Ellsberg paradox on ambiguity aver-
sion was introduced in the Ph.D.thesis of Daniel Ellsberg (Ellsberg, 1962), who went on to
become a military analyst at the RAND Corporation and to leak documents known as The
Pentagon Papers, which contributed to the end of the Vietnam war and the resignation of
President Nixon. Fox and Tversky (1995) describe a further study of ambiguity aversion.
Mark Machina (2005) gives an overview of choice under uncertainty and how it can vary
fromexpected utilitytheory.
Therehasbeenarecentoutpouring ofmore-or-less popularbooksonhumanirrational-
ity. The best known is Predictably Irrational (Ariely, 2009); others include Sway (Brafman
and Brafman, 2009), Nudge (Thaler and Sunstein, 2009), Kluge (Marcus, 2009), How We
Decide (Lehrer, 2009) and On Being Certain (Burton, 2009). They complement the classic
(Kahneman et al., 1982) and the article that started it all (Kahneman and Tversky, 1979).
Thefieldofevolutionary psychology (Buss,2005), ontheotherhand, hasruncountertothis
literature, arguing that humans are quite rational in evolutionarily appropriate contexts. Its
adherents pointoutthatirrationality ispenalized bydefinitioninanevolutionary contextand
showthatinsomecasesitisanartifactoftheexperimentalsetup(CumminsandAllen,1998).
There has been arecent resurgence of interest in Bayesian models of cognition, overturning
decadesofpessimism (OaksfordandChater,1998;Elio,2002;ChaterandOaksford,2008).
Keeney and Raiffa (1976) give a thorough introduction to multiattribute utility the-
ory. They describe early computer implementations of methods for eliciting the necessary
parameters for a multiattribute utility function and include extensive accounts of real appli-
cations of the theory. In AI, the principal reference for MAUT is Wellman’s (1985) paper,
which includes a system called URP (Utility Reasoning Package) that can use a collection
of statements about preference independence and conditional independence to analyze the
structure of decision problems. The use of stochastic dominance together with qualitative
probability models was investigated extensively by Wellman (1988, 1990a). Wellman and
Doyle (1992) provide apreliminary sketch of how acomplex set of utility-independence re-
lationships might be used to provide a structured model of a utility function, in much the
same way that Bayesian networks provide a structured model of joint probability distribu-
tions. BacchusandGrove(1995, 1996)andLaMuraandShoham(1999)givefurtherresults
alongtheselines.
Decision theory has been a standard tool in economics, finance, and management sci-
encesincethe1950s. Untilthe1980s,decisiontreeswerethemaintoolusedforrepresenting
simple decision problems. Smith (1988) gives an overview of the methodology of deci-
sion analysis. Influence diagrams were introduced by Howard and Matheson (1984), based
on earlier work at SRI (Miller et al., 1976). Howard and Matheson’s method involved the
Bibliographical andHistorical Notes 639
derivationofadecisiontreefromadecisionnetwork,butin generalthetreeisofexponential
size. Shachter(1986) developed amethod formaking decisions based directly onadecision
network, without the creation of an intermediate decision tree. This algorithm was also one
of the first to provide complete inference formultiply connected Bayesian networks. Zhang
etal.(1994)showedhowtotakeadvantageofconditionalindependenceofinformationtore-
ducethesizeoftreesinpractice;theyusethetermdecisionnetworkfornetworksthatusethis
approach(although othersuseitasasynonymforinfluencediagram). NilssonandLauritzen
(2000) link algorithms for decision networks to ongoing developments in clustering algo-
rithmsforBayesiannetworks. KollerandMilch(2003)showhowinfluencediagramscanbe
usedtosolvegamesthatinvolvegathering information byopposing players, andDetwarasiti
and Shachter(2005) show how influence diagrams can beused as anaid todecision making
for a team that shares goals but is unable to share all information perfectly. The collection
byOliverandSmith(1990)hasanumberofusefularticlesondecisionnetworks,asdoesthe
1990specialissueofthejournalNetworks. Papersondecisionnetworksandutilitymodeling
alsoappearregularly inthejournals ManagementScience andDecisionAnalysis.
The theory of information value was explored first in the context of statistical experi-
ments,whereaquasi-utility (entropy reduction) wasused(Lindley,1956). TheRussiancon-
troltheoristRuslanStratonovich(1965)developedthemoregeneraltheorypresentedhere,in
which information has value by virtue of its ability to affect decisions. Stratonovich’s work
was not known in the West, where Ron Howard (1966) pioneered the same idea. His paper
endswiththeremark“Ifinformationvaluetheoryandassociateddecisiontheoreticstructures
do not in the future occupy a large part of the education of engineers, then the engineering
professionwillfindthatitstraditionalroleofmanagingscientificandeconomicresourcesfor
the benefit of man has been forfeited to another profession.” Todate, the implied revolution
inmanagerial methodshasnotoccurred.
Recent work by Krause and Guestrin (2009) shows that computing the exact non-
myopicvalueofinformationisintractableeveninpolytree networks. Thereareothercases—
morerestricted thangeneral valueofinformation—in which themyopicalgorithm doespro-
vide a provably good approximation to the optimal sequence of observations (Krause et al.,
2008). Insomecases—forexample, looking fortreasure buried inoneofnplaces—ranking
experimentsinorderofsuccessprobabilitydividedbycostgivesanoptimalsolution(Kadane
andSimon,1977).
Surprisingly few early AI researchers adopted decision-theoretic tools after the early
applications inmedicaldecision makingdescribed inChapter13. Oneofthefewexceptions
was Jerry Feldman, who applied decision theory to problems in vision (Feldman and Yaki-
movsky,1974)andplanning(FeldmanandSproull,1977). Aftertheresurgenceofinterestin
probabilisticmethodsinAIinthe1980s,decision-theoreticexpertsystemsgainedwidespread
acceptance (Horvitz et al., 1988; Cowell et al., 2002). In fact, from 1991 onward, the cover
design of the journal Artificial Intelligence has depicted a decision network, although some
artisticlicenseappears tohavebeentakenwiththedirection ofthearrows.
640 Chapter 16. MakingSimpleDecisions
EXERCISES
16.1 (AdaptedfromDavidHeckerman.) ThisexerciseconcernstheAlmanacGame,which
is used by decision analysts to calibrate numeric estimation. For each of the questions that
follow, give your best guess of the answer, that is, a number that you think is as likely to be
too high as it is to be too low. Also give your guess at a 25th percentile estimate, that is, a
number that you think has a 25% chance of being too high, and a 75% chance of being too
low. Dothesameforthe75thpercentile. (Thus,youshould givethreeestimates inall—low,
median,andhigh—foreachquestion.)
a. Numberofpassengers whoflewbetweenNewYorkandLosAngelesin1989.
b. PopulationofWarsawin1992.
c. YearinwhichCoronadodiscovered theMississippi River.
d. NumberofvotesreceivedbyJimmyCarterinthe1976presidential election.
e. Ageoftheoldestlivingtree,asof2002.
f. HeightoftheHooverDaminfeet.
g. Numberofeggsproduced inOregonin1985.
h. NumberofBuddhistsintheworldin1992.
i. NumberofdeathsduetoAIDSintheUnitedStatesin1981.
j. NumberofU.S.patentsgranted in1901.
The correct answers appear after the last exercise of this chapter. From the point of view of
decisionanalysis,theinteresting thingisnothowcloseyourmedianguessescametothereal
answers, but rather how often the real answer came within your 25% and 75% bounds. If it
was about half the time, then your bounds are accurate. But if you’re like most people, you
will be more sure of yourself than you should be, and fewer than half the answers will fall
withinthebounds. Withpractice,youcancalibrateyourselftogiverealisticbounds,andthus
bemoreusefulinsupplyinginformationfordecisionmaking. Trythissecondsetofquestions
andseeifthereisanyimprovement:
a. YearofbirthofZsaZsaGabor.
b. Maximumdistance fromMarstothesuninmiles.
c. ValueindollarsofexportsofwheatfromtheUnitedStatesin1992.
d. TonshandledbytheportofHonolulu in1991.
e. AnnualsalaryindollarsofthegovernorofCalifornia in1993.
f. PopulationofSanDiegoin1990.
g. YearinwhichRogerWilliamsfoundedProvidence, RhodeIsland.
h. HeightofMt.Kilimanjaroinfeet.
i. LengthoftheBrooklynBridgeinfeet.
j. Numberofdeathsduetoautomobile accidents intheUnitedStatesin1992.
Exercises 641
16.2 Chris considers four used cars before buying the one withmaximum expected utility.
Pat considers ten cars and does the same. All other things being equal, which one is more
likelytohavethebettercar? Whichismorelikelytobedisappointedwiththeircar’squality?
Byhowmuch(intermsofstandard deviations ofexpectedquality)?
16.3 In 1713, Nicolas Bernoulli stated a puzzle, now called the St. Petersburg paradox,
which works as follows. You have the opportunity to play a game in which a fair coin is
tossed repeatedly until it comesup heads. Ifthefirstheads appears on thenthtoss, you win
2n dollars.
a. Showthattheexpectedmonetary valueofthisgameisinfinite.
b. Howmuchwouldyou,personally, paytoplaythegame?
c. Nicolas’scousinDanielBernoulliresolvedtheapparentparadoxin1738bysuggesting
thattheutilityofmoneyismeasuredonalogarithmicscale(i.e.,U(S ) = alog n+b,
n 2
whereS isthestateofhaving$n). Whatistheexpectedutilityofthegameunderthis
n
assumption?
d. Whatisthemaximumamountthatitwouldberationaltopaytoplaythegame,assum-
ingthatone’sinitialwealthis$k?
16.4 Write a computer program to automate the process in Exercise 16.9. Try your pro-
gram out on several people of different net worth and political outlook. Comment on the
consistency ofyourresults, bothforanindividual andacrossindividuals.
16.5 The Surprise Candy Company makes candy in two flavors: 70% are strawberry fla-
vor and 30% are anchovy flavor. Each new piece of candy starts out with a round shape;
as it moves along the production line, a machine randomly selects a certain percentage to
be trimmed into a square; then, each piece is wrapped in a wrapper whose color is chosen
randomly to be red or brown. 80% of the strawberry candies are round and 80% have a red
wrapper, while 90% of the anchovy candies are square and 90% have a brown wrapper. All
candiesaresoldindividually insealed,identical, blackboxes.
Nowyou, the customer, have justbought aSurprise candy atthestore buthave notyet
openedthebox. ConsiderthethreeBayesnetsinFigure16.11.
Wrapper Shape Wrapper Shape Flavor
Flavor Flavor Wrapper Shape
(i) (ii) (iii)
Figure16.11 ThreeproposedBayesnetsfortheSurpriseCandyproblem,Exercise16.5.
a. Whichnetwork(s)cancorrectly represent P(Flavor,Wrapper,Shape)?
b. Whichnetworkisthebestrepresentation forthisproblem?
642 Chapter 16. MakingSimpleDecisions
c. Doesnetwork(i)assertthat P(Wrapper|Shape)=P(Wrapper)?
d. Whatistheprobability thatyourcandyhasaredwrapper?
e. Intheboxisaroundcandywitharedwrapper. Whatistheprobability thatitsflavoris
strawberry?
f. A unwrapped strawberry candy is worth s on the open market and an unwrapped an-
chovycandyiswortha. Writeanexpression forthevalueofanunopened candybox.
g. Anewlawprohibits trading ofunwrapped candies, butitisstill legaltotradewrapped
candies (out ofthe box). Isanunopened candy box now worth more than less than, or
thesameasbefore?
16.6 Provethatthejudgments B ’ AandC ’ D intheAllaisparadox (page 620)violate
theaxiomofsubstitutability.
16.7 Consider the Allais paradox described on page 620: an agent who prefers B over
A (taking the sure thing), and C over D (taking the higher EMV) is not acting rationally,
accordingtoutilitytheory. Doyouthinkthisindicatesaproblemfortheagent,aproblemfor
thetheory, ornoproblematall? Explain.
16.8 Ticketstoalotterycost$1. Therearetwopossibleprizes: a $10payoffwithprobabil-
ity 1/50, and a$1,000,000 payoff withprobability 1/2,000,000. Whatis theexpected mone-
taryvalueofalotteryticket? When(ifever)isitrationaltobuyaticket? Beprecise—showan
equation involving utilities. Youmayassumecurrent wealthof$k andthatU(S ) = 0. You
k
may also assume that U(S ) = 10×U(S ), but you may not make any assumptions
k+10 k+1
about U(S ). Sociological studies show that people with lower income buy a dis-
k+1,000,000
proportionate numberoflotterytickets. Doyouthinkthisisbecause theyareworsedecision
makersorbecausetheyhaveadifferentutilityfunction? Considerthevalueofcontemplating
the possibility of winning the lottery versus the value of contemplating becoming an action
herowhilewatchinganadventuremovie.
16.9 Assessyourownutilityfordifferentincrementalamountsofmoneybyrunningaseries
ofpreferencetestsbetweensomedefiniteamountM andalottery[p,M ;(1−p),0]. Choose
1 2
differentvaluesofM andM ,andvarypuntilyouareindifferent betweenthetwochoices.
1 2
Plottheresulting utilityfunction.
16.10 How much is a micromort worth to you? Devise a protocol to determine this. Ask
questions basedbothonpayingtoavoidriskandbeingpaidtoacceptrisk.
16.11 Let continuous variables X ,...,X be independently distributed according to the
1 k
sameprobabilitydensityfunctionf(x). Provethatthedensityfunctionformax{X ,...,X }
1 k
isgivenbykf(x)(F(x))k−1,whereF
isthecumulativedistribution forf.
16.12 Economists often make use of an exponential utility function for money: U(x) =
−ex/R,whereRisapositiveconstant representing anindividual’s risktolerance. Risktoler-
ancereflectshowlikelyanindividualistoacceptalotterywithaparticularexpectedmonetary
value (EMV) versus some certain payoff. As R (which is measured in the same units as x)
becomeslarger, theindividual becomeslessrisk-averse.
Exercises 643
a. Assume Mary has an exponential utility function with R = $500. Mary is given the
choice between receiving $500 with certainty (probability 1) or participating in a lot-
tery which has a 60% probability of winning $5000 and a40% probability of winning
nothing. Assuming Marry acts rationally, which option would she choose? Show how
youderivedyouranswer.
b. Considerthechoicebetweenreceiving$100withcertainty (probability1)orparticipat-
inginalottery whichhasa50%probability ofwinning $500anda50%probability of
winningnothing. ApproximatethevalueofR(to3significant digits)inanexponential
utilityfunctionthatwouldcauseanindividualtobeindifferenttothesetwoalternatives.
(Youmightfindithelpful towriteashortprogram tohelpyousolvethisproblem.)
16.13 Repeat Exercise 16.16, using the action-utility representation shown in Figure 16.7.
16.14 For either of the airport-siting diagrams from Exercises 16.16 and 16.13, to which
conditional probability table entry is theutility most sensitive, given the available evidence?
16.15 Considerastudentwhohasthechoicetobuyornotbuyatextbookforacourse. We’ll
modelthisasadecision problem withoneBooleandecision node, B,indicating whetherthe
agent chooses to buy the book, and two Boolean chance nodes, M, indicating whether the
student has mastered the material in the book, and P, indicating whether the student passes
thecourse. Ofcourse, thereisalso autility node, U. Acertain student, Sam,hasanadditive
utilityfunction: 0fornotbuyingthebookand-$100forbuyingit;and$2000forpassingthe
courseand0fornotpassing. Sam’sconditional probability estimatesareasfollows:
P(p|b,m) = 0.9 P(m|b) = 0.9
P(p|b,¬m) = 0.5 P(m|¬b)= 0.7
P(p|¬b,m) = 0.8
P(p|¬b,¬m) = 0.3
You might think that P would be independent of B given M, But this course has an open-
bookfinal—sohavingthebookhelps.
a. Drawthedecisionnetworkforthisproblem.
b. Computetheexpected utilityofbuying thebookandofnotbuyingit.
c. WhatshouldSamdo?
16.16 Thisexercisecompletestheanalysisoftheairport-siting problem inFigure16.6.
a. Providereasonablevariabledomains,probabilities,andutilitiesforthenetwork,assum-
ingthattherearethreepossible sites.
b. Solvethedecision problem.
c. Whathappensifchangesintechnologymeanthateachaircraftgenerateshalfthenoise?
d. Whatifnoiseavoidance becomesthreetimesmoreimportant?
e. CalculatetheVPIforAirTraffic,Litigation,andConstruction inyourmodel.
644 Chapter 16. MakingSimpleDecisions
16.17 (Adapted from Pearl(1988).) Aused-car buyer can decide to carry out various tests
withvariouscosts(e.g.,kickthetires,takethecartoaqualifiedmechanic)andthen,depend-
ing on the outcome of the tests, decide which car to buy. We will assume that the buyer is
deciding whether to buy car c , that there is time tocarry out at most one test, and that t is
1 1
thetestofc andcosts$50.
1
Acarcan be in good shape (quality q+) orbad shape (quality q− ), and the tests might
helpindicatewhatshapethecarisin. Carc costs$1,500,anditsmarketvalueis$2,000ifit
1
isingoodshape;ifnot,$700inrepairswillbeneededtomakeitingoodshape. Thebuyer’s
estimateisthatc hasa70%chanceofbeingingoodshape.
1
a. Drawthedecisionnetworkthatrepresents thisproblem.
b. Calculatetheexpected netgainfrombuying c ,givennotest.
1
c. Testscanbedescribed bytheprobability thatthecarwillpassorfailthetestgiventhat
thecarisingoodorbadshape. Wehavethefollowinginformation:
P(pass(c ,t )|q+(c )) = 0.8
1 1 1
P(pass(c ,t )|q − (c )) = 0.35
1 1 1
UseBayes’theoremtocalculatetheprobabilitythatthecarwillpass(orfail)itstestand
hencetheprobability thatitisingood(orbad)shapegiveneachpossibletestoutcome.
d. Calculatetheoptimaldecisions giveneitherapassorafail,andtheirexpectedutilities.
e. Calculate the value of information of the test, and derive an optimal conditional plan
forthebuyer.
16.18 Recallthedefinition ofvalueofinformation inSection16.6.
a. Provethatthevalueofinformation isnonnegativeandorderindependent.
b. Explain why it is that some people would prefer not to get some information—for ex-
ample,notwantingtoknowthesexoftheirbabywhenanultrasound isdone.
c. A function f on sets is submodular if, for any element x and any sets A and B such
SUBMODULARITY
thatA⊆ B,addingxtoAgivesagreaterincrease in f thanaddingxtoB:
A⊆ B ⇒ (f(A∪{x})−f(A)) ≥ (f(B∪{x})−f(B)).
Submodularity captures the intuitive notion of diminishing returns. Is the value of in-
formation, viewedasafunction f onsetsofpossible observations, submodular? Prove
thisorfindacounterexample.
TheanswerstoExercise16.1(whereMstandsformillion): Firstset: 3M,1.6M,1541,41M,
4768, 221, 649M, 295M, 132, 25,546. Second set: 1917, 155M, 4,500M, 11M, 120,000,
1.1M,1636,19,340, 1,595,41,710.
17
MAKING COMPLEX
DECISIONS
In which we examine methods for deciding what to do today, given that we may
decideagaintomorrow.
Inthischapter,weaddressthecomputationalissuesinvolvedinmakingdecisionsinastochas-
tic environment. Whereas Chapter 16 was concerned with one-shot or episodic decision
problems, in which the utility of each action’s outcome was well known, we are concerned
SEQUENTIAL here withsequential decision problems, inwhich theagent’s utility depends onasequence
DECISIONPROBLEM
of decisions. Sequential decision problems incorporate utilities, uncertainty, and sensing,
and include search and planning problems as special cases. Section 17.1 explains how se-
quential decision problems are defined, and Sections 17.2 and 17.3 explain how they can
be solved to produce optimal behavior that balances the risks and rewards of acting in an
uncertain environment. Section 17.4 extends these ideas to the case of partially observable
environments, andSection17.4.3developsacompletedesignfordecision-theoretic agentsin
partially observable environments, combining dynamic Bayesian networks from Chapter 15
withdecision networksfromChapter16.
The second part of the chapter covers environments with multiple agents. In such en-
vironments, the notion of optimal behavior is complicated by the interactions among the
agents. Section 17.5 introduces the main ideas of game theory, including the idea that ra-
tional agents might need tobehave randomly. Section 17.6looks athow multiagent systems
canbedesigned sothatmultipleagentscanachieveacommongoal.
17.1 SEQUENTIAL DECISION PROBLEMS
Supposethatanagentissituatedinthe4×3environmentshowninFigure17.1(a). Beginning
inthestartstate,itmustchooseanactionateachtimestep. Theinteraction withtheenviron-
ment terminates when the agent reaches one of the goal states, marked +1 or–1. Just as for
search problems, the actions available to the agent in each state are given by ACTIONS(s),
sometimes abbreviated to A(s); in the 4×3 environment, the actions in every state are Up,
Down,Left,andRight. Weassume fornowthattheenvironment is fullyobservable, sothat
theagentalwaysknowswhereitis.
645
646 Chapter 17. MakingComplexDecisions
3 + 1 0.8
0.1 0.1
2 –1
1 START
1 2 3 4
(a) (b)
Figure 17.1 (a) A simple 4×3 environment that presents the agent with a sequential
decisionproblem. (b)Illustrationofthetransitionmodeloftheenvironment:the“intended”
outcomeoccurswithprobability0.8,butwithprobability0.2theagentmovesatrightangles
totheintendeddirection. Acollisionwithawallresultsinnomovement. Thetwoterminal
stateshavereward+1and–1,respectively,andallotherstateshavearewardof–0.04.
Iftheenvironment weredeterministic, asolutionwouldbeeasy: [Up,Up,Right,Right,
Right]. Unfortunately, theenvironmentwon’talwaysgoalongwiththissolution,becausethe
actions are unreliable. The particular model of stochastic motion that weadopt isillustrated
in Figure 17.1(b). Each action achieves the intended effect with probability 0.8, but the rest
ofthetime,theactionmovestheagentatrightanglestotheintendeddirection. Furthermore,
iftheagentbumpsintoawall,itstaysinthesamesquare. Forexample,fromthestartsquare
(1,1), theaction Upmovestheagentto(1,2)withprobability 0.8,butwithprobability 0.1,it
movesrightto(2,1),andwithprobability 0.1,itmovesleft,bumpsintothewall,andstaysin
(1,1). In such an environment, the sequence [Up,Up,Right,Right,Right] goes up around
thebarrierandreachesthegoalstateat(4,3)withprobability 0.85=0.32768. Thereisalsoa
smallchanceofaccidentallyreachingthegoalbygoingtheotherwayaroundwithprobability
0.14×0.8,foragrandtotalof0.32776. (SeealsoExercise17.1.)
As in Chapter 3, the transition model (or just “model,” whenever no confusion can
arise) describes the outcome of each action in each state. Here, the outcome is stochastic,
so we write P(s
(cid:2)|s,a)
to denote the probability of reaching state s
(cid:2)
if action a is done in
states. WewillassumethattransitionsareMarkovianinthesenseofChapter15,thatis,the
(cid:2)
probabilityofreachings fromsdependsonlyonsandnotonthehistoryofearlierstates. For
now, you can think of P(s
(cid:2)|s,a)
as a big three-dimensional table containing probabilities.
Later,inSection17.4.3,wewillseethatthetransitionmodelcanberepresentedasadynamic
Bayesiannetwork,justasinChapter15.
Tocompletethedefinitionofthetaskenvironment, wemustspecifytheutilityfunction
for the agent. Because the decision problem is sequential, the utility function will depend
on a sequence of states—an environment history—rather than on a single state. Later in
this section, we investigate how such utility functions can be specified in general; for now,
we simply stipulate that in each state s, the agent receives a reward R(s), which may be
REWARD
positive or negative, but must be bounded. For our particular example, the reward is −0.04
in all states except the terminal states (which have rewards +1 and –1). The utility of an
Section17.1. Sequential DecisionProblems 647
environment history is just (for now) the sum of the rewards received. For example, if the
agent reaches the +1 state after 10 steps, its total utility will be 0.6. The negative reward of
–0.04 gives the agent an incentive to reach (4,3) quickly, so our environment is a stochastic
generalization of the search problems of Chapter 3. Another way of saying this is that the
agentdoesnotenjoylivinginthisenvironment andsowantstoleaveassoonaspossible.
Tosumup: asequentialdecisionproblemforafullyobservable,stochasticenvironment
MARKOVDECISION withaMarkoviantransitionmodelandadditiverewardsiscalledaMarkovdecisionprocess,
PROCESS
orMDP,andconsistsofasetofstates(withaninitialstates
0
);aset ACTIONS(s)ofactions
ineachstate;atransition modelP(s (cid:2)|s,a);andarewardfunction R(s).1
Thenextquestion is,whatdoesasolution totheproblem look like? Wehaveseen that
anyfixedactionsequence won’tsolvetheproblem,becausetheagentmightendupinastate
otherthanthegoal. Therefore,asolutionmustspecifywhattheagentshoulddoforanystate
thattheagentmightreach. Asolutionofthiskindiscalledapolicy. Itistraditionaltodenote
POLICY
a policy by π, and π(s) is the action recommended by the policy π for state s. If the agent
has acomplete policy, then no matterwhat theoutcome of anyaction, theagent willalways
knowwhattodonext.
Eachtimeagivenpolicyisexecutedstartingfromtheinitialstate,thestochastic nature
of the environment may lead to a different environment history. The quality of a policy is
therefore measured by the expected utility of the possible environment histories generated
by that policy. An optimal policy is a policy that yields the highest expected utility. We
OPTIMALPOLICY
∗ ∗
use π to denote an optimal policy. Given π , the agent decides what to do by consulting
∗
its current percept, which tells it the current state s, and then executing the action π (s). A
policyrepresentstheagentfunctionexplicitlyandisthereforeadescriptionofasimplereflex
agent,computed fromtheinformation usedforautility-based agent.
An optimal policy for the world of Figure 17.1 is shown in Figure 17.2(a). Notice
that, because the cost of taking a step is fairly small compared with the penalty for ending
up in (4,2) by accident, the optimal policy for the state (3,1) is conservative. The policy
recommends taking the long way round, rather than taking the shortcut and thereby risking
entering (4,2).
Thebalanceofriskandrewardchanges depending onthevalue ofR(s)forthenonter-
minal states. Figure 17.2(b) shows optimal policies forfourdifferent ranges of R(s). When
R(s) ≤ −1.6284, life is so painful that the agent heads straight for the nearest exit, even if
the exit is worth –1. When −0.4278 ≤ R(s) ≤ −0.0850, life is quite unpleasant; the agent
takes the shortest route to the +1 state and is willing to risk falling into the –1 state by acci-
dent. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary
(−0.0221 < R(s) < 0), theoptimal policy takes norisks atall. In(4,1)and(3,2), theagent
heads directly away from the –1 state so that it cannot fall in by accident, even though this
means banging its head against the wall quite a few times. Finally, if R(s) > 0, then life is
positively enjoyable and the agent avoids both exits. As long as the actions in (4,1), (3,2),
1 SomedefinitionsofMDPsallowtherewardtodependontheactionandoutcometoo,sotherewardfunction
is R(s,a,s(cid:3)). This simplifies the description of some environments but does not change the problem in any
fundamentalway,asshowninExercise17.4.
648 Chapter 17. MakingComplexDecisions
+1 +1
–1 –1
3 + 1
R(s) < –1.6284 – 0.4278 < R(s) < – 0.0850
2 –1
+1 +1
1
–1 –1
1 2 3 4
– 0.0221 < R(s) < 0 R(s) > 0
(a) (b)
Figure17.2 (a)AnoptimalpolicyforthestochasticenvironmentwithR(s)= −0.04in
thenonterminalstates. (b)OptimalpoliciesforfourdifferentrangesofR(s).
and(3,3)areasshown,everypolicyisoptimal,andtheagent obtainsinfinitetotalrewardbe-
causeitneverentersaterminalstate. Surprisingly, itturnsoutthattherearesixotheroptimal
policies forvariousrangesof R(s);Exercise17.5asksyoutofindthem.
The careful balancing of risk and reward is a characteristic of MDPs that does not
arise in deterministic search problems; moreover, it is a characteristic of many real-world
decision problems. For this reason, MDPs have been studied in several fields, including
AI, operations research, economics, and control theory. Dozens of algorithms have been
proposed for calculating optimal policies. In sections 17.2 and 17.3 we describe two of the
most important algorithm families. First, however, we must complete our investigation of
utilities andpoliciesforsequential decision problems.
17.1.1 Utilitiesovertime
IntheMDPexampleinFigure17.1,theperformance oftheagentwasmeasuredbyasumof
rewards for the states visited. This choice of performance measure is not arbitrary, but it is
not the only possibility for the utility function on environment histories, which we write as
U ([s ,s ,...,s ]). Ouranalysisdrawsonmultiattributeutilitytheory(Section16.4)and
h 0 1 n
issomewhattechnical; theimpatient readermaywishtoskip tothenextsection.
Thefirstquestion toanswer is whetherthere is a finitehorizon oran infinitehorizon
FINITEHORIZON
fordecision making. Afinitehorizon means thatthere isa fixed timeN afterwhichnothing
INFINITEHORIZON
matters—the gameisover, sotospeak. Thus, U ([s ,s ,...,s ])=U ([s ,s ,...,s ])
h 0 1 N+k h 0 1 N
forallk > 0. Forexample,supposeanagentstartsat(3,1)inthe 4×3worldofFigure17.1,
and suppose that N =3. Then, to have any chance of reaching the +1 state, the agent must
head directly for it, and the optimal action is to go Up. On the other hand, if N=100,
then there is plenty of time to take the safe route by going Left. So, with a finite horizon,
Section17.1. Sequential DecisionProblems 649
the optimal action in a given state could change over time. We say that the optimal policy
NONSTATIONARY for a finite horizon is nonstationary. With no fixed time limit, on the other hand, there is
POLICY
no reason to behave differently in the same state at different times. Hence, the optimal ac-
tion depends only on the current state, and the optimal policy is stationary. Policies forthe
STATIONARYPOLICY
infinite-horizon casearetherefore simplerthanthoseforthefinite-horizon case,andwedeal
mainly with the infinite-horizon case in this chapter. (We will see later that for partially ob-
servableenvironments,theinfinite-horizoncaseisnotsosimple.) Notethat“infinitehorizon”
does not necessarily mean that all state sequences are infinite; it just means that there is no
fixed deadline. In particular, there can be finite state sequences in an infinite-horizon MDP
containing aterminalstate.
Thenext question wemust decide is how to calculate the utility of state sequences. In
theterminologyofmultiattribute utilitytheory,eachstates canbeviewedasanattributeof
i
thestatesequence [s ,s ,s ...]. Toobtainasimpleexpression intermsoftheattributes, we
0 1 2
will need to make some sort of preference-independence assumption. The most natural as-
STATIONARY sumptionisthattheagent’s preferences betweenstatesequences arestationary. Stationarity
PREFERENCE
(cid:2) (cid:2) (cid:2)
forpreferencesmeansthefollowing: iftwostatesequences [s ,s ,s ,...]and[s ,s ,s ,...]
0 1 2 0 1 2
(cid:2)
beginwiththesamestate(i.e.,s =s ),thenthetwosequencesshouldbepreference-ordered
0 0
(cid:2) (cid:2)
thesamewayasthesequences[s ,s ,...]and[s ,s ,...]. InEnglish,thismeansthatifyou
1 2 1 2
prefer one future to another starting tomorrow, then you should still prefer that future if it
were to start today instead. Stationarity is a fairly innocuous-looking assumption with very
strong consequences: it turns out that under stationarity there are just two coherent ways to
assignutilities tosequences:
1. Additiverewards: Theutilityofastatesequence is
ADDITIVEREWARD
U ([s ,s ,s ,...]) = R(s )+R(s )+R(s )+··· .
h 0 1 2 0 1 2
The 4×3 world in Figure 17.1 uses additive rewards. Notice that additivity was used
implicitlyinouruseofpathcostfunctions inheuristicsearchalgorithms (Chapter3).
DISCOUNTED 2. Discountedrewards: Theutilityofastatesequenceis
REWARD
U ([s ,s ,s ,...]) = R(s )+γR(s )+γ2R(s )+··· ,
h 0 1 2 0 1 2
wherethediscountfactorγisanumberbetween0and1. Thediscountfactordescribes
DISCOUNTFACTOR
the preference of an agent for current rewards over future rewards. When γ is close
to0, rewards inthe distant future are viewed as insignificant. When γ is 1, discounted
rewards are exactly equivalent to additive rewards, so additive rewards are a special
case of discounted rewards. Discounting appears to be a good model of both animal
andhumanpreferencesovertime. Adiscountfactorofγisequivalenttoaninterestrate
of(1/γ)−1.
For reasons that will shortly become clear, we assume discounted rewards in the remainder
ofthechapter, although sometimesweallowγ=1.
Lurking beneath our choice of infinite horizons is a problem: if the environment does
not contain aterminal state, orifthe agent neverreaches one, then all environment histories
will be infinitely long, and utilities with additive, undiscounted rewards will generally be
650 Chapter 17. MakingComplexDecisions
infinite. Whilewecanagreethat+∞isbetterthan−∞,comparingtwostatesequenceswith
+∞utilityismoredifficult. Therearethreesolutions, twoofwhichwehaveseenalready:
1. With discounted rewards, the utility of an infinite sequence is finite. In fact, if γ < 1
andrewardsareboundedby ±R ,wehave
max
(cid:12)∞ (cid:12)∞
U ([s ,s ,s ,...]) = γtR(s ) ≤ γtR = R /(1−γ), (17.1)
h 0 1 2 t max max
t=0 t=0
usingthestandard formulaforthesumofaninfinitegeometricseries.
2. Iftheenvironment contains terminal states and ifthe agent isguaranteed to getto one
eventually, then we will never need to compare infinite sequences. A policy that is
guaranteed toreachaterminalstateiscalledaproperpolicy. Withproperpolicies, we
PROPERPOLICY
can use γ=1 (i.e., additive rewards). The first three policies shown in Figure 17.2(b)
areproper,butthefourthisimproper. Itgainsinfinitetotalrewardbystayingawayfrom
theterminalstateswhentherewardforthenonterminalstatesispositive. Theexistence
of improper policies can cause the standard algorithms for solving MDPs to fail with
additiverewards,andsoprovides agoodreasonforusingdiscounted rewards.
3. Infinite sequences can be compared in terms of the average reward obtained pertime
AVERAGEREWARD
step. Suppose that square (1,1) in the 4×3 world has a reward of 0.1 while the other
nonterminal states have a reward of 0.01. Then a policy that does its best to stay in
(1,1)willhavehigheraverage rewardthanonethatstayselsewhere. Average rewardis
a useful criterion for some problems, but the analysis of average-reward algorithms is
beyondthescopeofthisbook.
Insum,discounted rewardspresent thefewestdifficulties inevaluating statesequences.
17.1.2 Optimalpoliciesand theutilities ofstates
Having decided that the utility of a given state sequence is the sum of discounted rewards
obtained during the sequence, we can compare policies by comparing the expected utilities
obtained when executing them. We assume the agent is in some initial state s and define S
t
(a random variable) to be the state the agent reaches at time t when executing a particular
policyπ. (Obviously, S =s,thestatetheagent isinnow.) Theprobability distribution over
0
statesequencesS ,S ,...,isdeterminedbytheinitialstates,thepolicyπ,andthetransition
1 2
modelfortheenvironment.
Theexpectedutilityobtained byexecutingπ startinginsisgivenby
" #
(cid:12)∞
Uπ(s) = E γtR(S ) , (17.2)
t
t=0
where the expectation is with respect tothe probability distribution overstate sequences de-
terminedbysandπ. Now,outofallthepoliciestheagentcouldchoosetoexecutestartingin
∗
s,one(ormore)willhavehigherexpectedutilitiesthanalltheothers. We’lluseπ todenote
s
oneofthesepolicies:
π ∗ = argmaxUπ(s). (17.3)
s
π
Section17.1. Sequential DecisionProblems 651
∗
Remember that π is a policy, so it recommends an action for every state; its connection
s
with s in particular is that it’s an optimal policy when s is the starting state. A remarkable
consequence of using discounted utilities with infinite horizons is that the optimal policy is
independent of the starting state. (Of course, the action sequence won’t be independent;
remember that a policy is a function specifying an action for each state.) This fact seems
intuitivelyobvious: ifpolicyπ∗ isoptimalstartinginaandpolicyπ∗ isoptimalstartinginb,
a b
then, when they reach a third state c, there’s no good reason for them to disagree with each
other, orwithπ ∗ ,aboutwhattodonext.2 Sowecansimplywriteπ ∗ foranoptimalpolicy.
c
Given this definition, the true utility of a state is just
Uπ∗
(s)—that is, the expected
sum of discounted rewards if the agent executes an optimal policy. We write this as U(s),
matchingthenotation usedinChapter16fortheutility ofan outcome. NoticethatU(s)and
R(s) are quite different quantities; R(s) is the “short term” reward for being in s, whereas
U(s) is the “long term” total reward from s onward. Figure 17.3 shows the utilities for the
4×3world. Noticethattheutilities arehigherforstatesclosertothe+1exit,because fewer
stepsarerequired toreachtheexit.
3 0.812 0.868 0.918 + 1
2 0.762 0.660 –1
1 0.705 0.655 0.611 0.388
1 2 3 4
Figure 17.3 The utilities of the states in the 4×3 world, calculated with γ=1 and
R(s)= −0.04fornonterminalstates.
The utility function U(s) allows the agent to select actions by using the principle of
maximum expected utility from Chapter 16—that is, choose the action that maximizes the
expectedutilityofthesubsequent state:
(cid:12)
π
∗
(s)= argmax P(s
(cid:2)|s,a)U(s (cid:2)
). (17.4)
a∈A(s)
s(cid:3)
Thenexttwosections describealgorithms forfindingoptimalpolicies.
2 Although this seems obvious, it does not hold for finite-horizon policies or for other ways of combining
rewardsovertime. Theprooffollowsdirectlyfromtheuniquenessoftheutilityfunctiononstates,asshownin
Section17.2.
652 Chapter 17. MakingComplexDecisions
17.2 VALUE ITERATION
In this section, we present an algorithm, called value iteration, for calculating an optimal
VALUEITERATION
policy. Thebasicideaistocalculate theutilityofeachstateandthenusethestateutilities to
selectanoptimalactionineachstate.
17.2.1 The Bellmanequation forutilities
Section17.1.2definedtheutilityofbeinginastateastheexpectedsumofdiscountedrewards
from that point onwards. From this, it follows that there is a direct relationship between the
utility ofastate and theutility ofitsneighbors: the utility ofastate istheimmediate reward
for that state plus the expected discounted utility of the next state, assuming that the agent
choosestheoptimalaction. Thatis,theutilityofastateisgivenby
(cid:12)
U(s) = R(s)+γ max P(s
(cid:2)|s,a)U(s (cid:2)
). (17.5)
a∈A(s)
s(cid:3)
This is called the Bellman equation, after Richard Bellman (1957). The utilities of the
BELLMANEQUATION
states—definedbyEquation(17.2)astheexpectedutilityofsubsequentstatesequences—are
solutions ofthe set of Bellman equations. In fact, they are the unique solutions, as weshow
inSection17.2.3.
Let us look at one of the Bellman equations for the 4×3 world. The equation for the
state(1,1)is
U(1,1) = −0.04+γ max[ 0.8U(1,2)+0.1U(2,1)+0.1U(1,1), (Up)
0.9U(1,1)+0.1U(1,2), (Left)
0.9U(1,1)+0.1U(2,1), (Down)
0.8U(2,1)+0.1U(1,2)+0.1U(1,1)]. (Right)
WhenwepluginthenumbersfromFigure17.3,wefindthat Upisthebestaction.
17.2.2 The valueiterationalgorithm
TheBellmanequation isthebasisofthevalueiterationalgorithm forsolvingMDPs. Ifthere
arenpossiblestates,thentherearenBellmanequations, oneforeachstate. Thenequations
containnunknowns—theutilitiesofthestates. Sowewouldliketosolvethesesimultaneous
equations tofindtheutilities. Thereisoneproblem: theequationsarenonlinear, becausethe
“max” operator is not a linear operator. Whereas systems of linear equations can be solved
quicklyusinglinearalgebratechniques,systemsofnonlinearequationsaremoreproblematic.
Onethingtotryisaniterativeapproach. Westartwitharbitraryinitialvaluesfortheutilities,
calculate the right-hand side of the equation, and plug it into the left-hand side—thereby
updating the utility of each state from the utilities of its neighbors. We repeat this until we
reachanequilibrium. LetU (s)betheutilityvalueforstatesattheithiteration. Theiteration
i
step,calledaBellmanupdate,lookslikethis:
BELLMANUPDATE
(cid:12)
U (s)← R(s)+γ max P(s (cid:2)|s,a)U (s (cid:2) ), (17.6)
i+1 i
a∈A(s)
s(cid:3)
Section17.2. ValueIteration 653
functionVALUE-ITERATION(mdp,(cid:2))returnsautilityfunction
inputs:mdp,anMDPwithstatesS,actionsA(s),transitionmodelP(s(cid:5)|s,a),
rewardsR(s),discountγ
(cid:2),themaximumerrorallowedintheutilityofanystate
localvariables: U,U(cid:5),vectorsofutilitiesforstatesinS,initiallyzero
δ,themaximumchangeintheutilityofanystateinaniteration
repeat
U ←U(cid:5);δ←0
foreachstates inS do (cid:12)
U(cid:5)[s]←R(s) + γ max P(s(cid:5)|s,a)U[s(cid:5)]
a∈A(s)
s(cid:3)
if|U(cid:5)[s] − U[s]| > δthenδ←|U(cid:5)[s] − U[s]|
untilδ < (cid:2)(1−γ)/γ
returnU
Figure17.4 Thevalueiterationalgorithmforcalculatingutilitiesof states. Thetermina-
tionconditionisfromEquation(17.8).
1
0.8
0.6
0.4
0.2
0
-0.2
0 5 10 15 20 25 30
setamitse
ytilitU
1e+07
(4,3)
(3,3) 1e+06
(1,1) 100000
(3,1)
10000
(4,1)
1000
100
10
1
0.50.550.60.650.70.750.80.850.90.95 1
Number of iterations
deriuqer
snoitaretI
c = 0.0001
c = 0.001
c = 0.01
c = 0.1
Discount factor γ
(a) (b)
Figure17.5 (a)Graphshowingtheevolutionoftheutilitiesofselectedstatesusingvalue
iteration. (b) The number of value iterations k required to guarantee an error of at most
(cid:2)=c·R ,fordifferentvaluesofc,asafunctionofthediscountfactorγ.
max
where the update is assumed to be applied simultaneously to all the states at each iteration.
If we apply the Bellman update infinitely often, we are guaranteed to reach an equilibrium
(see Section 17.2.3), in which case the final utility values must be solutions to the Bellman
equations. Infact, theyarealsotheuniquesolutions, andthecorresponding policy(obtained
using Equation (17.4)) is optimal. The algorithm, called VALUE-ITERATION, is shown in
Figure17.4.
Wecan apply value iteration to the 4×3 world in Figure 17.1(a). Starting with initial
valuesofzero,theutilitiesevolveasshowninFigure17.5(a). Noticehowthestatesatdiffer-
654 Chapter 17. MakingComplexDecisions
entdistancesfrom(4,3)accumulatenegativerewarduntilapathisfoundto(4,3),whereupon
the utilities start to increase. We can think of the value iteration algorithm as propagating
information through thestatespacebymeansoflocalupdates.
17.2.3 Convergence ofvalueiteration
Wesaid that value iteration eventually converges toaunique set ofsolutions ofthe Bellman
equations. In this section, we explain why this happens. We introduce some useful mathe-
maticalideasalongtheway,andweobtainsomemethodsforassessingtheerrorintheutility
function returned whenthealgorithm isterminatedearly;thisisusefulbecause itmeansthat
wedon’thavetorunforever. Thissection isquitetechnical.
Thebasicconceptusedinshowingthatvalueiterationconvergesisthenotionofacon-
traction. Roughlyspeaking,acontractionisafunctionofoneargumentthat,whenappliedto
CONTRACTION
twodifferent inputs inturn, produces twooutputvalues thatare“closertogether,” byatleast
some constant factor, than the original inputs. Forexample, the function “divide by two” is
a contraction, because, after we divide any two numbers by two, their difference is halved.
Noticethatthe“dividebytwo”function hasafixedpoint, namelyzero,thatisunchanged by
the application ofthe function. Fromthis example, wecan discern twoimportant properties
ofcontractions:
• A contraction has only one fixed point; if there were two fixed points they would not
getclosertogetherwhenthefunction wasapplied, soitwouldnotbeacontraction.
• When the function is applied to any argument, the value must get closer to the fixed
point (because the fixedpoint does not move), so repeated application of acontraction
alwaysreachesthefixedpointinthelimit.
Now,supposeweviewtheBellmanupdate(Equation(17.6))asanoperator B thatisapplied
simultaneously toupdate theutilityofeverystate. LetU denotethevectorofutilities forall
i
thestatesattheithiteration. ThentheBellmanupdateequationcanbewrittenas
U ← BU .
i+1 i
Next,weneedawaytomeasuredistancesbetweenutilityvectors. Wewillusethemaxnorm,
MAXNORM
whichmeasuresthe“length”ofavectorbytheabsolute value ofitsbiggest component:
||U|| =max|U(s)|.
s
With this definition, the “distance” between two vectors, ||U − U (cid:2)||, is the maximum dif-
ference between any two corresponding elements. The main result of this section is the
(cid:2)
following: LetU andU beanytwoutilityvectors. Thenwehave
i i
||BU −BU (cid:2)|| ≤γ||U −U (cid:2)||. (17.7)
i i i i
That is, the Bellman update is a contraction by a factor of γ on the space of utility vectors.
(Exercise17.6providessomeguidanceonprovingthisclaim.) Hence,fromthepropertiesof
contractions in general, it follows that value iteration always converges to a unique solution
oftheBellmanequations wheneverγ < 1.
Section17.2. ValueIteration 655
We can also use the contraction property to analyze the rate of convergence to a solu-
(cid:2)
tion. In particular, we can replace U in Equation (17.7) with the true utilities U, for which
i
BU=U. Thenweobtaintheinequality
||BU −U|| ≤ γ||U −U||.
i i
So,ifweview||U −U||astheerrorintheestimateU ,weseethattheerrorisreducedbya
i i
factorofatleast γ oneachiteration. Thismeansthatvalueiteration converges exponentially
fast. We can calculate the number of iterations required to reach a specified error bound (cid:2)
as follows: First, recall from Equation (17.1) that the utilities of all states are bounded by
±R /(1−γ). This means that the maximum initial error ||U −U|| ≤ 2R /(1−γ).
max 0 max
Suppose we run for N iterations to reach an error of at most (cid:2). Then, because the error is
reducedbyatleast γ eachtime,werequire γN ·2R /(1−γ) ≤ (cid:2). Takinglogs,wefind
max
N=(log(2R /(cid:2)(1−γ))/log(1/γ))
max
iterations suffice. Figure17.5(b)showshow N varieswithγ,fordifferent valuesoftheratio
(cid:2)/R . The good news is that, because of the exponentially fast convergence, N does not
max
dependmuchontheratio(cid:2)/R . ThebadnewsisthatN growsrapidlyasγ becomesclose
max
to 1. We can get fast convergence if we make γ small, but this effectively gives the agent a
shorthorizonandcouldmissthelong-term effectsoftheagent’s actions.
The error bound in the preceding paragraph gives some idea of the factors influencing
the run time of the algorithm, but is sometimes overly conservative as a method of deciding
when to stop the iteration. For the latter purpose, we can use a bound relating the error
to the size of the Bellman update on any given iteration. From the contraction property
(Equation(17.7)),itcanbeshownthatiftheupdateissmall(i.e.,nostate’sutilitychangesby
much),thentheerror,comparedwiththetrueutilityfunction, alsoissmall. Moreprecisely,
if ||U −U || < (cid:2)(1−γ)/γ then ||U −U|| < (cid:2). (17.8)
i+1 i i+1
Thisistheterminationcondition usedinthe VALUE-ITERATION algorithm ofFigure17.4.
Sofar, wehaveanalyzed theerrorintheutility function returned bythevalueiteration
algorithm. What the agent really cares about, however, is how well it will do if it makes its
decisionsonthebasisofthisutilityfunction. Supposethatafteriiterationsofvalueiteration,
the agent has an estimate U of the true utility U and obtains the MEU policy π based on
i i
one-step look-ahead using U (as in Equation (17.4)). Will the resulting behavior be nearly
i
asgoodastheoptimalbehavior? Thisisacrucialquestionforanyrealagent,anditturnsout
that the answer is yes. Uπi(s) is the utility obtained if π is executed starting in s, and the
i
policy loss ||Uπi −U||isthe mosttheagent canlose byexecuting π instead oftheoptimal
POLICYLOSS i
∗
policyπ . Thepolicylossofπ isconnected totheerrorin U bythefollowinginequality:
i i
if ||U −U||< (cid:2) then ||Uπi −U|| < 2(cid:2)γ/(1−γ). (17.9)
i
Inpractice,itoftenoccursthatπ becomesoptimallongbeforeU hasconverged. Figure17.6
i i
showshowthe maximumerrorin U and thepolicy lossapproach zeroasthevalue iteration
i
processproceedsforthe4×3environmentwithγ=0.9. Thepolicyπ isoptimalwheni=4,
i
eventhoughthemaximumerrorin U isstill0.46.
i
Now we have everything we need to use value iteration in practice. We know that
it converges to the correct utilities, we can bound the error in the utility estimates if we
656 Chapter 17. MakingComplexDecisions
stop after a finite number of iterations, and we can bound the policy loss that results from
executing the corresponding MEU policy. As a final note, all of the results in this section
depend on discounting with γ < 1. If γ=1 and the environment contains terminal states,
then a similar set of convergence results and error bounds can be derived whenever certain
technical conditions aresatisfied.
17.3 POLICY ITERATION
In the previous section, we observed that it is possible to get an optimal policy even when
the utility function estimate is inaccurate. If one action is clearly better than all others, then
the exact magnitude of the utilities on the states involved need not be precise. This insight
suggestsanalternativewaytofindoptimalpolicies. Thepolicyiterationalgorithmalternates
POLICYITERATION
thefollowingtwosteps,beginning fromsomeinitialpolicy π :
0
• Policy evaluation: given a policy π , calculate U =Uπi, the utility of each state if π
POLICYEVALUATION i i i
weretobeexecuted.
POLICY • Policy improvement: Calculate a new MEU policy π , using one-step look-ahead
IMPROVEMENT i+1
basedonU (asinEquation(17.4)).
i
Thealgorithm terminateswhenthepolicyimprovementstepyieldsnochangeintheutilities.
Atthis point, we know that the utility function U is a fixed point of the Bellman update, so
i
itisasolution totheBellmanequations, andπ mustbeanoptimalpolicy. Becausethereare
i
onlyfinitelymanypolicies forafinitestatespace, andeachiteration canbeshowntoyield a
betterpolicy,policyiteration mustterminate. Thealgorithm isshowninFigure17.7.
The policy improvement step is obviously straightforward, but how do we implement
the POLICY-EVALUATION routine? It turns out that doing so is much simpler than solving
the standard Bellman equations (which is what value iteration does), because the action in
eachstateisfixedbythepolicy. Attheithiteration, thepolicyπ specifiestheactionπ (s)in
i i
1
0.8
0.6
0.4
0.2
0
0 2 4 6 8 10 12 14
ssol
yciloP/rorre
xaM
Max error
Policy loss
Number of iterations
Figure 17.6 The maximum error ||Ui −U|| of the utility estimates and the policy loss
||Uπi −U||,asafunctionofthenumberofiterationsofvalueiteration.
Section17.3. PolicyIteration 657
states. Thismeansthatwehaveasimplifiedversion oftheBellmanequation (17.5)relating
theutilityofs(underπ )totheutilitiesofitsneighbors:
i
(cid:12)
U (s)= R(s)+γ P(s
(cid:2)|s,π
(s))U (s
(cid:2)
). (17.10)
i i i
s(cid:3)
Forexample,suppose π isthepolicyshowninFigure17.2(a). Thenwehaveπ (1,1)=Up,
i i
π (1,2)=Up,andsoon,andthesimplifiedBellmanequations are
i
U (1,1) = −0.04+0.8U (1,2)+0.1U (1,1)+0.1U (2,1),
i i i i
U (1,2) = −0.04+0.8U (1,3)+0.2U (1,2),
i i i
.
.
.
The important point is that these equations are linear, because the “max”operator has been
removed. For n states, we have n linear equations with n unknowns, which can be solved
exactlyintimeO(n3)bystandardlinearalgebramethods.
Forsmallstatespaces,policyevaluationusingexactsolutionmethodsisoftenthemost
efficient approach. For large state spaces, O(n3) time might be prohibitive. Fortunately, it
is not necessary to do exact policy evaluation. Instead, we can perform some number of
simplified value iteration steps (simplified because the policy is fixed) to give a reasonably
goodapproximation oftheutilities. ThesimplifiedBellmanupdateforthisprocessis
(cid:12)
U (s)← R(s)+γ P(s (cid:2)|s,π (s))U (s (cid:2) ),
i+1 i i
s(cid:3)
and this is repeated k times to produce the next utility estimate. The resulting algorithm is
MODIFIEDPOLICY calledmodifiedpolicyiteration. Itisoftenmuchmoreefficientthanstandardpolicyiteration
ITERATION
orvalueiteration.
functionPOLICY-ITERATION(mdp)returnsapolicy
inputs:mdp,anMDPwithstatesS,actionsA(s),transitionmodelP(s(cid:5)|s,a)
localvariables: U,avectorofutilitiesforstatesinS,initiallyzero
π,apolicyvectorindexedbystate,initiallyrandom
repeat
U ←POLICY-EVALUATION(π,U,mdp)
unchanged?←true
foreachstates(cid:12)inS do (cid:12)
if max P(s
(cid:5)|s,a)U[s (cid:5)
] > P(s
(cid:5)|s,π[s])U[s (cid:5)
]thendo
a∈A(s)
s(cid:3) (cid:12) s(cid:3)
π[s]←argmax P(s (cid:5)|s,a)U[s (cid:5) ]
a∈A(s)
s(cid:3)
unchanged?←false
untilunchanged?
returnπ
Figure17.7 Thepolicyiterationalgorithmforcalculatinganoptimalpolicy.
658 Chapter 17. MakingComplexDecisions
The algorithms we have described so far require updating the utility or policy for all
states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we
can pick any subset ofstates and apply either kind ofupdating (policy improvement orsim-
plified value iteration) to that subset. This very general algorithm is called asynchronous
ASYNCHRONOUS policy iteration. Given certain conditions on the initial policy and initial utility function,
POLICYITERATION
asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom
to choose any states to work on means that we can design much more efficient heuristic
algorithms—for example, algorithms that concentrate on updating the values of states that
arelikely tobereachedbyagoodpolicy. Thismakesalotofsenseinreallife: ifonehasno
intention ofthrowing oneself offacliff, oneshould notspend timeworrying about theexact
valueoftheresulting states.
17.4 PARTIALLY OBSERVABLE MDPS
Thedescription ofMarkov decision processes inSection 17.1assumed that theenvironment
was fully observable. With this assumption, the agent always knows which state it is in.
This,combinedwiththeMarkovassumptionforthetransitionmodel,meansthattheoptimal
policydependsonlyonthecurrentstate. Whentheenvironmentisonlypartiallyobservable,
the situation is, one might say, much less clear. The agent does not necessarily know which
stateitisin,soitcannotexecutetheactionπ(s)recommendedforthatstate. Furthermore,the
utility ofastatesandtheoptimalactioninsdepend notjustons,butalsoonhowmuchthe
PARTIALLY agent knows whenitisins. Forthese reasons, partially observable MDPs(orPOMDPs—
OBSERVABLEMDP
pronounced“pom-dee-pees”)areusuallyviewedasmuchmoredifficultthanordinaryMDPs.
WecannotavoidPOMDPs,however,becausetherealworldisone.
17.4.1 DefinitionofPOMDPs
To get a handle on POMDPs, we must first define them properly. A POMDP has the same
elements as an MDP—the transition model P(s(cid:2)|s,a), actions A(s), and reward function
R(s)—but, like the partially observable search problems of Section 4.4, italso has asensor
modelP(e|s). Here,asinChapter15,thesensormodelspecifiestheprobability ofperceiv-
ing evidence e in state s.3 For example, we can convert the 4×3 world of Figure 17.1 into
a POMDP by adding a noisy or partial sensor instead of assuming that the agent knows its
location exactly. Suchasensormightmeasure the number ofadjacent walls, whichhappens
to be 2 in all the nonterminal squares except for those in the third column, where the value
is1;anoisyversionmightgivethewrongvaluewithprobability 0.1.
In Chapters 4 and 11, we studied nondeterministic and partially observable planning
problems andidentified the beliefstate—the setofactual states the agent might bein—as a
keyconceptfordescribingandcalculatingsolutions. InPOMDPs,thebeliefstatebbecomesa
probability distribution overallpossiblestates,justasinChapter15. Forexample, theinitial
3 AswiththerewardfunctionforMDPs,thesensormodelcanalsodependontheactionandoutcomestate,but
againthischangeisnotfundamental.
Section17.4. PartiallyObservableMDPs 659
beliefstateforthe4×3POMDPcouldbetheuniformdistribution overtheninenonterminal
states, i.e., (cid:16)1,1, 1,1, 1,1,1, 1,1,0,0(cid:17). We write b(s) for the probability assigned to the
9 9 9 9 9 9 9 9 9
actualstatesbybeliefstateb. Theagentcancalculateitscurrentbeliefstateastheconditional
probability distribution over the actual states given the sequence of percepts and actions so
far. ThisisessentiallythefilteringtaskdescribedinChapter15. Thebasicrecursivefiltering
equation (15.5 on page 572) shows how to calculate the new belief state from the previous
belief state and thenew evidence. ForPOMDPs,wealso have anaction to consider, but the
result isessentially thesame. If b(s)wasthe previous belief state, and theagent does action
aandthenperceivesevidence e,thenthenewbeliefstateisgivenby
(cid:12)
b (cid:2) (s (cid:2) )= αP(e|s (cid:2) ) P(s (cid:2)|s,a)b(s),
s
where αis anormalizing constant that makes the belief state sum to 1. Byanalogy with the
updateoperatorforfiltering(page572),wecanwritethisas
(cid:2)
b = FORWARD(b,a,e). (17.11)
Inthe4×3POMDP,supposetheagentmovesLeftanditssensorreports1adjacentwall;then
it’s quite likely (although not guaranteed, because both the motion and the sensor are noisy)
thattheagentisnowin(3,1). Exercise17.13asksyoutocalculatetheexactprobabilityvalues
forthenewbeliefstate.
The fundamental insight required to understand POMDPs is this: the optimal action
depends onlyontheagent’s current belief state. Thatis,theoptimal policycanbedescribed
∗
by a mapping π (b) from belief states to actions. It does not depend on the actual state the
agentisin. Thisisagoodthing,becausetheagentdoesnotknowitsactualstate;allitknows
isthebeliefstate. Hence,thedecision cycleofaPOMDPagentcanbebroken downintothe
followingthreesteps:
∗
1. Giventhecurrentbeliefstate b,executetheactiona=π (b).
2. Receivepercept e.
3. SetthecurrentbeliefstatetoFORWARD(b,a,e)andrepeat.
Nowwecanthink ofPOMDPsasrequiring asearch inbelief-state space, justlikethemeth-
ods for sensorless and contingency problems in Chapter 4. The main difference is that the
POMDPbelief-state space iscontinuous, because aPOMDPbelief stateisaprobability dis-
tribution. For example, a belief state for the 4×3 world is a point in an 11-dimensional
continuous space. An action changes the belief state, not just the physical state. Hence, the
action isevaluated atleastinpartaccording totheinformation theagentacquires asaresult.
POMDPstherefore include the value of information (Section 16.6) as one component of the
decision problem.
Let’s look more carefully at the outcome of actions. In particular, let’s calculate the
(cid:2)
probability thatanagentinbeliefstatebreachesbeliefstateb afterexecutingactiona. Now,
if we knew the action and the subsequent percept, then Equation (17.11) would provide a
(cid:2)
deterministic update to the belief state: b = FORWARD(b,a,e). Of course, the subsequent
(cid:2)
percept isnot yetknown, sotheagent mightarrive inoneofseveral possible belief states b,
depending on the percept that is received. The probability of perceiving e, given that a was
660 Chapter 17. MakingComplexDecisions
(cid:2)
performed starting inbelief state b, is given by summing overall the actual states s that the
agentmightreach:
(cid:12)
P(e|a,b) = P(e|a,s (cid:2) ,b)P(s (cid:2)|a,b)
(cid:12)s(cid:3)
= P(e|s (cid:2) )P(s (cid:2)|a,b)
(cid:12)s(cid:3)
(cid:12)
= P(e|s (cid:2) ) P(s (cid:2)|s,a)b(s).
s(cid:3) s
Let us write the probability of reaching b
(cid:2)
from b, given action a, as P(b
(cid:2)|b,a)).
Then that
givesus
(cid:12)
P(b
(cid:2)|b,a)
= P(b
(cid:2)|a,b)
= P(b
(cid:2)|e,a,b)P(e|a,b)
(cid:12) e (cid:12) (cid:12)
= P(b (cid:2)|e,a,b) P(e|s (cid:2) ) P(s (cid:2)|s,a)b(s), (17.12)
e s(cid:3) s
whereP(b
(cid:2)|e,a,b)is1ifb (cid:2)
=FORWARD(b,a,e)and0otherwise.
Equation(17.12)canbeviewedasdefiningatransitionmodelforthebelief-statespace.
Wecanalsodefinearewardfunction forbeliefstates(i.e.,theexpectedrewardfortheactual
statestheagentmightbein):
(cid:12)
ρ(b) = b(s)R(s).
s
Together, P(b
(cid:2)|b,a)
and ρ(b) define an observable MDP on the space of belief states. Fur-
∗
thermore,itcanbeshownthatanoptimalpolicyforthisMDP,π (b),isalsoanoptimalpolicy
fortheoriginal POMDP.Inotherwords, solving aPOMDPonaphysical statespace canbe
reducedtosolvinganMDPonthecorresponding belief-statespace. Thisfactisperhapsless
surprisingifwerememberthatthebeliefstateisalwaysobservabletotheagent,bydefinition.
Notice that, although we have reduced POMDPs to MDPs, the MDPwe obtain has a
continuous (and usually high-dimensional) state space. None of the MDP algorithms de-
scribed in Sections 17.2 and 17.3 applies directly to such MDPs. The next two subsec-
tions describe a value iteration algorithm designed specifically for POMDPs and an online
decision-making algorithm, similartothosedeveloped for gamesinChapter5.
17.4.2 ValueiterationforPOMDPs
Section 17.2 described a value iteration algorithm that computed one utility value for each
state. With infinitely many belief states, we need to be more creative. Consider an optimal
∗
policy π and itsapplication in aspecific belief state b: the policy generates an action, then,
foreachsubsequent percept, thebeliefstateisupdated and anewaction isgenerated, andso
on. Forthisspecificb,therefore,thepolicyisexactlyequivalenttoaconditionalplan,asde-
finedinChapter4fornondeterministicandpartiallyobservableproblems. Insteadofthinking
about policies, letusthink about conditional plans andhowtheexpected utility ofexecuting
afixedconditional planvarieswiththeinitialbeliefstate. Wemaketwoobservations:
Section17.4. PartiallyObservableMDPs 661
1. Lettheutilityofexecutingafixedconditionalplanpstartinginphysicalstatesbeα (s).
(cid:2) p
Thentheexpected utility ofexecuting pinbelief state bisjust b(s)α (s),orb·α
s p p
if we think of them both as vectors. Hence, the expected utility of a fixed conditional
planvarieslinearly withb;thatis,itcorresponds toahyperplane inbeliefspace.
2. At any given belief state b, the optimal policy will choose to execute the conditional
planwithhighestexpectedutility;andtheexpectedutilityofbundertheoptimalpolicy
isjusttheutilityofthatconditional plan:
U(b) = Uπ∗ (b) = maxb·α .
p
p
∗
Iftheoptimalpolicyπ choosestoexecutepstartingatb,thenitisreasonabletoexpect
that it might choose to execute p in belief states that are very close to b; in fact, if we
bound the depth of the conditional plans, then there are only finitely many such plans
and the continuous space of belief states will generally be divided into regions, each
corresponding toaparticularconditional planthatisoptimalinthatregion.
Fromthese twoobservations, weseethattheutility function U(b)onbelief states, being the
maximumofacollection ofhyperplanes, willbepiecewiselinearandconvex.
Toillustrate this, weuseasimple two-state world. Thestates arelabeled 0and1,with
R(0)=0 and R(1)=1. There are two actions: Stay stays put with probability 0.9 and Go
switches to the other state with probability 0.9. Fornow we will assume the discount factor
γ=1. Thesensor reports the correct state with probability 0.6. Obviously, the agent should
Staywhenitthinksit’sinstate1andGowhenitthinksit’sinstate0.
The advantage of a two-state world is that the belief space can be viewed as one-
dimensional, because the two probabilities must sum to 1. In Figure 17.8(a), the x-axis
representsthebeliefstate,definedbyb(1),theprobabilityofbeinginstate1. Nowletuscon-
sider the one-step plans [Stay] and [Go], each of which receives the reward for the current
statefollowedbythe(discounted) rewardforthestatereached aftertheaction:
α (0) = R(0)+γ(0.9R(0)+0.1R(1)) = 0.1
[Stay]
α (1) = R(1)+γ(0.9R(1)+0.1R(0)) = 1.9
[Stay]
α (0) = R(0)+γ(0.9R(1)+0.1R(0)) = 0.9
[Go]
α (1) = R(1)+γ(0.9R(0)+0.1R(1)) = 1.1
[Go]
Thehyperplanes(lines,inthiscase)forb·α andb·α areshowninFigure17.8(a)and
[Stay] [Go]
their maximum is shown in bold. The bold line therefore represents the utility function for
the finite-horizon problem that allows just one action, and in each “piece” of the piecewise
linear utility function the optimal action is the first action of the corresponding conditional
plan. Inthiscase,theoptimalone-step policyistoStaywhenb(1) > 0.5andGootherwise.
Oncewehave utilities α (s)forallthe conditional plans pofdepth 1ineach physical
p
state s, we can compute the utilities for conditional plans of depth 2 by considering each
possible first action, each possible subsequent percept, and then each way of choosing a
depth-1plantoexecuteforeachpercept:
[Stay; ifPercept=0thenStay elseStay]
[Stay; ifPercept=0thenStay elseGo]...
662 Chapter 17. MakingComplexDecisions
3
2.5
2
1.5
1
0.5
0
0 0.2 0.4 0.6 0.8 1
ytilitU
3
2.5
2
[Stay]
1.5
[Go]
1
0.5
0
0 0.2 0.4 0.6 0.8 1
Probability of state 1
ytilitU
Probability of state 1
(a) (b)
3
2.5
2
1.5
1
0.5
0
0 0.2 0.4 0.6 0.8 1
ytilitU
7.5
7
6.5
6
5.5
5
4.5
0 0.2 0.4 0.6 0.8 1
Probability of state 1
ytilitU
Probability of state 1
(c) (d)
Figure17.8 (a)Utility oftwo one-stepplansasa functionoftheinitialbeliefstate b(1)
forthetwo-stateworld, with thecorrespondingutility functionshowninbold. (b)Utilities
for8 distinct two-step plans. (c) Utilities forfourundominatedtwo-step plans. (d) Utility
functionforoptimaleight-stepplans.
There are eight distinct depth-2 plans in all, and their utilities are shown in Figure 17.8(b).
Notice that four of the plans, shown as dashed lines, are suboptimal across the entire belief
space—we say these plans are dominated, and they need not be considered further. There
DOMINATEDPLAN
are four undominated plans, each of which is optimal in a specific region, as shown in Fig-
ure17.8(c). Theregionspartition thebelief-state space.
Werepeat theprocess fordepth 3,andsoon. Ingeneral, let pbeadepth-d conditional
planwhoseinitialactionisaandwhosedepth-d−1subplan forpercept eisp.e;then
(cid:31)
(cid:12) (cid:12)
α (s) = R(s)+γ P(s (cid:2)|s,a) P(e|s (cid:2) )α (s (cid:2) ) . (17.13)
p p.e
s(cid:3) e
Thisrecursionnaturallygivesusavalueiterationalgorithm,whichissketchedinFigure17.9.
Thestructureofthealgorithmanditserroranalysisaresimilartothoseofthebasicvalueiter-
ation algorithm inFigure 17.4onpage653; themaindifference isthat instead ofcomputing
one utility number for each state, POMDP-VALUE-ITERATION maintains a collection of
Section17.4. PartiallyObservableMDPs 663
functionPOMDP-VALUE-ITERATION(pomdp,(cid:2))returnsautilityfunction
inputs:pomdp,aPOMDPwithstatesS,actionsA(s),transitionmodelP(s(cid:5)|s,a),
sensormodelP(e|s),rewardsR(s),discountγ
(cid:2),themaximumerrorallowedintheutilityofanystate
localvariables: U,U(cid:5),setsofplanspwithassociatedutilityvectorsαp
U(cid:5)←asetcontainingjusttheemptyplan[],withα (s)= R(s)
[]
repeat
U ←U(cid:5)
U(cid:5)←thesetofallplansconsistingofanactionand,foreachpossiblenextpercept,
aplaninU withutilityvectorscomputedaccordingtoEquation(17.13)
U(cid:5)←REMOVE-DOMINATED-PLANS(U(cid:5))
untilMAX-DIFFERENCE(U,U(cid:5)) < (cid:2)(1−γ)/γ
returnU
Figure 17.9 A high-level sketch of the value iteration algorithm for POMDPs. The
REMOVE-DOMINATED-PLANSstepandMAX-DIFFERENCEtestaretypicallyimplemented
aslinearprograms.
undominated plans with their utility hyperplanes. The algorithm’s complexity depends pri-
marilyonhowmanyplansgetgenerated. Given |A|actions and|E|possibleobservations, it
iseasytoshowthatthereare|A|O(|E|d−1)distinctdepth-dplans.
Evenforthelowlytwo-state
world with d=8, the exact number is 2255. The elimination of dominated plans is essential
forreducing thisdoubly exponential growth: thenumberofundominated planswithd=8is
just144. Theutilityfunction forthese144plansisshowninFigure17.8(d).
Notice that even though state 0 has lower utility than state 1, the intermediate belief
states have even lower utility because the agent lacks the information needed to choose a
good action. This is why information has value in the sense defined in Section 16.6 and
optimalpoliciesinPOMDPsofteninclude information-gathering actions.
Givensuchautilityfunction,anexecutablepolicycanbeextractedbylookingatwhich
hyperplane is optimal at any given belief state b and executing the first action of the corre-
sponding plan. In Figure 17.8(d), the corresponding optimal policy is still the same as for
depth-1plans: Staywhenb(1) > 0.5andGootherwise.
In practice, the value iteration algorithm in Figure 17.9 is hopelessly inefficient for
larger problems—even the 4×3 POMDPis too hard. The main reason is that, given ncon-
ditional plans at level d, the algorithm constructs |A|·n |E| conditional plans at level d+1
beforeeliminatingthedominatedones. Sincethe1970s,whenthisalgorithmwasdeveloped,
therehavebeenseveraladvancesincludingmoreefficientformsofvalueiterationandvarious
kindsofpolicyiterationalgorithms. Someofthesearediscussedinthenotesattheendofthe
chapter. Forgeneral POMDPs,however, finding optimal policies isvery difficult (PSPACE-
hard, in fact—i.e., very hard indeed). Problems witha few dozen states are often infeasible.
Thenext section describes adifferent, approximate method forsolving POMDPs,onebased
onlook-ahead search.
664 Chapter 17. MakingComplexDecisions
A A A A A
t–2 t–1 t t+1 t+2
X X X X X U
t–1 t t+1 t+2 t+3 t+3
R R R R
t–1 t t+1 t+2
E E E E E
t–1 t t+1 t+2 t+3
Figure17.10 Thegenericstructureofadynamicdecisionnetwork.Variableswithknown
valuesareshaded.Thecurrenttimeistandtheagentmustdecidewhattodo—thatis,choose
avalueforAt. Thenetworkhasbeenunrolledintothefutureforthreestepsandrepresents
futurerewards,aswellastheutilityofthestateatthelook-aheadhorizon.
17.4.3 Onlineagents forPOMDPs
Inthissection,weoutlineasimpleapproachtoagentdesignforpartiallyobservable,stochas-
ticenvironments. Thebasicelementsofthedesignarealready familiar:
• The transition and sensor models are represented by a dynamic Bayesian network
(DBN),asdescribed inChapter15.
• Thedynamic Bayesian network is extended withdecision and utility nodes, as used in
decision networks in Chapter 16. The resulting model is called a dynamic decision
DYNAMICDECISION network,orDDN.
NETWORK
• A filtering algorithm is used to incorporate each new percept and action and to update
thebeliefstaterepresentation.
• Decisions are made by projecting forward possible action sequences and choosing the
bestone.
DBNs are factored representations in the terminology of Chapter 2; they typically have
an exponential complexity advantage over atomic representations and can model quite sub-
stantial real-world problems. Theagentdesign istherefore apractical implementation ofthe
utility-based agentsketched inChapter2.
In the DBN, the single state S becomes a set of state variables X , and there may be
t t
multipleevidencevariablesE . WewilluseA torefertotheactionattimet,sothetransition
t t
modelbecomes P(X |X ,A )andthesensormodelbecomes P(E |X ). Wewilluse R to
t+1 t t t t t
refertotherewardreceivedattime tandU torefertotheutility ofthestateattime t. (Both
t
ofthesearerandomvariables.) Withthisnotation,adynamicdecisionnetworklookslikethe
oneshowninFigure17.10.
DynamicdecisionnetworkscanbeusedasinputsforanyPOMDPalgorithm,including
thoseforvalueandpolicyiterationmethods. Inthissection,wefocusonlook-aheadmethods
thatprojectactionsequencesforwardfromthecurrentbeliefstateinmuchthesamewayasdo
the game-playing algorithms of Chapter 5. The network in Figure 17.10 has been projected
three steps into the future; the current and future decisions A and the future observations
Section17.4. PartiallyObservableMDPs 665
A in P(X | E )
t t 1:t
E . . .
t+1
... ... ... ...
A t+1 in P(X t+1 | E 1:t+1 ) . . .
... ... ...
E . . .
t+2
... ... ...
A t+2 in P(X t+2 | E 1:t+2 ) . . .
... ... ...
E t+3 . . .
... ... ...
U(X ) . . .
t+3
10 4 6 3
Figure17.11 Partofthelook-aheadsolutionoftheDDNinFigure17.10. Eachdecision
willbetakeninthebeliefstateindicated.
E and rewards R are all unknown. Notice that the network includes nodes for the rewards
for X and X , but the utility for X . This is because the agent must maximize the
t+1 t+2 t+3
(discounted) sum of all future rewards, and U(X ) represents the reward for X and all
t+3 t+3
subsequentrewards. AsinChapter5,weassumethatU isavailableonlyinsomeapproximate
form: ifexactutilityvalueswereavailable,look-aheadbeyonddepth1wouldbeunnecessary.
Figure 17.11 shows part of the search tree corresponding to the three-step look-ahead
DDNinFigure17.10. Eachofthetriangularnodesisabeliefstateinwhichtheagentmakes
a decision A for i=0,1,2,.... The round (chance) nodes correspond to choices by the
t+i
environment, namely, what evidence E arrives. Notice that there are no chance nodes
t+i
corresponding to the action outcomes; this is because the belief-state update for an action is
deterministic regardless oftheactualoutcome.
The belief state at each triangular node can be computed by applying a filtering al-
gorithm to the sequence of percepts and actions leading to it. In this way, the algorithm
takes into account the fact that, for decision A , the agent will have available percepts
t+i
E , ..., E , even though at time t it does not know what those percepts willbe. In this
t+1 t+i
way,adecision-theoretic agentautomatically takesintoaccountthevalueofinformation and
willexecuteinformation-gathering actionswhereappropriate.
A decision can be extracted from the search tree by backing up the utility values from
the leaves, taking an average at the chance nodes and taking the maximum at the decision
nodes. ThisissimilartotheEXPECTIMINIMAX algorithmforgametreeswithchancenodes,
except that (1) there can also be rewards at non-leaf states and (2) the decision nodes corre-
spond to belief states rather than actual states. The time complexity of an exhaustive search
todepthdisO(|A|d ·|E|d),where|A|isthenumberofavailable actionsand|E|isthenum-
ber of possible percepts. (Notice that this is far less than the number of depth-d conditional
666 Chapter 17. MakingComplexDecisions
plans generated by value iteration.) For problems in which the discount factor γ is not too
close to 1, a shallow search is often good enough to give near-optimal decisions. It is also
possible to approximate the averaging step atthe chance nodes, by sampling from the set of
possibleperceptsinsteadofsummingoverallpossiblepercepts. Therearevariousotherways
offindinggoodapproximate solutions quickly, butwedeferthemtoChapter21.
Decision-theoreticagentsbasedondynamicdecisionnetworkshaveanumberofadvan-
tagescompared withother, simpleragentdesigns presented inearlierchapters. Inparticular,
theyhandlepartiallyobservable,uncertainenvironments andcaneasilyrevisetheir“plans”to
handle unexpected evidence. Withappropriate sensormodels, theycanhandle sensorfailure
andcanplantogatherinformation. Theyexhibit“graceful degradation” undertimepressure
and incomplex environments, using various approximation techniques. Sowhatismissing?
OnedefectofourDDN-basedalgorithmisitsrelianceonforwardsearchthroughstatespace,
ratherthanusingthehierarchicalandotheradvancedplanningtechniquesdescribedinChap-
ter11. Therehavebeenattemptstoextendthesetechniquesintotheprobabilisticdomain,but
sofarthey haveproved tobeinefficient. Asecond, related problem isthebasically proposi-
tionalnature oftheDDNlanguage. Wewouldliketobeabletoextendsomeoftheideasfor
first-order probabilistic languages to the problem of decision making. Current research has
shownthatthisextension ispossible andhassignificant benefits, asdiscussed inthenotesat
theendofthechapter.
17.5 DECISIONS WITH MULTIPLE AGENTS: GAME THEORY
This chapter has concentrated on making decisions in uncertain environments. But what if
theuncertainty isduetootheragentsandthedecisionstheymake? Andwhatifthedecisions
of those agents are in turn influenced by our decisions? We addressed this question once
before, when westudied gamesinChapter 5. There, however, wewereprimarily concerned
with turn-taking games in fully observable environments, for which minimax search can be
usedtofindoptimalmoves. Inthissectionwestudytheaspectsofgametheorythatanalyze
GAMETHEORY
games with simultaneous moves and other sources of partial observability. (Game theorists
usethetermsperfectinformationandimperfectinformationratherthanfullyandpartially
observable.) Gametheorycanbeusedinatleasttwoways:
1. Agentdesign: Gametheorycananalyzetheagent’sdecisionsandcomputetheexpected
utility for each decision (under the assumption that other agents are acting optimally
according to game theory). Forexample, in the game two-finger Morra, two players,
O and E, simultaneously display one or two fingers. Let the total number of fingers
be f. If f is odd, O collects f dollars from E; and if f is even, E collects f dollars
from O. Gametheory can determine the best strategy against a rational player and the
expectedreturnforeachplayer.4
4 Morraisarecreationalversionofaninspectiongame.Insuchgames,aninspectorchoosesadaytoinspecta
facility(suchasarestaurantorabiologicalweaponsplant),andthefacilityoperatorchoosesadaytohideallthe
nastystuff.Theinspectorwinsifthedaysaredifferent,andthefacilityoperatorwinsiftheyarethesame.
Section17.5. DecisionswithMultipleAgents: GameTheory 667
2. Mechanism design: When an environment is inhabited by many agents, it might be
possible to define the rules of the environment (i.e., the game that the agents must
play)sothatthecollective goodofallagentsismaximizedwheneachagentadoptsthe
game-theoretic solution that maximizes its own utility. For example, game theory can
help design the protocols for a collection of Internet traffic routers so that each router
hasanincentive toactinsuch awaythat global throughput ismaximized. Mechanism
designcanalsobeusedtoconstruct intelligent multiagentsystemsthatsolvecomplex
problemsinadistributed fashion.
17.5.1 Single-movegames
Westartbyconsidering arestricted setofgames: oneswhere allplayerstakeactionsimulta-
neously and the result of the game is based on this single set of actions. (Actually, it is not
crucialthattheactionstakeplaceatexactlythesametime;whatmattersisthatnoplayerhas
knowledge ofthe otherplayers’ choices.) Therestriction to asingle move (and the very use
of the word “game”) might make this seem trivial, but in fact, game theory is serious busi-
ness. It is used in decision-making situations including the auctioning of oil drilling rights
and wireless frequency spectrum rights, bankruptcy proceedings, product development and
pricingdecisions,andnationaldefense—situations involvingbillionsofdollarsandhundreds
ofthousands oflives. Asingle-movegameisdefinedbythreecomponents:
• Players oragents who will be making decisions. Two-player games have received the
PLAYER
mostattention, although n-player games for n > 2are also common. Wegive players
capitalized names,likeAliceandBoborO andE.
• Actionsthattheplayerscanchoose. Wewillgiveactionslowercasenames,likeone or
ACTION
testify. Theplayersmayormaynothavethesamesetofactionsavailable.
• Apayoff function that gives the utility toeach player foreach combination of actions
PAYOFFFUNCTION
by all the players. Forsingle-move games the payoff function can be represented by a
matrix, a representation known as the strategic form (also called normal form). The
STRATEGICFORM
payoffmatrixfortwo-fingerMorraisasfollows:
O:one O:two
E:one E = +2,O = −2 E = −3,O = +3
E:two E = −3,O = +3 E = +4,O = −4
Forexample, the lower-right corner shows that when player O chooses action two and
E alsochoosestwo,thepayoffis+4forE and−4forO.
Each player in a game must adopt and then execute a strategy (which is the name used in
STRATEGY
gametheoryforapolicy). Apurestrategyisadeterministicpolicy;forasingle-movegame,
PURESTRATEGY
a pure strategy is just a single action. Formany games an agent can do better with a mixed
strategy, which is a randomized policy that selects actions according to aprobability distri-
MIXEDSTRATEGY
bution. The mixed strategy that chooses action a with probability p and action b otherwise
is written [p:a;(1 − p):b]. For example, a mixed strategy for two-finger Morra might be
[0.5:one;0.5:two]. A strategy profile is an assignment of a strategy to each player; given
STRATEGYPROFILE
thestrategyprofile,thegame’soutcomeisanumericvalueforeachplayer.
OUTCOME
668 Chapter 17. MakingComplexDecisions
Asolutiontoagameisastrategyprofileinwhicheachplayeradoptsarationalstrategy.
SOLUTION
We will see that the most important issue in game theory is to define what “rational” means
when each agent chooses only part of the strategy profile that determines the outcome. It is
important to realize that outcomes are actual results of playing a game, while solutions are
theoretical constructs used to analyze a game. We will see that some games have a solution
only in mixed strategies. But that does not mean that a player must literally be adopting a
mixedstrategytoberational.
Consider the following story: Two alleged burglars, Alice and Bob, are caught red-
handednearthesceneofaburglary andareinterrogated separately. Aprosecutorofferseach
a deal: if you testify against your partner as the leader of a burglary ring, you’ll go free for
being thecooperative one, while yourpartner willserve 10 years inprison. However, if you
both testify against each other, you’ll both get5years. Alice andBobalsoknow that ifboth
refuse to testify they will serve only 1 year each for the lesser charge of possessing stolen
PRISONER’S property. Now Alice and Bob face the so-called prisoner’s dilemma: should they testify
DILEMMA
or refuse? Being rational agents, Alice and Bob each want to maximize their own expected
utility. Let’sassumethatAliceiscallouslyunconcernedaboutherpartner’sfate,soherutility
decreases in proportion to the number of years she will spend in prison, regardless of what
happenstoBob. Bobfeelsexactlythesameway. Tohelpreacharational decision, theyboth
construct thefollowingpayoffmatrix:
Alice:testify Alice:refuse
Bob:testify A= −5,B = −5 A= −10,B = 0
Bob:refuse A = 0,B = −10 A = −1,B = −1
Alice analyzes the payoff matrix as follows: “Suppose Bob testifies. Then I get 5 years if I
testify and 10 years if I don’t, so in that case testifying is better. On the other hand, if Bob
refuses, thenIget0yearsifItestifyand1yearifIrefuse,sointhatcaseaswelltestifying is
better. Soineithercase,it’sbetterformetotestify, sothat’swhatImustdo.”
DOMINANT Alice has discovered that testify is a dominant strategy for the game. We say that a
STRATEGY
(cid:2)
STRONG strategysforplayerpstronglydominatesstrategys iftheoutcomeforsisbetterforpthan
DOMINATION
(cid:2)
the outcome for s, for every choice of strategies by the other player(s). Strategy s weakly
(cid:2) (cid:2)
dominates s if sis betterthan s on atleast one strategy profile and no worse on anyother.
WEAKDOMINATION
Adominantstrategyisastrategythatdominatesallothers. Itisirrationaltoplayadominated
strategy, and irrational not to play a dominant strategy if one exists. Being rational, Alice
choosesthedominantstrategy. Weneedjustabitmoreterminology: wesaythatanoutcome
isParetooptimal5 ifthere isnootheroutcome thatallplayers would prefer. An outcome is
PARETOOPTIMAL
Paretodominatedbyanotheroutcomeifallplayers wouldprefertheotheroutcome.
PARETODOMINATED
If Alice is clever as well as rational, she will continue to reason as follows: Bob’s
dominant strategy isalsototestify. Therefore, hewilltestifyandwewillbothgetfiveyears.
When each player has a dominant strategy, the combination of those strategies is called a
DOMINANT
STRATEGY dominant strategy equilibrium. In general, a strategy profile forms an equilibrium if no
EQUILIBRIUM
player canbenefit byswitching strategies, given that every otherplayer sticks withthesame
EQUILIBRIUM
5 ParetooptimalityisnamedaftertheeconomistVilfredoPareto(1848–1923).
Section17.5. DecisionswithMultipleAgents: GameTheory 669
strategy. Anequilibrium isessentially a local optimumin thespace of policies; itisthe top
ofapeak thatslopes downward along every dimension, wherea dimension corresponds toa
player’s strategychoices.
The mathematician John Nash (1928–) proved that every game has at least one equi-
librium. The general concept of equilibrium is now called Nash equilibrium in his honor.
Clearly, a dominant strategy equilibrium is a Nash equilibrium (Exercise 17.16), but some
NASHEQUILIBRIUM
gameshaveNashequilibria butnodominantstrategies.
The dilemma in the prisoner’s dilemma is that the equilibrium outcome is worse for
both players than the outcome they would get if they both refused totestify. Inother words,
(testify,testify)isParetodominatedbythe(-1,-1)outcomeof(refuse,refuse). Isthereany
way for Alice and Bob to arrive at the (-1, -1) outcome? It is certainly an allowable option
for both of them to refuse to testify, but is is hard to see how rational agents can get there,
giventhedefinitionofthegame. Eitherplayercontemplating playing refuse willrealizethat
he or she would do better by playing testify. That is the attractive power of an equilibrium
point. Gametheorists agree thatbeingaNashequilibrium is anecessary condition forbeing
asolution—although theydisagreewhetheritisasufficient condition.
It is easy enough to get to the (refuse,refuse) solution if we modify the game. For
example,wecouldchangetoarepeatedgameinwhichtheplayersknowthattheywillmeet
again. Ortheagents might havemoral beliefs that encourage cooperation andfairness. That
means they have a different utility function, necessitating a different payoff matrix, making
it a different game. We will see later that agents with limited computational powers, rather
thantheabilitytoreasonabsolutelyrationally,canreachnon-equilibriumoutcomes,ascanan
agentthatknowsthattheotheragenthaslimitedrationality. Ineachcase,weareconsidering
adifferent gamethantheonedescribed bythepayoffmatrixabove.
Now let’s look at a game that has no dominant strategy. Acme, a video game console
manufacturer, has todecide whether itsnext gamemachine willuse Blu-ray discs orDVDs.
Meanwhile, the video game software producer Best needs to decide whether to produce its
nextgameonBlu-rayorDVD.Theprofitsforbothwillbepositiveiftheyagreeandnegative
iftheydisagree, asshowninthefollowingpayoffmatrix:
Acme:bluray Acme:dvd
Best:bluray A = +9,B = +9 A= −4,B = −1
Best:dvd A = −3,B = −1 A= +5,B = +5
There is no dominant strategy equilibrium for this game, but there are two Nash equilibria:
(bluray, bluray) and (dvd, dvd). We know these are Nash equilibria because if either player
unilaterally movestoadifferent strategy, thatplayer willbeworse off. Nowtheagents have
a problem: there are multiple acceptable solutions, but if each agent aims for a different
solution, then both agents will suffer. How can they agree on a solution? One answer is
that both should choose the Pareto-optimal solution (bluray, bluray); that is, we can restrict
thedefinition of“solution” totheunique Pareto-optimal Nashequilibrium provided that one
exists. Every game has at least one Pareto-optimal solution, but a game might have several,
or they might not be equilibrium points. For example, if (bluray, bluray) had payoff (5,
5), then there would be two equal Pareto-optimal equilibrium points. To choose between
670 Chapter 17. MakingComplexDecisions
them the agents can either guess or communicate, which can be done either by establishing
a convention that orders the solutions before the game begins or by negotiating to reach a
mutually beneficial solution during the game (which would mean including communicative
actions aspartofasequential game). Communication thus arises ingametheory forexactly
thesamereasonsthatitaroseinmultiagentplanninginSection11.4. Gamesinwhichplayers
COORDINATION needtocommunicatelikethisarecalled coordination games.
GAME
A game can have more than one Nash equilibrium; how do we know that every game
must have at least one? Some games have no pure-strategy Nash equilibria. Consider, for
example, any pure-strategy profile for two-finger Morra (page 666). If the total number of
fingersiseven,thenOwillwanttoswitch;ontheotherhand(sotospeak),ifthetotalisodd,
thenE willwanttoswitch. Therefore, nopurestrategy profilecanbeanequilibrium andwe
mustlooktomixedstrategies instead.
Butwhichmixed strategy? In 1928, von Neumann developed amethod forfinding the
optimal mixed strategy for two-player, zero-sum games—games in which the sum of the
ZERO-SUMGAME
payoffsisalwayszero.6 Clearly,Morraissuchagame. Fortwo-player, zero-sum games,we
know that the payoffs are equal and opposite, so we need consider the payoffs of only one
player, whowillbethemaximizer(justasinChapter5). ForMorra,wepicktheevenplayer
Etobethemaximizer,sowecandefinethepayoffmatrixbythevaluesU (e,o)—thepayoff
E
toE ifE doeseandO doeso. (Forconvenience wecallplayer E “her”and O “him.”) Von
Neumann’smethodiscalledthethemaximintechnique, anditworksasfollows:
MAXIMIN
• Suppose we change the rules as follows: first E picks her strategy and reveals it to
O. Then O picks his strategy, with knowledge of E’s strategy. Finally, we evaluate
the expected payoff of the game based on the chosen strategies. This gives us a turn-
taking game to which we can apply the standard minimax algorithm from Chapter 5.
Let’s suppose this gives an outcome U . Clearly, this game favors O, so the true
E,O
utility U of the original game (from E’s point of view) is at least U . Forexample,
E,O
if we just look at pure strategies, the minimax game tree has a root value of −3 (see
Figure17.12(a)), soweknowthatU ≥ −3.
• Nowsuppose wechangetherulestoforce O torevealhisstrategyfirst,followedby E.
ThentheminimaxvalueofthisgameisU ,andbecausethisgamefavorsEweknow
O,E
that U is at mostU . With pure strategies, the value is +2(see Figure 17.12(b)), so
O,E
weknowU ≤ +2.
Combining these twoarguments, wesee that thetrue utility U ofthesolution totheoriginal
gamemustsatisfy
U ≤ U ≤ U orinthiscase, −3 ≤ U ≤ 2.
E,O O,E
TopinpointthevalueofU,weneedtoturnouranalysistomixedstrategies. First,observethe
following: once the first player has revealed his or her strategy, the second player might as
wellchooseapurestrategy. Thereasonissimple: ifthesecondplayerplaysamixedstrategy,
[p:one;(1−p):two],itsexpectedutilityisalinearcombination (p·u +(1−p)·u )of
one two
6 oraconstant—seepage162.
Section17.5. DecisionswithMultipleAgents: GameTheory 671
(a) E -3 (b) O 2
one two one two
O -3 -3 E 2 4
one two one two one two one two
2 -3 -3 4 2 -3 -3 4
(c) E (d) O
[p: one; (1 – p): two] [q: one; (1 – q): two]
O E
one two one two
2p – 3(1 – p) 3p + 4(1 – p) 2q – 3(1 – q) 3q + 4(1 – q)
(e) U (f) U
+4 +4
+3 +3
two two
+2 +2
one one
+1 +1
0 p 0 q
1 1
–1 –1
–2 –2
–3 –3
Figure 17.12 (a) and (b): Minimax game trees for two-finger Morra if the players take
turns playing pure strategies. (c) and (d): Parameterized game trees where the first player
plays a mixed strategy. The payoffs depend on the probability parameter (p or q) in the
mixedstrategy. (e)and(f): Foranyparticularvalueoftheprobabilityparameter,thesecond
player will choose the “better” of the two actions, so the value of the first player’s mixed
strategyisgivenbytheheavylines. Thefirstplayerwillchoosetheprobabilityparameterfor
themixedstrategyattheintersectionpoint.
theutilitiesofthepurestrategies, u andu . Thislinearcombination canneverbebetter
one two
thanthebetterofu andu ,sothesecondplayercanjustchoosethebetterone.
one two
Withthisobservation inmind,theminimaxtreescanbethought ofashaving infinitely
many branches at the root, corresponding to the infinitely many mixed strategies the first
672 Chapter 17. MakingComplexDecisions
player can choose. Each of these leads to a node with two branches corresponding to the
purestrategiesforthesecondplayer. Wecandepicttheseinfinitetreesfinitelybyhavingone
“parameterized” choiceattheroot:
• IfE chooses first, the situation isas shown inFigure 17.12(c). E chooses the strategy
[p:one;(1−p):two]attheroot,andthenOchoosesapurestrategy(andhenceamove)
giventhevalueofp. IfOchoosesone,theexpectedpayoff(toE)is2p−3(1−p)=5p−
3; if O chooses two, the expected payoff is −3p+4(1 −p)=4−7p. We can draw
thesetwopayoffsasstraightlinesonagraph,where prangesfrom0to1onthex-axis,
asshowninFigure17.12(e). O,theminimizer,willalwayschoosethelowerofthetwo
lines,asshownbytheheavylinesinthefigure. Therefore,thebestthatE candoatthe
rootistochoose ptobeattheintersection point, whichiswhere
5p−3 = 4−7p ⇒ p = 7/12.
TheutilityforE atthispointisU = −1/12.
E,O
• If O moves first, the situation is as shown in Figure 17.12(d). O chooses the strategy
[q:one;(1−q):two]attheroot, andthen E chooses amovegiventhevalueofq. The
payoffsare2q−3(1−q)=5q−3and−3q+4(1−q)=4−7q.7 Again,Figure17.12(f)
showsthatthebestO candoattherootistochoosetheintersection point:
5q−3 = 4−7q ⇒ q = 7/12.
TheutilityforE atthispointisU = −1/12.
O,E
Nowweknow that thetrue utility ofthe original gameliesbetween −1/12 and −1/12, that
is, it is exactly −1/12! (The moral is that it is better to be O than E if you are playing this
game.) Furthermore, thetrueutility isattained bythemixedstrategy [7/12:one;5/12:two],
MAXIMIN whichshouldbeplayedbybothplayers. Thisstrategyiscalledthemaximinequilibriumof
EQUILIBRIUM
the game, and is a Nash equilibrium. Note that each component strategy in an equilibrium
mixed strategy has the same expected utility. In this case, both one and two have the same
expectedutility, −1/12,asthemixedstrategyitself.
Our result for two-finger Morra is an example of the general result by von Neumann:
everytwo-playerzero-sumgamehasamaximinequilibriumwhenyouallowmixedstrategies.
Furthermore, every Nash equilibrium in a zero-sum game is a maximin for both players. A
player who adopts the maximin strategy has two guarantees: First, no other strategy can do
betteragainst anopponent whoplayswell(although someotherstrategies mightbebetterat
exploiting an opponent who makes irrational mistakes). Second, the player continues to do
justaswellevenifthestrategyisrevealedtotheopponent.
The general algorithm for finding maximin equilibria in zero-sum games is somewhat
moreinvolvedthanFigures17.12(e)and(f)mightsuggest. Whentherearenpossibleactions,
a mixed strategy is a point in n-dimensional space and the lines become hyperplanes. It’s
also possible for some pure strategies for the second player to be dominated by others, so
that they are not optimal against any strategy for the first player. After removing all such
strategies (which might have to be done repeatedly), the optimal choice at the root is the
7 It is a coincidence that these equations are the same as those for p; the coincidence arises because
UE(one,two)=UE(two,one)= −3.Thisalsoexplainswhytheoptimalstrategyisthesameforbothplayers.
Section17.5. DecisionswithMultipleAgents: GameTheory 673
highest (or lowest) intersection point of the remaining hyperplanes. Finding this choice is
anexampleofalinearprogrammingproblem: maximizinganobjective function subjectto
linear constraints. Such problems can be solved by standard techniques in time polynomial
inthenumberofactions(andinthenumberofbitsusedtospecifytherewardfunction,ifyou
wanttogettechnical).
Thequestionremains,whatshouldarationalagentactually doinplayingasinglegame
of Morra? The rational agent will have derived the fact that [7/12:one;5/12:two] is the
maximin equilibrium strategy, and willassume that thisismutual knowledge witharational
opponent. Theagentcouldusea12-sideddieorarandomnumbergeneratortopickrandomly
accordingtothismixedstrategy, inwhichcasetheexpected payoffwouldbe-1/12forE. Or
the agent could just decide to play one, or two. In either case, the expected payoff remains
-1/12forE. Curiously,unilaterallychoosingaparticularactiondoesnotharmone’sexpected
payoff,butallowingtheotheragenttoknowthatonehasmadesuchaunilateraldecisiondoes
affecttheexpected payoff,because thentheopponent canadjusthisstrategy accordingly.
Findingequilibria innon-zero-sum gamesissomewhatmorecomplicated. Thegeneral
approach hastwosteps: (1)Enumerateallpossible subsets ofactions thatmightformmixed
strategies. Forexample, first try all strategy profiles where each player uses a single action,
then those where each player uses either one or two actions, and so on. This is exponential
inthenumberofactions, andsoonlyappliestorelatively smallgames. (2)Foreachstrategy
profileenumeratedin(1),checktoseeifitisanequilibrium. Thisisdonebysolvingasetof
equationsandinequalitiesthataresimilartotheonesusedinthezero-sumcase. Fortwoplay-
ers these equations are linear and can be solved with basic linear programming techniques,
butforthreeormoreplayers theyarenonlinearandmaybeverydifficulttosolve.
17.5.2 Repeated games
Sofarwehave looked only atgames that last a single move. The simplest kind of multiple-
movegameistherepeatedgame,inwhichplayersfacethesamechoicerepeatedly, buteach
REPEATEDGAME
time with knowledge of the history of all players’ previous choices. A strategy profile for a
repeated gamespecifies anaction choice foreach playerateachtimestep foreverypossible
historyofpreviouschoices. AswithMDPs,payoffsareadditiveovertime.
Let’sconsidertherepeatedversionoftheprisoner’sdilemma. WillAliceandBobwork
together and refuse to testify, knowing they will meet again? The answer depends on the
details of the engagement. For example, suppose Alice and Bob know that they must play
exactly100roundsofprisoner’sdilemma. Thentheybothknowthatthe100throundwillnot
bearepeated game—thatis,itsoutcomecanhavenoeffect onfuture rounds—and therefore
theywillbothchoose thedominant strategy, testify,inthatround. Butoncethe100thround
is determined, the 99th round can have no effect on subsequent rounds, so it too will have
adominant strategy equilibrium at (testify,testify). By induction, both players willchoose
testify oneveryround,earning atotaljailsentence of500yearseach.
We can get different solutions by changing the rules of the interaction. For example,
suppose that after each round there is a 99% chance that the players will meet again. Then
the expected number of rounds is still 100, but neither player knows for sure which round
674 Chapter 17. MakingComplexDecisions
willbethelast. Undertheseconditions, morecooperative behaviorispossible. Forexample,
one equilibrium strategy is foreach player to refuse unless the other player has ever played
PERPETUAL testify. This strategy could be called perpetual punishment. Suppose both players have
PUNISHMENT
adoptedthisstrategy,andthisismutualknowledge. Thenaslongasneitherplayerhasplayed
testify,thenatanypointintimetheexpected futuretotalpayoffforeachplayeris
(cid:12)∞
0.99t·(−1) = −100.
t=0
Aplayerwhodeviatesfromthestrategyandchooses testify willgainascoreof0ratherthan
−1 on the very next move, but from then on both players will play testify and the player’s
totalexpected futurepayoffbecomes
(cid:12)∞
0+ 0.99t·(−5) = −495.
t=1
Therefore, at every step, there is no incentive to deviate from (refuse,refuse). Perpetual
punishment is the “mutually assured destruction” strategy of the prisoner’s dilemma: once
either player decides to testify, it ensures that both players suffer a great deal. But it works
asadeterrentonlyiftheotherplayerbelievesyouhaveadoptedthisstrategy—oratleastthat
youmighthaveadopted it.
Otherstrategies aremoreforgiving. Themostfamous, calledtit-for-tat, callsforstart-
TIT-FOR-TAT
ingwithrefuse andthenechoing theotherplayer’s previous moveonallsubsequent moves.
SoAlicewouldrefuse aslong asBobrefuses andwouldtestify themoveafterBobtestified,
but would go back to refusing if Bob did. Although very simple, this strategy has proven to
behighlyrobustandeffectiveagainstawidevarietyofstrategies.
We can also get different solutions by changing the agents, rather than changing the
rules of engagement. Suppose the agents are finite-state machines with n states and they
are playing a game with m > n total steps. The agents are thus incapable of representing
the number of remaining steps, and must treat it as an unknown. Therefore, they cannot do
the induction, and are free to arrive at the more favorable (refuse, refuse) equilibrium. In
thiscase,ignorance isbliss—orrather, havingyouropponent believethatyouareignorantis
bliss. Yoursuccess in these repeated games depends onthe other player’s perception of you
asabullyorasimpleton, andnotonyouractualcharacteristics.
17.5.3 Sequential games
Inthegeneralcase,agameconsistsofasequenceofturnsthatneednotbeallthesame. Such
gamesarebestrepresentedbyagametree,whichgametheoristscalltheextensiveform. The
EXTENSIVEFORM
tree includes all the same information we saw in Section 5.1: an initial state S , a function
0
PLAYER(s) that tells which player has the move, a function ACTIONS(s) enumerating the
possible actions, a function RESULT(s,a) that defines the transition to a new state, and a
partial function UTILITY(s,p), which is defined only on terminal states, to give the payoff
foreachplayer.
To represent stochastic games, such as backgammon, we add a distinguished player,
chance, that can take random actions. Chance’s “strategy” is part of the definition of the
Section17.5. DecisionswithMultipleAgents: GameTheory 675
game, specified as a probability distribution over actions (the other players get to choose
their own strategy). To represent games with nondeterministic actions, such as billiards, we
breaktheactionintotwopieces: theplayer’s actionitself hasadeterministic result, andthen
chance hasaturntoreacttotheaction initsowncapricious way. Torepresent simultaneous
moves,asintheprisoner’sdilemmaortwo-fingerMorra,weimposeanarbitraryorderonthe
players,butwehavetheoptionofassertingthattheearlierplayer’sactionsarenotobservable
to the subsequent players: e.g., Alice must choose refuse or testify first, then Bob chooses,
but Bob does not know what choice Alice made at that time (we can also represent the fact
that the moveis revealed later). However, we assume the players always remember all their
ownprevious actions;thisassumption iscalled perfectrecall.
The key idea of extensive form that sets it apart from the game trees of Chapter 5 is
the representation of partial observability. We saw in Section 5.6 that a player in a partially
observable game such as Kriegspiel can create a game tree over the space of belief states.
With that tree, wesaw that in somecases aplayer can findasequence of moves (astrategy)
thatleadstoaforcedcheckmateregardlessofwhatactualstatewestartedin,andregardlessof
whatstrategytheopponentuses. However,thetechniquesofChapter5couldnottellaplayer
what to do when there is no guaranteed checkmate. If the player’s best strategy depends
on the opponent’s strategy and vice versa, then minimax (or alpha–beta) by itself cannot
find a solution. The extensive form does allow us to find solutions because it represents the
belief states (game theorists call them information sets) of all players at once. From that
INFORMATIONSETS
representation wecanfindequilibrium solutions, justaswe didwithnormal-form games.
Asasimple example ofasequential game, place twoagents inthe4×3world ofFig-
ure17.1andhavethemmovesimultaneously untiloneagentreachesanexitsquare,andgets
the payoff for that square. If we specify that no movement occurs when the two agents try
to move into the same square simultaneously (a common problem at many traffic intersec-
tions), then certain pure strategies can get stuck forever. Thus, agents need a mixedstrategy
toperform wellinthisgame: randomly choosebetweenmoving aheadandstaying put. This
isexactlywhatisdonetoresolvepacketcollisions inEthernetnetworks.
Next we’ll consider a very simple variant of poker. The deck has only four cards, two
aces and two kings. One card is dealt to each player. The first player then has the option
to raise the stakes of the game from 1 point to 2, or to check. If player 1 checks, the game
is over. If he raises, then player 2 has the option to call, accepting that the game is worth 2
points, or fold, conceding the 1 point. If the game does not end with a fold, then the payoff
depends on the cards: it is zero for both players if they have the same card; otherwise the
playerwiththekingpaysthestakestotheplayerwiththeace.
Theextensive-form treeforthisgameisshowninFigure17.13. Nonterminalstatesare
shownascircles, withtheplayertomoveinsidethecircle;player0ischance. Eachactionis
depictedasanarrowwithalabel,correspondingtoaraise,check,call,orfold,or,forchance,
thefourpossible deals(“AK”meansthatplayer1getsanaceandplayer2aking). Terminal
states are rectangles labeled by their payoff to player 1 and player 2. Information sets are
shown as labeled dashed boxes; for example, I is the information set where it is player
1,1
1’s turn, and he knows he has an ace (but does not know what player 2 has). In information
set I , it is player 2’s turn and she knows that she has an ace and that player 1 has raised,
2,1
676 Chapter 17. MakingComplexDecisions
r c
1 2 0,0!
f
k
I I +1,-1!
1,1 0,0! 2,1
1/6: AA
r c
1 2 +2,-2
f
k
1/3: AK
I +1,-1!
0 +1,-1! 2,2
1/6: KK
r c
1 2 0,0
f
k
1/3: KA
I
1,2
0,0!
I
+1,-1!
2,1
r c
1 22 -2,+2
f
k
-1,+1! +1,-1!
Figure17.13 Extensiveformofasimplifiedversionofpoker.
but does not know what card player 1 has. (Due to the limits of two-dimensional paper, this
information setisshownastwoboxesratherthanone.)
Onewaytosolveanextensivegameistoconvertittoanormal-form game. Recallthat
thenormalformisamatrix,eachrowofwhichislabeledwithapurestrategyforplayer1,and
each column byapure strategy forplayer 2. Inan extensive gameapure strategy forplayer
icorresponds toanaction foreachinformation setinvolving thatplayer. SoinFigure17.13,
one pure strategy forplayer 1 is “raise when in I (that is, when I have an ace), and check
1,1
when in I (when I have a king).” In the payoff matrix below, this strategy is called rk.
1,2
Similarly, strategy cf for player 2 means “call when I have an ace and fold when I have a
king.” Since this is a zero-sum game, the matrix below gives only the payoff for player 1;
player2alwayshastheopposite payoff:
2:cc 2:cf 2:ff 2:fc
1:rr 0 -1/6 1 7/6
1:kr -1/3 -1/6 5/6 2/3
1:rk 1/3 0 1/6 1/2
1:kk 0 0 0 0
This game is so simple that it has two pure-strategy equilibria, shown in bold: cf for player
2 and rk or kk for player 1. But in general we can solve extensive games by converting
to normal form and then finding a solution (usually a mixed strategy) using standard linear
programming methods. That works in theory. But if a player has I information sets and
a actions per set, then that player will have aI pure strategies. In other words, the size of
the normal-form matrix is exponential in the number of information sets, so in practice the
Section17.5. DecisionswithMultipleAgents: GameTheory 677
approach works only forvery small game trees, on the order of a dozen states. A game like
Texashold’em pokerhasabout1018 states,makingthisapproach completelyinfeasible.
What are the alternatives? In Chapter 5 we saw how alpha–beta search could handle
games of perfect information with huge game trees by generating the tree incrementally, by
pruningsomebranches,andbyheuristicallyevaluatingnonterminalnodes. Butthatapproach
does not work well forgames with imperfect information, for two reasons: first, it is harder
toprune,becauseweneedtoconsidermixedstrategiesthatcombinemultiplebranches,nota
purestrategythatalwayschoosesthebestbranch. Second,itishardertoheuristicallyevaluate
anonterminal node,because wearedealing withinformation sets,notindividual states.
Koller et al. (1996) come to the rescue with an alternative representation of extensive
games, called the sequence form, that is only linear in the size of the tree, rather than ex-
SEQUENCEFORM
ponential. Rather than represent strategies, it represents paths through the tree; the number
of paths is equal to the number of terminal nodes. Standard linear programming methods
can again be applied to this representation. The resulting system can solve poker variants
with 25,000 states in a minute ortwo. This is an exponential speedup overthe normal-form
approach, butstillfallsfarshortofhandling fullpoker, with1018 states.
If we can’t handle 1018 states, perhaps we can simplify the problem by changing the
gametoasimplerform. Forexample,ifIholdanaceandamconsidering thepossibility that
thenextcardwillgivemeapairofaces,thenIdon’tcareaboutthesuitofthenextcard;any
suit will do equally well. This suggests forming an abstraction of the game, one in which
ABSTRACTION
suits are ignored. The resulting game tree will be smaller by a factor of 4!=24. Suppose I
can solve this smaller game; how will the solution to that game relate to the original game?
Ifnoplayerisgoingforaflush(orbluffingso),thenthesuits don’t mattertoanyplayer, and
thesolution fortheabstraction willalsobeasolution fortheoriginal game. However, ifany
playeriscontemplatingaflush,thentheabstractionwillbeonlyanapproximatesolution(but
itispossibletocomputeboundsontheerror).
Therearemanyopportunitiesforabstraction. Forexample, atthepointinagamewhere
each player has two cards, if I hold a pair of queens, then the other players’ hands could be
abstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of
queens) or worse (everything else). However, this abstraction might be too coarse. A better
abstraction would divide worse into, say, medium pair (nines through jacks), low pair, and
nopair. Theseexamples areabstractions ofstates; itisalso possible toabstract actions. For
example,instead ofhavingabetactionforeachintegerfrom 1to1000,wecouldrestrict the
bets to 100, 101, 102 and 103. Or we could cut out one of the rounds of betting altogether.
We can also abstract over chance nodes, by considering only a subset of the possible deals.
ThisisequivalenttotherollouttechniqueusedinGoprograms. Puttingalltheseabstractions
together, we can reduce the 1018 states of poker to 107 states, a size that can be solved with
currenttechniques.
Pokerprograms basedonthisapproach caneasily defeatnoviceandsomeexperienced
human players, but are not yet at the level of master players. Part of the problem is that
thesolution theseprograms approximate—the equilibrium solution—is optimalonlyagainst
an opponent who also plays the equilibrium strategy. Against fallible human players it is
important to be able to exploit an opponent’s deviation from the equilibrium strategy. As
678 Chapter 17. MakingComplexDecisions
GautamRao(aka“TheCount”),theworld’sleadingonlinepokerplayer,said(Billings etal.,
2003), “You have a very strong program. Once you add opponent modeling to it, it will kill
everyone.” However,goodmodelsofhumanfallability remainelusive.
Inasense,extensivegameformistheoneofthemostcompleterepresentationswehave
seen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic
environments—most of the hard cases from the list of environment properties on page 42.
However,therearetwolimitationsofgametheory. First,itdoesnotdealwellwithcontinuous
states and actions (although there have been some extensions to the continuous case; for
COURNOT example,thetheoryofCournotcompetitionusesgametheorytosolveproblemswheretwo
COMPETITION
companies choose prices for their products from a continuous space). Second, game theory
assumes the gameis known. Partsof the gamemay be specified asunobservable tosome of
theplayers, butitmustbeknown whatparts areunobservable. Incases inwhichtheplayers
learn the unknown structure of the game over time, the model begins to break down. Let’s
examineeachsourceofuncertainty, andwhethereachcanberepresented ingametheory.
Actions: There is no easy way to represent a game where the players have to discover
what actions are available. Consider the game between computer virus writers and security
experts. Partoftheproblem isanticipating whatactionthe viruswriterswilltrynext.
Strategies: Game theory is very good at representing the idea that the other players’
strategies are initially unknown—as long as we assume all agents are rational. The theory
itself does not say whatto do when the other players are less than fully rational. The notion
BAYES–NASH ofaBayes–Nashequilibriumpartiallyaddressesthispoint: itisanequilibrium withrespect
EQUILIBRIUM
toaplayer’s priorprobability distribution overtheother players’ strategies—in otherwords,
itexpresses aplayer’s beliefsabouttheotherplayers’likelystrategies.
Chance: Ifagamedependsontherollofadie,itiseasyenoughtomodelachancenode
withuniform distribution overtheoutcomes. Butwhatifitis possible that thedie isunfair?
Wecanrepresent thatwithanotherchance node, higherupinthetree,withtwobranches for
“die is fair” and “die is unfair,” such that the corresponding nodes in each branch are in the
sameinformation set(thatis,theplayersdon’tknowifthedieisfairornot). Andwhatifwe
suspect the other opponent does know? Thenweadd another chance node, with one branch
representing thecasewheretheopponent doesknow,andonewherehedoesn’t.
Utilities: What if wedon’t know ouropponent’s utilities? Again, that can be modeled
with a chance node, such that the other agent knows its own utilities in each branch, but we
don’t. But what if we don’t know our own utilities? For example, how do I know if it is
rational toordertheChef’ssaladifIdon’tknowhowmuchIwilllikeit? Wecanmodelthat
withyetanotherchancenodespecifying anunobservable “intrinsic quality” ofthesalad.
Thus,weseethatgametheoryisgoodatrepresentingmostsourcesofuncertainty—but
at the cost of doubling the size of the tree every time we add another node; a habit which
quickly leads to intractably large trees. Because of these and other problems, game theory
hasbeenusedprimarilytoanalyzeenvironmentsthatareatequilibrium,ratherthantocontrol
agentswithinanenvironment. Nextweshallseehowitcanhelpdesignenvironments.
Section17.6. Mechanism Design 679
17.6 MECHANISM DESIGN
In the previous section, we asked, “Given a game, what is a rational strategy?” In this sec-
tion,weask,“Giventhatagentspickrationalstrategies,whatgameshouldwedesign?” More
specifically,wewouldliketodesignagamewhosesolutions, consistingofeachagentpursu-
ingitsownrational strategy, result inthemaximization of someglobal utility function. This
problem is called mechanism design, or sometimes inverse game theory. Mechanism de-
MECHANISMDESIGN
signisastapleofeconomics andpolitical science. Capitalism 101saysthatifeveryonetries
to get rich, the total wealth of society will increase. But the examples we will discuss show
thatpropermechanismdesignisnecessarytokeeptheinvisiblehandontrack. Forcollections
ofagents,mechanismdesignallowsustoconstructsmartsystemsoutofacollectionofmore
limitedsystems—evenuncooperative systems—inmuchthesamewaythatteamsofhumans
canachievegoalsbeyondthereachofanyindividual.
Examples of mechanism design include auctioning off cheap airline tickets, routing
TCPpacketsbetweencomputers, decidinghowmedicalinternswillbeassignedtohospitals,
and deciding how robotic soccer players will cooperate with their teammates. Mechanism
design becamemorethananacademic subject inthe1990swhen severalnations, facedwith
theproblemofauctioning offlicensestobroadcastinvariousfrequencybands,losthundreds
of millions of dollars in potential revenue as a result of poor mechanism design. Formally,
a mechanism consists of (1) a language for describing the set of allowable strategies that
MECHANISM
agentsmayadopt,(2)adistinguishedagent,calledthecenter,thatcollectsreportsofstrategy
CENTER
choices from the agents in the game, and (3) an outcome rule, known to all agents, that the
centerusestodeterminethepayoffstoeachagent, giventheirstrategychoices.
17.6.1 Auctions
Let’sconsider auctionsfirst. Anauction isamechanism forselling somegoods tomembers
AUCTION
of a pool of bidders. For simplicity, we concentrate on auctions with a single item for sale.
Each bidder i has a utility value v for having the item. In some cases, each bidder has a
i
private value for the item. For example, the first item sold on eBay was a broken laser
pointer, whichsoldfor$14.83toacollectorofbrokenlaser pointers. Thus,weknowthatthe
collector has v ≥ $14.83, but most other people would have v * $14.83. In other cases,
i j
such as auctioning drilling rights for an oil tract, the item has a common value—the tract
willproduce someamount ofmoney, X, and all bidders value adollarequally—but there is
uncertainty astowhattheactual valueof X is. Different bidders havedifferent information,
andhencedifferentestimatesoftheitem’struevalue. Ineithercase,biddersendupwiththeir
own v . Given v , each bidder gets a chance, at the appropriate time ortimes in the auction,
i i
to make a bid b . The highest bid, b wins the item, but the price paid need not be b ;
i max max
that’spartofthemechanism design.
The best-known auction mechanism is the ascending-bid,8 or English auction, in
ASCENDING-BID
which the center starts by asking for a minimum (or reserve) bid b . If some bidder is
ENGLISHAUCTION min
8 Theword“auction”comesfromtheLatinaugere,toincrease.
680 Chapter 17. MakingComplexDecisions
willing to pay that amount, the center then asks for b + d, for some increment d, and
min
continues up from there. The auction ends when nobody is willing to bid anymore; then the
lastbidderwinstheitem,paying thepricehebid.
How do we know if this is a good mechanism? One goal is to maximize expected
revenue for the seller. Another goal is to maximize a notion of global utility. These goals
overlap tosome extent, because one aspect ofmaximizing global utility isto ensure that the
winner of the auction is the agent who values the item the most (and thus is willing to pay
themost). Wesayanauction isefficientifthe goods gototheagent whovalues them most.
EFFICIENT
Theascending-bidauctionisusuallybothefficientandrevenuemaximizing,butifthereserve
price is set too high, the bidder who values it most may not bid, and if the reserve is set too
low,thesellerlosesnetrevenue.
Probably the most important things that an auction mechanism can do is encourage a
sufficient numberofbidders toenterthe gameand discourage them from engaging in collu-
sion. Collusionisanunfairorillegalagreementbytwoormorebidderstomanipulateprices.
COLLUSION
Itcanhappeninsecretbackroom dealsortacitly, withinthe rulesofthemechanism.
For example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a
simultaneous auction (bidsweretakenonalltenblocksatthesametime),usingtherulethat
anybidmustbeaminimumofa10%raiseoverthepreviousbidonablock. Therewereonly
two credible bidders, and the first, Mannesman, entered the bid of 20 million deutschmark
onblocks1-5and18.18milliononblocks6-10. Why18.18M?OneofT-Mobile’smanagers
said they “interpreted Mannesman’s first bid as an offer.” Both parties could compute that
a 10% raise on 18.18M is 19.99M; thus Mannesman’s bid was interpreted as saying “we
can each get half the blocks for 20M; let’s not spoil it by bidding the prices up higher.”
And in fact T-Mobile bid 20M on blocks 6-10 and that was the end of the bidding. The
German government got less than they expected, because the two competitors were able to
use the bidding mechanism to come to a tacit agreement on how not to compete. From
the government’s point of view, a better result could have been obtained by any of these
changes to the mechanism: a higher reserve price; a sealed-bid first-price auction, so that
the competitors could not communicate through their bids; or incentives to bring in a third
bidder. Perhaps the 10% rule was an error in mechanism design, because it facilitated the
precisesignaling fromMannesmantoT-Mobile.
In general, both the seller and the global utility function benefit if there are more bid-
ders, although global utility can suffer if you count the cost of wasted time of bidders that
have no chance of winning. One way to encourage more bidders is to make the mechanism
easier for them. After all, if it requires too much research orcomputation on the part of the
bidders, they may decide to take their money elsewhere. So it is desirable that the bidders
haveadominantstrategy. Recallthat “dominant” meansthatthestrategy worksagainst all
other strategies, which in turn means that an agent can adopt it without regard for the other
strategies. Anagentwithadominantstrategy canjustbid,without wastingtimecontemplat-
ing other agents’ possible strategies. A mechanism where agents have a dominant strategy
is called a strategy-proof mechanism. If, as is usually the case, that strategy involves the
STRATEGY-PROOF
biddersrevealingtheirtruevalue, v ,thenitiscalledatruth-revealing, ortruthful,auction;
TRUTH-REVEALING i
REVELATION theterm incentivecompatibleisalsoused. Therevelation principlestatesthatanymecha-
PRINCIPLE
Section17.6. Mechanism Design 681
nismcanbetransformedintoanequivalenttruth-revealing mechanism,sopartofmechanism
designisfindingtheseequivalent mechanisms.
It turns out that the ascending-bid auction has most of the desirable properties. The
bidder with the highest value v gets the goods at a price of b +d, where b is the highest
i o o
bid among all the other agents and d is the auctioneer’s increment.9 Bidders have a simple
dominantstrategy: keepbiddingaslongasthecurrentcostisbelowyourv . Themechanism
i
isnotquitetruth-revealing, because thewinningbidderrevealsonlythathisv ≥ b +d;we
i o
havealowerboundonv butnotanexactamount.
i
A disadvantage (from the point of view of the seller) of the ascending-bid auction is
that it can discourage competition. Suppose that in a bid for cell-phone spectrum there is
one advantaged company that everyone agrees would be able to leverage existing customers
and infrastructure, and thus can make alarger profit than anyone else. Potential competitors
can see that they have no chance in an ascending-bid auction, because the advantaged com-
pany can always bid higher. Thus, the competitors may not enter at all, and the advantaged
companyendsupwinningatthereserveprice.
AnothernegativepropertyoftheEnglishauctionisitshighcommunicationcosts. Either
theauction takesplaceinoneroomorallbiddershavetohave high-speed, securecommuni-
cationlines;ineithercasetheyhavetohavethetimeavailabletogothroughseveralroundsof
bidding. Analternative mechanism,whichrequires muchlesscommunication, isthesealed-
SEALED-BID bidauction. Eachbiddermakes asingle bidandcommunicates ittotheauctioneer, without
AUCTION
theotherbidders seeing it. Withthismechanism, thereisno longer asimple dominant strat-
egy. If your value is v and you believe that the maximum of all the other agents’ bids will
i
be b , then you should bid b + (cid:2), for some small (cid:2), if that is less than v . Thus, your bid
o o i
depends on your estimation of the other agents’ bids, requiring you to do more work. Also,
note that the agent with the highest v might not win the auction. This is offset by the fact
i
thattheauctionismorecompetitive, reducing thebiastowardanadvantaged bidder.
A small change in the mechanism for sealed-bid auctions produces the sealed-bid
SEALED-BID second-priceauction,alsoknownasaVickreyauction.10 Insuchauctions,thewinnerpays
SECOND-PRICE
AUCTION
the price of the second-highest bid, b , rather than paying his own bid. This simple modifi-
VICKREYAUCTION o
cation completely eliminates thecomplexdeliberations required forstandard (or first-price)
sealed-bidauctions, becausethedominantstrategyisnowsimplytobidv ;themechanismis
i
truth-revealing. Notethattheutilityofagentiintermsofhisbidb ,hisvaluev ,andthebest
i i
bidamongtheotheragents, b ,is
o
(cid:24)
(v −b ) ifb > b
u = i o i o
i 0 otherwise.
To see that b = v is a dominant strategy, note that when (v − b ) is positive, any bid
i i i o
that wins the auction is optimal, and bidding v in particular wins the auction. On the other
i
hand, when (v −b )isnegative, anybid that loses the auction isoptimal, and bidding v in
i o i
9 Thereisactuallyasmallchancethattheagentwithhighestvifailstogetthegoods,inthecaseinwhich
bo <vi <bo+d.Thechanceofthiscanbemadearbitrarilysmallbydecreasingtheincrementd.
10 NamedafterWilliamVickrey(1914–1996), whowonthe1996NobelPrizeineconomicsforthisworkand
diedofaheartattackthreedayslater.
682 Chapter 17. MakingComplexDecisions
particularlosestheauction. Sobidding v isoptimalforallpossiblevaluesofb ,andinfact,
i o
v istheonlybidthathasthisproperty. Becauseofitssimplicityandtheminimalcomputation
i
requirements for both seller and bidders, the Vickrey auction is widely used in constructing
distributed AI systems. Also, Internet search engines conduct over a billion auctions a day
to sell advertisements along with their search results, and online auction sites handle $100
billionayearingoods,allusingvariantsoftheVickreyauction. Notethattheexpectedvalue
to the seller is b , which is the same expected return as the limit of the English auction as
o
theincrement dgoestozero. Thisisactually averygeneral result: the revenueequivalence
REVENUE
theorem states that, with a few minor caveats, any auction mechanism where risk-neutral
EQUIVALENCE
THEOREM
bidders have values v known only to themselves (but know a probability distribution from
i
whichthosevaluesaresampled),willyieldthesameexpectedrevenue. Thisprinciplemeans
thatthevariousmechanismsarenotcompetingonthebasisofrevenuegeneration, butrather
onotherqualities.
Althoughthesecond-priceauctionistruth-revealing, itturnsoutthatextendingtheidea
tomultiplegoods andusinganext-price auction isnottruth-revealing. ManyInternet search
engines use a mechanism where they auction k slots for ads on a page. The highest bidder
wins the top spot, the second highest gets the second spot, and so on. Each winner pays the
price bid by the next-lower bidder, with the understanding that payment is made only if the
searcher actually clicks on the ad. The top slots are considered more valuable because they
are more likely to be noticed and clicked on. Imagine that three bidders, b ,b and b , have
1 2 3
valuations foraclick of v =200,v =180, and v =100, and thatk = 2slots are available,
1 2 3
where it is known that the top spot is clicked on 5% of the time and the bottom spot 2%. If
all bidders bid truthfully, then b wins the top slot and pays 180, and has an expected return
1
of(200−180)×0.05=1. Thesecondslotgoestob . Butb canseethatifsheweretobid
2 1
anythingintherange101–179,shewouldconcedethetopslottob ,winthesecondslot,and
2
yieldanexpectedreturnof(200−100)×.02=2. Thus,b candoubleherexpectedreturnby
1
biddinglessthanhertruevalueinthiscase. Ingeneral,biddersinthismultislotauctionmust
spendalotofenergyanalyzing thebidsofotherstodeterminetheirbeststrategy; thereisno
simpledominantstrategy. Aggarwal etal.(2006)showthatthereisauniquetruthfulauction
mechanism for this multislot problem, in which the winner of slot j pays the full price for
slot j just for those additional clicks that are available at slot j and not at slot j + 1. The
winner pays the price for the lower slot for the remaining clicks. In our example, b would
1
bid200truthfully, andwouldpay180fortheadditional .05−.02=.03clicksinthetopslot,
but would pay only the cost of the bottom slot, 100, for the remaining .02 clicks. Thus, the
totalreturnto b wouldbe(200−180)×.03+(200−100)×.02=2.6.
1
Anotherexample ofwhere auctions can come into play within AI is whenacollection
of agents are deciding whether to cooperate on a joint plan. Hunsberger and Grosz (2000)
show that this can be accomplished efficiently with an auction in which the agents bid for
rolesinthejointplan.
Section17.6. Mechanism Design 683
17.6.2 Commongoods
Now let’s consider another type of game, in which countries set their policy for controlling
airpollution. Eachcountry hasachoice: theycanreduce pollution atacostof-10points for
implementing thenecessary changes, ortheycancontinue topollute, whichgivesthemanet
utility of-5(inadded health costs, etc.) andalsocontributes -1points toeveryothercountry
(because the air is shared across countries). Clearly, the dominant strategy for each country
is“continue topollute,” butifthereare100countries andeachfollowsthispolicy, theneach
country gets a total utility of -104, whereas if every country reduced pollution, they would
TRAGEDYOFTHE each have a utility of -10. This situation is called the tragedy of the commons: if nobody
COMMONS
has to pay for using a common resource, then it tends to be exploited in a way that leads to
a lower total utility for all agents. It is similar to the prisoner’s dilemma: there is another
solution to the game that is better for all parties, but there appears to be no way for rational
agentstoarriveatthatsolution.
The standard approach for dealing with the tragedy of the commons is to change the
mechanism toone that charges each agent forusing the commons. More generally, weneed
to ensure that all externalities—effects on global utility that are not recognized in the in-
EXTERNALITIES
dividual agents’ transactions—are made explicit. Setting the prices correctly is the difficult
part. In the limit, this approach amounts to creating a mechanism in which each agent is
effectively requiredtomaximizeglobalutility, butcando sobymakingalocaldecision. For
this example, a carbon tax would be an example of a mechanism that charges for use of the
commonsinawaythat,ifimplemented well,maximizesglobalutility.
Asafinalexample,considertheproblemofallocatingsomecommongoods. Supposea
citydecidesitwantstoinstallsomefreewirelessInternet transceivers. However,thenumber
oftransceivers theycanaffordislessthanthenumberofneighborhoods thatwantthem. The
city wants to allocate the goods efficiently, to the neighborhoods that would value them the
(cid:2)
most. That is, they want to maximize the global utility V = v . The problem is that if
i i
theyjustaskeachneighborhood council“howmuchdoyouvaluethisfreegift?” theywould
allhaveanincentive tolie,andreportahighvalue. Itturns outthereisamechanism, known
VICKREY-CLARKE- as the Vickrey-Clarke-Groves, or VCG, mechanism, that makes it a dominant strategy for
GROVES
eachagent toreport itstrueutility andthatachieves anefficient allocation ofthegoods. The
VCG
trick is that each agent pays a tax equivalent to the loss in global utility that occurs because
oftheagent’s presenceinthegame. Themechanism workslike this:
1. Thecenteraskseachagenttoreportitsvalueforreceivinganitem. Callthisb .
i
2. Thecenterallocatesthegoodstoasubsetofthebidders. WecallthissubsetA,anduse
thenotation b (A) to meanthe result to iunder this allocation: b if iisin A(that is, i
i i
is a winner), and 0 otherwise. The center chooses A to maximize total reported utility
(cid:2)
B = b (A).
i i
3. The center calculates (for each i) the sum of the reported utilities for all the winners
(cid:2)
except i. Weusethe notation B−i =
j(cid:14)=i
b
j
(A). Thecenteralso computes (foreach
i)theallocation thatwould maximizetotal global utility ifiwerenotinthegame; call
thatsumW−i .
4. EachagentipaysataxequaltoW−i −B−i .
684 Chapter 17. MakingComplexDecisions
In this example, the VCG rule means that each winner would pay a tax equal to the highest
reported value among the losers. That is, if I report my value as5, and that causes someone
withvalue2tomissoutonanallocation, thenIpayataxof2. Allwinners should behappy
because theypayataxthatislessthantheirvalue, andalllosersareashappy astheycanbe,
becausetheyvaluethegoodslessthantherequiredtax.
Why is it that this mechanism is truth-revealing? First, consider the payoff to agent i,
whichisthevalueofgettinganitem,minusthetax:
v
i
(A)−(W−i −B−i ). (17.14)
Herewedistinguish the agent’s true utility, v , from his reported utility b (but weare trying
i i
to show that a dominant strategy is b =v ). Agent i knows that the center will maximize
i i
globalutilityusingthereportedvalues,
(cid:12) (cid:12)
b (A) = b (A)+ b (A)
j i j
j j(cid:14)=i
whereasagentiwantsthecentertomaximize(17.14), whichcanberewritten as
(cid:12)
v
i
(A)+ b
j
(A)−W−i .
j(cid:14)=i
Since agent i cannot affect the value of W−i (it depends only on the other agents), the only
wayicanmakethecenteroptimizewhatiwantsistoreportthetrueutility, b =v .
i i
17.7 SUMMARY
Thischapter showshowtouse knowledge about theworld tomakedecisions even whenthe
outcomesofanactionareuncertainandtherewardsforactingmightnotbereapeduntilmany
actionshavepassed. Themainpointsareasfollows:
• Sequential decision problems inuncertain environments, also called Markov decision
processes, or MDPs, are defined by a transition model specifying the probabilistic
outcomesofactionsandarewardfunctionspecifying therewardineachstate.
• Theutilityofastatesequence isthesumofalltherewardsoverthesequence, possibly
discounted over time. The solution of an MDP is a policy that associates a decision
witheverystate thattheagent mightreach. Anoptimalpolicy maximizes theutility of
thestatesequences encountered whenitisexecuted.
• The utility of a state is the expected utility of the state sequences encountered when
an optimal policy is executed, starting in that state. The value iteration algorithm for
solvingMDPsworksbyiterativelysolvingtheequationsrelatingtheutilityofeachstate
tothoseofitsneighbors.
• Policy iteration alternates between calculating the utilities of states under the current
policyandimprovingthecurrentpolicywithrespect tothecurrentutilities.
• Partially observable MDPs, or POMDPs, are much more difficult to solve than are
MDPs. Theycanbesolved byconversion toanMDPinthecontinuous spaceofbelief
Bibliographical andHistorical Notes 685
states; both value iteration and policy iteration algorithms have been devised. Optimal
behavior in POMDPs includes information gathering to reduce uncertainty and there-
foremakebetterdecisions inthefuture.
• A decision-theoretic agent can be constructed for POMDP environments. The agent
uses a dynamic decision network to represent the transition and sensor models, to
updateitsbeliefstate,andtoprojectforwardpossible actionsequences.
• Game theory describes rational behavior for agents in situations in which multiple
agentsinteract simultaneously. Solutions ofgamesare Nashequilibria—strategy pro-
filesinwhichnoagenthasanincentivetodeviatefromthespecifiedstrategy.
• Mechanism design can be used to set the rules by which agents will interact, in order
to maximize some global utility through the operation of individually rational agents.
Sometimes, mechanisms exist that achieve this goal without requiring each agent to
considerthechoicesmadebyotheragents.
We shall return to the world of MDPs and POMDP in Chapter 21, when we study rein-
forcement learningmethods that allow anagent toimproveitsbehavior from experience in
sequential, uncertain environments.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
RichardBellmandevelopedtheideasunderlying themodernapproachtosequential decision
problems while working atthe RANDCorporation beginning in 1949. According to his au-
tobiography (Bellman, 1984), he coined the exciting term “dynamic programming” to hide
from a research-phobic Secretary of Defense, Charles Wilson, the fact that his group was
doingmathematics. (Thiscannotbestrictlytrue,becausehisfirstpaperusingtheterm(Bell-
man,1952)appeared beforeWilsonbecameSecretary ofDefensein1953.) Bellman’sbook,
DynamicProgramming(1957),gavethenewfieldasolidfoundationandintroducedthebasic
algorithmic approaches. Ron Howard’s Ph.D. thesis (1960) introduced policy iteration and
the idea of average reward for solving infinite-horizon problems. Several additional results
were introduced by Bellman and Dreyfus (1962). Modified policy iteration is due to van
Nunen (1976) and Puterman and Shin (1978). Asynchronous policy iteration was analyzed
byWilliamsandBaird(1993),whoalsoprovedthepolicylossboundinEquation(17.9). The
analysis of discounting in terms of stationary preferences is due to Koopmans (1972). The
texts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide a
rigorous introduction to sequential decision problems. Papadimitriou and Tsitsiklis (1987)
describe resultsonthecomputational complexity ofMDPs.
SeminalworkbySutton(1988)andWatkins(1989)onreinforcementlearningmethods
for solving MDPs played a significant role in introducing MDPs into the AI community, as
did the later survey by Barto et al. (1995). (Earlier work by Werbos (1977) contained many
similar ideas, but was not taken up to the same extent.) The connection between MDPsand
AIplanning problemswasmadefirstbySvenKoenig(1991), whoshowedhowprobabilistic
STRIPS operators provide acompactrepresentation fortransition models(seealsoWellman,
686 Chapter 17. MakingComplexDecisions
1990b). Work by Dean et al. (1993) and Tash and Russell (1994) attempted to overcome
the combinatorics oflarge state spaces by using alimited search horizon and abstract states.
Heuristics based on the value of information can be used to select areas of the state space
wherealocalexpansionofthehorizonwillyieldasignificantimprovementindecisionqual-
ity. Agents using this approach can tailor their effort to handle time pressure and generate
someinteresting behaviors suchasusingfamiliar“beatenpaths”tofindtheirwayaroundthe
statespacequicklywithouthavingtorecomputeoptimaldecisions ateachpoint.
As one might expect, AI researchers have pushed MDPs in the direction of more ex-
pressive representations that can accommodate much larger problems than the traditional
atomicrepresentations basedontransition matrices. TheuseofadynamicBayesiannetwork
to represent transition models was an obvious idea, but work on factored MDPs (Boutilier
FACTOREDMDP
et al., 2000; Koller and Parr, 2000; Guestrin et al., 2003b) extends the idea to structured
representations ofthevaluefunction withprovable improvements incomplexity. Relational
MDPs (Boutilier et al., 2001; Guestrin et al., 2003a) go one step further, using structured
RELATIONALMDP
representations tohandledomainswithmanyrelatedobjects.
TheobservationthatapartiallyobservableMDPcanbetransformedintoaregularMDP
overbelief states isduetoAstrom (1965) andAoki(1965). Thefirstcomplete algorithm for
the exact solution of POMDPs—essentially the value iteration algorithm presented in this
chapter—was proposed by EdwardSondik (1971) in hisPh.D.thesis. (Alater journal paper
by Smallwood and Sondik (1973) contains some errors, but is more accessible.) Lovejoy
(1991) surveyed the first twenty-five years of POMDP research, reaching somewhat pes-
simistic conclusions about the feasibility of solving large problems. The first significant
contribution within AI was the Witness algorithm (Cassandra et al., 1994; Kaelbling et al.,
1998), an improved version of POMDPvalue iteration. Otheralgorithms soon followed, in-
cluding anapproach duetoHansen(1998)thatconstructs apolicyincrementally intheform
ofafinite-state automaton. Inthispolicy representation, thebelief statecorresponds directly
to a particular state in the automaton. More recent work in AI has focused on point-based
value iteration methods that, at each iteration, generate conditional plans and α-vectors for
a finite set of belief states rather than for the entire belief space. Lovejoy (1991) proposed
such an algorithm for a fixed grid of points, an approach taken also by Bonet (2002). An
influential paper by Pineau et al. (2003) suggested generating reachable points by simulat-
ing trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that one
need generate plans for only a small, randomly selected subset of points to improve on the
plans from the previous iteration for all points in the set. Current point-based methods—
suchaspoint-based policyiteration(Jietal.,2007)—cangeneratenear-optimal solutionsfor
POMDPswiththousands ofstates. BecausePOMDPsarePSPACE-hard(Papadimitriou and
Tsitsiklis, 1987), furtherprogress mayrequire takingadvantage ofvariouskinds ofstructure
withinafactored representation.
Theonlineapproach—using look-ahead searchtoselectanactionforthecurrentbelief
state—was first examined by Satia and Lave (1973). The use of sampling at chance nodes
was explored analytically by Kearns et al. (2000) and Ng and Jordan (2000). The basic
ideas for an agent architecture using dynamic decision networks were proposed by Dean
and Kanazawa (1989a). ThebookPlanning and Control by Deanand Wellman (1991) goes
Bibliographical andHistorical Notes 687
into much greater depth, making connections between DBN/DDN models and the classical
control literature on filtering. Tatman and Shachter (1990) showed how to apply dynamic
programming algorithms to DDN models. Russell (1998) explains various ways in which
suchagentscanbescaledupandidentifiesanumberofopenresearchissues.
The roots of game theory can be traced back to proposals made in the 17th century
by Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human
interactions scientifically and mathematically. Throughout the 19th century, several leading
economists created simple mathematical examples to analyze particular examples of com-
petitive situations. The first formal results in game theory are due to Zermelo (1913) (who
had,theyearbefore,suggestedaformofminimaxsearchforgames,albeitanincorrectone).
Emile Borel (1921) introduced the notion of a mixed strategy. John von Neumann (1928)
provedthateverytwo-person, zero-sumgamehasamaximinequilibrium inmixedstrategies
and a well-defined value. Von Neumann’s collaboration with the economist Oskar Morgen-
stern led to the publication in 1944 of the Theory of Games and Economic Behavior, the
defining book for game theory. Publication of the book was delayed by the wartime paper
shortage untilamemberoftheRockefellerfamilypersonally subsidized itspublication.
In1950,attheageof21,JohnNashpublishedhisideasconcerningequilibriaingeneral
(non-zero-sum) games. Hisdefinition ofanequilibrium solution, although originating inthe
work of Cournot (1838), became known as Nash equilibrium. After a long delay because
oftheschizophrenia hesuffered from 1959 onward, Nashwasawarded theNobelMemorial
PrizeinEconomics(alongwithReinhartSeltenandJohnHarsanyi)in1994. TheBayes–Nash
equilibrium is described by Harsanyi (1967) and discussed by Kadane and Larkey (1982).
SomeissuesintheuseofgametheoryforagentcontrolarecoveredbyBinmore(1982).
The prisoner’s dilemma was invented as a classroom exercise by Albert W. Tucker in
1950(basedonanexamplebyMerrillFloodandMelvinDresher)andiscoveredextensively
by Axelrod (1985) and Poundstone (1993). Repeated games were introduced by Luce and
Raiffa(1957), and gamesofpartial information inextensive form byKuhn(1953). Thefirst
practical algorithm for sequential, partial-information games was developed within AI by
Kolleretal. (1996); the paper by Kollerand Pfeffer(1997) provides a readable introduction
tothefieldanddescribeaworkingsystemforrepresenting andsolving sequential games.
The use of abstraction to reduce a game tree to a size that can be solved with Koller’s
technique is discussed by Billings et al. (2003). Bowling et al. (2008) show how to use
importance sampling to get a better estimate of the value of a strategy. Waugh et al. (2009)
showthattheabstractionapproachisvulnerabletomakingsystematicerrorsinapproximating
the equilibrium solution, meaning that the whole approach is on shaky ground: it works for
some games but not others. Korb et al. (1999) experiment with an opponent model in the
form of a Bayesian network. It plays five-card stud about as well as experienced humans.
(Zinkevich et al., 2008) show how an approach that minimizes regret can find approximate
equilibria forabstractions with 1012 states,100timesmorethanpreviousmethods.
Game theory and MDPs are combined in the theory of Markov games, also called
stochasticgames(Littman,1994;HuandWellman,1998). Shapley(1953)actuallydescribed
the value iteration algorithm independently of Bellman, but his results were not widely ap-
preciated, perhaps because they were presented in the context of Markov games. Evolu-
688 Chapter 17. MakingComplexDecisions
tionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your
opponent’s strategy is changing, how should you react? Textbooks on game theory from
aneconomics point ofview include those byMyerson (1991), Fudenberg and Tirole(1991),
Osborne(2004),andOsborneandRubinstein(1994);MailathandSamuelson(2006)concen-
trateonrepeated games. FromanAIperspective wehaveNisan etal.(2007),Leyton-Brown
andShoham(2008), andShohamandLeyton-Brown(2009).
The2007NobelMemorialPrizeinEconomicswenttoHurwicz,Maskin,andMyerson
“forhaving laidthefoundations ofmechanism designtheory” (Hurwicz,1973). Thetragedy
ofthecommons,amotivatingproblemforthefield,waspresentedbyHardin(1968). Therev-
elation principle isdue to Myerson (1986), and the revenue equivalence theorem wasdevel-
oped independently by Myerson (1981) and Riley and Samuelson (1981). Two economists,
Milgrom(1997)andKlemperer(2002),writeaboutthemultibillion-dollar spectrumauctions
theywereinvolved in.
Mechanism designisusedinmultiagent planning (HunsbergerandGrosz,2000;Stone
etal.,2009)andscheduling(Rassentietal.,1982). Varian(1995)givesabriefoverviewwith
connectionstothecomputerscienceliterature, andRosenscheinandZlotkin(1994)presenta
book-lengthtreatmentwithapplicationstodistributedAI.RelatedworkondistributedAIalso
goesunderothernames,includingcollectiveintelligence(TumerandWolpert,2000;Segaran,
2007) and market-based control (Clearwater, 1996). Since 2001 there has been an annual
Trading Agents Competition (TAC), in which agents try to make the best profit on a series
ofauctions (Wellmanetal.,2001; Arunachalam andSadeh, 2005). Papersoncomputational
issuesinauctions oftenappearintheACMConferences onElectronicCommerce.
EXERCISES
17.1 For the 4×3 world shown in Figure 17.1, calculate which squares can be reached
from(1,1)bytheactionsequence[Up,Up,Right,Right,Right]andwithwhatprobabilities.
Explainhowthiscomputationisrelatedtothepredictiontask(seeSection15.2.1)forahidden
Markovmodel.
17.2 SelectaspecificmemberofthesetofpoliciesthatareoptimalforR(s)> 0asshown
inFigure17.2(b),andcalculatethefractionoftimetheagentspendsineachstate,inthelimit,
if the policy is executed forever. (Hint: Construct the state-to-state transition probability
matrixcorresponding tothepolicyandseeExercise15.2.)
17.3 Suppose that we define the utility of a state sequence to be the maximum reward ob-
tainedinanystateinthesequence. Showthatthisutilityfunctiondoesnotresultinstationary
preferences between state sequences. Is it still possible to define a utility function on states
suchthatMEUdecision makinggivesoptimalbehavior?
17.4 Sometimes MDPsare formulated with a reward function R(s,a) that depends on the
(cid:2)
actiontakenorwitharewardfunction R(s,a,s)thatalsodepends ontheoutcomestate.
a. WritetheBellmanequations fortheseformulations.
Exercises 689
(cid:2)
b. ShowhowanMDPwithrewardfunction R(s,a,s)canbetransformedintoadifferent
MDP with reward function R(s,a), such that optimal policies in the new MDP corre-
spondexactlytooptimalpolicies intheoriginal MDP.
c. Nowdothesametoconvert MDPswithR(s,a)intoMDPswithR(s).
17.5 Fortheenvironment showninFigure17.1, findallthethreshold values forR(s)such
thattheoptimalpolicychanges whenthethreshold iscrossed. Youwillneedawaytocalcu-
late the optimal policy and its value for fixed R(s). (Hint: Prove that the value of any fixed
policyvarieslinearly withR(s).)
17.6 Equation(17.7)onpage654statesthattheBellmanoperator isacontraction.
a. Showthat,foranyfunctions f andg,
|maxf(a)−maxg(a)| ≤ max|f(a)−g(a)|.
a a a
b. Write out an expression for |(BU − BU (cid:2) )(s)| and then apply the result from (a) to
i i
completetheproofthattheBellmanoperatorisacontraction.
17.7 This exercise considers two-player MDPs that correspond to zero-sum, turn-taking
games like those in Chapter 5. Let the players be A and B, and let R(s) be the reward for
playerAinstates. (Therewardfor B isalwaysequalandopposite.)
a. LetU (s)betheutilityofstateswhenitisA’sturntomoveins,andletU (s)bethe
A B
utilityofstateswhenitisB’sturntomoveins. Allrewardsandutilitiesarecalculated
fromA’spointofview(justasinaminimaxgametree). WritedownBellmanequations
definingU (s)andU (s).
A B
b. Explainhowtodotwo-playervalueiterationwiththeseequations,anddefineasuitable
terminationcriterion.
c. Consider the game described in Figure 5.17 on page 197. Draw the state space (rather
thanthe gametree), showing themovesby Aassolid linesand movesbyB asdashed
lines. MarkeachstatewithR(s). Youwillfindithelpfultoarrange thestates (s ,s )
A B
onatwo-dimensional grid,using s ands as“coordinates.”
A B
d. Nowapplytwo-playervalueiterationtosolvethisgame,andderivetheoptimalpolicy.
17.8 Considerthe 3×3worldshowninFigure17.14(a). Thetransition modelisthesame
asinthe4×3Figure17.1: 80%ofthetimetheagentgoesinthedirection itselects; therest
ofthetimeitmovesatrightanglestotheintended direction.
Implement value iteration for this world for each value of r below. Use discounted
rewards with a discount factor of 0.99. Show the policy obtained in each case. Explain
intuitively whythevalueofr leadstoeachpolicy.
a. r = 100
b. r = −3
c. r = 0
d. r = +3
690 Chapter 17. MakingComplexDecisions
r -1 +10 +50 -1 -1 -1 -1 -1 -1 -1
···
-1 -1 -1 Start
···
-1 -1 -1 -50 +1 +1 +1 +1 +1 +1 +1
···
(a) (b)
Figure 17.14 (a) 3×3 world for Exercise 17.8. The reward for each state is indicated.
Theupperrightsquareisaterminalstate. (b) 101×3worldforExercise17.9(omitting93
identicalcolumnsinthemiddle).Thestartstatehasreward0.
17.9 Consider the 101×3world shown inFigure 17.14(b). Inthe start state the agent has
a choice of two deterministic actions, Up or Down, but in the other states the agent has one
deterministic action, Right. Assuming a discounted reward function, for what values of the
discount γ should the agent choose Up and for which Down? Compute the utility of each
action as a function of γ. (Note that this simple example actually reflects many real-world
situations in which one must weigh the value of an immediate action versus the potential
continual long-term consequences, suchaschoosing todumppollutants intoalake.)
17.10 Consider an undiscounted MDPhaving three states, (1, 2, 3), with rewards −1, −2,
0, respectively. State 3isaterminal state. In states 1and 2there are twopossible actions: a
andb. Thetransition modelisasfollows:
• Instate 1,actionamovestheagent tostate 2withprobability 0.8and makestheagent
stayputwithprobability 0.2.
• Instate 2,actionamovestheagent tostate 1withprobability 0.8and makestheagent
stayputwithprobability 0.2.
• In either state 1 orstate 2, action b moves the agent to state 3 with probability 0.1 and
makestheagentstayputwithprobability 0.9.
Answerthefollowingquestions:
a. Whatcanbedetermined qualitatively abouttheoptimalpolicyinstates1and2?
b. Apply policy iteration, showing each step in full, to determine the optimal policy and
thevaluesofstates1and2. Assumethattheinitialpolicyhasactionbinbothstates.
c. What happens to policy iteration if the initial policy has action a in both states? Does
discounting help? Doestheoptimalpolicydependonthediscount factor?
17.11 Considerthe4×3worldshowninFigure17.1.
a. Implementanenvironment simulator forthisenvironment, such thatthespecific geog-
raphy of the environment is easily altered. Some code for doing this is already in the
onlinecoderepository.
Exercises 691
b. Create an agent that uses policy iteration, and measure its performance in the environ-
ment simulator from various starting states. Perform several experiments from each
starting state, and compare the average total reward received perrun withtheutility of
thestate,asdetermined byyouralgorithm.
c. Experiment with increasing the size of the environment. How does the run time for
policyiteration varywiththesizeoftheenvironment?
17.12 How can the value determination algorithm be used to calculate the expected loss
experienced by an agent using a given set of utility estimates U and an estimated model P,
comparedwithanagentusingcorrect values?
17.13 Let the initial belief state b for the 4×3 POMDP on page 658 be the uniform dis-
0
tribution over the nonterminal states, i.e., (cid:16)1, 1,1,1, 1,1, 1,1,1,0,0(cid:17). Calculate the exact
9 9 9 9 9 9 9 9 9
beliefstateb aftertheagentmovesLeftanditssensorreports1adjacentwall. Alsocalculate
1
b assumingthatthesamethinghappens again.
2
17.14 What is the time complexity of d steps of POMDP value iteration for a sensorless
environment?
17.15 Consider a version of the two-state POMDP on page 661 in which the sensor is
90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 with
equal probability). Analyze, eitherqualitatively orquantitatively, theutility function andthe
optimalpolicyforthisproblem.
17.16 Showthatadominant strategy equilibrium isaNashequilibrium, butnotviceversa.
17.17 In the children’s game of rock–paper–scissors each player reveals at the same time
a choice of rock, paper, or scissors. Paper wraps rock, rock blunts scissors, and scissors cut
paper. In the extended version rock–paper–scissors–fire–water, fire beats rock, paper, and
scissors; rock, paper, and scissors beat water; and water beats fire. Write out the payoff
matrixandfindamixed-strategy solutiontothisgame.
17.18 Thefollowingpayoffmatrix,fromBlinder(1983)bywayofBernstein(1996),shows
agamebetweenpoliticians andtheFederalReserve.
Fed: contract Fed: donothing Fed: expand
Pol: contract F = 7,P = 1 F = 9,P = 4 F = 6,P = 6
Pol: donothing F = 8,P = 2 F = 5,P = 5 F = 4,P = 9
Pol: expand F = 3,P = 3 F = 2,P = 7 F = 1,P = 8
Politicians can expand or contract fiscal policy, while the Fed can expand or contract mon-
etary policy. (And of course either side can choose to do nothing.) Each side also has pref-
erences for who should do what—neither side wants to look like the bad guys. The payoffs
shown are simply the rank orderings: 9 for first choice through 1 for last choice. Find the
Nashequilibriumofthegameinpurestrategies. IsthisaPareto-optimalsolution? Youmight
wishtoanalyzethepolicies ofrecentadministrations inthislight.
692 Chapter 17. MakingComplexDecisions
17.19 ADutchauction issimilarinanEnglishauction, butratherthanstarting thebidding
atalowpriceandincreasing, inaDutchauctionthesellerstartsatahighpriceandgradually
lowers the price until some buyer is willing to accept that price. (If multiple bidders accept
the price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a
price p and gradually lowers p by increments of d until at least one buyer accepts the price.
Assumingallbidders actrationally, isittruethatforarbitrarily small d,aDutchauctionwill
alwaysresultinthebidderwiththehighestvaluefortheitemobtainingtheitem? Ifso,show
mathematically why. Ifnot,explainhowitmaybepossible forthebidderwithhighestvalue
fortheitemnottoobtainit.
17.20 Imagine anauction mechanism thatisjustlikeanascending-bid auction, except that
at the end, the winning bidder, the one who bid b , pays only b /2 rather than b .
max max max
Assuming all agents are rational, what is the expected revenue to the auctioneer for this
mechanism, comparedwithastandard ascending-bid auction?
17.21 Teams in the National Hockey League historically received 2 points for winning a
game and 0 for losing. If the game is tied, an overtime period is played; if nobody wins in
overtime, the game is a tie and each team gets 1 point. But league officials felt that teams
were playing too conservatively inovertime (to avoid aloss), and it would be more exciting
if overtime produced a winner. So in 1999 the officials experimented in mechanism design:
theruleswerechanged, giving ateam thatloses inovertime1 point, not0. Itisstill2points
forawinand1foratie.
a. Washockeyazero-sum gamebeforetherulechange? After?
b. Suppose thatatacertain timetinagame,thehometeam hasprobability pofwinning
in regulation time, probability 0.78 − p of losing, and probability 0.22 of going into
overtime, where they have probability q of winning, .9−q of losing, and .1 of tying.
Giveequations fortheexpected valueforthehomeandvisiting teams.
c. Imagine that it werelegal and ethical forthetwo teamsto enterinto apact where they
agreethat theywillskate toatieinregulation time,and thenboth tryinearnest towin
in overtime. Under what conditions, in terms of p and q, would it be rational for both
teamstoagreetothispact?
d. LongleyandSankaran(2005)reportthatsincetherulechange,thepercentageofgames
with a winner in overtime went up 18.2%, as desired, but the percentage of overtime
games also went up 3.6%. What does that suggest about possible collusion or conser-
vativeplayaftertherulechange?
18
LEARNING FROM
EXAMPLES
In which we describe agents that can improve their behavior through diligent
studyoftheirownexperiences.
Anagentislearningifitimprovesitsperformanceonfuturetasksaftermakingobservations
LEARNING
about the world. Learning can range from the trivial, as exhibited by jotting down a phone
number, to the profound, as exhibited by Albert Einstein, who inferred a new theory of the
universe. Inthis chapter wewillconcentrate on one class of learning problem, which seems
restricted but actually has vast applicability: from a collection of input–output pairs, learn a
function thatpredictstheoutputfornewinputs.
Why would we want an agent to learn? If the design of the agent can be improved,
whywouldn’tthedesigners justprogram inthatimprovement tobeginwith? Therearethree
main reasons. First, the designers cannot anticipate all possible situations that the agent
might find itself in. For example, a robot designed to navigate mazes must learn the layout
of each new maze it encounters. Second, the designers cannot anticipate all changes over
time;aprogramdesignedtopredicttomorrow’sstockmarket pricesmustlearntoadaptwhen
conditions change from boom to bust. Third, sometimes human programmers have no idea
howtoprogramasolutionthemselves. Forexample,mostpeoplearegoodatrecognizingthe
faces of family members, but even the best programmers are unable to program a computer
to accomplish that task, except by using learning algorithms. This chapter first gives an
overview of the various forms of learning, then describes one popular approach, decision-
tree learning, inSection 18.3, followed byatheoretical analysis oflearning inSections 18.4
and 18.5. We look at various learning systems used in practice: linear models, nonlinear
models(inparticular, neuralnetworks),nonparametricmodels,andsupportvectormachines.
Finallyweshowhowensembles ofmodelscanoutperform asinglemodel.
18.1 FORMS OF LEARNING
Anycomponent ofanagent canbeimprovedbylearning from data. Theimprovements, and
thetechniques usedtomakethem,dependonfourmajorfactors:
• Whichcomponentistobeimproved.
693
694 Chapter 18. LearningfromExamples
• Whatpriorknowledgetheagentalreadyhas.
• Whatrepresentation isusedforthedataandthecomponent.
• Whatfeedback isavailabletolearnfrom.
Componentstobelearned
Chapter2described severalagentdesigns. Thecomponents oftheseagentsinclude:
1. Adirectmappingfromconditions onthecurrentstatetoactions.
2. Ameanstoinferrelevantproperties oftheworldfromthepercept sequence.
3. Information about the way the world evolves and about the results of possible actions
theagentcantake.
4. Utilityinformation indicating thedesirability ofworldstates.
5. Action-value information indicating thedesirability ofactions.
6. Goalsthatdescribe classesofstateswhoseachievement maximizestheagent’s utility.
Eachofthesecomponentscanbelearned. Consider,forexample,anagenttrainingtobecome
a taxi driver. Every time the instructor shouts “Brake!” the agent might learn a condition–
action rule for when to brake (component 1); the agent also learns every time the instructor
does not shout. By seeing many camera images that it is told contain buses, it can learn
to recognize them (2). By trying actions and observing the results—for example, braking
hard on a wet road—it can learn the effects of its actions (3). Then, when it receives no tip
from passengers who have been thoroughly shaken up during the trip, it can learn a useful
component ofitsoverallutilityfunction (4).
Representation andpriorknowledge
We have seen several examples of representations for agent components: propositional and
first-order logical sentences for the components in a logical agent; Bayesian networks for
the inferential components of adecision-theoretic agent, and so on. Effective learning algo-
rithms have been devised for all of these representations. This chapter (and most of current
machine learning research) covers inputs that form a factored representation—a vector of
attribute values—and outputs that can be either a continuous numerical value or a discrete
value. Chapter 19 covers functions and prior knowledge composed of first-order logic sen-
tences, andChapter20concentrates onBayesiannetworks.
There is another way to look at the various types of learning. We say that learning
a (possibly incorrect) general function or rule from specific input–output pairs is called in-
INDUCTIVE ductive learning. We will see in Chapter 19 that we can also do analytical or deductive
LEARNING
DEDUCTIVE learning: going from a known general rule to a new rule that is logically entailed, but is
LEARNING
usefulbecauseitallowsmoreefficientprocessing.
Feedbacktolearnfrom
Therearethree typesoffeedback thatdeterminethethreemaintypesoflearning:
UNSUPERVISED Inunsupervisedlearningtheagentlearnspatternsintheinputeventhoughnoexplicit
LEARNING
feedback issupplied. Themostcommonunsupervised learning taskisclustering: detecting
CLUSTERING
Section18.2. Supervised Learning 695
potentially useful clusters of input examples. For example, a taxi agent might gradually
develop a concept of “good traffic days” and “bad traffic days” without ever being given
labeledexamplesofeachbyateacher.
REINFORCEMENT In reinforcement learning the agent learns from a series of reinforcements—rewards
LEARNING
orpunishments. Forexample,thelackofatipattheendofthejourneygivesthetaxiagentan
indication that itdid something wrong. The twopoints fora win at the end of achess game
tellstheagentitdidsomethingright. Itisuptotheagenttodecidewhichoftheactionsprior
tothereinforcement weremostresponsible forit.
SUPERVISED Insupervisedlearningtheagentobservessomeexampleinput–output pairsandlearns
LEARNING
afunctionthatmapsfrominputtooutput. Incomponent1above,theinputsarepercepts and
the output are provided by ateacher who says “Brake!” or“Turn left.” In component 2, the
inputsarecameraimagesandtheoutputsagaincomefromateacherwhosays“that’sabus.”
In3, thetheory ofbraking isafunction from states andbraking actions tostopping distance
in feet. In this case the output value is available directly from the agent’s percepts (after the
fact);theenvironment istheteacher.
SEMI-SUPERVISED In practice, these distinction are not always so crisp. In semi-supervised learning we
LEARNING
are given a few labeled examples and must make what we can of a large collection of un-
labeled examples. Even the labels themselves may not be the oracular truths that we hope
for. Imagine thatyou are trying tobuild asystem toguess aperson’s age from aphoto. You
gather some labeled examples by snapping pictures of people and asking their age. That’s
supervised learning. But in reality some of the people lied about their age. It’s not just
that there is random noise in the data; rather the inaccuracies are systematic, and to uncover
themisanunsupervised learningprobleminvolving images, self-reported ages,andtrue(un-
known)ages. Thus,bothnoiseandlackoflabelscreateacontinuum betweensupervised and
unsupervised learning.
18.2 SUPERVISED LEARNING
Thetaskofsupervised learning isthis:
GivenatrainingsetofN exampleinput–output pairs
TRAININGSET
(x ,y ),(x ,y ),...(x ,y ),
1 1 2 2 N N
whereeach y wasgenerated byanunknownfunction y =f(x),
j
discoverafunction hthatapproximates thetruefunction f.
Here x and y can be any value; they need not be numbers. The function h isa hypothesis.1
HYPOTHESIS
Learningisasearchthrough thespaceofpossiblehypotheses foronethatwillperform well,
even on new examples beyond the training set. Tomeasure the accuracy of a hypothesis we
give it a test set of examples that are distinct from the training set. We say a hypothesis
TESTSET
1 Anoteonnotation:exceptwherenoted,wewillusejtoindextheNexamples;xjwillalwaysbetheinputand
yj theoutput. Incaseswheretheinputisspecificallyavectorofattributevalues(beginningwithSection18.3),
wewillusexj forthejthexampleandwewilluseitoindexthenattributesofeachexample. Theelementsof
xj arewrittenxj,1,xj,2,...,xj,n.
696 Chapter 18. LearningfromExamples
f(x) f(x) f(x) f(x)
x x x x
(a) (b) (c) (d)
Figure18.1 (a) Example(x,f(x)) pairsand a consistent, linearhypothesis. (b)A con-
sistent,degree-7polynomialhypothesisforthesamedataset. (c)Adifferentdataset,which
admits an exact degree-6 polynomial fit or an approximate linear fit. (d) A simple, exact
sinusoidalfittothesamedataset.
generalizes well if it correctly predicts the value of y for novel examples. Sometimes the
GENERALIZATION
function f is stochastic—it is not strictly a function of x, and what we have to learn is a
conditional probability distribution, P(Y |x).
When the output y is one of a finite set of values (such as sunny, cloudy or rainy),
the learning problem is called classification, and is called Boolean or binary classification
CLASSIFICATION
if there are only two values. When y is a number (such as tomorrow’s temperature), the
learning problem is called regression. (Technically, solving a regression problem is finding
REGRESSION
a conditional expectation or average value of y, because the probability that we have found
exactlytherightreal-valued numberfor y is0.)
Figure18.1showsafamiliarexample: fittingafunctionofasinglevariabletosomedata
points. Theexamplesarepoints inthe (x,y)plane, wherey = f(x). Wedon’t knowwhatf
is, butwewillapproximate itwithafunction hselected fromahypothesisspace, H,which
HYPOTHESISSPACE
forthisexamplewewilltaketobethesetofpolynomials,suchasx5+3x2+2. Figure18.1(a)
shows some data with an exact fit by a straight line (the polynomial 0.4x +3). The line is
calledaconsistenthypothesisbecauseitagreeswithallthedata. Figure18.1(b)showsahigh-
CONSISTENT
degree polynomial that is also consistent with the same data. This illustrates a fundamental
problemininductivelearning: howdowechoosefromamongmultipleconsistenthypotheses?
One answer is to prefer the simplest hypothesis consistent with the data. This principle is
calledOckham’srazor,afterthe14th-centuryEnglishphilosopherWilliamofOckham,who
OCKHAM’SRAZOR
usedittoarguesharplyagainstallsortsofcomplications. Definingsimplicityisnoteasy,but
itseemsclearthatadegree-1polynomial issimplerthanadegree-7polynomial, andthus(a)
shouldbepreferred to(b). WewillmakethisintuitionmorepreciseinSection18.4.3.
Figure 18.1(c) shows a second data set. There is no consistent straight line for this
data set; in fact, it requires a degree-6 polynomial for an exact fit. There are just 7 data
points, so a polynomial with 7 parameters does not seem to be finding any pattern in the
data and we do not expect it to generalize well. A straight line that is not consistent with
any ofthe data points, but mightgeneralize fairly wellforunseen values of x, isalso shown
in (c). In general, there is a tradeoff between complex hypotheses that fit the training data
well and simpler hypotheses that may generalize better. In Figure 18.1(d) we expand the
Section18.3. LearningDecisionTrees 697
hypothesis space H to allow polynomials over both x and sin(x), and find that the data in
(c) can be fitted exactly by asimple function of the form ax+b+csin(x). This shows the
importanceofthechoiceofhypothesisspace. Wesaythatalearningproblemisrealizableif
REALIZABLE
thehypothesisspacecontainsthetruefunction. Unfortunately, wecannotalwaystellwhether
agivenlearning problem isrealizable, becausethetruefunction isnotknown.
In some cases, an analyst looking at a problem is willing to make more fine-grained
distinctions about thehypothesis space, tosay—even beforeseeing anydata—not justthat a
hypothesis is possible or impossible, but rather how probable it is. Supervised learning can
∗
bedonebychoosing thehypothesis h thatismostprobable giventhedata:
h ∗ = argmaxP(h|data).
h∈H
ByBayes’rulethisisequivalentto
h ∗ = argmaxP(data|h)P(h).
h∈H
Then we can say that the prior probability P(h) is high for a degree-1 or -2 polynomial,
lower for a degree-7 polynomial, and especially low for degree-7 polynomials with large,
sharpspikes asinFigure18.1(b). Weallowunusual-looking functions whenthedatasaywe
reallyneedthem,butwediscourage thembygivingthemalowpriorprobability.
Why not let H be the class of all Java programs, orTuring machines? Afterall, every
computable function can be represented by some Turing machine, and that is the best we
can do. One problem with this idea is that it does not take into account the computational
complexity oflearning. Thereisatradeoff between theexpressiveness ofahypothesis space
and the complexity of finding a good hypothesis within that space. For example, fitting a
straight line to data is an easy computation; fitting high-degree polynomials is somewhat
harder; and fitting Turing machines is in general undecidable. A second reason to prefer
simple hypothesis spaces is that presumably we will want to use h after we have learned it,
and computing h(x) when h is a linear function is guaranteed to be fast, while computing
anarbitrary Turingmachine program isnotevenguaranteed toterminate. Forthese reasons,
mostworkonlearninghasfocused onsimplerepresentations.
Wewillseethattheexpressiveness–complexitytradeoffisnotassimpleasitfirstseems:
itisoftenthecase,aswesawwithfirst-orderlogicinChapter8,thatanexpressive language
makesitpossibleforasimplehypothesistofitthedata,whereasrestrictingtheexpressiveness
of the language means that any consistent hypothesis must be very complex. For example,
therulesofchesscanbewritteninapageortwooffirst-order logic,butrequirethousandsof
pageswhenwritteninpropositional logic.
18.3 LEARNING DECISION TREES
Decision tree induction is one of the simplest and yet most successful forms of machine
learning. Wefirstdescribe therepresentation—the hypothesis space—and then showhowto
learnagoodhypothesis.
698 Chapter 18. LearningfromExamples
18.3.1 The decisiontree representation
A decision tree represents a function that takes as input a vector of attribute values and
DECISIONTREE
returns a “decision”—a single output value. The input and output values can be discrete or
continuous. Fornow wewillconcentrate on problems where the inputs have discrete values
and the output has exactly two possible values; this is Boolean classification, where each
exampleinputwillbeclassifiedastrue(apositiveexample)orfalse(anegativeexample).
POSITIVE
A decision tree reaches its decision by performing a sequence of tests. Each internal
NEGATIVE
node in the tree corresponds to a test of the value of one of the input attributes, A , and
i
the branches from the node are labeled with the possible values of the attribute, A =v .
i ik
Each leaf node in the tree specifies avalue to be returned by the function. The decision tree
representation is natural for humans; indeed, many “How To” manuals (e.g., for car repair)
arewrittenentirelyasasingledecision treestretching overhundreds ofpages.
As an example, we will build a decision tree to decide whether to wait for a table at a
restaurant. The aim here is to learn a definition for the goal predicate WillWait. First we
GOALPREDICATE
listtheattributes thatwewillconsideraspartoftheinput:
1. Alternate: whetherthereisasuitablealternative restaurant nearby.
2. Bar: whethertherestaurant hasacomfortable barareatowaitin.
3. Fri/Sat: trueonFridaysandSaturdays.
4. Hungry: whetherwearehungry.
5. Patrons: howmanypeopleareintherestaurant (valuesare None,Some,andFull).
6. Price: therestaurant’s pricerange($,$$,$$$).
7. Raining: whetheritisrainingoutside.
8. Reservation: whetherwemadeareservation.
9. Type: thekindofrestaurant (French,Italian,Thai,orburger).
10. WaitEstimate: thewaitestimatedbythehost(0–10minutes,10–30, 30–60, or>60).
Note that every variable has a small set of possible values; the value of WaitEstimate, for
example, isnot aninteger, rather itisone ofthefourdiscrete values 0–10, 10–30, 30–60, or
>60. Thedecisiontreeusuallyusedbyoneofus(SR)forthisdomainisshowninFigure18.2.
NoticethatthetreeignoresthePrice andType attributes. Examplesareprocessedbythetree
starting attherootandfollowing theappropriate branch untilaleafisreached. Forinstance,
an example with Patrons=Full and WaitEstimate=0–10 will be classified as positive
(i.e.,yes,wewillwaitforatable).
18.3.2 Expressiveness ofdecisiontrees
A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true
if and only if the input attributes satisfy one of the paths leading to a leaf with value true.
Writingthisoutinpropositional logic,wehave
Goal ⇔ (Path ∨Path ∨···),
1 2
where each Path is aconjunction of attribute-value tests required to follow that path. Thus,
the whole expression is equivalent to disjunctive normal form (see page 283), which means
Section18.3. LearningDecisionTrees 699
that any function in propositional logic can be expressed as a decision tree. As an example,
therightmostpathinFigure18.2is
Path = (Patrons=Full ∧WaitEstimate=0–10).
For a wide variety of problems, the decision tree format yields a nice, concise result. But
some functions cannot be represented concisely. Forexample, the majority function, which
returns true if and only if more than half of the inputs are true, requires an exponentially
large decision tree. In other words, decision trees are good for some kinds of functions and
bad forothers. Is there any kind ofrepresentation that is efficient for all kinds offunctions?
Unfortunately, the answer is no. We can show this in a general way. Consider the set of all
Boolean functions onnattributes. Howmany different functions are inthis set? Thisis just
the number of different truth tables that we can write down, because the function is defined
by its truth table. A truth table over n attributes has 2n rows, one for each combination of
valuesoftheattributes. Wecanconsiderthe“answer”columnofthetableasa2n-bitnumber
thatdefinesthefunction.
Thatmeansthereare22n
differentfunctions(andtherewillbemore
than that number of trees, since more than one tree can compute the same function). Thisis
a scary number. Forexample, with just the ten Boolean attributes of our restaurant problem
there are 21024 or about 10308 different functions to choose from, and for20 attributes there
areover 10300,000. Wewillneed someingenious algorithms tofindgood hypotheses insuch
alargespace.
18.3.3 Inducing decisiontrees from examples
AnexampleforaBooleandecisiontreeconsistsofan(x,y)pair,wherexisavectorofvalues
fortheinputattributes, and y isasingleBooleanoutput value. Atraining setof12examples
Patrons?
None Some Full
No Yes WaitEstimate?
>60 30-60 10-30 0-10
No Alternate? Hungry? Yes
No Yes No Yes
Reservation? Fri/Sat? Yes Alternate?
No Yes No Yes No Yes
Bar? Yes No Yes Yes Raining?
No Yes No Yes
No Yes No Yes
Figure18.2 Adecisiontreefordecidingwhethertowaitforatable.
700 Chapter 18. LearningfromExamples
InputAttributes Goal
Example
Alt Bar Fri Hun Pat Price Rain Res Type Est WillWait
x Yes No No Yes Some $$$ No Yes French 0–10 y =Yes
1 1
x Yes No No Yes Full $ No No Thai 30–60 y =No
2 2
x No Yes No No Some $ No No Burger 0–10 y =Yes
3 3
x Yes No Yes Yes Full $ Yes No Thai 10–30 y =Yes
4 4
x Yes No Yes No Full $$$ No Yes French >60 y =No
5 5
x No Yes No Yes Some $$ Yes Yes Italian 0–10 y =Yes
6 6
x No Yes No No None $ Yes No Burger 0–10 y =No
7 7
x No No No Yes Some $$ Yes Yes Thai 0–10 y =Yes
8 8
x No Yes Yes No Full $ Yes No Burger >60 y =No
9 9
x Yes Yes Yes Yes Full $$$ No Yes Italian 10–30 y =No
10 10
x No No No No None $ No No Thai 0–10 y =No
11 11
x Yes Yes Yes Yes Full $ No No Burger 30–60 y =Yes
12 12
Figure18.3 Examplesfortherestaurantdomain.
is shown in Figure 18.3. The positive examples are the ones in which the goal WillWait is
true(x ,x ,...);thenegativeexamplesaretheonesinwhichitisfalse (x ,x ,...).
1 3 2 5
We want a tree that is consistent with the examples and is as small as possible. Un-
fortunately, no matter how we measure size, it is an intractable problem to find the smallest
consistent tree; thereisnowaytoefficiently search through
the22n
trees. Withsomesimple
heuristics, however,wecanfindagoodapproximate solution: asmall(butnotsmallest)con-
sistenttree. TheDECISION-TREE-LEARNING algorithmadoptsagreedydivide-and-conquer
strategy: always test the most important attribute first. This test divides the problem up into
smaller subproblems that can then be solved recursively. By “most important attribute,” we
meantheonethatmakesthemostdifferencetotheclassificationofanexample. Thatway,we
hopetogettothecorrectclassificationwithasmallnumberoftests,meaningthatallpathsin
thetreewillbeshortandthetreeasawholewillbeshallow.
Figure18.4(a)showsthatType isapoorattribute,becauseitleavesuswithfourpossible
outcomes,eachofwhichhasthesamenumberofpositiveasnegativeexamples. Ontheother
hand,in(b)weseethatPatrons isafairlyimportantattribute,becauseifthevalueisNone or
Some,thenweareleftwithexamplesetsforwhichwecananswerdefinitively(No andYes,
respectively). Ifthe valueis Full,weareleftwithamixedsetofexamples. Ingeneral, after
the first attribute test splits up the examples, each outcome is a new decision tree learning
probleminitself,withfewerexamplesandonelessattribute. Therearefourcasestoconsider
fortheserecursive problems:
1. If the remaining examples are all positive (or all negative), then we are done: we can
answer Yes or No. Figure 18.4(b) shows examples ofthis happening in the None and
Some branches.
2. Iftherearesomepositiveandsomenegativeexamples,thenchoosethebestattributeto
splitthem. Figure18.4(b)showsHungry beingusedtosplittheremainingexamples.
3. Iftherearenoexamplesleft,itmeansthatnoexamplehasbeenobserved forthiscom-
Section18.3. LearningDecisionTrees 701
1 3 4 6 8 12 1 3 4 6 8 12
2 5 7 9 10 11 2 5 7 9 10 11
Type? Patrons?
French Italian Thai Burger None Some Full
1 6 4 8 3 12 1 3 6 8 4 12
5 10 2 11 7 9 7 11 2 5 9 10
No Yes Hungry?
No Yes
4 12
5 9 2 10
(a) (b)
Figure 18.4 Splitting the examples by testing on attributes. At each node we show the
positive(lightboxes) and negative(darkboxes)examplesremaining. (a) Splitting on Type
bringsusnonearertodistinguishingbetweenpositiveandnegativeexamples. (b)Splitting
onPatronsdoesagoodjobofseparatingpositiveandnegativeexamples. Aftersplittingon
Patrons,Hungryisafairlygoodsecondtest.
bination ofattribute values, and wereturn adefault value calculated from the plurality
classificationofalltheexamplesthatwereusedinconstructingthenode’sparent. These
arepassedalonginthevariable parent examples.
4. If there are no attributes left, but both positive and negative examples, it means that
theseexampleshaveexactlythesamedescription,butdifferentclassifications. Thiscan
happen because there is an error or noise in the data; because the domain is nondeter-
NOISE
ministic; orbecause wecan’t observe anattribute thatwoulddistinguish theexamples.
Thebestwecandoisreturntheplurality classification oftheremainingexamples.
The DECISION-TREE-LEARNING algorithm is shown in Figure 18.5. Note that the set of
examples iscrucial for constructing thetree, butnowheredotheexamplesappearinthetree
itself. A tree consists of just tests on attributes in the interior nodes, values of attributes on
the branches, and output values on the leaf nodes. Thedetails of the IMPORTANCE function
are given in Section 18.3.4. The output of the learning algorithm on our sample training
set is shown in Figure 18.6. The tree is clearly different from the original tree shown in
Figure 18.2. One might conclude that the learning algorithm is not doing a very good job
oflearning the correct function. Thiswould bethewrong conclusion todraw, however. The
learningalgorithmlooksattheexamples,notatthecorrectfunction,andinfact,itshypothesis
(see Figure 18.6) not only is consistent with all the examples, but is considerably simpler
thantheoriginal tree! Thelearning algorithm hasnoreason toinclude testsforRaining and
Reservation, because it can classify all the examples without them. It has also detected an
interesting and previously unsuspected pattern: the first author will wait for Thai food on
weekends. It is also bound tomake some mistakes forcases where ithas seen no examples.
Forexample,ithasneverseenacasewherethewaitis0–10minutesbuttherestaurantisfull.
702 Chapter 18. LearningfromExamples
function DECISION-TREE-LEARNING(examples,attributes,parent examples) returns
atree
ifexamples isemptythenreturnPLURALITY-VALUE(parent examples)
elseifallexamples havethesameclassificationthenreturntheclassification
elseifattributes isemptythenreturnPLURALITY-VALUE(examples)
else
A←argmax
a∈attributes
IMPORTANCE(a,examples)
tree←anewdecisiontreewithroottestA
foreachvaluevk ofAdo
exs←{e : e∈examples and e.A = vk }
subtree←DECISION-TREE-LEARNING(exs,attributes−A,examples)
addabranchtotree withlabel(A = vk)andsubtreesubtree
returntree
Figure 18.5 The decision-tree learning algorithm. The function IMPORTANCE is de-
scribedinSection18.3.4.ThefunctionPLURALITY-VALUEselectsthemostcommonoutput
valueamongasetofexamples,breakingtiesrandomly.
Patrons?
None Some Full
No Yes Hungry?
No Yes
No Type?
French Italian Thai Burger
Yes No Fri/Sat? Yes
No Yes
No Yes
Figure18.6 Thedecisiontreeinducedfromthe12-exampletrainingset.
In that case it says not to wait when Hungry is false, but I (SR)would certainly wait. With
moretrainingexamplesthelearningprogram couldcorrectthismistake.
Wenotethereisadangerofover-interpreting thetreethatthealgorithm selects. When
thereareseveralvariables ofsimilarimportance, thechoice betweenthemissomewhatarbi-
trary: withslightly different input examples, adifferent variable wouldbechosen tosplit on
first,andthewholetreewouldlookcompletely different. Thefunction computed bythetree
wouldstillbesimilar,butthestructure ofthetreecanvary widely.
Wecanevaluate the accuracy ofalearning algorithm witha learningcurve, asshown
LEARNINGCURVE
inFigure 18.7. Wehave 100 examples atourdisposal, which we split into atraining set and
Section18.3. LearningDecisionTrees 703
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20 40 60 80 100
tes
tset
no
tcerroc
noitroporP
Training set size
Figure 18.7 A learning curve for the decision tree learning algorithm on 100 randomly
generatedexamplesintherestaurantdomain.Eachdatapointistheaverageof20trials.
atestset. Welearnahypothesis hwiththetrainingsetandmeasureitsaccuracywiththetest
set. We do this starting with a training set of size 1 and increasing one at a time up to size
99. Foreach size weactually repeat the process ofrandomly splitting 20times, andaverage
the results of the 20 trials. The curve shows that as the training set size grows, the accuracy
increases. (Forthis reason, learning curves are also called happygraphs.) Inthis graph we
reach95%accuracy, anditlookslikethecurvemightcontinue toincreasewithmoredata.
18.3.4 Choosingattribute tests
The greedy search used in decision tree learning is designed to approximately minimize the
depth of the final tree. The idea is to pick the attribute that goes as far as possible toward
providing an exact classification of the examples. A perfect attribute divides the examples
intosets,eachofwhichareallpositiveorallnegativeandthuswillbeleavesofthetree. The
Patrons attribute isnotperfect,butitisfairlygood. Areallyuselessattribute, suchasType,
leaves the example sets with roughly the same proportion of positive and negative examples
astheoriginal set.
Allweneed,then,isaformalmeasureof“fairlygood”and“reallyuseless”andwecan
implement the IMPORTANCE function ofFigure 18.5. Wewillusethe notion ofinformation
gain, which is defined in terms of entropy, the fundamental quantity in information theory
ENTROPY
(ShannonandWeaver,1949).
Entropyisameasureoftheuncertaintyofarandomvariable; acquisitionofinformation
corresponds to a reduction in entropy. A random variable with only one value—a coin that
always comesupheads—has nouncertainty andthus itsentropy isdefined aszero; thus, we
gain no information by observing its value. Aflipof afair coin is equally likely to come up
heads or tails, 0 or 1, and we will soon show that this counts as “1 bit” of entropy. The roll
ofafairfour-sided diehas2bitsofentropy, because ittakestwobitstodescribe oneoffour
equallyprobablechoices. Nowconsideranunfaircointhatcomesupheads99%ofthetime.
Intuitively,thiscoinhaslessuncertaintythanthefaircoin—ifweguessheadswe’llbewrong
only1%ofthetime—sowewouldlikeittohaveanentropymeasurethatisclosetozero,but
704 Chapter 18. LearningfromExamples
positive. Ingeneral,theentropyofarandomvariable V withvaluesv ,eachwithprobability
k
P(v ),isdefinedas
k
(cid:12) (cid:12)
1
Entropy: H(V) = P(v )log = − P(v )log P(v ).
k 2 P(v ) k 2 k
k
k k
Wecancheckthattheentropy ofafaircoinflipisindeed1bit:
H(Fair) = −(0.5log 0.5+0.5log 0.5) = 1.
2 2
Ifthecoinisloadedtogive99%heads, weget
H(Loaded)= −(0.99log 0.99+0.01log 0.01) ≈ 0.08bits.
2 2
It will help to define B(q) as the entropy of a Boolean random variable that is true with
probability q:
B(q)=−(qlog q+(1−q)log (1−q)).
2 2
Thus, H(Loaded)=B(0.99) ≈ 0.08. Now let’s get back to decision tree learning. If a
training set contains p positive examples and n negative examples, then the entropy of the
goalattribute onthewholesetis
(cid:13) (cid:14)
p
H(Goal)= B .
p+n
The restaurant training set in Figure 18.3 has p = n = 6, so the corresponding entropy is
B(0.5)orexactly1bit. Atestonasingleattribute Amightgiveusonlypartofthis1bit. We
canmeasureexactlyhowmuchbylookingattheentropyremainingaftertheattribute test.
Anattribute AwithddistinctvaluesdividesthetrainingsetE intosubsetsE ,...,E .
1 d
Each subset E has p positive examples and n negative examples, so if we go along that
k k k
branch, wewillneed anadditional B(p /(p +n ))bitsofinformation toanswertheques-
k k k
tion. Arandomlychosenexamplefromthetrainingsethasthekthvaluefortheattributewith
probability (p +n )/(p+n),sotheexpectedentropy remainingaftertestingattribute Ais
k k
(cid:12)d
Remainder(A) = pk+nkB( pk ).
p+n pk+nk
k=1
Theinformationgainfromtheattribute testonAistheexpectedreduction inentropy:
INFORMATIONGAIN
Gain(A) = B( p )−Remainder(A).
p+n
InfactGain(A)isjustwhatweneedtoimplementthe IMPORTANCE function. Returningto
theattributes considered inFigure18.4,wehave
$ %
Gain(Patrons)= 1− 2 B(0)+ 4 B(4)+ 6 B(2) ≈ 0.541bits,
$ 12 2 12 4 12 6 %
Gain(Type) = 1− 2 B(1)+ 2 B(1)+ 4 B(2)+ 4 B(2) = 0bits,
12 2 12 2 12 4 12 4
confirming our intuition that Patrons is a better attribute to split on. In fact, Patrons has
themaximumgainofanyoftheattributes andwouldbechosenbythedecision-tree learning
algorithm astheroot.
Section18.3. LearningDecisionTrees 705
18.3.5 Generalizationandoverfitting
On some problems, the DECISION-TREE-LEARNING algorithm will generate a large tree
when there is actually no pattern to be found. Consider the problem of trying to predict
whether the roll of a die will come up as 6 ornot. Suppose that experiments are carried out
with various dice and that the attributes describing each training example include the color
of the die, its weight, the time when the roll was done, and whether the experimenters had
their fingers crossed. If the dice are fair, the right thing to learn is a tree with a single node
that says “no,” But the DECISION-TREE-LEARNING algorithm will seize on any pattern it
can find in the input. If it turns out that there are 2 rolls of a 7-gram blue die with fingers
crossed and theyboth come out6, then the algorithm mayconstruct apath that predicts 6in
thatcase. Thisproblemiscalled overfitting. Ageneral phenomenon, overfittingoccurswith
OVERFITTING
alltypesoflearners,evenwhenthetargetfunctionisnotatallrandom. InFigure18.1(b)and
(c),wesawpolynomialfunctionsoverfittingthedata. Overfittingbecomesmorelikelyasthe
hypothesis spaceandthenumberofinputattributes grows,andlesslikelyasweincrease the
numberoftrainingexamples.
DECISIONTREE Fordecisiontrees,atechniquecalled decisiontreepruningcombatsoverfitting. Prun-
PRUNING
ing works by eliminating nodes that are not clearly relevant. We start with a full tree, as
generated by DECISION-TREE-LEARNING. We then look at a test node that has only leaf
nodes asdescendants. Ifthetestappears tobeirrelevant—detecting onlynoiseinthedata—
then weeliminate the test, replacing it witha leaf node. Werepeat this process, considering
eachtestwithonlyleafdescendants, untileachonehaseitherbeenpruned oraccepted asis.
Thequestionis,howdowedetectthatanodeistestinganirrelevantattribute? Suppose
weareatanodeconsistingofppositiveandnnegativeexamples. Iftheattributeisirrelevant,
wewouldexpectthatitwouldsplittheexamplesintosubsetsthateachhaveroughlythesame
proportion ofpositiveexamplesasthewholeset, p/(p+n),andsotheinformationgainwill
beclosetozero.2 Thus,theinformation gainisagoodcluetoirrelevance. Now thequestion
is,howlargeagainshouldwerequireinordertosplitonaparticular attribute?
Wecan answerthisquestion byusing astatistical significance test. Suchatest begins
SIGNIFICANCETEST
byassumingthatthereisnounderlying pattern (theso-called nullhypothesis). Thentheac-
NULLHYPOTHESIS
tualdataareanalyzed tocalculate theextenttowhichtheydeviate fromaperfect absence of
pattern. Ifthe degree ofdeviation isstatistically unlikely (usually taken to meana5% prob-
ability or less), then that is considered to be good evidence for the presence of a significant
patterninthedata. Theprobabilities arecalculatedfromstandarddistributions oftheamount
ofdeviation onewouldexpecttoseeinrandomsampling.
In this case, the null hypothesis is that the attribute is irrelevant and, hence, that the
information gain for an infinitely large sample would be zero. We need to calculate the
probability that, under the null hypothesis, a sample of size v=n + p would exhibit the
observeddeviationfromtheexpecteddistribution ofpositiveandnegativeexamples. Wecan
measurethedeviation bycomparingtheactualnumbersofpositiveandnegativeexamplesin
2 Thegainwillbestrictlypositiveexceptfortheunlikelycasewherealltheproportionsare exactlythesame.
(SeeExercise18.5.)
706 Chapter 18. LearningfromExamples
eachsubset, p andn ,withtheexpectednumbers, pˆ andnˆ ,assumingtrueirrelevance:
k k k k
p +n p +n
pˆ = p× k k nˆ =n× k k .
k k
p+n p+n
Aconvenient measureofthetotaldeviation isgivenby
(cid:12)d (p −pˆ )2 (n −nˆ )2
k k k k
Δ = + .
pˆ nˆ
k k
k=1
Under the null hypothesis, the value of Δ is distributed according to the χ2 (chi-squared)
distribution with v −1 degrees of freedom. We can use a χ2 table or a standard statistical
library routine to see if a particular Δ value confirms or rejects the null hypothesis. For
example, consider the restaurant type attribute, with four values and thus three degrees of
freedom. AvalueofΔ=7.82ormorewouldrejectthenullhypothesisatthe5%level(anda
valueofΔ=11.35ormorewouldrejectatthe1%level). Exercise18.8asksyoutoextendthe
DECISION-TREE-LEARNING algorithm toimplement this form of pruning, which is known
χ2 asχ2 pruning.
PRUNING
Withpruning,noiseintheexamplescanbetolerated. Errorsintheexample’slabel(e.g.,
anexample(x,Yes)thatshouldbe(x,No))givealinearincreaseinpredictionerror,whereas
errorsinthedescriptions ofexamples(e.g., Price=$whenitwasactuallyPrice=$$)have
an asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees
perform significantly better than unpruned trees when the data contain a large amount of
noise. Also,theprunedtreesareoftenmuchsmallerandhenceeasiertounderstand.
Onefinalwarning: Youmightthink that χ2 pruning andinformation gainlooksimilar,
so why not combine them using an approach called early stopping—have the decision tree
EARLYSTOPPING
algorithm stopgenerating nodeswhenthereisnogoodattribute tospliton,ratherthangoing
to all the trouble of generating nodes and then pruning them away. The problem with early
stopping is that it stops us from recognizing situations where there is no one good attribute,
buttherearecombinations ofattributes thatareinformative. Forexample,considertheXOR
function of twobinary attributes. If there are roughly equal numberof examples forallfour
combinations of input values, then neither attribute willbeinformative, yet thecorrect thing
todo istosplit onone ofthe attributes (it doesn’t matterwhich one), and then atthesecond
level we will get splits that are informative. Early stopping would miss this, but generate-
and-then-prune handles itcorrectly.
18.3.6 Broadening the applicabilityofdecisiontrees
Inordertoextend decision tree induction toawidervariety ofproblems, anumberofissues
must be addressed. We will briefly mention several, suggesting that a full understanding is
bestobtained bydoingtheassociated exercises:
• Missing data: In many domains, not all the attribute values will be known for every
example. The values might have gone unrecorded, or they might be too expensive to
obtain. This gives rise to two problems: First, given a complete decision tree, how
should one classify an example that is missing one of the test attributes? Second, how
Section18.3. LearningDecisionTrees 707
should one modify the information-gain formula when some examples have unknown
valuesfortheattribute? Thesequestions areaddressed inExercise18.9.
• Multivalued attributes: When an attribute has many possible values, the information
gain measure gives an inappropriate indication of the attribute’s usefulness. In the ex-
treme case, an attribute such as ExactTime has a different value for every example,
which means each subset of examples is a singleton with a unique classification, and
theinformationgainmeasurewouldhaveitshighestvalueforthisattribute. Butchoos-
ingthissplitfirstisunlikely toyieldthebesttree. Onesolution istousethegainratio
GAINRATIO
(Exercise18.10). AnotherpossibilityistoallowaBooleantestoftheformA=v ,that
k
is, picking out just one of the possible values for an attribute, leaving the remaining
valuestopossibly betestedlaterinthetree.
• Continuous and integer-valued input attributes: Continuous or integer-valued at-
tributessuchasHeight andWeight,haveaninfinitesetofpossiblevalues. Ratherthan
generate infinitely many branches, decision-tree learning algorithms typically find the
split point that gives the highest information gain. For example, at a given node in
SPLITPOINT
the tree, it might be the case that testing on Weight > 160 gives the most informa-
tion. Efficient methods exist for finding good split points: start by sorting the values
of the attribute, and then consider only split points that are between two examples in
sortedorderthathavedifferentclassifications, whilekeepingtrackoftherunningtotals
of positive and negative examples on each side of the split point. Splitting is the most
expensivepartofreal-world decision treelearningapplications.
• Continuous-valued output attributes: If we are trying to predict a numerical output
value, such as the price of an apartment, then we need a regression tree rather than a
REGRESSIONTREE
classification tree. A regression tree has at each leaf a linear function of some subset
of numerical attributes, rather than a single value. For example, the branch for two-
bedroom apartments might end with a linear function of square footage, number of
bathrooms, and average income for the neighborhood. The learning algorithm must
decide when to stop splitting and begin applying linear regression (see Section 18.6)
overtheattributes.
A decision-tree learning system for real-world applications must be able to handle all of
these problems. Handling continuous-valued variables isespecially important, because both
physical and financial processes provide numerical data. Several commercial packages have
been built that meet these criteria, and they have been used to develop thousands of fielded
systems. Inmanyareasofindustryandcommerce,decisiontreesareusuallythefirstmethod
triedwhenaclassification methodistobeextracted from adataset. Oneimportant property
ofdecisiontreesisthatitispossibleforahumantounderstandthereasonfortheoutputofthe
learningalgorithm. (Indeed,thisisalegalrequirementforfinancialdecisionsthataresubject
to anti-discrimination laws.) This is a property not shared by some other representations,
suchasneuralnetworks.
708 Chapter 18. LearningfromExamples
18.4 EVALUATING AND CHOOSING THE BEST HYPOTHESIS
We want to learn a hypothesis that fits the future data best. To make that precise we need
STATIONARITY to define “future data” and “best.” We make the stationarity assumption: that there is a
ASSUMPTION
probability distribution overexamples that remains stationary overtime. Each example data
point(beforeweseeit)isarandomvariableE whoseobservedvaluee =(x ,y )issampled
j j j j
fromthatdistribution, andisindependent oftheprevious examples:
P(E j |E j−1 ,E j−2 ,...)= P(E j ),
andeachexamplehasanidentical priorprobability distribution:
P(E j )= P(E j−1 ) = P(E j−2 ) = ··· .
Examplesthatsatisfytheseassumptionsarecalledindependent andidenticallydistributed or
i.i.d.. Ani.i.d.assumption connects thepasttothefuture; withoutsomesuchconnection, all
I.I.D.
bets are off—the future could be anything. (We will see later that learning can still occur if
thereare slowchangesinthedistribution.)
The next step is to define “best fit.” We define the error rate of a hypothesis as the
ERRORRATE
proportionofmistakesitmakes—theproportionoftimesthath(x) (cid:7)= yforan(x,y)example.
Now, just because a hypothesis h has a low error rate on the training set does not mean that
itwillgeneralize well. Aprofessor knowsthatanexamwillnotaccurately evaluate students
if they have already seen the exam questions. Similarly, to get an accurate evaluation of a
hypothesis,weneedtotestitonasetofexamplesithasnotseenyet. Thesimplestapproachis
theonewehaveseenalready: randomlysplittheavailabledataintoatrainingsetfromwhich
thelearningalgorithmproduces handatestsetonwhichtheaccuracyofhisevaluated. This
HOLDOUT method, sometimes called holdoutcross-validation, hasthedisadvantage thatitfails touse
CROSS-VALIDATION
alltheavailable data;ifweusehalfthedataforthetestset,thenweareonlytraining onhalf
the data, and we may get a poor hypothesis. On the other hand, if we reserve only 10% of
the data for the test set, then we may, by statistical chance, get a poor estimate of the actual
accuracy.
Wecansqueezemoreoutofthedataandstillgetanaccurateestimateusingatechnique
K-FOLD calledk-foldcross-validation. Theideaisthateachexampleservesdoubleduty—astraining
CROSS-VALIDATION
data and test data. First wesplit the data into k equal subsets. Wethen perform k rounds of
learning; on each round 1/k of the data is held out as a test set and the remaining examples
are used as training data. The average test set score of the k rounds should then be a better
estimate than a single score. Popular values for k are 5 and 10—enough to give an estimate
that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.
LEAVE-ONE-OUT Theextremeisk = n,alsoknownasleave-one-outcross-validation orLOOCV.
CROSS-VALIDATION
Despite the best efforts of statistical methodologists, users frequently invalidate their
LOOCV
results by inadvertently peeking at the test data. Peeking can happen like this: A learning
PEEKING
algorithmhasvarious“knobs”thatcanbetwiddledtotuneitsbehavior—forexample,various
different criteria for choosing the next attribute in decision tree learning. The researcher
generateshypotheses forvariousdifferentsettingsofthe knobs,measurestheirerrorrateson
thetestset,andreportstheerrorrateofthebesthypothesis. Alas,peekinghasoccurred! The
Section18.4. EvaluatingandChoosing theBestHypothesis 709
reasonisthatthehypothesiswasselectedonthebasisofitstestseterrorrate,soinformation
aboutthetestsethasleakedintothelearning algorithm.
Peekingisaconsequenceofusingtest-setperformancetobothchooseahypothesisand
evaluate it. The way to avoid this is to really hold the test set out—lock it away until you
are completely done with learning and simply wish to obtain an independent evaluation of
the finalhypothesis. (And then, if you don’t like the results ... you have toobtain, and lock
away, a completely new test set if you want to go back and find a better hypothesis.) If the
testsetislockedaway,butyoustillwanttomeasureperformanceonunseendataasawayof
selectingagoodhypothesis,thendividetheavailabledata(withoutthetestset)intoatraining
set and a validation set. The next section shows how to use validation sets to find a good
VALIDATIONSET
tradeoffbetweenhypothesis complexityandgoodness offit.
18.4.1 Model selection: Complexity versus goodnessoffit
InFigure18.1(page696)weshowedthathigher-degree polynomialscanfitthetrainingdata
better,butwhenthedegreeistoohightheywilloverfit,andperformpoorlyonvalidationdata.
Choosingthedegreeofthepolynomialisaninstanceoftheproblemofmodelselection. You
MODELSELECTION
can think ofthe task offinding the best hypothesis astwotasks: model selection defines the
hypothesis spaceandthenoptimization findsthebesthypothesis withinthatspace.
OPTIMIZATION
Inthis section weexplain how to select among models that are parameterized by size.
Forexample,withpolynomialswehavesize=1forlinearfunctions, size=2forquadratics,
and so on. Fordecision trees, the size could be the number of nodes in the tree. In all cases
wewanttofindthevalueofthesize parameterthatbestbalancesunderfitting andoverfitting
togivethebesttestsetaccuracy.
Analgorithm to perform model selection and optimization is shown in Figure 18.8. It
WRAPPER
is awrapperthat takes alearning algorithm as an argument (DECISION-TREE-LEARNING,
forexample). Thewrapperenumerates modelsaccording toaparameter, size. Foreachsize,
it uses cross validation on Learner to compute the average error rate on the training and
testsets. Westartwiththesmallest, simplest models (whichprobably underfitthedata), and
iterate, considering more complex models at each step, until the models start to overfit. In
Figure 18.9 we see typical curves: the training set error decreases monotonically (although
there may in general be slight random variation), while the validation set error decreases at
first, and then increases when the model begins to overfit. The cross-validation procedure
picksthevalueofsize withthelowestvalidationseterror;thebottomoftheU-shapedcurve.
Wethengenerate ahypothesis ofthat size,using allthedata(without holding outanyofit).
Finally,ofcourse,weshouldevaluatethereturned hypothesis onaseparatetestset.
Thisapproachrequiresthatthelearningalgorithmaccepta parameter, size,anddeliver
ahypothesisofthatsize. Aswesaid,fordecisiontreelearning,thesizecanbethenumberof
nodes. We can modify DECISION-TREE-LEARNER so that it takes the number of nodes as
an input, builds the tree breadth-first rather than depth-first (but at each level it still chooses
thehighestgainattribute first),andstopswhenitreachesthedesirednumberofnodes.
710 Chapter 18. LearningfromExamples
functionCROSS-VALIDATION-WRAPPER(Learner,k,examples)returnsahypothesis
localvariables: errT,anarray,indexedbysize,storingtraining-seterrorrates
errV,anarray,indexedbysize,storingvalidation-seterrorrates
forsize =1to∞do
errT[size],errV[size]←CROSS-VALIDATION(Learner,size,k,examples)
iferrT hasconvergedthendo
best size←thevalueofsize withminimumerrV[size]
return Learner(best size,examples)
functionCROSS-VALIDATION(Learner,size,k,examples)returnstwovalues:
averagetrainingseterrorrate,averagevalidationseterrorrate
fold errT ←0;fold errV ←0
forfold =1to k do
training set,validation set←PARTITION(examples,fold,k)
h←Learner(size,training set)
fold errT←fold errT +ERROR-RATE(h,training set)
fold errV ←fold errV +ERROR-RATE(h,validation set)
returnfold errT/k,fold errV/k
Figure18.8 Analgorithmtoselectthemodelthathasthelowesterrorrateonvalidation
data by building models of increasing complexity, and choosing the one with best empir-
ical error rate on validation data. Here errT means error rate on the training data, and
errV means error rate on the validation data. Learner(size,examples) returns a hypoth-
esis whose complexityis set by the parametersize, and which is trained on the examples.
PARTITION(examples,fold,k)splitsexamplesintotwosubsets: avalidationsetofsizeN/k
andatrainingsetwithalltheotherexamples.Thesplitisdifferentforeachvalueoffold.
18.4.2 From error rates to loss
So far, we have been trying to minimize error rate. This is clearly better than maximizing
error rate, but it is not the full story. Consider the problem of classifying email messages
as spam or non-spam. It is worse to classify non-spam as spam (and thus potentially miss
an important message) then to classify spam as non-spam (and thus suffer a few seconds of
annoyance). Soaclassifierwitha1%errorrate, wherealmost alltheerrorswereclassifying
spam as non-spam, would be better than a classifier with only a 0.5% error rate, if most of
thoseerrorswereclassifying non-spam asspam. WesawinChapter16thatdecision-makers
should maximize expected utility, and utility is what learners should maximize as well. In
machine learning it is traditional to express utilities by means of a loss function. The loss
LOSSFUNCTION
function L(x,y,yˆ) is defined as the amount of utility lost by predicting h(x)=yˆwhen the
correctansweris f(x)=y:
L(x,y,yˆ) = Utility(resultofusing y givenaninputx)
− Utility(resultofusing yˆgivenaninputx)
Section18.4. EvaluatingandChoosing theBestHypothesis 711
60
50
40
30
20
10
0
1 2 3 4 5 6 7 8 9 10
etar
rorrE
Validation Set Error
Training Set Error
Tree size
Figure18.9 Errorrateson training data (lower, dashed line) and validationdata (upper,
solidline)fordifferentsizedecisiontrees. We stopwhenthetrainingseterrorrateasymp-
totes,andthenchoosethetreewithminimalerroronthevalidationset;inthiscasethetree
ofsize7nodes.
Thisisthe mostgeneral formulation of theloss function. Oftenasimplified version is used,
L(y,yˆ), that is independent of x. We will use the simplified version for the rest of this
chapter, which means we can’t say that it is worse to misclassify a letter from Mom than it
is to misclassify a letter from our annoying cousin, but we can say it is 10 times worse to
classifynon-spam asspamthanvice-versa:
L(spam,nospam) = 1, L(nospam,spam) = 10.
NotethatL(y,y)isalwayszero; bydefinition thereisnoloss whenyouguess exactly right.
For functions with discrete outputs, we can enumerate a loss value for each possible mis-
classification, but we can’t enumerate all the possibilities for real-valued data. If f(x) is
137.035999, wewould be fairly happy with h(x) = 137.036, but just how happy should we
be? Ingeneralsmallerrors arebetterthanlargeones;twofunctions thatimplementthatidea
are the absolute value of the difference (called the L loss), and the square of the difference
1
(called the L loss). If we are content with the idea of minimizing error rate, we can use
2
the L loss function, which has a loss of 1 for an incorrect answer and is appropriate for
0/1
discrete-valued outputs:
Absolutevalueloss: L (y,yˆ) = |y−yˆ|
1
Squarederrorloss: L (y,yˆ) = (y−yˆ)2
2
0/1loss: L (y,yˆ)= 0ify = yˆ, else1
0/1
The learning agent can theoretically maximize its expected utility by choosing the hypoth-
esis that minimizes expected loss over all input–output pairs it will see. It is meaningless
to talk about this expectation without defining aprior probability distribution, P(X,Y)over
examples. Let E bethe set of all possible input–output examples. Thenthe expected gener-
GENERALIZATION alization lossforahypothesis h(withrespecttolossfunction L)is
LOSS
712 Chapter 18. LearningfromExamples
(cid:12)
GenLoss (h) = L(y,h(x))P(x,y) ,
L
(x,y)∈E
∗
andthebesthypothesis, h ,istheonewiththeminimumexpectedgeneralization loss:
∗
h = argminGenLoss (h).
L
h∈H
Because P(x,y)is notknown, thelearning agent canonly estimate generalization loss with
empiricallossonasetofexamples, E:
EMPIRICALLOSS
(cid:12)
1
EmpLoss (h) = L(y,h(x)).
L,E N
(x,y)∈E
Theestimatedbesthypothesis
hˆ∗
isthentheonewithminimumempiricalloss:
hˆ∗
= argminEmpLoss (h).
L,E
h∈H
Therearefourreasonswhy
hˆ∗
maydifferfromthetruefunction, f: unrealizability, variance,
noise, and computational complexity. First, f may not be realizable—may not be in H—or
may be present in such a way that other hypotheses are preferred. Second, a learning algo-
rithm will return different hypotheses for different sets of examples, even if those sets are
drawn from the same true function f, and those hypotheses will make different predictions
on new examples. Thehigher the variance among the predictions, the higher the probability
ofsignificant error. Notethat evenwhentheproblem isrealizable, therewillstill berandom
variance, but that variance decreases towards zero as the number of training examples in-
creases. Third, f may benondeterministic or noisy—it mayreturn different values for f(x)
NOISE
eachtimexoccurs. Bydefinition, noisecannotbepredicted; inmanycases,itarisesbecause
theobservedlabelsyaretheresultofattributesoftheenvironmentnotlistedinx. Andfinally,
whenHiscomplex, itcanbecomputationally intractable tosystematically searchthewhole
hypothesis space. The best we can do is a local search (hill climbing or greedy search) that
exploresonlypartofthespace. Thatgivesusanapproximationerror. Combiningthesources
oferror, we’releftwithanestimationofanapproximation ofthetruefunction f.
Traditional methods in statistics and the early years of machine learning concentrated
SMALL-SCALE on small-scale learning, where the number of training examples ranged from dozens to the
LEARNING
low thousands. Here the generalization error mostly comes from the approximation error of
nothavingthetruef inthehypothesisspace,andfromestimationerrorofnothavingenough
training examples to limit variance. In recent years there has been more emphasis on large-
LARGE-SCALE scale learning, often with millions of examples. Here the generalization error is dominated
LEARNING
bylimitsofcomputation: thereisenoughdataandarichenoughmodelthatwecouldfindan
h that is very close to the true f, but the computation to find it is too complex, so we settle
forasub-optimal approximation.
18.4.3 Regularization
InSection18.4.1,wesawhowtodomodelselectionwithcross-validation onmodelsize. An
alternativeapproachistosearchforahypothesisthatdirectlyminimizestheweightedsumof
Section18.5. TheTheoryofLearning 713
empiricallossandthecomplexity ofthehypothesis, whichwewillcallthetotalcost:
Cost(h) = EmpLoss(h)+λComplexity(h)
hˆ∗
= argminCost(h).
h∈H
Here λ is a parameter, a positive number that serves as a conversion rate between loss and
hypothesis complexity (which after all are not measured on the same scale). This approach
combines loss and complexity into one metric, allowing us to find the best hypothesis all at
once. Unfortunately we still need to do a cross-validation search to find the hypothesis that
generalizes best, but this time it is with different values of λ rather than size. We select the
valueofλthatgivesusthebestvalidation setscore.
Thisprocess ofexplicitly penalizing complex hypotheses iscalled regularization (be-
REGULARIZATION
causeitlooksforafunctionthatismoreregular,orlesscomplex). Notethatthecostfunction
requires us to make two choices: the loss function and the complexity measure, which is
called a regularization function. The choice of regularization function depends on the hy-
pothesis space. For example, a good regularization function for polynomials is the sum of
thesquaresofthecoefficients—keeping thesumsmallwouldguideusawayfromthewiggly
polynomialsinFigure18.1(b)and(c). Wewillshowanexampleofthistypeofregularization
inSection18.6.
Anotherwaytosimplifymodelsistoreducethedimensionsthatthemodelsworkwith.
Aprocess of feature selection can beperformed todiscard attributes thatappear tobeirrel-
FEATURESELECTION
evant. χ2 pruning isakindoffeatureselection.
It is in fact possible to have the empirical loss and the complexity measured on the
samescale, without theconversion factor λ: theycan bothbemeasured inbits. Firstencode
the hypothesis as a Turing machine program, and count the number of bits. Then count
the number of bits required to encode the data, where a correctly predicted example costs
zero bits and the cost ofan incorrectly predicted example depends on how large the error is.
MINIMUM
The minimum description length or MDL hypothesis minimizes the total number of bits
DESCRIPTION
LENGTH
required. This works well in the limit, but for smaller problems there is a difficulty in that
the choice of encoding for the program—for example, how best to encode a decision tree
as a bit string—affects the outcome. In Chapter 20 (page 805), we describe a probabilistic
interpretation oftheMDLapproach.
18.5 THE THEORY OF LEARNING
The main unanswered question in learning is this: How can we be sure that our learning
algorithm hasproduced ahypothesis thatwillpredictthecorrectvalueforpreviously unseen
inputs? Informalterms,howdoweknowthatthehypothesis hisclosetothetargetfunction
f if we don’t know what f is? These questions have been pondered for several centuries.
In more recent decades, other questions have emerged: how many examples do we need
to get a good h? What hypothesis space should we use? If the hypothesis space is very
complex, can we even find the best h, or do we have to settle for a local maximum in the
714 Chapter 18. LearningfromExamples
spaceofhypotheses? Howcomplexshouldhbe? Howdoweavoidoverfitting? Thissection
examinesthesequestions.
We’ll start with the question of how many examples are needed for learning. We saw
from the learning curve fordecision tree learning on the restaurant problem (Figure 18.7 on
page 703) that improves with more training data. Learning curves are useful, but they are
specifictoaparticularlearningalgorithm onaparticularproblem. Aretheresomemoregen-
eralprinciples governing thenumberofexamples needed ingeneral? Questions likethis are
COMPUTATIONAL addressed bycomputationallearningtheory, whichliesattheintersection ofAI,statistics,
LEARNINGTHEORY
andtheoreticalcomputerscience. Theunderlyingprincipleisthatanyhypothesisthatisseri-
ouslywrongwillalmostcertainly be“foundout”withhighprobability afterasmallnumber
ofexamples,becauseitwillmakeanincorrectprediction. Thus,anyhypothesisthatisconsis-
tentwithasufficientlylargesetoftrainingexamplesisunlikelytobeseriouslywrong: thatis,
PROBABLY
it must be probably approximately correct. Any learning algorithm that returns hypotheses
APPROXIMATELY
CORRECT
that areprobably approximately correct iscalled a PAClearningalgorithm; wecanuse this
PACLEARNING
approach toprovidebounds ontheperformance ofvariouslearning algorithms.
PAC-learning theorems, like all theorems, are logical consequences of axioms. When
a theorem (as opposed to, say, a political pundit) states something about the future based on
the past, the axioms have to provide the “juice” to make that connection. ForPAClearning,
the juice isprovided bythe stationarity assumption introduced onpage 708, whichsays that
future examples are going to be drawn from the same fixed distribution P(E)=P(X,Y)
as past examples. (Note that we do not have to know what distribution that is, just that it
doesn’t change.) In addition, to keep things simple, we will assume that the true function f
isdeterministic andisamemberofthehypothesis class Hthatisbeingconsidered.
The simplest PAC theorems deal with Boolean functions, for which the 0/1 loss is ap-
propriate. The error rate of a hypothesis h, defined informally earlier, is defined formally
hereastheexpectedgeneralization errorforexamplesdrawnfromthestationarydistribution:
(cid:12)
error(h) = GenLoss (h) = L (y,h(x))P(x,y) .
L0/1 0/1
x,y
In other words, error(h) is the probability that h misclassifies a new example. This is the
samequantitybeingmeasuredexperimentally bythelearning curvesshownearlier.
A hypothesis h is called approximately correct if error(h) ≤ (cid:2), where (cid:2) is a small
constant. Wewillshow thatwecanfindanN such that, afterseeing N examples, withhigh
probability, all consistent hypotheses will be approximately correct. One can think of an
approximately correcthypothesisasbeing“close”tothetruefunctioninhypothesisspace: it
(cid:2) liesinside whatiscalled the(cid:2)-ballaround thetruefunction f. Thehypothesis spaceoutside
-BALL
thisballiscalledH .
bad
We can calculate the probability that a “seriously wrong” hypothesis h ∈ H is
b bad
consistent with the first N examples as follows. We know that error(h ) > (cid:2). Thus, the
b
probability that it agrees with a given example is at most 1 − (cid:2). Since the examples are
independent, theboundforN examplesis
P(h agreeswithN examples) ≤ (1−(cid:2))N .
b
Section18.5. TheTheoryofLearning 715
The probability that H contains at least one consistent hypothesis is bounded by the sum
bad
oftheindividual probabilities:
P(H contains aconsistent hypothesis)≤ |H |(1−(cid:2))N ≤ |H|(1−(cid:2))N ,
bad bad
where we have used the fact that |H | ≤ |H|. We would like to reduce the probability of
bad
thiseventbelowsomesmallnumberδ:
|H|(1−(cid:2))N ≤ δ .
Giventhat1−(cid:2) ≤ e −(cid:2),wecanachievethisifweallowthealgorithm tosee
(cid:13) (cid:14)
1 1
N ≥ ln +ln|H| (18.1)
(cid:2) δ
examples. Thus,ifalearningalgorithm returnsahypothesis thatisconsistentwiththismany
examples, then with probability at least 1 − δ, it has error at most (cid:2). In other words, it is
probably approximately correct. Thenumberofrequired examples, asafunction of(cid:2) andδ,
SAMPLE iscalledthesamplecomplexityofthehypothesis space.
COMPLEXITY
As we saw earlier, if H is the set of all Boolean functions on n attributes, then |H| =
22n . Thus, thesample complexity ofthespace growsas 2n. Becausethenumberofpossible
examples is also 2n, this suggests that PAC-learning in the class of all Boolean functions
requires seeing all, or nearly all, of the possible examples. A moment’s thought reveals the
reason for this: H contains enough hypotheses to classify any given set of examples in all
possibleways. Inparticular, foranysetof N examples,thesetofhypothesesconsistent with
those examples contains equal numbers of hypotheses that predict x to be positive and
N+1
hypotheses thatpredict x tobenegative.
N+1
To obtain real generalization to unseen examples, then, it seems we need to restrict
the hypothesis space H in some way; but of course, if we do restrict the space, we might
eliminatethetruefunctionaltogether. Therearethreewaystoescapethisdilemma. Thefirst,
which wewillcoverin Chapter19, is tobring priorknowledge tobear onthe problem. The
second, which we introduced in Section 18.4.3, is to insist that the algorithm return not just
anyconsistenthypothesis,butpreferablyasimpleone(asisdoneindecisiontreelearning). In
cases where finding simple consistent hypotheses istractable, the sample complexity results
are generally better than for analyses based only on consistency. The third escape, which
we pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean
functions. This approach relies on the assumption that the restricted language contains a
hypothesis h that is close enough to the true function f; the benefits are that the restricted
hypothesisspaceallowsforeffectivegeneralization andistypicallyeasiertosearch. Wenow
examineonesuchrestricted language inmoredetail.
18.5.1 PAClearning example: Learning decisionlists
We now show how to apply PAC learning to a new hypothesis space: decision lists. A
DECISIONLISTS
decision list consists of a series of tests, each of which is a conjunction of literals. If a
test succeeds when applied to an example description, the decision list specifies the value
to be returned. If the test fails, processing continues with the next test in the list. Decision
lists resemble decision trees, but their overall structure is simpler: they branch only in one
716 Chapter 18. LearningfromExamples
No No
Patrons(x, Some) Patrons(x, Full) ^ Fri/Sat(x) No
Yes Yes
Yes Yes
Figure18.10 Adecisionlistfortherestaurantproblem.
direction. In contrast, the individual tests are more complex. Figure 18.10 shows a decision
listthatrepresents thefollowinghypothesis:
WillWait ⇔ (Patrons = Some)∨(Patrons =Full ∧Fri/Sat).
If we allow tests of arbitrary size, then decision lists can represent any Boolean function
(Exercise 18.14). On the other hand, if we restrict the size of each test to at most k literals,
then itis possible forthe learning algorithm to generalize successfully from asmall number
k-DL ofexamples. Wecallthislanguagek-DL. TheexampleinFigure18.10isin2-DL.Itiseasyto
k-DT show(Exercise18.14)thatk-DLincludesasasubsetthelanguagek-DT,thesetofalldecision
trees ofdepth at most k. Itis important to remember that the particular language referred to
by k-DL depends on the attributes used to describe the examples. We will use the notation
k-DL(n)todenoteak-DLlanguage usingnBooleanattributes.
The first task is to show that k-DL is learnable—that is, that any function in k-DL can
be approximated accurately after training on a reasonable number of examples. To do this,
we need to calculate the number of hypotheses in the language. Let the language of tests—
conjunctions ofatmostk literals using nattributes—be Conj(n,k). Because adecision list
isconstructed oftests,andbecauseeachtestcanbeattachedtoeitheraYes oraNo outcome
orcanbeabsentfromthedecisionlist,thereareatmost3
|Conj(n,k)|
distinctsetsofcomponent
tests. Eachofthesesetsoftestscanbeinanyorder, so
|k-DL(n)| ≤ 3 |Conj(n,k)||Conj(n,k)|!.
Thenumberofconjunctions ofk literalsfrom nattributes isgivenby
(cid:13) (cid:14)
(cid:12)k
2n
|Conj(n,k)| = = O(nk).
i
i=0
Hence,aftersomework,weobtain
|k-DL(n)| = 2O(nklog2(nk)) .
WecanplugthisintoEquation (18.1)toshowthatthenumberofexamplesneeded forPAC-
learning ak-DLfunction ispolynomial inn:
(cid:13) (cid:14)
1 1
N ≥ ln +O(nklog (nk)) .
(cid:2) δ 2
Therefore,anyalgorithmthatreturnsaconsistentdecisionlistwillPAC-learnak-DLfunction
inareasonable numberofexamples,forsmall k.
The next task is to find an efficient algorithm that returns a consistent decision list.
We will use a greedy algorithm called DECISION-LIST-LEARNING that repeatedly finds a
Section18.6. RegressionandClassification withLinearModels 717
functionDECISION-LIST-LEARNING(examples)returnsadecisionlist,orfailure
ifexamples isemptythenreturnthetrivialdecisionlistNo
t←atestthatmatchesanonemptysubsetexamples ofexamples
t
suchthatthemembersofexamples areallpositiveorallnegative
t
ifthereisnosucht thenreturnfailure
iftheexamplesinexamples arepositivetheno←Yes elseo←No
t
returnadecisionlistwithinitialtestt andoutcomeo andremainingtestsgivenby
DECISION-LIST-LEARNING(examples − examples
t
)
Figure18.11 Analgorithmforlearningdecisionlists.
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20 40 60 80 100
tes
tset
no
tcerroc
noitroporP
Decision tree
Decision list
Training set size
Figure18.12 LearningcurveforDECISION-LIST-LEARNINGalgorithmontherestaurant
data. ThecurveforDECISION-TREE-LEARNINGisshownforcomparison.
test that agrees exactly with some subset of the training set. Once it finds such a test, it
adds it to the decision list under construction and removes the corresponding examples. It
thenconstructs theremainderofthedecision list, using justtheremaining examples. Thisis
repeated untiltherearenoexamplesleft. Thealgorithm isshowninFigure18.11.
This algorithm does not specify the method for selecting the next test to add to the
decisionlist. Althoughtheformalresultsgivenearlierdonotdependontheselectionmethod,
it would seem reasonable to prefer small tests that match large sets of uniformly classified
examples,sothattheoveralldecisionlistwillbeascompactaspossible. Thesimpleststrategy
istofindthesmallesttesttthatmatchesanyuniformlyclassifiedsubset,regardlessofthesize
ofthesubset. Eventhisapproach worksquitewell,asFigure 18.12suggests.
18.6 REGRESSION AND CLASSIFICATION WITH LINEAR MODELS
Now it is time to move on from decision trees and lists to a different hypothesis space, one
that has been used for hundred of years: the class of linear functions of continuous-valued
LINEARFUNCTION
718 Chapter 18. LearningfromExamples
1000
900
800
700
600
500
400
300
500 1000 1500 2000 2500 3000 3500
0001$
ni
ecirp
esuoH
Loss
w
0
w
1
House size in square feet
(a) (b)
Figure 18.13 (a) Data points of price versus floor space of houses for sale in Berkeley,
CA, in July 2009, along with the linear function hypot(cid:2)hesis that minimizes squared error
loss: y = 0.232x+246. (b) Plot of the loss function
j
(w
1
xj +w
0
−yj)2 for various
valuesofw ,w . Notethatthelossfunctionisconvex,withasingleglobalminimum.
0 1
inputs. We’ll start with the simplest case: regression with a univariate linear function, oth-
erwise known as “fitting a straight line.” Section 18.6.2 covers the multivariate case. Sec-
tions 18.6.3 and 18.6.4 show how to turn linear functions into classifiers by applying hard
andsoftthresholds.
18.6.1 Univariatelinearregression
Aunivariatelinearfunction(astraightline)withinputxandoutputyhastheformy=w x+
1
w ,where w and w arereal-valued coefficients tobelearned. Weusethe letter w because
0 0 1
we think of the coefficients as weights; the value of y is changed by changing the relative
WEIGHT
weightofonetermoranother. We’lldefine wtobethevector[w ,w ],anddefine
0 1
h (x)=w x+w .
w 1 0
Figure 18.13(a) shows an example of a training set of n points in the x,y plane, each point
representing the size in square feet and the price of a house offered for sale. The task of
findingtheh thatbestfitsthesedataiscalled linearregression. Tofitalinetothedata,all
LINEARREGRESSION w
wehavetodoisfindthevaluesoftheweights[w ,w ]thatminimizetheempiricalloss. Itis
0 1
traditional (going back to Gauss3)to use the squared loss function, L , summed overall the
2
training examples:
(cid:12)N (cid:12)N (cid:12)N
Loss(h )= L (y ,h (x )) = (y −h (x ))2 = (y −(w x +w ))2 .
w 2 j w j j w j j 1 j 0
j=1 j=1 j=1
3 Gaussshowedthatiftheyj valueshavenormallydistributednoise,thenthemostlikelyvaluesofw1andw0
areobtainedbyminimizingthesumofthesquaresoftheerrors.
Section18.6. RegressionandClassification withLinearModels 719
(cid:2)
We would like to find w ∗ = argmin Loss(h ). The sum N (y − (w x + w ))2 is
w w j=1 j 1 j 0
minimizedwhenitspartialderivativeswithrespectto w andw arezero:
0 1
(cid:12)N (cid:12)N
∂ ∂
(y −(w x +w ))2 = 0and (y −(w x +w ))2 = 0. (18.2)
j 1 j 0 j 1 j 0
∂w ∂w
0 1
j=1 j=1
Theseequations haveauniquesolution:
(cid:2) (cid:2) (cid:2)
(cid:12) (cid:12)
N( x y )−( x )( y )
w = (cid:2)j j (cid:2)j j ; w =( y −w ( x ))/N . (18.3)
1 N( x2)−( x )2 0 j 1 j
j j
Forthe example in Figure 18.13(a), the solution is w =0.232, w = 246, and the line with
1 0
thoseweightsisshownasadashedlineinthefigure.
Many forms of learning involve adjusting weights to minimize a loss, so it helps to
haveamentalpicture ofwhat’sgoingoninweightspace—the space definedbyallpossible
WEIGHTSPACE
settings of the weights. Forunivariate linear regression, the weight space defined by w and
0
w istwo-dimensional, sowecangraphthelossasafunction ofw andw ina3Dplot(see
1 0 1
Figure18.13(b)). Weseethatthelossfunction isconvex,asdefinedonpage133;thisistrue
for every linear regression problem with an L loss function, and implies that there are no
2
local minima. In some sense that’s the end of the story for linear models; if we need to fit
linestodata,weapplyEquation(18.3).4
To go beyond linear models, we will need to face the fact that the equations defining
minimum loss (as in Equation (18.2)) will often have no closed-form solution. Instead, we
will face a general optimization search problem in a continuous weight space. As indicated
inSection 4.2(page 129), such problems can beaddressed by a hill-climbing algorithm that
follows the gradient of the function to be optimized. In this case, because we are trying to
minimize the loss, we will use gradient descent. We choose any starting point in weight
GRADIENTDESCENT
space—here, a point in the (w , w ) plane—and then move to a neighboring point that is
0 1
downhill,repeating untilweconverge ontheminimumpossibleloss:
w ← anypointintheparameterspace
loopuntilconvergence do
foreachw inwdo
i
∂
w ← w −α Loss(w) (18.4)
i i
∂w
i
Theparameter α,whichwecalledthestepsizeinSection4.2,isusually called thelearning
ratewhenwearetryingtominimizelossinalearningproblem. Itcanbeafixedconstant,or
LEARNINGRATE
itcandecayovertimeasthelearning processproceeds.
Forunivariateregression, thelossfunctionisaquadratic function, sothepartialderiva-
tive will be a linear function. (The only calculus you need to know is that ∂ x2=2x and
∂x
∂ x=1.) Let’s first work out the partial derivatives—the slopes—in the simplified case of
∂x
4 Withsomecaveats: theL2 lossfunctionisappropriatewhenthereisnormally-distributednoisethatisinde-
pendentofx;allresultsrelyonthestationarityassumption;etc.
720 Chapter 18. LearningfromExamples
onlyonetraining example, (x,y):
∂ ∂
Loss(w) = (y−h (x))2
w
∂w ∂w
i i
∂
= 2(y−h (x))× (y−h (x))
w w
∂w
i
∂
= 2(y−h (x))× (y−(w x+w )), (18.5)
w 1 0
∂w
i
applying thistobothw andw weget:
0 1
∂ ∂
Loss(w)= −2(y−h (x)); Loss(w) = −2(y−h (x))×x
w w
∂w ∂w
0 1
Then,pluggingthisbackintoEquation(18.4),andfoldingthe2intotheunspecifiedlearning
rateα,wegetthefollowinglearning rulefortheweights:
w ← w +α(y−h (x)); w ← w +α(y−h (x))×x
0 0 w 1 1 w
These updates make intuitive sense: if h (x) > y, i.e., the output of the hypothesis is too
w
large, reduce w a bit, and reduce w if x was a positive input but increase w if x was a
0 1 1
negativeinput.
Theprecedingequationscoveronetrainingexample. ForN trainingexamples,wewant
tominimizethesumoftheindividuallossesforeachexample. Thederivativeofasumisthe
sumofthederivatives, sowehave:
(cid:12) (cid:12)
w ← w +α (y −h (x )); w ← w +α (y −h (x ))×x .
0 0 j w j 1 1 j w j j
j j
BATCHGRADIENT These updates constitute the batch gradient descent learning rule for univariate linear re-
DESCENT
gression. Convergence to the unique global minimum is guaranteed (as long as we pick α
smallenough) butmaybeveryslow: wehavetocyclethrough allthetraining dataforevery
step,andtheremaybemanysteps.
STOCHASTIC There is another possibility, called stochastic gradient descent, where we consider
GRADIENTDESCENT
only a single training point at a time, taking a step after each one using Equation (18.5).
Stochastic gradient descent can be used in an online setting, where new data are coming in
one at a time, or offline, where we cycle through the same data as many times as is neces-
sary,takingastepafterconsideringeachsingleexample. Itisoftenfasterthanbatchgradient
descent. Withafixedlearning rate α, however, itdoes not guarantee convergence; itcan os-
cillatearoundtheminimumwithoutsettlingdown. Insomecases,asweseelater,aschedule
ofdecreasing learning rates(asinsimulatedannealing) doesguarantee convergence.
18.6.2 Multivariatelinearregression
MULTIVARIATE Wecaneasilyextendtomultivariatelinearregression problems, inwhicheachexamplex
LINEARREGRESSION j
isann-elementvector.5 Ourhypothesis spaceisthesetoffunctions oftheform
(cid:12)
h (x ) = w +w x +···+w x = w + w x .
sw j 0 1 j,1 n j,n 0 i j,i
i
5 ThereadermaywishtoconsultAppendixAforabriefsummaryoflinearalgebra.
Section18.6. RegressionandClassification withLinearModels 721
Thew term,theintercept,standsoutasdifferentfromtheothers. Wecanfixthatbyinventing
0
a dummy input attribute, x , which is defined as always equal to 1. Then h is simply the
j,0
dot product of the weights and the input vector (or equivalently, the matrix product of the
transpose oftheweightsandtheinputvector):
(cid:12)
h (x ) = w·x = w (cid:12) x = w x .
sw j j j i j,i
i
∗
Thebestvectorofweights, w ,minimizessquared-error lossovertheexamples:
(cid:12)
w ∗ = argmin L (y ,w·x ).
2 j j
w
j
Multivariatelinearregressionisactuallynotmuchmorecomplicatedthantheunivariatecase
wejust covered. Gradient descent willreach the(unique) minimum oftheloss function; the
updateequation foreachweightw is
(cid:12) i
w ← w +α x (y −h (x )). (18.6)
i i j,i j w j
j
It is also possible to solve analytically for the w that minimizes loss. Let y be the vector of
outputs for the training examples, and X be the data matrix, i.e., the matrix of inputs with
DATAMATRIX
onen-dimensional exampleperrow. Thenthesolution
w
∗
= (X
(cid:12)
X)
−1X (cid:12)
y
minimizesthesquared error.
With univariate linear regression we didn’t have to worry about overfitting. But with
multivariate linear regression in high-dimensional spaces it is possible that some dimension
thatisactually irrelevant appearsbychancetobeuseful,resulting inoverfitting.
Thus,itiscommontouseregularizationonmultivariatelinearfunctionstoavoidover-
fitting. Recall that with regularization we minimize the total cost of a hypothesis, counting
boththeempiricallossandthecomplexity ofthehypothesis:
Cost(h) = EmpLoss(h)+λComplexity(h).
For linear functions the complexity can be specified as a function of the weights. We can
considerafamilyofregularization functions:
(cid:12)
Complexity(h ) =L (w) = |w |q .
w q i
i
As with loss functions,6 with q=1 we have L regularization, which minimizes the sum of
1
theabsolute values; withq=2, L regularization minimizes thesum ofsquares. Whichreg-
2
ularization function should you pick? Thatdepends onthe specific problem, but L regular-
1
ization hasanimportant advantage: ittendstoproduce a sparsemodel. Thatis,itoften sets
SPARSEMODEL
manyweightstozero,effectivelydeclaringthecorresponding attributestobeirrelevant—just
as DECISION-TREE-LEARNING does(although byadifferent mechanism). Hypotheses that
discardattributes canbeeasierforahumantounderstand, andmaybelesslikelytooverfit.
6 ItisperhapsconfusingthatL1andL2areusedforbothlossfunctionsandregularizationfunctions.Theyneed
notbeusedinpairs:youcoulduseL2losswithL1regularization,orviceversa.
722 Chapter 18. LearningfromExamples
w w
2 2
w*
w*
w w
1 1
Figure18.14 WhyL regularizationtendstoproduceasparsemodel. (a)With L regu-
1 1
larization(box),the minimalachievableloss(concentriccontours)oftenoccurson anaxis,
meaninga weightof zero. (b)With L regularization(circle), the minimalloss islikely to
2
occuranywhereonthecircle,givingnopreferencetozeroweights.
Figure18.14givesanintuitiveexplanationofwhyL regularizationleadstoweightsof
1
zero, while L regularization does not. Note that minimizing Loss(w)+λComplexity(w)
2
is equivalent to minimizing Loss(w) subject to the constraint that Complexity(w) ≤ c, for
someconstant cthatisrelated to λ. Now,inFigure 18.14(a) thediamond-shaped box repre-
sents theset of points win two-dimensional weight space that have L complexity less than
1
c; our solution will have to be somewhere inside this box. The concentric ovals represent
contours ofthelossfunction, withtheminimumlossatthecenter. Wewanttofindthepoint
intheboxthatisclosest totheminimum;youcanseefromthediagram that,foranarbitrary
positionoftheminimumanditscontours, itwillbecommonforthecorneroftheboxtofind
itswayclosesttotheminimum,justbecausethecornersarepointy. Andofcoursethecorners
are the points that have a value of zero in some dimension. In Figure 18.14(b), we’ve done
the same for the L complexity measure, which represents a circle rather than a diamond.
2
Here you can see that, in general, there is no reason for the intersection to appear on one of
theaxes; thusL regularization does nottend toproduce zeroweights. Theresult isthat the
2
numberofexamplesrequiredtofindagoodhislinearinthenumberofirrelevantfeaturesfor
L regularization, but only logarithmic with L regularization. Empirical evidence onmany
2 1
problemssupports thisanalysis.
AnotherwaytolookatitisthatL regularization takesthedimensionalaxesseriously,
1
while L treats them as arbitrary. The L function is spherical, which makes it rotationally
2 2
invariant: Imagine a set of points in a plane, measured by their x and y coordinates. Now
imagine rotating the axes by 45o. You’d get a different set of (x (cid:2) ,y (cid:2) ) values representing
the same points. If you apply L regularization before and after rotating, you get exactly
2
(cid:2) (cid:2)
the same point as the answer (although the point would be described with the new (x,y )
coordinates). Thatisappropriate whenthechoiceofaxesreallyisarbitrary—when itdoesn’t
matterwhetheryourtwodimensions aredistancesnorthandeast;ordistancesnorth-east and
Section18.6. RegressionandClassification withLinearModels 723
south-east. WithL regularizationyou’dgetadifferentanswer,becausetheL functionisnot
1 1
rotationally invariant. That is appropriate when the axes are not interchangeable; it doesn’t
makesensetorotate“numberofbathrooms” 45o towards“lotsize.”
18.6.3 Linearclassifiers witha hardthreshold
Linear functions can be used to do classification as well as regression. For example, Fig-
ure18.15(a)showsdatapointsoftwoclasses: earthquakes(whichareofinteresttoseismolo-
gists)andunderground explosions (whichareofinterestto armscontrolexperts). Eachpoint
is defined by two input values, x and x , that refer to body and surface wave magnitudes
1 2
computed from the seismic signal. Given these training data, the task of classification is to
learn ahypothesis hthat willtake new (x ,x )points andreturn either 0forearthquakes or
1 2
1forexplosions.
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
4.5 5 5.5 6 6.5 7
x
2
7.5
7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
4.5 5 5.5 6 6.5 7
x
1
x
2
x
1
(a) (b)
Figure18.15 (a)Plotoftwoseismicdataparameters,bodywavemagnitudex andsur-
1
face wave magnitudex , forearthquakes(white circles) andnuclearexplosions(blackcir-
2
cles)occurringbetween1982and1990inAsiaandtheMiddleEast(Kebeasyetal.,1998).
Alsoshownisadecisionboundarybetweentheclasses. (b)Thesamedomainwithmoredata
points.Theearthquakesandexplosionsarenolongerlinearlyseparable.
DECISION A decision boundary is a line (or a surface, in higher dimensions) that separates the
BOUNDARY
two classes. In Figure 18.15(a), the decision boundary is a straight line. A linear decision
boundaryiscalledalinearseparatoranddatathatadmitsuchaseparatorarecalled linearly
LINEARSEPARATOR
LINEAR separable. Thelinearseparatorinthiscaseisdefinedby
SEPARABILITY
x = 1.7x −4.9 or −4.9+1.7x −x = 0.
2 1 1 2
Theexplosions,whichwewanttoclassifywithvalue1,aretotherightofthislinewithhigher
values of x and lower values of x , so they are points for which −4.9+1.7x −x > 0,
1 2 1 2
while earthquakes have −4.9 + 1.7x − x < 0. Using the convention of a dummy input
1 2
x =1,wecanwritetheclassification hypothesis as
0
h (x)= 1ifw·x ≥ 0and0otherwise.
w
724 Chapter 18. LearningfromExamples
Alternatively, we can think of h as the result of passing the linear function w·x through a
THRESHOLD thresholdfunction:
FUNCTION
h (x)= Threshold(w·x)whereThreshold(z)=1ifz ≥0and0otherwise.
w
Thethreshold function isshowninFigure18.17(a).
Now that the hypothesis h (x) has a well-defined mathematical form, we can think
w
about choosing the weights w to minimize the loss. In Sections 18.6.1 and 18.6.2, we did
this both in closed form (by setting the gradient to zero and solving for the weights) and
by gradient descent in weight space. Here, we cannot do either of those things because the
gradient is zero almost everywhere in weight space except at those points where w·x=0,
andatthosepointsthegradientisundefined.
There is, however, a simple weight update rule that converges to a solution—that is, a
linearseparatorthatclassifiesthedataperfectly–provided thedataarelinearlyseparable. For
asingleexample(x,y),wehave
w ← w +α(y−h (x))×x (18.7)
i i w i
whichisessentiallyidenticaltotheEquation(18.6),theupdateruleforlinearregression! This
PERCEPTRON ruleiscalledtheperceptronlearningrule,forreasonsthatwillbecomeclearinSection18.7.
LEARNINGRULE
Because weareconsidering a0/1classification problem, however, thebehavior issomewhat
different. Boththetruevalue yandthehypothesisoutputh (x)areeither0or1,sothereare
w
threepossibilities:
• Iftheoutputiscorrect, i.e.,y=h (x),thentheweightsarenotchanged.
w
• Ifyis1buth (x)is0,thenw isincreasedwhenthecorresponding inputx ispositive
w i i
and decreased when x is negative. This makes sense, because we want to make w·x
i
biggersothath (x)outputsa1.
w
• Ifyis0buth (x)is1,thenw isdecreasedwhenthecorrespondinginputx ispositive
w i i
and increased when x is negative. This makes sense, because we want to make w·x
i
smallersothat h (x)outputsa0.
w
Typically the learning rule is applied one example at a time, choosing examples at random
(as in stochastic gradient descent). Figure 18.16(a) shows atraining curve forthis learning
TRAININGCURVE
rule applied to the earthquake/explosion data shown in Figure 18.15(a). A training curve
measures the classifier performance on a fixed training set as the learning process proceeds
on that same training set. The curve shows the update rule converging to a zero-error linear
separator. The“convergence”processisn’texactlypretty,butitalwaysworks. Thisparticular
runtakes657stepstoconverge,foradatasetwith63examples,soeachexampleispresented
roughly 10timesonaverage. Typically, thevariation acrossrunsisverylarge.
We have said that the perceptron learning rule converges to a perfect linear separator
when the data points are linearly separable, but what if they are not? This situation is all
too common in the real world. For example, Figure 18.15(b) adds back in the data points
left out by Kebeasy et al. (1998) when they plotted the data shown in Figure 18.15(a). In
Figure 18.16(b), we show the perceptron learning rule failing to converge even after 10,000
steps: even though it hits the minimum-error solution (three errors) many times, the algo-
rithm keeps changing the weights. In general, the perceptron rule may not converge to a
Section18.6. RegressionandClassification withLinearModels 725
1
0.9
0.8
0.7
0.6
0.5
0.4
0 100 200 300 400 500 600 700
tcerroc
noitroporP
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20000 40000 60000 80000 100000
Number of weight updates
tcerroc
noitroporP
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20000 40000 60000 80000 100000
Number of weight updates
tcerroc
noitroporP
Number of weight updates
(a) (b) (c)
Figure 18.16 (a) Plot of total training-setaccuracyvs. numberof iterationsthroughthe
training set for the perceptron learning rule, given the earthquake/explosion data in Fig-
ure 18.15(a). (b) The same plot for the noisy, non-separable data in Figure 18.15(b); note
thechangeinscale ofthex-axis. (c)Thesame plotasin(b),witha learningrateschedule
α(t)=1000/(1000+t).
stable solution for fixed learning rate α, but if α decays as O(1/t) where t is the iteration
number,thentherulecanbeshowntoconverge toaminimum-errorsolution whenexamples
are presented in a random sequence.7 It can also be shown that finding the minimum-error
solution isNP-hard,sooneexpectsthatmanypresentations oftheexampleswillberequired
for convergence to be achieved. Figure 18.16(b) shows the training process with a learning
rate schedule α(t)=1000/(1000 + t): convergence is not perfect after 100,000 iterations,
butitismuchbetterthanthefixed-αcase.
18.6.4 Linearclassificationwithlogisticregression
We have seen that passing the output of a linear function through the threshold function
creates a linear classifier; yet the hard nature of the threshold causes some problems: the
hypothesis h (x)isnotdifferentiable andisinfactadiscontinuous function ofitsinputsand
w
itsweights;thismakeslearningwiththeperceptronruleaveryunpredictable adventure. Fur-
thermore, the linear classifier always announces acompletely confident prediction of 1or0,
even for examples that are very close to the boundary; in many situations, we really need
moregradatedpredictions.
Alloftheseissuescanberesolvedtoalargeextentbysofteningthethresholdfunction—
approximating the hard threshold with a continuous, differentiable function. In Chapter 14
(page 522), we saw two functions that look like soft thresholds: the integral of the standard
normal distribution (used for the probit model) and the logistic function (used for the logit
model). Although thetwofunctions areverysimilarinshape, thelogistic function
1
Logistic(z) =
1+e−z
P P
7 Technically, we require that ∞ α(t)=∞ and ∞ α2(t) < ∞. The decay α(t)=O(1/t) satisfies
t=1 t=1
theseconditions.
726 Chapter 18. LearningfromExamples
1 1
1
0.8
0.6
0.5 0.5 0.4
0.2 -4
-2
0 -2 0 x 2 4 6 10 8 6 4 2 0 x 2
1
0 0
-8 -6 -4 -2 0 2 4 6 8 -6 -4 -2 0 2 4 6
(a) (b) (c)
Figure 18.17 (a) The hard threshold function Threshold(z) with 0/1 output. Note
that the function is nondifferentiable at z=0. (b) The logistic function, Logistic(z) =
1 , also known as the sigmoid function. (c) Plot of a logistic regression hypothesis
1+e−z
h
w
(x)=Logistic(w·x)forthedatashowninFigure18.15(b).
has more convenient mathematical properties. The function is shown in Figure 18.17(b).
Withthelogisticfunction replacing thethreshold function, wenowhave
1
h (x)= Logistic(w·x) = .
w 1+e−w·x
Anexampleofsuchahypothesisforthetwo-inputearthquake/explosion problemisshownin
Figure 18.17(c). Notice that the output, being anumberbetween 0and 1, can beinterpreted
as a probability of belonging to the class labeled 1. The hypothesis forms a soft boundary
in the input space and gives a probability of 0.5 for any input at the center of the boundary
region,andapproaches 0or1aswemoveawayfromtheboundary.
Theprocess offittingtheweights ofthis modeltominimize lossonadatasetiscalled
LOGISTIC logisticregression. Thereisnoeasyclosed-formsolutiontofindtheoptimalvalueofwwith
REGRESSION
thismodel,butthegradient descent computation isstraightforward. Becauseourhypotheses
no longer output just 0 or 1, we will use the L loss function; also, to keep the formulas
2
(cid:2)
readable, we’lluseg tostandforthelogisticfunction, withg itsderivative.
For a single example (x,y), the derivation of the gradient is the same as for linear
regression (Equation (18.5)) up to the point where the actual form of his inserted. (Forthis
(cid:2)
derivation, wewillneedthechainrule: ∂g(f(x))/∂x=g (f(x))∂f(x)/∂x.) Wehave
CHAINRULE
∂ ∂
Loss(w) = (y−h (x))2
w
∂w ∂w
i i
∂
= 2(y−h (x))× (y−h (x))
w w
∂w
i
∂
= −2(y−h (x))×g (cid:2) (w·x)× w·x
w
∂w
i
= −2(y−h (x))×g (cid:2) (w·x)×x .
w i
Section18.7. ArtificialNeuralNetworks 727
1
0.9
0.8
0.7
0.6
0.5
0.4
0 1000 2000 3000 4000 5000
elpmaxe
rep
rorre
derauqS
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20000 40000 60000 80000 100000
Number of weight updates
elpmaxe
rep
rorre
derauqS
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20000 40000 60000 80000 100000
Number of weight updates
elpmaxe
rep
rorre
derauqS
Number of weight updates
(a) (b) (c)
Figure 18.18 Repeat of the experiments in Figure 18.16 using logistic regression and
squarederror. Theplotin(a)covers5000iterationsrather than1000,while(b)and(c)use
thesamescale.
Thederivative g (cid:2) ofthelogistic functionsatisfiesg (cid:2) (z)=g(z)(1−g(z)), sowehave
g (cid:2) (w·x) = g(w·x)(1−g(w·x)) =h (x)(1−h (x))
w w
sotheweightupdateforminimizingthelossis
w ← w +α(y−h (x))×h (x)(1−h (x))×x . (18.8)
i i w w w i
Repeating the experiments of Figure 18.16 with logistic regression instead of the linear
threshold classifier, we obtain the results shown in Figure 18.18. In (a), the linearly sep-
arable case, logistic regression is somewhat slower to converge, but behaves much more
predictably. In (b) and (c), where the data are noisy and nonseparable, logistic regression
converges farmorequickly andreliably. Theseadvantages tendtocarryoverintoreal-world
applications and logistic regression has become one of the most popular classification tech-
niquesforproblemsinmedicine,marketingandsurveyanalysis,creditscoring,publichealth,
andotherapplications.
18.7 ARTIFICIAL NEURAL NETWORKS
We turn now to what seems to be a somewhat unrelated topic: the brain. In fact, as we
will see, the technical ideas we have discussed so far in this chapter turn out to be useful in
buildingmathematicalmodelsofthebrain’sactivity;conversely, thinkingaboutthebrainhas
helpedinextendingthescopeofthetechnical ideas.
Chapter 1 touched briefly on the basic findings of neuroscience—in particular, the hy-
pothesis that mental activity consists primarily of electrochemical activity in networks of
brain cells called neurons. (Figure 1.2on page 11 showed a schematic diagram of atypical
neuron.) Inspired by this hypothesis, some of the earliest AI work aimed to create artificial
neural networks. (Other names for the field include connectionism, parallel distributed
NEURALNETWORK
processing, and neural computation.) Figure 18.19 shows a simple mathematical model
of the neuron devised by McCulloch and Pitts (1943). Roughly speaking, it “fires” when a
linearcombinationofitsinputsexceedssome(hardorsoft) threshold—that is,itimplements
728 Chapter 18. LearningfromExamples
Bias Weight
a 0 = 1 w a j = g(in j )
0,j
g
in
w Σ j
i,j
a a
i j
Input Input Activation Output
Output
Links Function Function Links
Figure(cid:2)18.19 Asimplemathematicalmodelforaneuron. Theunit’soutputactivationis
n
aj=g(
i=0
wi,jai),whereaiistheoutputactivationofunitiandwi,j istheweightonthe
linkfromunititothisunit.
a linear classifier of the kind described in the preceding section. A neural network is just a
collection of units connected together; the properties of the network are determined by its
topology andtheproperties ofthe“neurons.”
Since 1943, much more detailed and realistic models have been developed, both for
neurons and for larger systems in the brain, leading to the modern field of computational
COMPUTATIONAL neuroscience. On the other hand, researchers in AI and statistics became interested in the
NEUROSCIENCE
moreabstractproperties ofneuralnetworks, suchastheirabilitytoperformdistributed com-
putation, totolerate noisyinputs, andtolearn. Although weunderstand nowthatotherkinds
of systems—including Bayesian networks—have these properties, neural networks remain
one of the most popular and effective forms of learning system and are worthy of study in
theirownright.
18.7.1 Neural network structures
Neural networks are composed of nodes or units (see Figure 18.19) connected by directed
UNIT
links. Alinkfromunititounitj servestopropagatetheactivationa fromitoj.8 Eachlink
LINK i
also hasanumeric weightw associated withit, which determines the strength andsign of
ACTIVATION i,j
theconnection. Justasinlinearregression models,eachunithasadummyinputa =1with
WEIGHT 0
anassociated weightw . Eachunitj firstcomputesaweightedsumofitsinputs:
0,j
(cid:12)n
in = w a .
j i,j i
i=0
ACTIVATION Thenitappliesanactivation functiong tothissumtoderivetheoutput:
FUNCTION (cid:31)
(cid:12)n
a = g(in )= g w a . (18.9)
j j i,j i
i=0
8 Anoteonnotation: forthissection,weareforcedtosuspendourusualconventions. Inputattributesarestill
indexed by i , so that an “external” activation ai is given by input xi; but index j will refer to internal units
ratherthanexamples. Throughoutthissection,themathematicalderivationsconcernasinglegenericexamplex,
omittingtheusualsummationsoverexamplestoobtainresultsforthewholedataset.
Section18.7. ArtificialNeuralNetworks 729
Theactivation function g istypically eitherahardthreshold (Figure18.17(a)), inwhichcase
theunitiscalledaperceptron,oralogisticfunction(Figure18.17(b)),inwhichcasetheterm
PERCEPTRON
SIGMOID sigmoid perceptron is sometimes used. Both of these nonlinear activation function ensure
PERCEPTRON
theimportantpropertythattheentirenetworkofunitscanrepresentanonlinearfunction(see
Exercise18.22). Asmentionedinthediscussionoflogisticregression(page725),thelogistic
activation functionhastheaddedadvantage ofbeingdifferentiable.
Having decided on the mathematical model for individual “neurons,” the next task is
to connect them together to form a network. There are two fundamentally distinct ways to
FEED-FORWARD do this. A feed-forward network has connections only in one direction—that is, it forms a
NETWORK
directedacyclicgraph. Everynodereceivesinputfrom“upstream”nodesanddeliversoutput
to“downstream” nodes; therearenoloops. Afeed-forward networkrepresents afunction of
itscurrentinput;thus,ithasnointernalstateotherthantheweightsthemselves. Arecurrent
RECURRENT network, on the other hand, feeds its outputs back into its own inputs. This means that
NETWORK
theactivation levelsofthenetworkformadynamical system thatmayreach astablestateor
exhibitoscillationsorevenchaoticbehavior. Moreover,theresponseofthenetworktoagiven
input depends on its initial state, which may depend on previous inputs. Hence, recurrent
networks (unlike feed-forward networks) can support short-term memory. This makes them
more interesting as models of the brain, but also more difficult to understand. This section
will concentrate on feed-forward networks; some pointers for further reading on recurrent
networksaregivenattheendofthechapter.
Feed-forwardnetworksareusuallyarrangedin layers,suchthateachunitreceivesinput
LAYERS
onlyfromunitsintheimmediatelyprecedinglayer. Inthenexttwosubsections, wewilllook
at single-layer networks, in which every unit connects directly from the network’s inputs to
itsoutputs, andmultilayernetworks, whichhaveoneormore layersofhiddenunitsthatare
HIDDENUNIT
not connected tothe outputs of the network. Sofarinthis chapter, wehave considered only
learningproblemswithasingleoutputvariable y,butneuralnetworksareoftenusedincases
where multiple outputs are appropriate. For example, if we want to train a network to add
twoinput bits, each a0ora1, wewillneed oneoutput forthesum bit andone forthecarry
bit. Also, whenthelearning problem involves classification intomorethan twoclasses—for
example,whenlearningtocategorizeimagesofhandwritten digits—itiscommontouseone
outputunitforeachclass.
18.7.2 Single-layerfeed-forward neural networks (perceptrons)
Anetworkwithalltheinputsconnecteddirectlytotheoutputsiscalledasingle-layerneural
PERCEPTRON network, or a perceptron network. Figure 18.20 shows a simple two-input, two-output
NETWORK
perceptronnetwork. Withsuchanetwork,wemighthopetolearnthetwo-bitadderfunction,
forexample. Hereareallthetrainingdatawewillneed:
x x y (carry) y (sum)
1 2 3 4
0 0 0 0
0 1 0 1
1 0 0 1
1 1 1 0
730 Chapter 18. LearningfromExamples
Thefirstthingtonoticeisthataperceptronnetworkwithmoutputsisreallymseparate
networks, because each weight affects only one of the outputs. Thus, there will be m sepa-
rate training processes. Furthermore, depending on the type of activation function used, the
trainingprocesseswillbeeitherthe perceptronlearningrule(Equation(18.7)onpage724)
orgradient descentruleforthe logistic regression (Equation(18.8)onpage727).
Ifyoutryeithermethodonthetwo-bit-adderdata,somethinginteresting happens. Unit
3 learns the carry function easily, but unit 4 completely fails to learn the sum function. No,
unit 4 isnot defective! The problem iswith the sum function itself. Wesawin Section 18.6
thatlinearclassifiers(whetherhardorsoft)canrepresent lineardecisionboundariesinthein-
putspace. Thisworksfineforthecarryfunction,whichisalogicalAND(seeFigure18.21(a)).
Thesum function, however, is an XOR (exclusive OR)of the two inputs. AsFigure 18.21(c)
illustrates, thisfunction isnotlinearly separable sothe perceptron cannotlearnit.
The linearly separable functions constitute just a small fraction of all Boolean func-
tions; Exercise 18.20 asksyou toquantify thisfraction. Theinability ofperceptrons tolearn
even such simple functions as XOR was a significant setback to the nascent neural network
w w w
1,3 1,3 3,5
1 3 1 3 5
w w w
1,4 1,4 3,6
w w w
2,3 2,3 4,5
2 4 2 4 6
w w w
2,4 2,4 4,6
(a) (b)
Figure18.20 (a)Aperceptronnetworkwithtwoinputsandtwooutputunits.(b)Aneural
networkwithtwoinputs,onehiddenlayeroftwounits,andoneoutputunit. Notshownare
thedummyinputsandtheirassociatedweights.
x x x
1 1 1
1 1 1
?
0 0 0
0 1 x 2 0 1 x 2 0 1 x 2
(a)x andx (b)x orx (c)x xorx
1 2 1 2 1 2
Figure18.21 Linearseparabilityinthresholdperceptrons. Blackdotsindicateapointin
theinputspacewherethevalueofthefunctionis1,andwhitedotsindicateapointwherethe
valueis0. Theperceptronreturns1ontheregiononthenon-shadedsideoftheline. In(c),
nosuchlineexiststhatcorrectlyclassifiestheinputs.
Section18.7. ArtificialNeuralNetworks 731
1
0.9
0.8
0.7
0.6
0.5
0.4
0 10 20 30 40 50 60 70 80 90 100
tes
tset
no
tcerroc
noitroporP
1
0.9
0.8
0.7
Perceptron 0.6
Decision tree
0.5
0.4
0 10 20 30 40 50 60 70 80 90 100
Training set size
tes
tset
no
tcerroc
noitroporP
Perceptron
Decision tree
Training set size
(a) (b)
Figure18.22 Comparingtheperformanceofperceptronsanddecisiontrees. (a)Percep-
tronsarebetteratlearningthemajorityfunctionof11inputs. (b)Decisiontreesarebetterat
learningtheWillWait predicateintherestaurantexample.
community in the 1960s. Perceptrons are far from useless, however. Section 18.6.4 noted
that logistic regression (i.e., training asigmoid perceptron) is eventoday a very popular and
effective tool. Moreover, a perceptron can represent some quite “complex” Boolean func-
tions very compactly. For example, the majority function, which outputs a 1 only if more
than half ofitsninputs are 1, canberepresented byaperceptron witheach w =1andwith
i
w =−n/2. Adecisiontreewouldneedexponentiallymanynodestorepresentthisfunction.
0
Figure 18.22 shows the learning curve fora perceptron on two different problems. On
the left, we show the curve for learning the majority function with 11 Boolean inputs (i.e.,
thefunctionoutputsa1if6ormoreinputsare1). Aswewouldexpect,theperceptronlearns
the function quite quickly, because the majority function is linearly separable. On the other
hand,thedecision-tree learnermakesnoprogress, because themajorityfunctionisveryhard
(althoughnotimpossible) torepresentasadecisiontree. Ontheright,wehavetherestaurant
example. The solution problem is easily represented as a decision tree, but is not linearly
separable. Thebestplanethrough thedatacorrectlyclassifiesonly65%.
18.7.3 Multilayerfeed-forward neural networks
(McCulloch andPitts,1943)werewellawarethatasinglethreshold unitwouldnotsolveall
their problems. In fact, their paper proves that such a unit can represent the basic Boolean
functions AND, OR,and NOT andthen goes ontoargue that anydesired functionality canbe
obtainedbyconnecting largenumbersofunitsinto(possibly recurrent)networksofarbitrary
depth. Theproblem wasthatnobodyknewhowtotrainsuchnetworks.
This turns out to be an easy problem if we think of a network the right way: as a
function h (x)parameterized bytheweights w. Considerthesimplenetwork showninFig-
w
ure 18.20(b), which hastwoinput units, twohidden units, and twooutput unit. (Inaddition,
each unit has a dummy input fixed at 1.) Given an input vector x=(x ,x ), the activations
1 2
732 Chapter 18. LearningfromExamples
h (x,x) h (x,x)
W 1 2 W 1 2
1 1
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
4 4
0 2 0 2
-4 -2 x 1 0 2 4 -4 -2 0 x 2 -4 -2 x 1 0 2 4 -4 -2 0 x 2
(a) (b)
Figure18.23 (a)Theresultofcombiningtwoopposite-facingsoftthresholdfunctionsto
producearidge.(b)Theresultofcombiningtworidgestoproduceabump.
oftheinputunitsaresetto(a ,a )=(x ,x ). Theoutputatunit5isgivenby
1 2 1 2
a = g(w w a +w a )
5 0,5,+ 3,5 3 4,5 4
= g(w w g(w +w a +w a )+w g(w 4+w a +w a ))
0,5,+ 3,5 0,3 1,3 1 2,3 2 4,5 0 1,4 1 2,4 2
= g(w w g(w +w x +w x )+w g(w 4+w x +w x )).
0,5,+ 3,5 0,3 1,3 1 2,3 2 4,5 0 1,4 1 2,4 2
Thus, we have the output expressed as a function of the inputs and the weights. A similar
expression holds for unit 6. As long as we can calculate the derivatives of such expressions
with respect to the weights, we can use the gradient-descent loss-minimization method to
train the network. Section 18.7.4 shows exactly how to do this. And because the function
representedbyanetworkcanbehighlynonlinear—composed, asitis,ofnestednonlinearsoft
NONLINEAR threshold functions—we canseeneuralnetworksasatoolfor doingnonlinearregression.
REGRESSION
Before delving into learning rules, let us look at the ways in which networks generate
complicated functions. First,rememberthateachunitinasigmoidnetwork represents asoft
threshold in its input space, as shown in Figure 18.17(c) (page 726). With one hidden layer
and one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholded
linear combination of several such functions. For example, by adding two opposite-facing
softthresholdfunctionsandthresholdingtheresult,wecanobtaina“ridge”functionasshown
inFigure 18.23(a). Combining twosuch ridges atright angles toeach other(i.e., combining
theoutputsfromfourhiddenunits), weobtaina“bump”asshowninFigure18.23(b).
Withmorehiddenunits, wecanproduce morebumpsofdifferentsizesinmoreplaces.
Infact,withasingle,sufficientlylargehiddenlayer,itispossibletorepresentanycontinuous
function oftheinputs witharbitrary accuracy; withtwolayers, evendiscontinuous functions
canberepresented.9 Unfortunately, forany particular network structure, itishardertochar-
acterizeexactlywhichfunctions canberepresented andwhichonescannot.
9 Theproofiscomplex,butthemainpointisthattherequirednumberofhiddenunitsgrowsexponentiallywith
thenumberofinputs.Forexample,2n/nhiddenunitsareneededtoencodeallBooleanfunctionsofninputs.
Section18.7. ArtificialNeuralNetworks 733
18.7.4 Learning inmultilayernetworks
First,letusdispensewithoneminorcomplicationarisinginmultilayernetworks: interactions
among the learning problems when the network has multiple outputs. In such cases, we
shouldthinkofthenetworkasimplementingavectorfunctionh ratherthanascalarfunction
w
h ; for example, the network in Figure 18.20(b) returns a vector [a ,a ]. Similarly, the
w 5 6
target output willbe avector y. Whereas aperceptron network decomposes into m separate
learningproblemsforanm-outputproblem,thisdecompositionfailsinamultilayernetwork.
Forexample, both a and a in Figure 18.20(b) depend on all of the input-layer weights, so
5 6
updatestothoseweightswilldependonerrorsinbotha anda . Fortunately,thisdependency
5 6
is very simple in the case of any loss function that is additive across the components of the
errorvector y−h (x). FortheL loss,wehave,foranyweightw,
w 2
(cid:12) (cid:12)
∂ ∂ ∂ ∂
Loss(w)= |y−h (x)|2 = (y −a )2 = (y −a )2 (18.10)
w k k k k
∂w ∂w ∂w ∂w
k k
where the index k ranges over nodes in the output layer. Each term in the final summation
is just the gradient of the loss for the kth output, computed as if the other outputs did not
exist. Hence, we can decompose an m-output learning problem into m learning problems,
providedweremembertoaddupthegradientcontributionsfromeachofthemwhenupdating
theweights.
The major complication comes from the addition of hidden layers to the network.
Whereas the error y − h at the output layer is clear, the error at the hidden layers seems
w
mysterious because the training data do not say what value the hidden nodes should have.
Fortunately, it turns out that we can back-propagate the error from the output layer to the
BACK-PROPAGATION
hiddenlayers. Theback-propagationprocessemergesdirectlyfromaderivationoftheoverall
errorgradient. First,wewilldescribetheprocesswithanintuitivejustification; then,wewill
showthederivation.
At the output layer, the weight-update rule is identical to Equation (18.8). We have
multiple output units, so let Err be the kth component of the error vector y−h . Wewill
k w
also find it convenient to define a modified error Δ =Err ×g (cid:2) (in ), so that the weight-
k k k
updaterulebecomes
w ← w +α×a ×Δ . (18.11)
j,k j,k j k
Toupdate the connections between the input units and the hidden units, weneed to define a
quantity analogous to the error term for output nodes. Here is where we do the error back-
propagation. Theideaisthathiddennodej is“responsible”forsomefractionoftheerror Δ
k
in each of the output nodes to which itconnects. Thus, the Δ values are divided according
k
tothestrength oftheconnection betweenthehidden nodeand theoutput node andareprop-
agated back to provide the Δ values for the hidden layer. The propagation rule for the Δ
j
valuesisthefollowing:
(cid:12)
(cid:2)
Δ = g (in ) w Δ . (18.12)
j j j,k k
k
734 Chapter 18. LearningfromExamples
functionBACK-PROP-LEARNING(examples,network)returnsaneuralnetwork
inputs:examples,asetofexamples,eachwithinputvectorxandoutputvectory
network,amultilayernetworkwithLlayers,weightswi,j,activationfunctiong
localvariables: Δ,avectoroferrors,indexedbynetworknode
repeat
foreachweightwi,j innetwork do
wi,j ←asmallrandomnumber
foreachexample(x,y)inexamples do
/*Propagatetheinputsforwardtocomputetheoutputs*/
foreachnodeiintheinputlayerdo
ai ←xi
for(cid:3)=2toLdo
foreachno(cid:2)dejinlayer(cid:3)do
inj ←
i
wi,j ai
aj ←g(inj)
/*Propagatedeltasbackwardfromoutputlayertoinputlayer*/
foreachnodej intheoutputlayerdo
Δ[j]←g(cid:5)(inj) × (yj − aj)
for(cid:3)=L−1to1do
foreachnodeiinl(cid:2)ayer(cid:3)do
Δ[i]←g(cid:5)(ini)
j
wi,j Δ[j]
/*Updateeveryweightinnetworkusingdeltas*/
foreachweightwi,j innetwork do
wi,j ←wi,j + α × ai × Δ[j]
untilsomestoppingcriterionissatisfied
returnnetwork
Figure18.24 Theback-propagationalgorithmforlearninginmultilayernetworks.
Nowtheweight-updaterulefortheweightsbetweentheinputsandthehiddenlayerisessen-
tiallyidenticaltotheupdaterulefortheoutput layer:
w ← w +α×a ×Δ .
i,j i,j i j
Theback-propagation processcanbesummarizedasfollows:
• ComputetheΔvaluesfortheoutputunits, usingtheobserved error.
• Starting with output layer, repeat the following foreach layer in the network, until the
earliesthiddenlayerisreached:
– PropagatetheΔvaluesbacktothepreviouslayer.
– Updatetheweightsbetweenthetwolayers.
Thedetailed algorithm isshowninFigure18.24.
For the mathematically inclined, we will now derive the back-propagation equations
from first principles. The derivation is quite similar to the gradient calculation for logistic
Section18.7. ArtificialNeuralNetworks 735
regression (leading uptoEquation (18.8) onpage 727), except that wehavetouse thechain
rulemorethanonce.
Following Equation (18.10), we compute just the gradient for Loss = (y −a )2 at
k k k
the kthoutput. Thegradient ofthis loss with respect toweights connecting the hidden layer
to the output layer will be zero except for weights w that connect to the kth output unit.
j,k
Forthoseweights, wehave
∂Loss ∂a ∂g(in )
k = −2(y −a ) k = −2(y −a ) k
k k k k
∂w ∂w ∂w
j,k j,k j,k ⎛ ⎞
(cid:12)
∂in ∂
= −2(y −a )g (cid:2) (in ) k = −2(y −a )g (cid:2) (in ) ⎝ w a ⎠
k k k k k k j,k j
∂w ∂w
j,k j,k
j
= −2(y −a )g (cid:2) (in )a = −a Δ ,
k k k j j k
withΔ definedasbefore. Toobtainthegradientwithrespecttothe w weightsconnecting
k i,j
theinputlayertothehidden ¡layer, wehavetoexpandoutthe activations a andreapply the
j
chainrule. Wewillshowthederivation ingorydetail because itisinteresting toseehowthe
derivativeoperatorpropagates backthrough thenetwork:
∂Loss ∂a ∂g(in )
k = −2(y −a ) k = −2(y −a ) k
k k k k
∂w ∂w ∂w
i,j i,j i,j⎛ ⎞
(cid:12)
∂in ∂
= −2(y −a )g (cid:2) (in ) k = −2Δ ⎝ w a ⎠
k k k k j,k j
∂w ∂w
i,j i,j
j
∂a ∂g(in )
= −2Δ w j = −2Δ w j
k j,k k j,k
∂w ∂w
i,j i,j
∂in
= −2Δ w g (cid:2) (in ) j
k j,k j
∂w
i,j (cid:31)
(cid:12)
∂
= −2Δ w g (cid:2) (in ) w a
k j,k j i,j i
∂w
i,j
i
= −2Δ w g (cid:2) (in )a = −a Δ ,
k j,k j i i j
whereΔ isdefinedasbefore. Thus,weobtaintheupdaterulesobtainedearlierfromintuitive
j
considerations. Itisalsoclearthattheprocess canbecontinued fornetworkswithmorethan
onehiddenlayer, whichjustifiesthegeneralalgorithm giveninFigure18.24.
Having made it through (or skipped over) all the mathematics, let’s see how a single-
hidden-layer network performs on the restaurant problem. First, we need to determine the
structure of the network. We have 10 attributes describing each example, so we will need
10 input units. Should we have one hidden layer or two? How many nodes in each layer?
Shouldtheybefullyconnected? Thereisnogoodtheorythatwilltellustheanswer. (Seethe
nextsection.) Asalways,wecanusecross-validation: tryseveraldifferentstructures andsee
whichoneworksbest. Itturnsoutthatanetworkwithonehiddenlayercontainingfournodes
is about right for this problem. In Figure 18.25, we show two curves. The first is a training
curve showing the mean squared error on a given training set of 100 restaurant examples
736 Chapter 18. LearningfromExamples
14
12
10
8
6
4
2
0
0 50 100 150 200 250 300 350 400
tes
gniniart
no
rorre
latoT
1
0.9
0.8
0.7
0.6
0.5
0.4
0 10 20 30 40 50 60 70 80 90 100
Number of epochs
tes
tset
no
tcerroc
noitroporP Decision tree
Multilayer network
Training set size
(a) (b)
Figure 18.25 (a) Training curve showing the gradual reduction in error as weights are
modified over several epochs, for a given set of examples in the restaurant domain. (b)
Comparativelearningcurvesshowingthatdecision-treelearningdoesslightlybetteronthe
restaurantproblemthanback-propagationinamultilayernetwork.
duringtheweight-updatingprocess. Thisdemonstratesthatthenetworkdoesindeedconverge
to a perfect fit to the training data. The second curve is the standard learning curve for the
restaurant data. The neural network does learn well, although not quite as fast as decision-
tree learning; this is perhaps not surprising, because the data were generated from a simple
decision treeinthefirstplace.
Neural networks are capable of farmore complex learning tasks of course, although it
must be said that a certain amount of twiddling is needed to get the network structure right
andtoachieveconvergence tosomethingclosetotheglobaloptimuminweightspace. There
are literally tens of thousands of published applications of neural networks. Section 18.11.1
looksatonesuchapplication inmoredepth.
18.7.5 Learning neural network structures
Sofar,wehaveconsidered theproblem oflearning weights, givenafixednetwork structure;
just as with Bayesian networks, we also need to understand how to find the best network
structure. Ifwechooseanetworkthatistoobig,itwillbeabletomemorizealltheexamples
by forming a large lookup table, but will not necessarily generalize well to inputs that have
notbeenseenbefore.10 Inotherwords,likeallstatisticalmodels,neuralnetworksaresubject
tooverfitting whenthere aretoo manyparameters inthe model. Wesawthis in Figure 18.1
(page 696), where the high-parameter models in (b) and (c) fit all the data, but might not
generalize aswellasthelow-parametermodelsin(a)and(d).
Ifwesticktofullyconnectednetworks,theonlychoicestobemadeconcernthenumber
10 Ithasbeenobservedthatverylargenetworks dogeneralizewellaslongastheweightsarekeptsmall. This
restrictionkeepstheactivationvaluesinthelinearregionofthesigmoidfunctiong(x)wherexisclosetozero.
This,inturn,meansthatthenetworkbehaveslikealinearfunction(Exercise18.22)withfarfewerparameters.
Section18.8. Nonparametric Models 737
of hidden layers and their sizes. The usual approach is to try several and keep the best. The
cross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test
set. Thatis,wechoosethenetworkarchitecture thatgivesthehighestprediction accuracyon
thevalidation sets.
If we want to consider networks that are not fully connected, then we need to find
someeffectivesearchmethodthroughtheverylargespaceofpossibleconnectiontopologies.
OPTIMALBRAIN The optimal brain damage algorithm begins with a fully connected network and removes
DAMAGE
connections from it. After the network is trained for the first time, an information-theoretic
approach identifies an optimal selection of connections that can be dropped. The network
is then retrained, and if its performance has not decreased then the process is repeated. In
additiontoremovingconnections, itisalsopossible toremoveunitsthatarenotcontributing
muchtotheresult.
Severalalgorithmshavebeenproposedforgrowingalargernetworkfromasmallerone.
One, the tiling algorithm, resembles decision-list learning. The idea is to start with a single
TILING
unit that does its best to produce the correct output on as many of the training examples as
possible. Subsequentunitsareaddedtotakecareoftheexamplesthatthefirstunitgotwrong.
Thealgorithm addsonlyasmanyunitsasareneededtocoveralltheexamples.
18.8 NONPARAMETRIC MODELS
Linearregression and neural networks use thetraining data toestimate afixedset ofparam-
eters w. Thatdefinesourhypothesis h (x),andatthatpointwecanthrowawaythetraining
w
data, because they are all summarized by w. A learning model that summarizes data with a
set of parameters of fixed size (independent of the number of training examples) is called a
parametricmodel.
PARAMETRICMODEL
No matter how much data you throw at a parametric model, it won’t change its mind
abouthowmanyparametersitneeds. Whendatasetsaresmall,itmakessensetohaveastrong
restrictionontheallowablehypotheses, toavoidoverfitting. Butwhentherearethousandsor
millionsorbillionsofexamplestolearnfrom,itseemslike abetterideatoletthedataspeak
for themselves rather than forcing them to speak through a tiny vector of parameters. If the
data say that the correct answer is a very wiggly function, we shouldn’t restrict ourselves to
linearorslightly wigglyfunctions.
NONPARAMETRIC Anonparametricmodelisonethatcannotbecharacterizedbyaboundedsetofparam-
MODEL
eters. Forexample, suppose that each hypothesis wegenerate simply retains within itself all
ofthetraining examples andusesallofthemtopredict thenextexample. Suchahypothesis
family would benonparametric because the effective number of parameters isunbounded—
INSTANCE-BASED it grows with the number of examples. This approach is called instance-based learning or
LEARNING
memory-basedlearning. Thesimplestinstance-basedlearningmethodistablelookup: take
TABLELOOKUP
allthetrainingexamples,puttheminalookuptable,andthenwhenaskedforh(x),seeifxis
inthe table; ifitis, return thecorresponding y. Theproblem withthismethod is thatitdoes
notgeneralize well: whenxisnotinthetableallitcandoisreturnsomedefault value.
738 Chapter 18. LearningfromExamples
7.5 7.5
7 7
6.5 6.5
6 6
5.5 5.5
x x
1 5 1 5
4.5 4.5
4 4
3.5 3.5
3 3
2.5 2.5
4.5 5 5.5 6 6.5 7 4.5 5 5.5 6 6.5 7
x x
2 2
(k=1) (k=5)
Figure18.26 (a)Ak-nearest-neighbormodelshowingtheextentoftheexplosionclassfor
thedatainFigure18.15,withk=1. Overfittingisapparent. (b)With k=5,theoverfitting
problemgoesawayforthisdataset.
18.8.1 Nearestneighbor models
Wecanimproveontablelookupwithaslightvariation: givenaqueryx ,findthekexamples
q
NEAREST that are nearest to x . This is called k-nearest neighbors lookup. We’ll use the notation
NEIGHBORS q
NN(k,x )todenotethesetofk nearestneighbors.
q
To do classification, first find NN(k,x ), then take the plurality vote of the neighbors
q
(which is the majority vote in the case of binary classification). To avoid ties, k is always
chosen to be an odd number. To do regression, we can take the mean or median of the k
neighbors, orwecansolvealinearregression problem onthe neighbors.
In Figure 18.26, we show the decision boundary of k-nearest-neighbors classification
for k= 1 and 5 on the earthquake data set from Figure 18.15. Nonparametric methods are
stillsubjecttounderfittingandoverfitting,justlikeparametricmethods. Inthiscase1-nearest
neighborsisoverfitting;itreactstoomuchtotheblackoutlierintheupperrightandthewhite
outlier at (5.4, 3.7). The 5-nearest-neighbors decision boundary is good; higher k would
underfit. Asusual, cross-validation canbeusedtoselectthebestvalueofk.
The very word “nearest” implies a distance metric. How do we measure the distance
from a query point x to an example point x ? Typically, distances are measured with a
q j
MINKOWSKI MinkowskidistanceorLp norm,definedas
DISTANCE (cid:12)
Lp(x ,x ) = ( |x −x |p)1/p .
j q j,i q,i
i
With p=2this isEuclidean distance and withp=1itis Manhattan distance. With Boolean
attribute values, the number of attributes on which the two points differ is called the Ham-
mingdistance. Often p=2isused ifthedimensions aremeasuring similarproperties, such
HAMMINGDISTANCE
asthewidth, height and depth ofparts onaconveyor belt, and Manhattan distance isused if
they aredissimilar, such as age, weight, and gender ofapatient. Note that ifweuse the raw
numbers from each dimension then the total distance will be affected by a change in scale
in any dimension. That is, if we change dimension i from measurements in centimeters to
Section18.8. Nonparametric Models 739
mileswhilekeeping theotherdimensions thesame,we’llgetdifferent nearest neighbors. To
avoidthis,itiscommontoapplynormalizationtothemeasurementsineachdimension. One
NORMALIZATION
simple approach is to compute the mean μ and standard deviation σ of the values in each
i i
dimension, and rescale them so that x becomes (x − μ )/σ . A more complex metric
j,i j,i i i
MAHALANOBIS knownastheMahalanobisdistancetakesintoaccountthecovariance betweendimensions.
DISTANCE
In low-dimensional spaces with plenty of data, nearest neighbors works very well: we
are likely to have enough nearby data points to get a good answer. But as the number of
dimensions rises weencounter aproblem: the nearest neighbors inhigh-dimensional spaces
areusually notverynear! Consider k-nearest-neighbors onadata setof N points uniformly
distributed throughout the interior of an n-dimensional unit hypercube. We’ll define the k-
neighborhood ofapointasthesmallesthypercube thatcontains thek-nearest neighbors. Let
(cid:3)betheaveragesidelengthofaneighborhood. Thenthevolumeoftheneighborhood(which
contains k points) is (cid:3)n and the volume of the full cube (which contains N points) is 1. So,
onaverage, (cid:3)n=k/N. Takingnthrootsofbothsidesweget(cid:3) = (k/N)1/n.
To be concrete, let k=10 and N=1,000,000. In two dimensions (n=2; a unit
square), the average neighborhood has (cid:3)=0.003, a small fraction of the unit square, and
in3dimensions (cid:3)isjust2%oftheedgelengthoftheunitcube. Butbythetimewegetto17
dimensions, (cid:3)ishalftheedgelengthoftheunithypercube, andin200dimensions itis94%.
CURSEOF Thisproblem hasbeencalledthecurseofdimensionality.
DIMENSIONALITY
Anotherwaytolookatit: considerthepointsthatfallwithinathinshellmakingupthe
outer 1% of the unit hypercube. These are outliers; in general it will be hard to find a good
value forthembecause wewillbeextrapolating ratherthaninterpolating. Inonedimension,
these outliers are only 2% of the points on the unit line (those points where x < .01 or
x > .99), but in 200 dimensions, over 98% of the points fall within this thin shell—almost
allthepoints areoutliers. Youcanseeanexample ofapoornearest-neighbors fitonoutliers
ifyoulookaheadtoFigure18.28(b).
TheNN(k,x )function isconceptually trivial: givenasetofN examplesandaquery
q
x ,iteratethroughtheexamples,measurethedistanceto x fromeachone,andkeepthebest
q q
k. Ifwearesatisfiedwithanimplementation thattakesO(N)executiontime,thenthatisthe
end of the story. But instance-based methods are designed for large data sets, so we would
like an algorithm with sublinear run time. Elementary analysis of algorithms tells us that
exact table lookup is O(N) with a sequential table, O(logN) with a binary tree, and O(1)
with a hash table. We will now see that binary trees and hash tables are also applicable for
findingnearest neighbors.
18.8.2 Finding nearest neighbors with k-dtrees
Abalancedbinarytreeoverdatawithanarbitrarynumberofdimensionsiscalledak-dtree,
K-DTREE
for k-dimensional tree. (In our notation, the number of dimensions is n, so they would be
n-d trees. The construction of a k-d tree is similar to the construction of a one-dimensional
balancedbinarytree. Westartwithasetofexamplesandattherootnodewesplitthemalong
theithdimension bytesting whetherx ≤ m. Wechose thevaluemtobethemedian ofthe
i
examplesalongtheithdimension;thushalftheexampleswillbeintheleftbranchofthetree
740 Chapter 18. LearningfromExamples
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
25 50 75 100 125 150 175 200
doohrobhgien
fo
htgnel
egdE
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
25 50 75 100 125 150 175 200
Number of dimensions
llehs
roiretxe
ni
stniop
fo
noitroporP
Number of dimensions
(a) (b)
Figure18.27 Thecurseofdimensionality:(a)Thelengthoftheaverageneighborhoodfor
10-nearest-neighborsinaunithypercubewith1,000,000points,asafunctionofthenumber
of dimensions. (b) The proportion of points that fall within a thin shell consisting of the
outer1%ofthehypercube,asafunctionofthenumberofdimensions.Sampledfrom10,000
randomlydistributedpoints.
and half inthe right. Wethen recursively makeatree fortheleft and right sets ofexamples,
stopping when there are fewerthan two examples left. To choose a dimension to split on at
each node of the tree, one can simply select dimension i mod n at level i of the tree. (Note
thatwemayneedtosplitonanygivendimensionseveraltimesasweproceeddownthetree.)
Anotherstrategyistosplitonthedimension thathasthewidestspreadofvalues.
Exact lookup from a k-d tree is just like lookup from a binary tree (with the slight
complicationthatyouneedtopayattentiontowhichdimensionyouaretestingateachnode).
But nearest neighbor lookup is more complicated. As we go down the branches, splitting
the examples in half, in some cases we can discard the other half of the examples. But not
always. Sometimes the point we are querying for falls very close to the dividing boundary.
The query point itself might be on the left hand side of the boundary, but one or more of
the k nearest neighbors might actually be on the right-hand side. We have to test for this
possibility by computing the distance of the query point to the dividing boundary, and then
searching bothsides ifwecan’t findk examplesontheleftthatarecloserthanthisdistance.
Because ofthisproblem, k-dtreesareappropriate onlywhen therearemanymoreexamples
than dimensions, preferably at least 2n examples. Thus, k-d trees work well with up to 10
dimensionswiththousandsofexamplesorupto20dimensionswithmillionsofexamples. If
wedon’thaveenoughexamples, lookupisnofasterthanalinearscanoftheentiredataset.
18.8.3 Locality-sensitivehashing
Hash tables have the potential to provide even faster lookup than binary trees. But how can
wefindnearestneighbors usingahashtable, whenhashcodesrelyonanexactmatch? Hash
codes randomly distribute values among the bins, but we want to have near points grouped
LOCALITY-SENSITIVE togetherinthesamebin;wewantalocality-sensitive hash(LSH).
HASH
Section18.8. Nonparametric Models 741
We can’t use hashes to solve NN(k,x ) exactly, but with a clever use of randomized
q
algorithms, we can find an approximate solution. First we define the approximate near-
APPROXIMATE neighborsproblem: given adata setofexample points and aquery point x ,find, withhigh
NEAR-NEIGHBORS q
probability, an example point (orpoints) that isnear x . Tobe moreprecise, werequire that
q
if there is a point x that is within a radius r of x , then with high probability the algorithm
j q
willfindapointx j(cid:3) thatiswithindistancecrofq. Ifthereisnopointwithinradiusrthenthe
algorithm isallowed toreport failure. Thevalues of cand “high probability” areparameters
ofthealgorithm.
To solve approximate near neighbors, we will need a hash function g(x) that has the
propertythat,foranytwopoints x
j
andx j(cid:3),theprobabilitythattheyhavethesamehashcode
is small if their distance is more than cr, and is high if their distance is less than r. For
simplicity we will treat each point as a bit string. (Any features that are not Boolean can be
encoded intoasetofBooleanfeatures.)
The intuition we rely on is that if two points are close together in an n-dimensional
space,thentheywillnecessarilybeclosewhenprojecteddownontoaone-dimensionalspace
(aline). Infact, wecandiscretize thelineinto bins—hash buckets—so that, withhigh prob-
ability, nearpoints project downto exactly the same bin. Points that are faraway from each
otherwilltendtoproject downinto different binsformostprojections, but therewillalways
be a few projections that coincidentally project far-apart points into the same bin. Thus, the
bin forpoint x contains many(but not all)points thatare nearto x ,as wellassome points
q q
thatarefaraway.
ThetrickofLSHistocreatemultiplerandomprojectionsandcombinethem. Arandom
projection is just a random subset of the bit-string representation. We choose (cid:3) different
randomprojectionsandcreate (cid:3)hashtables,g (x),...,g (x). Wethenenteralltheexamples
1 (cid:3)
intoeachhashtable. Thenwhengivenaquerypointx ,wefetchthesetofpointsinbing (q)
q k
foreach k,andunion these setstogether intoasetofcandidate points, C. Thenwecompute
theactualdistancetox foreachofthepointsinC andreturnthekclosestpoints. Withhigh
q
probability, eachofthepointsthatarenearto x willshowupinatleastoneofthebins, and
q
although some far-away points will show up as well, we can ignore those. With large real-
world problems, such as finding the near neighbors in a data set of 13 million Web images
using512dimensions(Torralbaetal.,2008),locality-sensitivehashingneedstoexamineonly
a few thousand images out of 13 million to find nearest neighbors; a thousand-fold speedup
overexhaustiveork-dtreeapproaches.
18.8.4 Nonparametric regression
Now we’ll look at nonparametric approaches to regression rather than classification. Fig-
ure 18.28 shows an example of some different models. In (a), wehave perhaps the simplest
method of all, known informally as “connect-the-dots,” and superciliously as “piecewise-
linear nonparametric regression.” This model creates a function h(x) that, when given a
query x , solves the ordinary linear regression problem with just two points: the training
q
examples immediately to the left and right of x . When noise is low, this trivial method is
q
actuallynottoobad,whichiswhyitisastandardfeatureofchartingsoftwareinspreadsheets.
742 Chapter 18. LearningfromExamples
8 8
7 7
6 6
5 5
4 4
3 3
2 2
1 1
0 0
0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14
(a) (b)
8 8
7 7
6 6
5 5
4 4
3 3
2 2
1 1
0 0
0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14
(c) (d)
Figure18.28 Nonparametricregressionmodels:(a)connectthedots,(b)3-nearestneigh-
borsaverage,(c)3-nearest-neighborslinearregression, (d)locallyweightedregressionwith
aquadratickernelofwidthk=10.
Butwhenthedataarenoisy, theresulting functionisspiky, anddoesnotgeneralize well.
NEAREST- k-nearest-neighbors regression (Figure 18.28(b)) improves on connect-the-dots. In-
NEIGHBORS
REGRESSION
stead of using just the two examples to the left and right of a query point x , we use the
q
k nearest neighbors (here 3). A larger value of k tends to smooth out the magnitude of
thespikes, although theresulting function hasdiscontinuities. In(b), wehave thek-nearest-
(cid:2)
neighborsaverage: h(x)isthemeanvalueofthekpoints, y /k. Noticethatattheoutlying
j
points,nearx=0andx=14,theestimatesarepoorbecausealltheevidencecomesfromone
side(theinterior),andignoresthetrend. In(c),wehavek-nearest-neighbor linearregression,
whichfindsthebestlinethroughthekexamples. Thisdoesabetterjobofcapturingtrendsat
theoutliers, butisstilldiscontinuous. Inboth(b)and(c),we’releftwiththequestionofhow
tochoose agoodvaluefork. Theanswer,asusual, iscross-validation.
LOCALLYWEIGHTED Locallyweightedregression(Figure18.28(d))givesustheadvantagesofnearestneigh-
REGRESSION
bors,withoutthediscontinuities. Toavoiddiscontinuities inh(x),weneedtoavoiddisconti-
Section18.8. Nonparametric Models 743
1
0.5
0
-10 -5 0 5 10
Figure 18.29 A quadratic kernel, K(d)= max(0,1 − (2|x|/k)2), with kernel width
k=10,centeredonthequerypointx=0.
nuitiesinthesetofexamplesweusetoestimateh(x). Theideaoflocallyweightedregression
isthatateachquerypoint x ,theexamplesthatareclosetox areweightedheavily, andthe
q q
examplesthatarefartherawayareweightedlessheavilyornotatall. Thedecreaseinweight
overdistanceisalwaysgradual, notsudden.
We decide how much to weight each example with a function known as a kernel. A
KERNEL
kernelfunctionlookslikeabump;inFigure18.29weseethespecifickernelusedtogenerate
Figure 18.28(d). We can see that the weight provided by this kernel is highest in the center
andreacheszeroatadistanceof±5. Canwechoosejustanyfunctionforakernel? No. First,
notethatweinvokeakernelfunction KwithK(Distance(x ,x )),wherex isaquerypoint
j q q
that is a given distance from x , and we want to know how much to weight that distance.
j
So K should be symmetric around 0 and have a maximum at 0. The area under the kernel
mustremainbounded aswegoto±∞. Othershapes, suchasGaussians, havebeenusedfor
kernels, but the latest research suggests that the choice of shape doesn’t matter much. We
do have to be careful about the width of the kernel. Again, this is a parameter of the model
thatisbestchosen bycross-validation. Justasinchoosing thek fornearest neighbors, ifthe
kernels aretoowidewe’llgetunderfitting andiftheyaretoo narrow we’llgetoverfitting. In
Figure18.29(d), thevalueofk=10givesasmoothcurvethatlooksaboutright—but maybe
it does not pay enough attention to the outlier at x=6; a narrower kernel width would be
moreresponsivetoindividual points.
Doing locally weighted regression with kernels is now straightforward. For a given
querypointx wesolvethefollowingweightedregression problem usinggradientdescent:
q (cid:12)
w ∗ = argmin K(Distance(x ,x ))(y −w·x )2 ,
q j j j
w
j
where Distance is any of the distance metrics discussed for nearest neighbors. Then the
answerish(x )=w
∗·x
.
q q
Notethatweneedtosolveanewregressionproblemforeveryquerypoint—that’swhat
it means to be local. (In ordinary linear regression, we solved the regression problem once,
globally, and then used thesameh forany query point.) Mitigating against thisextra work
w
744 Chapter 18. LearningfromExamples
is the fact that each regression problem will be easier to solve, because it involves only the
examples withnonzero weight—the examples whosekernels overlap thequery point. When
kernelwidthsaresmall,thismaybejustafewpoints.
Mostnonparametricmodelshavetheadvantagethatitiseasytodoleave-one-outcross-
validation without having to recompute everything. With a k-nearest-neighbors model, for
instance, whengivenatestexample(x,y)weretrievetheknearestneighborsonce,compute
the per-example loss L(y,h(x)) from them, and record that as the leave-one-out result for
everyexamplethatisnotoneoftheneighbors. Thenweretrievethek+1nearestneighbors
and record distinct results for leaving out each of the k neighbors. With N examples the
wholeprocessisO(k),notO(kN).
18.9 SUPPORT VECTOR MACHINES
SUPPORTVECTOR ThesupportvectormachineorSVMframeworkiscurrently themostpopularapproach for
MACHINE
“off-the-shelf” supervised learning: ifyoudon’thaveanyspecialized priorknowledge about
a domain, then the SVM is an excellent method to try first. There are three properties that
makeSVMsattractive:
1. SVMsconstructamaximummarginseparator—adecisionboundarywiththelargest
possibledistance toexamplepoints. Thishelpsthemgeneralize well.
2. SVMs create a linear separating hyperplane, but they have the ability to embed the
dataintoahigher-dimensional space, using theso-called kerneltrick. Often,datathat
arenot linearly separable in the original input space are easily separable inthe higher-
dimensional space. The high-dimensional linear separator is actually nonlinear in the
original space. Thismeansthehypothesis space isgreatly expanded overmethods that
usestrictly linearrepresentations.
3. SVMsareanonparametricmethod—theyretaintrainingexamplesandpotentiallyneed
to store them all. On the other hand, in practice they often end up retaining only a
smallfractionofthenumberofexamples—sometimes asfewas asmallconstant times
the numberof dimensions. Thus SVMscombine the advantages of nonparametric and
parametric models: they have the flexibility to represent complex functions, but they
areresistant tooverfitting.
You could say that SVMs are successful because of one key insight and one neat trick. We
willcovereachinturn. InFigure18.30(a),wehaveabinaryclassificationproblemwiththree
candidate decision boundaries, each a linear separator. Each of them is consistent with all
the examples, so from the point of view of 0/1 loss, each would be equally good. Logistic
regression would find some separating line; the exact location of the line depends on all the
example points. The key insight of SVMs is that some examples are more important than
others, andthatpayingattention tothemcanleadtobettergeneralization.
Consider the lowest of the three separating lines in (a). It comes very close to 5 of the
black examples. Although itclassifies allthe examples correctly, andthus minimizes loss, it
Section18.9. SupportVectorMachines 745
1 1
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0 0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
(a) (b)
Figure18.30 Supportvectormachineclassification: (a)Twoclassesofpoints(blackand
white circles) and three candidate linear separators. (b) The maximum margin separator
(heavy line), is at the midpoint of the margin (area between dashed lines). The support
vectors(pointswithlargecircles)aretheexamplesclosesttotheseparator.
should make you nervous that so many examples are close to the line; it may be that other
blackexampleswillturnouttofallontheothersideoftheline.
SVMsaddressthisissue: Insteadofminimizingexpectedempiricallossonthetraining
data, SVMs attempt to minimize expected generalization loss. We don’t know where the
as-yet-unseen points may fall, but under the probabilistic assumption that they are drawn
from the same distribution as the previously seen examples, there are some arguments from
computationallearningtheory(Section18.5)suggestingthatweminimizegeneralizationloss
by choosing the separator that is farthest away from the examples we have seen so far. We
MAXIMUMMARGIN callthisseparator, showninFigure18.30(b) the maximummargin separator. Themargin
SEPARATOR
is the width of the area bounded by dashed lines in the figure—twice the distance from the
MARGIN
separatortothenearestexamplepoint.
Now, how do we find this separator? Before showing the equations, some notation:
Traditionally SVMsuse the convention that class labels are +1and -1, instead ofthe +1 and
0wehave been using so far. Also, where weput the intercept into the weight vector w(and
a corresponding dummy 1 value into x ), SVMs do not do that; they keep the intercept
j,0
as a separate parameter, b. With that in mind, the separator is defined as the set of points
{x : w·x+b=0}. We could search the space of w and b with gradient descent to find the
parameters thatmaximizethemarginwhilecorrectly classifying alltheexamples.
However, it turns out there is another approach to solving this problem. We won’t
show the details, but will just say that there is an alternative representation called the dual
746 Chapter 18. LearningfromExamples
representation, inwhichtheoptimalsolution isfoundbysolving
(cid:12) (cid:12)
1
argmax α − α α y y (x ·x ) (18.13)
j j k j k j k
2
α
j j,k
(cid:2)
QUADRATIC subject to the constraints α ≥ 0 and α y =0. This is a quadratic programming
PROGRAMMING j j j j
optimization problem, forwhichthere aregoodsoftware packages. Oncewehave found the
(cid:2)
vector α we can get back to w with the equation w= α x , or we can stay in the dual
j j j
representation. TherearethreeimportantpropertiesofEquation(18.13). First,theexpression
isconvex;ithasasingleglobalmaximumthatcanbefoundefficiently. Second,thedataenter
theexpressiononlyintheformofdotproductsofpairsofpoints. Thissecondpropertyisalso
trueoftheequation fortheseparatoritself;oncetheoptimalα havebeencalculated, itis
j
⎛ ⎞
(cid:12)
h(x) = sign ⎝ α y (x·x )−b ⎠ . (18.14)
j j j
j
Afinalimportant property isthattheweights α associated witheachdatapointarezeroex-
j
ceptforthe supportvectors—the points closest totheseparator. (Theyarecalled“support”
SUPPORTVECTOR
vectors because they “hold up” the separating plane.) Because there are usually manyfewer
supportvectorsthanexamples,SVMsgainsomeoftheadvantages ofparametricmodels.
Whatiftheexamples arenotlinearly separable? Figure18.31(a) showsaninput space
defined by attributes x=(x ,x ), with positive examples (y= +1) inside a circular region
1 2
andnegativeexamples(y=−1)outside. Clearly,thereisnolinearseparatorforthisproblem.
Now,supposewere-expresstheinputdata—i.e.,wemapeachinputvectorxtoanewvector
offeature values, F(x). Inparticular, letususethethreefeatures
√
f =x2 , f =x2 , f = 2x x . (18.15)
1 1 2 2 3 1 2
We will see shortly where these came from, but for now, just look at what happens. Fig-
ure18.31(b)showsthedatainthenew,three-dimensionalspacedefinedbythethreefeatures;
the data are linearly separable in this space! This phenomenon is actually fairly general: if
dataaremappedintoaspace ofsufficiently high dimension, thentheywillalmostalwaysbe
linearlyseparable—ifyoulookatasetofpointsfromenough directions, you’llfindawayto
makethemlineup. Here,weusedonlythreedimensions;11 Exercise18.16asksyoutoshow
thatfourdimensions sufficeforlinearly separating acircle anywhereintheplane(notjustat
theorigin),andfivedimensionssufficetolinearlyseparate anyellipse. Ingeneral(withsome
specialcasesexcepted)ifwehaveN datapointsthentheywillalwaysbeseparableinspaces
ofN −1dimensions ormore(Exercise18.25).
Now, we would not usually expect to find a linear separator in the input space x, but
wecanfindlinearseparatorsinthehigh-dimensional featurespaceF(x)simplybyreplacing
x ·x inEquation(18.13)withF(x )·F(x ). Thisbyitselfisnotremarkable—replacing xby
j k j k
F(x)inanylearningalgorithmhastherequiredeffect—butthedotproducthassomespecial
properties. It turns out that F(x )·F(x )can often becomputed without firstcomputing F
j k
11 Thereadermaynoticethatwecouldhaveusedjustf1andf2,butthe3Dmappingillustratestheideabetter.
Section18.9. SupportVectorMachines 747
1.5
1
0.5
0
-0.5
-1
-1.5
-1.5 -1 -0.5 0 0.5 1 1.5
x 2
√2xx
1 2
3
2
1
0
-1
-2 2.5
-3 2
0
1.5 0.5
1 1 x2
2
x2 1.5 0.5 1 2
x
1
(a) (b)
Figure 18.31 (a) A two-dimensional training set with positive examples as black cir-
cles and negative examples as white circles. The true decision boundary, x2 + x2 ≤ 1,
1 2
is also s√hown. (b) The same data after mapping into a three-dimensional input space
(x2,x2, 2x x ). Thecirculardecisionboundaryin(a)becomesalineardecisionboundary
1 2 1 2
inthreedimensions.Figure18.30(b)givesacloseupoftheseparatorin(b).
foreachpoint. Inourthree-dimensional featurespacedefinedbyEquation(18.15),alittlebit
ofalgebra showsthat
F(x )·F(x ) = (x ·x )2 .
j k j k
√
(That’s why the 2 is in f .) The expression (x · x )2 is called a kernel function,12 and
KERNELFUNCTION 3 j k
is usually written as K(x ,x ). The kernel function can be applied to pairs of input data to
j k
evaluate dotproducts insomecorresponding featurespace. So,wecanfindlinearseparators
inthehigher-dimensional featurespace F(x)simplybyreplacing x ·x inEquation(18.13)
j k
withakernelfunctionK(x ,x ). Thus,wecanlearninthehigher-dimensional space,butwe
j k
computeonlykernelfunctions ratherthanthefulllistoffeatures foreachdatapoint.
Thenextstepistoseethatthere’snothingspecialaboutthekernelK(x ,x )=(x ·x )2.
j k j k
It corresponds to a particular higher-dimensional feature space, but other kernel functions
correspond to other feature spaces. A venerable result in mathematics, Mercer’s theo-
rem (1909), tells us that any “reasonable”13 kernel function corresponds to some feature
MERCER’STHEOREM
space. These feature spaces can be very large, even for innocuous-looking kernels. For ex-
POLYNOMIAL ample, the polynomial kernel, K(x ,x )=(1 + x · x )d, corresponds to a feature space
KERNEL j k j k
whosedimension isexponential ind.
12 Thisusage of “kernel function” isslightly different fromthe kernels in locally weighted regression. Some
SVMkernelsaredistancemetrics,butnotallare.
13 Here,“reasonable”meansthatthematrixKjk=K(xj,xk)ispositivedefinite.
748 Chapter 18. LearningfromExamples
This then is the clever kernel trick: Plugging these kernels into Equation (18.13),
KERNELTRICK
optimal linear separators can be found efficiently in feature spaces with billions of (or, in
somecases, infinitely many)dimensions. Theresulting linearseparators, whenmappedback
to the original input space, can correspond to arbitrarily wiggly, nonlinear decision bound-
ariesbetweenthepositiveandnegativeexamples.
In the case of inherently noisy data, we may not want a linear separator in some high-
dimensional space. Rather, we’d like a decision surface in a lower-dimensional space that
does not cleanly separate the classes, but reflects the reality of the noisy data. That is pos-
sible withthe soft margin classifier, which allows examples to fall on the wrong side of the
SOFTMARGIN
decision boundary, but assigns them a penalty proportional to the distance required to move
thembackonthecorrectside.
The kernel method can be applied not only with learning algorithms that find optimal
linear separators, but also with any other algorithm that can be reformulated to work only
with dot products of pairs of data points, as in Equations 18.13 and 18.14. Once this is
done, the dot product is replaced by a kernel function and we have a kernelized version
KERNELIZATION
of the algorithm. This can be done easily for k-nearest-neighbors and perceptron learning
(Section18.7.2),amongothers.
18.10 ENSEMBLE LEARNING
So far we have looked at learning methods in which a single hypothesis, chosen from a
ENSEMBLE hypothesis space, is used to make predictions. The idea of ensemble learning methods is
LEARNING
to select a collection, or ensemble, of hypotheses from the hypothesis space and combine
their predictions. For example, during cross-validation we might generate twenty different
decision trees,andhavethemvoteonthebestclassification foranewexample.
The motivation for ensemble learning is simple. Consider an ensemble of K=5 hy-
pothesesandsupposethatwecombinetheirpredictionsusingsimplemajorityvoting. Forthe
ensembletomisclassify anewexample,atleastthreeofthefivehypotheses havetomisclas-
sifyit. Thehopeisthatthisismuchlesslikelythanamisclassificationbyasinglehypothesis.
Suppose we assume that each hypothesis h in the ensemble has an error of p—that is, the
k
probabilitythatarandomlychosenexampleismisclassifiedbyh isp. Furthermore,suppose
k
weassumethattheerrorsmadebyeachhypothesisareindependent. Inthatcase,ifpissmall,
then the probability of a large number of misclassifications occurring is minuscule. For ex-
ample,asimplecalculation(Exercise18.18)showsthatusinganensembleoffivehypotheses
reduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now, obviously
the assumption of independence isunreasonable, because hypotheses arelikely tobemisled
in the same way by any misleading aspects of the training data. But ifthe hypotheses are at
leastalittlebitdifferent,therebyreducingthecorrelationbetweentheirerrors,thenensemble
learning canbeveryuseful.
Another way to think about the ensemble idea is as a generic way of enlarging the
hypothesisspace. Thatis,thinkoftheensembleitselfasahypothesisandthenewhypothesis
Section18.10. EnsembleLearning 749
–
–
– – – –
–
–
– – – –
–
– + – –
– –
– + + –
+ ++ + – –
– + + –
+ ++ + +
– – –
– – – – – – – – – –
– –
– –
Figure18.32 Illustrationoftheincreasedexpressivepowerobtainedby ensemblelearn-
ing. We take three linear threshold hypotheses, each of which classifies positively on the
unshaded side, and classify as positive any example classified positively by all three. The
resultingtriangularregionisahypothesisnotexpressibleintheoriginalhypothesisspace.
spaceasthesetofallpossibleensemblesconstructablefromhypothesesintheoriginalspace.
Figure18.32showshowthiscanresultinamoreexpressivehypothesis space. Iftheoriginal
hypothesis space allows for a simple and efficient learning algorithm, then the ensemble
methodprovidesawaytolearnamuchmoreexpressiveclassofhypotheseswithoutincurring
muchadditional computational oralgorithmic complexity.
Themostwidelyusedensemblemethodiscalledboosting. Tounderstandhowitworks,
BOOSTING
WEIGHTEDTRAINING we need first to explain the idea of a weighted training set. In such a training set, each
SET
example has an associated weight w ≥ 0. The higher the weight of an example, the higher
j
is the importance attached to it during the learning of a hypothesis. It is straightforward to
modifythelearning algorithmswehaveseensofartooperate withweightedtraining sets.14
Boosting starts with w =1foralltheexamples (i.e., anormal training set). From this
j
set,itgeneratesthefirsthypothesis, h . Thishypothesiswillclassifysomeofthetrainingex-
1
amplescorrectly andsomeincorrectly. Wewouldlikethenexthypothesis todobetteronthe
misclassifiedexamples,soweincreasetheirweightswhiledecreasingtheweightsofthecor-
rectly classified examples. From this new weighted training set, wegenerate hypothesis h .
2
TheprocesscontinuesinthiswayuntilwehavegeneratedK hypotheses,whereK isaninput
totheboostingalgorithm. Thefinalensemblehypothesisisaweighted-majoritycombination
ofalltheKhypotheses,eachweightedaccordingtohowwellitperformedonthetrainingset.
Figure18.33showshowthealgorithmworksconceptually. Therearemanyvariantsoftheba-
sicboostingidea,withdifferentwaysofadjustingtheweightsandcombiningthehypotheses.
Onespecificalgorithm,calledADABOOST,isshowninFigure18.34. ADABOOSThasavery
important property: if the input learning algorithm L is a weak learning algorithm—which
WEAKLEARNING
14 Forlearningalgorithmsinwhichthisisnotpossible,onecaninsteadcreateareplicatedtrainingsetwhere
thejthexampleappearswj times,usingrandomizationtohandlefractionalweights.
750 Chapter 18. LearningfromExamples
h = h = h = h =
1 2 3 4
h
Figure18.33 Howtheboostingalgorithmworks. Eachshadedrectanglecorrespondsto
anexample; the heightof therectanglecorrespondsto theweight. Thechecksandcrosses
indicatewhethertheexamplewasclassifiedcorrectlybythecurrenthypothesis. Thesizeof
thedecisiontreeindicatestheweightofthathypothesisinthefinalensemble.
means that L always returns a hypothesis with accuracy on the training set that is slightly
betterthanrandomguessing(i.e.,50%+(cid:2)forBooleanclassification)—then ADABOOSTwill
return a hypothesis that classifies the training data perfectly for large enough K. Thus, the
algorithm boosts the accuracy of the original learning algorithm on the training data. This
result holds no matter how inexpressive the original hypothesis space and no matter how
complexthefunction beinglearned.
Letusseehowwellboostingdoesontherestaurantdata. Wewillchooseasouroriginal
hypothesis spacetheclassofdecision stumps,whicharedecision treeswithjustonetest,at
DECISIONSTUMP
the root. The lower curve in Figure 18.35(a) shows that unboosted decision stumps are not
veryeffectiveforthisdataset,reachingapredictionperformanceofonly81%on100training
examples. When boosting is applied (with K=5), the performance is better, reaching 93%
after100examples.
Aninteresting thing happens astheensemble size K increases. Figure18.35(b) shows
the training set performance (on 100 examples) as a function of K. Notice that the error
reaches zero when K is 20; that is, a weighted-majority combination of 20 decision stumps
sufficestofitthe100examplesexactly. Asmorestumpsareaddedtotheensemble, theerror
remains at zero. The graph also shows that the test set performance continues to increase
long after the training set error has reached zero. At K = 20, the test performance is 0.95
(or 0.05 error), and the performance increases to 0.98 as late as K = 137, before gradually
dropping to0.95.
Thisfinding,whichisquiterobustacrossdatasetsandhypothesisspaces,cameasquite
a surprise when it was first noticed. Ockham’s razor tells us not to make hypotheses more
Section18.10. EnsembleLearning 751
functionADABOOST(examples,L,K)returnsaweighted-majorityhypothesis
inputs:examples,setofN labeledexamples(x
1
,y
1
),...,(xN,yN)
L,alearningalgorithm
K,thenumberofhypothesesintheensemble
localvariables: w,avectorofN exampleweights,initially1/N
h,avectorofK hypotheses
z,avectorofK hypothesisweights
fork =1toK do
h[k]←L(examples,w)
error←0
forj =1toN do
ifh[k](xj)(cid:7)= yj thenerror←error + w[j]
forj =1toN do
ifh[k](xj)=yj thenw[j]←w[j]· error/(1−error)
w←NORMALIZE(w)
z[k]←log(1−error)/error
returnWEIGHTED-MAJORITY(h,z)
Figure18.34 TheADABOOSTvariantoftheboostingmethodforensemblelearning.The
algorithmgenerateshypothesesbysuccessivelyreweightingthetrainingexamples.Thefunc-
tion WEIGHTED-MAJORITY generates a hypothesis that returns the output value with the
highestvotefromthehypothesesinh,withvotesweightedbyz.
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0 20 40 60 80 100
tes
tset
no
tcerroc
noitroporP
1
0.95
0.9
0.85
0.8
0.75
Boosted decision stumps
Decision stump 0.7
0.65
0.6
0 50 100 150 200
Training set size
ycarucca
tset/gniniarT
Training error
Test error
Number of hypotheses K
(a) (b)
Figure18.35 (a)GraphshowingtheperformanceofboosteddecisionstumpswithK=5
versusunboosteddecisionstumpsonthe restaurantdata. (b)Theproportioncorrectonthe
trainingset and the test set as a functionof K, the numberof hypothesesin the ensemble.
Noticethatthetestsetaccuracyimprovesslightlyevenafterthetrainingaccuracyreaches1,
i.e.,aftertheensemblefitsthedataexactly.
752 Chapter 18. LearningfromExamples
complex than necessary, but the graph tells us that the predictions improve as the ensemble
hypothesis getsmorecomplex! Various explanations have been proposed forthis. Oneview
is that boosting approximates Bayesian learning (see Chapter 20), which can be shown to
be an optimal learning algorithm, and the approximation improves as more hypotheses are
added. Another possible explanation is that the addition of further hypotheses enables the
ensembletobemoredefiniteinitsdistinctionbetweenpositiveandnegativeexamples,which
helpsitwhenitcomestoclassifying newexamples.
18.10.1 OnlineLearning
Sofar,everything wehavedoneinthischapterhasreliedontheassumption thatthedataare
i.i.d. (independentandidenticallydistributed). Ontheonehand,thatisasensibleassumption:
ifthefuturebearsnoresemblancetothepast,thenhowcanwepredictanything? Ontheother
hand,itistoostronganassumption: itisrarethatourinputshavecapturedalltheinformation
thatwouldmakethefuturetrulyindependent ofthepast.
Inthissectionweexaminewhattodowhenthedataarenoti.i.d.;whentheycanchange
overtime. Inthiscase,itmatterswhenwemakeaprediction,sowewilladopttheperspective
calledonlinelearning: anagentreceivesaninputx fromnature,predictsthecorresponding
ONLINELEARNING j
y , and then is told the correct answer. Then the process repeats with x , and so on. One
j j+1
might think this task is hopeless—if nature is adversarial, all the predictions may be wrong.
Itturnsoutthattherearesomeguarantees wecanmake.
Let us consider the situation where our input consists of predictions from a panel of
experts. Forexample, each dayasetof K pundits predicts whetherthestock marketwillgo
upordown, and ourtask istopool those predictions and makeourown. Onewaytodo this
is to keep track of how welleach expert performs, and choose to believe them in proportion
RANDOMIZED
WEIGHTED totheirpastperformance. Thisiscalledthe randomizedweightedmajorityalgorithm. We
MAJORITY
ALGORITHM candescribed itmoreformally:
1. Initializeasetofweights{w ,...,w }allto1.
1 K
2. Receivethepredictions {yˆ ,...,yˆ }fromtheexperts.
1 K (cid:2)
∗
3. Randomlychooseanexpertk ,inproportion toitsweight: P(k)=w
k
/(
k(cid:3)
w k(cid:3)).
4. Predictyˆ k∗.
5. Receivethecorrectanswer y.
6. Foreachexpert k suchthatyˆ (cid:7)= y,updatew ←βw
k k k
Hereβ isanumber, 0 < β < 1,thattellshowmuchtopenalizeanexpertforeachmistake.
We measure the success of this algorithm in terms of regret, which is defined as the
REGRET
number of additional mistakes we make compared to the expert who, in hindsight, had the
∗
bestprediction record. Let M bethenumberofmistakesmadebythebestexpert. Thenthe
numberofmistakes, M,madebytherandom weightedmajorityalgorithm, isbounded by15
∗
M ln(1/β)+lnK
M < .
1−β
15 See(Blum,1996)fortheproof.
Section18.11. PracticalMachineLearning 753
This bound holds for any sequence of examples, even ones chosen by adversaries trying to
do their worst. To be specific, when there are K=10 experts, if we choose β=1/2 then
∗ ∗
our number of mistakes is bounded by 1.39M +4.6, and if β=3/4 by 1.15M +9.2. In
general,ifβiscloseto1thenweareresponsivetochangeoverthelongrun;ifthebestexpert
changes, wewillpick up onitbefore too long. However, wepay apenalty atthe beginning,
when we start with all experts trusted equally; we may accept the advice of the bad experts
fortoolong. Whenβ iscloserto0,thesetwofactorsarereversed. Notethatwecanchooseβ
∗
NO-REGRET togetasymptotically closetoM inthelongrun; thisiscalled no-regret learning(because
LEARNING
theaverageamountofregretpertrialtendsto0asthenumber oftrialsincreases).
Online learning is helpful when the data may be changing rapidly over time. It is also
useful forapplications thatinvolve alargecollection ofdatathatisconstantly growing, even
ifchangesaregradual. Forexample,withadatabaseofmillionsofWebimages,youwouldn’t
wanttotrain,say,alinearregressionmodelonallthedata, andthenretrainfromscratchevery
timeanewimageisadded. Itwouldbemorepracticaltohaveanonlinealgorithmthatallows
images to be added incrementally. For most learning algorithms based on minimizing loss,
thereisanonlineversionbasedonminimizingregret. Itisa bonusthatmanyoftheseonline
algorithms comewithguaranteed bounds onregret.
Tosomeobservers, itissurprising thattherearesuchtight bounds onhowwellwecan
do compared to a panel of experts. Toothers, the really surprising thing is that when panels
of human experts congregate—predicting stock market prices, sports outcomes, or political
contests—the viewing public is so willing to listen to them pontificate and so unwilling to
quantify theirerrorrates.
18.11 PRACTICAL MACHINE LEARNING
Wehaveintroducedawiderangeofmachinelearningtechniques,eachillustratedwithsimple
learningtasks. Inthissection,weconsidertwoaspectsofpracticalmachinelearning. Thefirst
involvesfindingalgorithmscapableoflearningtorecognizehandwrittendigitsandsqueezing
every last drop of predictive performance out of them. The second involves anything but—
pointingoutthatobtaining, cleaning,andrepresentingthedatacanbeatleastasimportantas
algorithm engineering.
18.11.1 Casestudy: Handwrittendigitrecognition
Recognizing handwritten digits is an important problem with many applications, including
automated sorting of mail by postal code, automated reading of checks and tax returns, and
dataentryforhand-held computers. Itisanareawhererapid progresshasbeenmade,inpart
because ofbetterlearning algorithms andinpartbecauseof theavailability ofbettertraining
sets. TheUnited StatesNational Institute ofScience and Technology (NIST)has archived a
database of 60,000 labeled digits, each 20×20=400 pixels with 8-bit grayscale values. It
hasbecomeoneofthestandardbenchmarkproblemsforcomparingnewlearningalgorithms.
SomeexampledigitsareshowninFigure18.36.
754 Chapter 18. LearningfromExamples
Figure18.36 ExamplesfromtheNISTdatabaseofhandwrittendigits.Toprow:examples
ofdigits0–9thatareeasytoidentify.Bottomrow:moredifficultexamplesofthesamedigits.
Manydifferent learning approaches have been tried. Oneofthe first, and probably the
simplest, is the 3-nearest-neighbor classifier, which also has the advantage of requiring no
training time. As a memory-based algorithm, however, it must store all 60,000 images, and
itsruntimeperformance isslow. Itachievedatesterrorrateof2.4%.
A single-hidden-layer neural network was designed for this problem with 400 input
units(oneperpixel)and10outputunits(oneperclass). Usingcross-validation, itwasfound
thatroughly300hiddenunitsgavethebestperformance. Withfullinterconnections between
layers, therewereatotalof123,300weights. Thisnetworkachieveda1.6%errorrate.
Aseries of specialized neuralnetworks called LeNetwere devised totake advantage
ofthestructure oftheproblem—that theinput consists ofpixelsinatwo–dimensional array,
and that small changes in the position or slant of an image are unimportant. Each network
hadaninputlayerof32×32units,ontowhichthe20×20pixelswerecenteredsothateach
inputunitispresentedwithalocalneighborhood ofpixels. Thiswasfollowedbythreelayers
of hidden units. Each layer consisted of several planes of n×n arrays, where n is smaller
thanthepreviouslayersothatthenetworkisdown-samplingtheinput,andwheretheweights
ofeveryunitinaplaneareconstrained tobeidentical, sothattheplaneisacting asafeature
detector: itcanpickoutafeaturesuchasalongverticallineorashortsemi-circulararc. The
output layerhad10units. Manyversions ofthisarchitecture weretried; arepresentative one
had hidden layers with768, 192, and 30units, respectively. Thetraining setwasaugmented
byapplyingaffinetransformations totheactualinputs: shifting,slightlyrotating,andscaling
theimages. (Ofcourse, thetransformations havetobesmall, orelse a6willbetransformed
intoa9!) ThebesterrorrateachievedbyLeNetwas0.9%.
A boosted neural network combined three copies of the LeNet architecture, with the
second one trained on a mix of patterns that the first one got 50% wrong, and the third one
trainedonpatternsforwhichthefirsttwodisagreed. During testing,thethreenetsvotedwith
themajorityruling. Thetesterrorratewas0.7%.
Asupportvectormachine(seeSection18.9)with25,000supportvectorsachievedan
error rate of 1.1%. This is remarkable because the SVM technique, like the simple nearest-
neighbor approach, required almostnothought oriterated experimentation onthepartofthe
developer, yetitstillcameclosetotheperformance ofLeNet,whichhadhadyearsofdevel-
opment. Indeed, the support vector machine makes no use of the structure of the problem,
andwouldperform justaswellifthepixelswerepresented in apermutedorder.
Section18.11. PracticalMachineLearning 755
VIRTUALSUPPORT A virtual support vector machine starts with a regular SVM and then improves it
VECTORMACHINE
withatechniquethatisdesignedtotakeadvantageofthestructureoftheproblem. Insteadof
allowingproducts ofallpixelpairs,thisapproach concentrates onkernels formedfrompairs
ofnearby pixels. Italsoaugments thetraining setwithtransformations ofthe examples, just
asLeNetdid. AvirtualSVMachievedthebesterrorraterecorded todate,0.56%.
Shapematchingisatechniquefromcomputervisionusedtoaligncorrespondingparts
of two different images of objects (Belongie et al., 2002). The idea is to pick out a set
of points in each of the two images, and then compute, for each point in the first image,
which point in the second image itcorresponds to. From this alignment, wethen compute a
transformation between the images. The transformation gives us a measure of the distance
betweentheimages. Thisdistancemeasureisbettermotivatedthanjustcountingthenumber
of differing pixels, and it turns out that a 3–nearest neighbor algorithm using this distance
measure performs very well. Training on only 20,000 of the 60,000 digits, and using 100
sample points per image extracted from a Canny edge detector, a shape matching classifier
achieved0.63%testerror.
Humansareestimatedtohaveanerrorrateofabout0.2%onthisproblem. Thisfigure
is somewhat suspect because humans have not been tested as extensively as have machine
learning algorithms. On a similar data set of digits from the United States Postal Service,
humanerrorswereat2.5%.
The following figure summarizes the error rates, run time performance, memory re-
quirements, and amount oftraining timeforthe seven algorithms wehave discussed. Italso
adds another measure, the percentage of digits that must be rejected to achieve 0.5% error.
For example, if the SVM is allowed to reject 1.8% of the inputs—that is, pass them on for
someone else to make the final judgment—then its error rate on the remaining 98.2% of the
inputsisreduced from1.1%to0.5%.
The following table summarizes the error rate and some of the other characteristics of
theseventechniques wehavediscussed.
3 300 Boosted Virtual Shape
NN Hidden LeNet LeNet SVM SVM Match
Errorrate(pct.) 2.4 1.6 0.9 0.7 1.1 0.56 0.63
Runtime(millisec/digit) 1000 10 30 50 2000 200
Memoryrequirements (Mbyte) 12 .49 .012 .21 11
Trainingtime(days) 0 7 14 30 10
%rejectedtoreach0.5%error 8.1 3.2 1.8 0.5 1.8
18.11.2 Casestudy: Wordsenses and houseprices
In a textbook weneed to deal with simple, toy data to get the ideas across: a small data set,
usually in two dimensions. But in practical applications of machine learning, the data set
is usually large, multidimensional, and messy. The data are not handed to the analyst in a
prepackaged setof(x,y)values;rathertheanalystneedstogooutandacquiretherightdata.
There is a task to be accomplished, and most of the engineering problem is deciding what
data are necessary to accomplish the task; a smaller part is choosing and implementing an
756 Chapter 18. LearningfromExamples
1
0.95
0.9
0.85
0.8
0.75
1 10 100 1000
tes
tset
no
tcerroc
noitroporP
Training set size (millions of words)
Figure18.37 Learningcurvesforfivelearningalgorithmsona commontask. Notethat
there appears to be more room for improvementin the horizontal direction (more training
data) than in the vertical direction (different machine learning algorithm). Adapted from
BankoandBrill(2001).
appropriate machine learning method toprocess the data. Figure18.37 showsatypical real-
world example, comparing five learning algorithms on the task of word-sense classification
(given a sentence such as “The bank folded,” classify the word “bank” as “money-bank” or
“river-bank”). The point is that machine learning researchers have focused mainly on the
verticaldirection: CanIinventanewlearningalgorithm thatperformsbetterthanpreviously
published algorithms on a standard training set of 1 million words? But the graph shows
there is more room for improvement in the horizontal direction: instead of inventing a new
algorithm,allIneedtodoisgather10millionwordsoftrainingdata;eventheworstalgorithm
at 10 million words is performing better than the best algorithm at 1 million. As we gather
evenmoredata,thecurvescontinuetorise,dwarfingthedifferences betweenalgorithms.
Consider another problem: the task of estimating the true value of houses that are for
sale. In Figure 18.13 we showed a toy version of this problem, doing linear regression of
house size to asking price. You probably noticed many limitations of this model. First, it is
measuring the wrong thing: we want to estimate the selling price of a house, not the asking
price. To solve this task we’ll need data on actual sales. But that doesn’t mean we should
throw away the data about asking price—we can use it as one of the input features. Besides
the size of the house, we’ll need more information: the number of rooms, bedrooms and
bathrooms; whether the kitchen and bathrooms have been recently remodeled; the age of
the house; we’ll also need information about the lot, and the neighborhood. But how do
we define neighborhood? By zip code? What if part of one zip code is on the “wrong”
side of the highway or train tracks, and the other part is desirable? What about the school
district? Should the name of the school district be a feature, or the average test scores? In
additiontodecidingwhatfeaturestoinclude,wewillhavetodealwithmissingdata;different
areas have different customs on what data are reported, and individual cases will always be
missing some data. If the data you want are not available, perhaps you can set up a social
networking site to encourage people to share and correct data. In the end, this process of
Section18.12. Summary 757
deciding whatfeatures touse,andhowtousethem,isjustasimportant aschoosing between
linearregression, decision trees,orsomeotherformoflearning.
That said, one does have to pick a method (or methods) for a problem. There is no
guaranteed way to pick the best method, but there are some rough guidelines. Decision
trees are good when there are a lot of discrete features and you believe that many of them
maybeirrelevant. Nonparametricmethodsaregoodwhenyouhavealotofdataandnoprior
knowledge,andwhenyoudon’twanttoworrytoomuchaboutchoosingjusttherightfeatures
(aslongastherearefewerthan20orso). However,nonparametric methodsusuallygiveyou
afunction hthatismoreexpensive torun. Supportvectormachines areoftenconsidered the
bestmethodtotryfirst,providedthedatasetisnottoolarge.
18.12 SUMMARY
This chapter has concentrated on inductive learning of functions from examples. The main
pointswereasfollows:
• Learningtakesmanyforms,depending onthenatureoftheagent, thecomponent tobe
improved,andtheavailablefeedback.
• Iftheavailablefeedbackprovidesthecorrectanswerforexampleinputs,thenthelearn-
ing problem is called supervised learning. The task is to learn a function y = h(x).
Learningadiscrete-valued functioniscalledclassification;learningacontinuousfunc-
tioniscalledregression.
• Inductive learning involves finding a hypothesis that agrees well with the examples.
Ockham’s razor suggests choosing the simplest consistent hypothesis. The difficulty
ofthistaskdepends onthechosen representation.
• Decision trees can represent all Boolean functions. The information-gain heuristic
providesanefficientmethodforfindingasimple,consistent decision tree.
• The performance of a learning algorithm is measured by the learning curve, which
showstheprediction accuracy onthe testsetasafunctionofthetraining-setsize.
• Whentherearemultiplemodelstochoosefrom, cross-validation canbeusedtoselect
amodelthatwillgeneralize well.
• Sometimes not all errors are equal. A loss function tells us how bad each error is; the
goalisthentominimizelossoveravalidation set.
• Computational learning theory analyzes the sample complexity and computational
complexityofinductive learning. Thereisatradeoff betweentheexpressiveness ofthe
hypothesis language andtheeaseoflearning.
• Linear regression is a widely used model. The optimal parameters of a linear regres-
sionmodelcanbefoundbygradientdescent search,orcomputedexactly.
• Alinearclassifierwithahardthreshold—also knownasa perceptron—can betrained
by a simple weight update rule to fit data that are linearly separable. In other cases,
therulefailstoconverge.
758 Chapter 18. LearningfromExamples
• Logistic regression replaces the perceptron’s hard threshold with a soft threshold de-
fined by a logistic function. Gradient descent works well even for noisy data that are
notlinearly separable.
• Neural networks represent complex nonlinear functions with a network of linear-
threshold units. termMultilayer feed-forward neural networks can represent any func-
tion, given enough units. The back-propagation algorithm implements a gradient de-
scentinparameterspacetominimizetheoutputerror.
• Nonparametric models use all the data to make each prediction, rather than trying to
summarize the data first with a few parameters. Examples include nearest neighbors
andlocallyweightedregression.
• Support vector machines find linear separators with maximum margin to improve
the generalization performance of the classifier. Kernel methodsimplicitly transform
theinputdataintoahigh-dimensional spacewherealinearseparatormayexist,evenif
theoriginal dataarenon-separable.
• Ensemble methods such as boosting often perform better than individual methods. In
onlinelearningwecanaggregatetheopinionsofexpertstocomearbitrarily closetothe
bestexpert’sperformance, evenwhenthedistribution ofthedataisconstantly shifting.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Chapter1outlinedthehistoryofphilosophicalinvestigationsintoinductivelearning. William
of Ockham16 (1280–1349), the most influential philosopher of his century and a majorcon-
tributortomedievalepistemology, logic,andmetaphysics, iscreditedwithastatementcalled
“Ockham’sRazor”—inLatin,Entianonsuntmultiplicanda praeternecessitatem, andinEn-
glish, “Entitiesarenottobemultipliedbeyond necessity.” Unfortunately, thislaudable piece
of advice is nowhere to be found in his writings in precisely these words (although he did
say “Pluralitas non est ponenda sine necessitate,” or “plurality shouldn’t be posited without
necessity”). A similar sentiment was expressed by Aristotle in 350 B.C. in Physics book I,
chapterVI:“Forthemorelimited,ifadequate, isalwayspreferable.”
The first notable use of decision trees was in EPAM, the “Elementary Perceiver And
Memorizer” (Feigenbaum, 1961), which was a simulation of human concept learning. ID3
(Quinlan, 1979)addedthecrucialideaofchoosing theattribute withmaximumentropy; itis
thebasisforthedecisiontreealgorithminthischapter. Informationtheorywasdevelopedby
Claude Shannon to aid in the study of communication (Shannon and Weaver, 1949). (Shan-
non also contributed one of the earliest examples of machine learning, a mechanical mouse
named Theseus that learned to navigate through a maze by trial and error.) The χ2 method
of tree pruning was described by Quinlan (1986). C4.5, an industrial-strength decision tree
package, canbefound inQuinlan (1993). Anindependent tradition ofdecision treelearning
exists inthestatistical literature. Classification and Regression Trees(Breiman etal.,1984),
knownasthe“CARTbook,”istheprincipal reference.
16 Thenameisoftenmisspelledas“Occam,”perhapsfromtheFrenchrendering,“Guillaumed’Occam.”
Bibliographical andHistorical Notes 759
Cross-validation was first introduced by Larson (1931), and in a form close to what
we show by Stone (1974) and Golub et al. (1979). The regularization procedure is due to
Tikhonov (1963). Guyon and Elisseeff (2003) introduce ajournal issue devoted tothe prob-
lemoffeature selection. Bankoand Brill(2001) andHalevy etal.(2009) discuss theadvan-
tages of using large amounts of data. It was Robert Mercer, a speech researcher who said
in 1985 “There is no data like more data.” (Lyman and Varian, 2003) estimate that about 5
exabytes (5 ×1018 bytes) of data was produced in 2002, and that the rate of production is
doubling every3years.
Theoretical analysis of learning algorithms began with the work of Gold (1967) on
identification in the limit. This approach was motivated in part by models of scientific
discovery fromthephilosophy ofscience (Popper, 1962), buthasbeenapplied mainlytothe
problem oflearning grammarsfromexamplesentences (Oshersonetal.,1986).
Whereastheidentification-in-the-limit approachconcentratesoneventualconvergence,
KOLMOGOROV the study ofKolmogorov complexityor algorithmic complexity, developed independently
COMPLEXITY
bySolomonoff(1964,2009)andKolmogorov(1965),attemptstoprovideaformaldefinition
for the notion of simplicity used in Ockham’s razor. To escape the problem that simplicity
depends on the way in which information is represented, it is proposed that simplicity be
measuredbythelengthoftheshortest program forauniversal Turingmachinethatcorrectly
reproduces theobserved data. Although there aremany possible universal Turing machines,
and hence many possible “shortest” programs, these programs differ in length by at most a
constant that is independent of the amount of data. This beautiful insight, which essentially
shows that any initial representation bias will eventually be overcome by the data itself, is
marredonlybytheundecidability ofcomputing thelength of theshortest program. Approx-
MINIMUM
imate measures such as the minimum description length, or MDL (Rissanen, 1984, 2007)
DESCRIPTION
LENGTH
can be used instead and have produced excellent results in practice. The text by Li and Vi-
tanyi(1993)isthebestsourceforKolmogorovcomplexity.
ThetheoryofPAC-learningwasinauguratedbyLeslieValiant(1984). Hisworkstressed
theimportanceofcomputationalandsamplecomplexity. WithMichaelKearns(1990),Valiant
showed that several concept classes cannot be PAC-learned tractably, even though sufficient
informationisavailableintheexamples. Somepositiveresultswereobtainedforclassessuch
asdecision lists(Rivest,1987).
Anindependent traditionofsample-complexity analysishasexistedinstatistics, begin-
UNIFORM
ningwiththeworkonuniformconvergence theory(VapnikandChervonenkis, 1971). The
CONVERGENCE
THEORY
so-calledVCdimensionprovidesameasureroughlyanalogousto,butmoregeneralthan,the
VCDIMENSION
ln|H|measureobtainedfromPACanalysis. TheVCdimensioncanbeappliedtocontinuous
function classes, to which standard PAC analysis does not apply. PAC-learning theory and
VCtheory were firstconnected by the “four Germans” (none ofwhom actually is German):
Blumer,Ehrenfeucht, Haussler, andWarmuth(1989).
Linear regression with squared error loss goes back to Legendre (1805) and Gauss
(1809), who were both working on predicting orbits around the sun. The modern use of
multivariate regression for machine learning is covered in texts such as Bishop (2007). Ng
(2004)analyzed thedifferences between L andL regularization.
1 2
760 Chapter 18. LearningfromExamples
Theterm logistic functioncomesfromPierre-Franc¸oisVerhulst (1804–1849), astatis-
tician whoused the curve tomodel population growth withlimited resources, amore realis-
tic model than the unconstrained geometric growth proposed by Thomas Malthus. Verhulst
called it the courbe logistique, because of its relation tothe logarithmic curve. Theterm re-
gression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin,
andinitiatorofthefieldsofmeteorology, fingerprintanalysis, andstatistical correlation, who
useditinthesenseofregressiontothemean. Thetermcurseofdimensionalitycomesfrom
RichardBellman(1961).
Logistic regression can be solved with gradient descent, or with the Newton-Raphson
method (Newton,1671; Raphson, 1690). Avariant oftheNewtonmethodcalled L-BFGSis
sometimesusedforlarge-dimensional problems;theLstandsfor“limitedmemory,”meaning
that it avoids creating the full matrices all at once, and instead creates parts of them on the
fly. BFGSareauthors’initials (Byrd etal.,1995).
Nearest-neighbors modelsdatebackatleasttoFixandHodges(1951)andhavebeena
standardtoolinstatisticsandpatternrecognitioneversince. WithinAI,theywerepopularized
byStanfillandWaltz(1986),whoinvestigatedmethodsforadaptingthedistancemetrictothe
data. HastieandTibshirani(1996)developedawaytolocalizethemetrictoeachpointinthe
space,dependingonthedistributionofdataaroundthatpoint. Gionisetal.(1999)introduced
locality-sensitive hashing, which has revolutionized the retrieval of similar objects in high-
dimensional spaces, particularly in computer vision. Andoni and Indyk (2006) provide a
recentsurveyofLSHandrelatedmethods.
The ideas behind kernel machines come from Aizerman et al. (1964) (who also in-
troduced the kernel trick), but the full development of the theory is due to Vapnik and his
colleagues (Boser et al., 1992). SVMs were made practical with the introduction of the
soft-margin classifier for handling noisy data in a paper that won the 2008 ACM Theory
and Practice Award(Cortes and Vapnik, 1995), andof theSequential Minimal Optimization
(SMO)algorithm forefficientlysolving SVMproblemsusingquadratic programming (Platt,
1999). SVMshave proven tobevery popular and effective fortasks such astextcategoriza-
tion(Joachims,2001),computationalgenomics(CristianiniandHahn,2007),andnaturallan-
guageprocessing,suchasthehandwrittendigitrecognitionofDeCosteandScho¨lkopf(2002).
As part of this process, many new kernels have been designed that work with strings, trees,
and othernonnumerical data types. Arelated technique that also uses the kernel trick toim-
plicitly represent an exponential feature space isthe voted perceptron (Freund andSchapire,
1999; Collins and Duffy, 2002). Textbooks on SVMs include Cristianini and Shawe-Taylor
(2000)andScho¨lkopf andSmola(2002). Afriendlierexposition appearsintheAIMagazine
article by Cristianini and Scho¨lkopf (2002). Bengio and LeCun (2007) show some of the
limitations ofSVMsandotherlocal,nonparametric methods forlearning functions thathave
aglobalstructure butdonothavelocalsmoothness.
Ensemblelearningisanincreasingly populartechnique for improvingtheperformance
of learning algorithms. Bagging (Breiman, 1996), the first effective method, combines hy-
BAGGING
potheseslearnedfrommultiple bootstrapdatasets,eachgeneratedbysubsamplingtheorig-
inaldataset. Theboostingmethoddescribedinthischapteroriginatedwiththeoretical work
by Schapire (1990). The ADABOOST algorithm was developed by Freund and Schapire
Bibliographical andHistorical Notes 761
(1996) and analyzed theoretically bySchapire (2003). Friedman etal. (2000) explain boost-
ing from a statistician’s viewpoint. Online learning is covered in a survey by Blum (1996)
and a book by Cesa-Bianchi and Lugosi (2006). Dredze et al. (2008) introduce the idea of
confidence-weighted online learning for classification: in addition to keeping a weight for
eachparameter, theyalsomaintain ameasureofconfidence, sothatanewexamplecanhave
a large effect on features that were rarely seen before (and thus had low confidence) and a
smalleffectoncommonfeaturesthathavealready beenwell-estimated.
The literature on neural networks is rather too large (approximately 150,000 papers to
date)tocoverindetail. CowanandSharp(1988b, 1988a)surveytheearlyhistory, beginning
with the work of McCulloch and Pitts (1943). (As mentioned in Chapter 1, John McCarthy
haspointedtotheworkofNicolasRashevsky(1936,1938)astheearliestmathematicalmodel
of neural learning.) Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,
1948), worked with McCulloch and Pitts and influenced a number of young researchers in-
cludingMarvinMinsky,whomayhavebeenthefirsttodevelopaworkingneuralnetworkin
hardware in 1951 (see Minsky and Papert, 1988, pp. ix–x). Turing (1948) wrote a research
report titled Intelligent Machinery thatbegins withthesentence “Ipropose toinvestigate the
questionastowhetheritispossibleformachinerytoshowintelligentbehaviour”andgoeson
todescribe arecurrent neuralnetworkarchitecture hecalled“B-typeunorganized machines”
andanapproachtotrainingthem. Unfortunately, thereport wentunpublisheduntil1969,and
wasallbutignoreduntilrecently.
Frank Rosenblatt (1957) invented the modern “perceptron” and proved the percep-
tron convergence theorem (1960), although it had been foreshadowed by purely mathemat-
ical work outside the context of neural networks (Agmon, 1954; Motzkin and Schoenberg,
1954). Some early work was also done on multilayer networks, including Gamba percep-
trons (Gamba et al., 1961) and madalines (Widrow, 1962). Learning Machines (Nilsson,
1965) covers muchof thisearly workand more. Thesubsequent demise ofearly perceptron
research efforts was hastened (or, the authors later claimed, merely explained) by the book
Perceptrons (Minsky and Papert, 1969), which lamented the field’s lack of mathematical
rigor. Thebook pointed outthat single-layer perceptrons could represent only linearly sepa-
rableconcepts andnotedthelackofeffectivelearning algorithms formultilayernetworks.
The papers in (Hinton and Anderson, 1981), based on a conference in San Diego in
1979, can be regarded as marking a renaissance of connectionism. The two-volume “PDP”
(Parallel Distributed Processing) anthology (Rumelhart et al., 1986a) and a short article in
Nature (Rumelhart et al., 1986b) attracted a great deal of attention—indeed, the number of
papers on “neural networks” multiplied by a factor of 200 between 1980–84 and 1990–94.
The analysis of neural networks using the physical theory of magnetic spin glasses (Amit
et al., 1985) tightened the links between statistical mechanics and neural network theory—
providingnotonlyusefulmathematicalinsightsbutalsorespectability. Theback-propagation
techniquehadbeeninventedquiteearly(BrysonandHo,1969)butitwasrediscoveredseveral
times(Werbos,1974;Parker,1985).
Theprobabilisticinterpretationofneuralnetworkshasseveralsources,includingBaum
and Wilczek (1988) and Bridle (1990). The role of the sigmoid function is discussed by
Jordan (1995). Bayesian parameter learning for neural networks was proposed by MacKay
762 Chapter 18. LearningfromExamples
(1992) and is explored further by Neal(1996). Thecapacity ofneural networks to represent
functionswasinvestigated byCybenko(1988,1989),whoshowedthattwohiddenlayersare
enough to represent any function and a single layer is enough to represent any continuous
function. The“optimalbraindamage”methodforremovinguselessconnectionsisbyLeCun
et al. (1989), and Sietsma and Dow (1988) show how to remove useless units. The tiling
algorithm for growing larger structures is due to Me´zard and Nadal (1989). LeCun et al.
(1995)surveyanumberofalgorithmsforhandwrittendigitrecognition. Improvederrorrates
since then were reported by Belongie et al. (2002) for shape matching and DeCoste and
Scho¨lkopf (2002) for virtual support vectors. At the time of writing, the best test error rate
reported is0.39%byRanzato etal.(2007)usingaconvolutional neuralnetwork.
Thecomplexityofneuralnetworklearninghasbeeninvestigatedbyresearchersincom-
putational learning theory. Early computational results were obtained by Judd (1990), who
showedthatthegeneralproblemoffindingasetofweightsconsistentwithasetofexamples
isNP-complete,evenunderveryrestrictiveassumptions. Someofthefirstsamplecomplexity
results were obtained by Baum and Haussler (1989), who showed that the number of exam-
ples required for effective learning grows as roughly W logW, where W is the number of
weights.17 Since then, a much more sophisticated theory has been developed (Anthony and
Bartlett, 1999), including theimportant resultthattherepresentational capacity ofanetwork
depends on the size of the weights as well as on their number, a result that should not be
surprising inthelightofourdiscussion ofregularization.
The most popular kind of neural network that we did not cover is the radial basis
RADIALBASIS function,orRBF,network. Aradialbasisfunctioncombinesaweightedcollectionofkernels
FUNCTION
(usuallyGaussians,ofcourse)todofunctionapproximation. RBFnetworkscanbetrainedin
two phases: first, an unsupervised clustering approach is used to train the parameters of the
Gaussians—the meansandvariances—are trained, asinSection20.3.1. Inthesecondphase,
the relative weights of the Gaussians are determined. This is a system of linear equations,
whichweknowhowtosolvedirectly. Thus,bothphasesofRBFtraininghaveanicebenefit:
thefirstphaseisunsupervised, andthusdoesnotrequirelabeledtrainingdata,andthesecond
phase,although supervised, isefficient. SeeBishop(1995) formoredetails.
Recurrentnetworks,inwhichunitsarelinked incycles, werementioned inthechap-
HOPFIELDNETWORK ter but not explored in depth. Hopfield networks (Hopfield, 1982) are probably the best-
understood class of recurrent networks. They use bidirectional connections with symmetric
weights (i.e., w = w ), all of the units are both input and output units, the activation
i,j j,i
function g isthesignfunction, andtheactivation levelscanonlybe±1. AHopfieldnetwork
ASSOCIATIVE functions as an associative memory: after the network trains on a set of examples, a new
MEMORY
stimulus will cause it to settle into an activation pattern corresponding to the example in the
trainingsetthatmostcloselyresemblesthenewstimulus. Forexample,ifthetrainingsetcon-
sistsofasetofphotographs, andthenewstimulusisasmallpieceofoneofthephotographs,
then the network activation levels will reproduce the photograph from which the piece was
taken. Notice that the original photographs are not stored separately in the network; each
17 Thisapproximatelyconfirmed“UncleBernie’srule.” TherulewasnamedafterBernieWidrow,whorecom-
mendedusingroughlytentimesasmanyexamplesasweights.
Exercises 763
weight is a partial encoding of all the photographs. One of the most interesting theoretical
resultsisthatHopfieldnetworkscanreliablystoreupto0.138N trainingexamples,whereN
isthenumberofunitsinthenetwork.
BOLTZMANN Boltzmannmachines(HintonandSejnowski,1983,1986)alsousesymmetricweights,
MACHINE
but include hidden units. In addition, they use a stochastic activation function, such that
the probability of the output being 1 is some function of the total weighted input. Boltz-
mannmachinesthereforeundergostatetransitionsthatresembleasimulatedannealingsearch
(see Chapter4)forthe configuration that bestapproximates thetraining set. Itturns out that
BoltzmannmachinesareverycloselyrelatedtoaspecialcaseofBayesiannetworksevaluated
withastochastic simulationalgorithm. (SeeSection14.5.)
Forneuralnets,Bishop(1995),Ripley(1996),andHaykin(2008)aretheleadingtexts.
Thefieldofcomputational neuroscience iscoveredbyDayanandAbbott(2001).
TheapproachtakeninthischapterwasinfluencedbytheexcellentcoursenotesofDavid
Cohn,TomMitchell,AndrewMoore,andAndrewNg. Thereareseveraltop-notchtextbooks
inMachineLearning(Mitchell,1997;Bishop,2007)andinthecloselyalliedandoverlapping
fields of pattern recognition (Ripley, 1996; Duda et al., 2001), statistics (Wasserman, 2004;
Hastieetal.,2001), datamining(Handetal.,2001; WittenandFrank, 2005), computational
learning theory(KearnsandVazirani,1994;Vapnik, 1998)andinformation theory(Shannon
and Weaver, 1949; MacKay, 2002; Cover and Thomas, 2006). Other books concentrate on
implementations (Segaran, 2007; Marsland, 2009) and comparisons of algorithms (Michie
et al., 1994). Current research in machine learning is published in the annual proceedings
oftheInternational Conference on Machine Learning (ICML)and theconference on Neural
Information Processing Systems (NIPS), in Machine Learning and the Journal of Machine
LearningResearch,andinmainstreamAIjournals.
EXERCISES
18.1 Considertheproblem facedbyaninfantlearningtospeakand understand alanguage.
Explain how this process fits into the general learning model. Describe the percepts and
actions oftheinfant, andthetypes oflearning the infant mustdo. Describe thesubfunctions
theinfantistryingtolearnintermsofinputsandoutputs, andavailable exampledata.
18.2 Repeat Exercise 18.1 forthe case oflearning to play tennis (orsome other sport with
whichyouarefamiliar). Isthissupervised learningorreinforcement learning?
18.3 Suppose we generate a training set from a decision tree and then apply decision-tree
learning to that training set. Is it the case that the learning algorithm will eventually return
thecorrecttreeasthetraining-set sizegoestoinfinity? Whyorwhynot?
18.4 In the recursive construction of decision trees, it sometimes happens that a mixed set
of positive and negative examples remains at a leaf node, even after all the attributes have
beenused. Supposethatwehaveppositiveexamplesandnnegativeexamples.
764 Chapter 18. LearningfromExamples
a. ShowthatthesolutionusedbyDECISION-TREE-LEARNING,whichpicksthemajority
classification, minimizestheabsolute erroroverthesetof examplesattheleaf.
b. Showthattheclassprobabilityp/(p+n)minimizesthesumofsquared errors.
CLASSPROBABILITY
18.5 Suppose that an attribute splits the set of examples E into subsets E and that each
k
subsethasp positiveexamplesandn negativeexamples. Showthattheattributehasstrictly
k k
positiveinformation gainunlesstheratio p /(p +n )isthesameforallk.
k k k
18.6 Considerthefollowingdatasetcomprisedofthreebinaryinputattributes(A ,A ,and
1 2
A )andonebinaryoutput:
3
Example A A A Outputy
1 2 3
x 1 0 0 0
1
x 1 0 1 0
2
x 0 1 0 0
3
x 1 1 1 1
4
x 1 1 0 1
5
Usethealgorithm inFigure18.5(page 702)tolearn adecision treeforthesedata. Showthe
computations madetodeterminetheattribute tosplitateachnode.
18.7 Adecisiongraphisageneralizationofadecisiontreethatallowsnodes(i.e.,attributes
used forsplits) tohavemultiple parents, ratherthan justa single parent. Theresulting graph
muststillbeacyclic. Now,considertheXORfunction ofthreebinaryinputattributes, which
produces thevalue1ifandonlyifanoddnumberofthethreeinputattributes hasvalue1.
a. Drawaminimal-sized decision treeforthethree-input XORfunction.
b. Drawaminimal-sized decision graphforthethree-input XORfunction.
18.8 Thisexerciseconsiders χ2 pruning ofdecision trees(Section18.3.5).
a. Createadata setwithtwoinput attributes, such that theinformation gain atthe rootof
thetreeforbothattributesiszero,butthereisadecisiontreeofdepth2thatisconsistent
with all the data. What would χ2 pruning do on this data set if applied bottom up? If
appliedtopdown?
b. Modify DECISION-TREE-LEARNING to include χ2-pruning. You might wish to con-
sultQuinlan(1986)orKearnsandMansour(1998)fordetails.
18.9 The standard DECISION-TREE-LEARNING algorithm described in the chapter does
nothandlecasesinwhichsomeexampleshavemissingattribute values.
a. First,weneedtofindawaytoclassifysuchexamples,givenadecisiontreethatincludes
tests onthe attributes forwhich values can be missing. Suppose that anexample xhas
a missing value for attribute A and that the decision tree tests for A at a node that x
reaches. One way to handle this case is to pretend that the example has all possible
values for the attribute, but to weight each value according to its frequency among all
of the examples that reach that node in the decision tree. The classification algorithm
shouldfollowallbranchesatanynodeforwhichavalueismissingandshouldmultiply
Exercises 765
theweightsalongeachpath. Writeamodifiedclassificationalgorithmfordecisiontrees
thathasthisbehavior.
b. Now modify the information-gain calculation so that in any given collection of exam-
ples C at a given node in the tree during the construction process, the examples with
missingvaluesforanyoftheremaining attributes aregiven “as-if”values according to
thefrequencies ofthosevaluesinthesetC.
18.10 In Section 18.3.6, we noted that attributes with many different possible values can
causeproblemswiththegainmeasure. Suchattributes tendtosplittheexamplesintonumer-
oussmallclassesorevensingletonclasses,therebyappearingtobehighlyrelevantaccording
tothegainmeasure. Thegain-ratiocriterionselectsattributesaccordingtotheratiobetween
their gain and their intrinsic information content—that is, the amount of information con-
tainedintheanswertothequestion,“Whatisthevalueofthisattribute?” Thegain-ratiocrite-
rionthereforetriestomeasurehowefficientlyanattribute providesinformationonthecorrect
classification ofanexample. Writeamathematical expression fortheinformation content of
anattribute, andimplementthegainratiocriterion inDECISION-TREE-LEARNING.
18.11 SupposeyouarerunningalearningexperimentonanewalgorithmforBooleanclas-
sification. You have a data set consisting of 100 positive and 100 negative examples. You
plantouseleave-one-outcross-validation andcompareyouralgorithmtoabaselinefunction,
a simple majority classifier. (A majority classifier is given a set of training data and then
always outputs the class that is in the majority in the training set, regardless of the input.)
You expect the majority classifier to score about 50% on leave-one-out cross-validation, but
toyoursurprise, itscoreszeroeverytime. Canyouexplainwhy?
18.12 Construct a decision list to classify the data below. Select tests to be as small as
possible(intermsofattributes), breakingtiesamongtestswiththesamenumberofattributes
byselectingtheonethatclassifiesthegreatestnumberofexamplescorrectly. Ifmultipletests
havethesamenumberofattributesandclassifythesamenumberofexamples,thenbreakthe
tieusingattributes withlowerindexnumbers(e.g.,select A overA ).
1 2
Example A A A A y
1 2 3 4
x 1 0 0 0 1
1
x 1 0 1 1 1
2
x 0 1 0 0 1
3
x 0 1 1 0 0
4
x 1 1 0 1 1
5
x 0 1 0 1 0
6
x 0 0 1 1 1
7
x 0 0 1 0 0
8
18.13 Prove that a decision list can represent the same function as a decision tree while
using atmostasmanyrulesasthereareleavesinthedecision treeforthatfunction. Givean
exampleofafunctionrepresentedbyadecisionlistusingstrictlyfewerrulesthanthenumber
ofleavesinaminimal-sized decisiontreeforthatsamefunction.
766 Chapter 18. LearningfromExamples
18.14 Thisexerciseconcernstheexpressiveness ofdecisionlists(Section18.5).
a. Show that decision lists can represent any Boolean function, if the size of the tests is
notlimited.
b. Showthatifthetestscancontainatmostkliteralseach,thendecisionlistscanrepresent
anyfunction thatcanberepresented byadecision treeofdepthk.
18.15 Supposea7-nearest-neighbors regressionsearchreturns {7,6,8,4,7,11,100} asthe
7 nearest y values for a given x value. What is the value of yˆ that minimizes the L loss
1
functiononthisdata? Thereisacommonnameinstatisticsforthisvalueasafunctionofthe
y values;whatisit? Answerthesametwoquestions forthe L lossfunction.
2
18.16 Figure18.31showedhowacircleattheorigincanbelinearly separated bymapping
fromthefeatures (x ,x )tothetwodimensions(x2,x2). Butwhatifthecircleisnotlocated
1 2 1 2
at the origin? What if it is an ellipse, not a circle? The general equation for a circle (and
hencethedecisionboundary) is(x −a)2+(x −b)2−r2=0,andthegeneralequationfor
1 2
anellipseisc(x −a)2+d(x −b)2−1=0.
1 2
a. Expand out the equation for the circle and show what the weights w would be for the
i
decision boundary in the four-dimensional feature space (x ,x ,x2,x2). Explain why
1 2 1 2
thismeansthatanycircleislinearly separable inthisspace.
b. Dothesameforellipses inthefive-dimensional featurespace(x ,x ,x2,x2,x x ).
1 2 1 2 1 2
18.17 Construct a support vector machine that computes the XOR function. Use values of
+1 and –1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like
([−1,1],1)or([−1,−1],−1). Maptheinput[x ,x ]intoaspaceconsistingofx andx x .
1 2 1 1 2
Draw the four input points in this space, and the maximal margin separator. What is the
margin? Nowdrawtheseparating linebackintheoriginalEuclideaninputspace.
18.18 Consider an ensemble learning algorithm that uses simple majority voting among
K learned hypotheses. Suppose that each hypothesis has error (cid:2) and that the errors made
by each hypothesis are independent of the others’. Calculate a formula for the error of the
ensemble algorithm in terms of K and (cid:2), and evaluate it for the cases where K=5, 10, and
20and(cid:2)=0.1,0.2,and0.4. Iftheindependence assumptionisremoved,isitpossibleforthe
ensembleerrortobe worsethan(cid:2)?
18.19 Construct by hand a neural network that computes the XOR function of two inputs.
Makesuretospecify whatsortofunitsyouareusing.
18.20
RecallfromChapter18thatthereare22n
distinctBooleanfunctionsofninputs. How
manyofthesearerepresentable byathreshold perceptron?
18.21 Section 18.6.4 (page 725) noted that the output of the logistic function could be in-
terpreted asa probability passigned bythemodeltotheproposition that f(x)=1;theprob-
ability that f(x)=0 is therefore 1 − p. Write down the probability p as a function of x
and calculate the derivative of logp with respect to each weight w . Repeat the process for
i
log(1−p). Thesecalculationsgivealearningruleforminimizingthenegative-log-likelihood
Exercises 767
loss function for a probabilistic hypothesis. Comment on any resemblance to other learning
rulesinthechapter.
18.22 Suppose youhad aneural network withlinearactivation functions. Thatis, foreach
unittheoutputissomeconstant ctimestheweightedsumoftheinputs.
a. Assume that the network has one hidden layer. For a given assignment to the weights
w, write down equations for the value of the units in the output layer as a function of
wandtheinput layer x,without anyexplicit mention oftheoutput ofthehidden layer.
Showthatthereisanetworkwithnohiddenunitsthatcomputesthesamefunction.
b. Repeat the calculation inpart (a), but this timedo itfora network with any numberof
hiddenlayers.
c. Suppose a network with one hidden layer and linear activation functions has n input
and output nodes and h hidden nodes. What effect does the transformation in part (a)
to a network with no hidden layers have on the total number of weights? Discuss in
particularthecase h* n.
18.23 Suppose that a training set contains only a single example, repeated 100 times. In
80 of the 100 cases, the single output value is 1; in the other 20, it is 0. What will a back-
propagation network predict for this example, assuming that it has been trained and reaches
aglobaloptimum? (Hint: tofindtheglobaloptimum, differentiate theerrorfunction andset
ittozero.)
18.24 TheneuralnetworkwhoselearningperformanceismeasuredinFigure18.25hasfour
hidden nodes. Thisnumberwaschosen somewhatarbitrarily. Useacross-validation method
tofindthebestnumberofhiddennodes.
18.25 ConsidertheproblemofseparatingN datapointsintopositiveandnegativeexamples
using a linear separator. Clearly, this can always be done for N =2 points on a line of
dimension d=1, regardless of how the points are labeled or where they are located (unless
thepointsareinthesameplace).
a. ShowthatitcanalwaysbedoneforN=3pointsonaplaneofdimensiond=2,unless
theyarecollinear.
b. Showthatitcannot alwaysbedoneforN=4pointsonaplaneofdimensiond=2.
c. ShowthatitcanalwaysbedoneforN=4pointsinaspaceofdimension d=3,unless
theyarecoplanar.
d. Showthatitcannot alwaysbedoneforN =5pointsinaspaceofdimension d=3.
e. The ambitious student may wish to prove that N points in general position (but not
N +1)arelinearlyseparable inaspaceofdimension N −1.
19
KNOWLEDGE IN
LEARNING
Inwhichweexaminetheproblem oflearning whenyouknowsomething already.
Inalloftheapproaches tolearning described intheprevious chapter, theideaistoconstruct
afunction thathastheinput–output behaviorobserved inthedata. Ineachcase,thelearning
methodscanbeunderstoodassearchingahypothesisspacetofindasuitablefunction,starting
from only a very basic assumption about the form of the function, such as “second-degree
polynomial” or“decision tree” and perhaps a preference for simpler hypotheses. Doing this
amounts to saying that before you can learn something new, you must first forget (almost)
everything you know. In this chapter, we study learning methods that can take advantage
of prior knowledge about the world. In most cases, the prior knowledge is represented
PRIORKNOWLEDGE
as general first-order logical theories; thus for the first time we bring together the work on
knowledgerepresentation andlearning.
19.1 A LOGICAL FORMULATION OF LEARNING
Chapter 18 defined pure inductive learning as a process of finding a hypothesis that agrees
withtheobservedexamples. Here,wespecializethisdefinitiontothecasewherethehypoth-
esisisrepresentedbyasetoflogicalsentences. Exampledescriptionsandclassificationswill
also be logical sentences, and a new example can be classified by inferring a classification
sentence from the hypothesis and the example description. This approach allows for incre-
mentalconstructionofhypotheses,onesentenceatatime. Italsoallowsforpriorknowledge,
because sentences that are already known can assist in the classification of new examples.
Thelogical formulation oflearning mayseemlikealotofextraworkatfirst,butitturns out
toclarify manyoftheissues inlearning. Itenables ustogowellbeyond the simplelearning
methodsofChapter18byusingthefullpoweroflogicalinferenceintheserviceoflearning.
19.1.1 Examples andhypotheses
RecallfromChapter18therestaurant learningproblem: learningarulefordecidingwhether
towaitforatable. ExamplesweredescribedbyattributessuchasAlternate,Bar,Fri/Sat,
768
Section19.1. ALogicalFormulationofLearning 769
and so on. In a logical setting, an example is described by a logical sentence; the attributes
become unary predicates. Let us generically call the ith example X . For instance, the first
i
examplefromFigure18.3(page700)isdescribed bythesentences
Alternate(X )∧¬Bar(X )∧¬Fri/Sat(X )∧Hungry(X )∧...
1 1 1 1
WewillusethenotationD (X )torefertothedescriptionofX ,whereD canbeanylogical
i i i i
expression taking a single argument. The classification of the example is given by a literal
usingthegoalpredicate, inthiscase
WillWait(X ) or ¬WillWait(X ).
1 1
Thecompletetrainingsetcanthusbeexpressedastheconjunctionofalltheexampledescrip-
tionsandgoalliterals.
The aim of inductive learning in general is to find a hypothesis that classifies the ex-
amples well and generalizes well to new examples. Herewe are concerned withhypotheses
expressed inlogic;eachhypothesis h willhavetheform
j
∀x Goal(x) ⇔ C (x),
j
where C (x) is a candidate definition—some expression involving the attribute predicates.
j
Forexample,adecisiontreecanbeinterpretedasalogicalexpressionofthisform. Thus,the
tree in Figure 18.6 (page 702) expresses the following logical definition (which we will call
h forfuturereference):
r
∀r WillWait(r) ⇔ Patrons(r,Some)
∨ Patrons(r,Full)∧Hungry(r)∧Type(r,French)
∨ Patrons(r,Full)∧Hungry(r)∧Type(r,Thai) (19.1)
∧Fri/Sat(r)
∨ Patrons(r,Full)∧Hungry(r)∧Type(r,Burger).
Eachhypothesis predicts thatacertain setofexamples—namely, thosethat satisfy itscandi-
date definition—will be examples of the goal predicate. This set is called the extension of
EXTENSION
the predicate. Two hypotheses with different extensions are therefore logically inconsistent
with each other, because they disagree on their predictions for at least one example. If they
havethesameextension, theyarelogically equivalent.
ThehypothesisspaceHisthesetofallhypotheses{h ,...,h }thatthelearningalgo-
1 n
rithmisdesigned toentertain. Forexample, the DECISION-TREE-LEARNING algorithm can
entertain anydecision treehypothesis defined intermsoftheattributes provided; itshypoth-
esis space therefore consists of all these decision trees. Presumably, the learning algorithm
believesthatoneofthehypotheses iscorrect;thatis,itbelievesthesentence
h ∨h ∨h ∨...∨h . (19.2)
1 2 3 n
As the examples arrive, hypotheses that are not consistent with the examples can be ruled
out. Letusexaminethisnotion ofconsistency morecarefully. Obviously, ifhypothesis h is
j
consistentwiththeentiretrainingset,ithastobeconsistentwitheachexampleinthetraining
set. What would it mean for it to be inconsistent with an example? There are two possible
waysthatthiscanhappen:
770 Chapter 19. KnowledgeinLearning
• Anexample canbeafalsenegative forthehypothesis, ifthehypothesis saysitshould
FALSENEGATIVE
benegativebutinfactitispositive. Forinstance, thenewexampleX described by
13
Patrons(X ,Full)∧¬Hungry(X )∧...∧WillWait(X )
13 13 13
wouldbeafalsenegativeforthehypothesis h givenearlier. From h andtheexample
r r
description, we can deduce both WillWait(X ), which is what the example says,
13
and ¬WillWait(X ), which is what the hypothesis predicts. The hypothesis and the
13
examplearethereforelogically inconsistent.
• Anexample can be a false positive forthe hypothesis, if the hypothesis says it should
FALSEPOSITIVE
bepositivebutinfactitisnegative.1
If an example is afalse positive or false negative fora hypothesis, then the example and the
hypothesis arelogically inconsistent witheachother. Assumingthattheexampleisacorrect
observation offact,thenthehypothesis canberuledout. Logically, thisisexactlyanalogous
to the resolution rule of inference (see Chapter 9), where the disjunction of hypotheses cor-
responds toaclause andtheexample corresponds toaliteral thatresolves against oneofthe
literalsintheclause. Anordinarylogicalinferencesystemthereforecould,inprinciple,learn
from the example by eliminating one or more hypotheses. Suppose, for example, that the
exampleisdenotedbythesentenceI ,andthehypothesisspaceish ∨h ∨h ∨h . Thenif
1 1 2 3 4
I isinconsistentwithh andh ,thelogicalinferencesystemcandeducethenewhypothesis
1 2 3
spaceh ∨h .
1 4
We therefore can characterize inductive learning in a logical setting as a process of
gradually eliminating hypotheses that are inconsistent with the examples, narrowing down
the possibilities. Because the hypothesis space is usually vast (oreveninfinite inthe case of
first-order logic), we do not recommend trying to build a learning system using resolution-
basedtheoremprovingandacompleteenumerationofthehypothesisspace. Instead,wewill
describe twoapproaches thatfindlogically consistent hypotheses withmuchlesseffort.
19.1.2 Current-best-hypothesis search
CURRENT-BEST- The idea behind current-best-hypothesis search is to maintain a single hypothesis, and to
HYPOTHESIS
adjust it as new examples arrive in order to maintain consistency. The basic algorithm was
described byJohnStuartMill(1843), andmaywellhaveappeared evenearlier.
Suppose we have some hypothesis such as h , of which we have grown quite fond.
r
As long as each new example is consistent, we need do nothing. Then along comes a false
negativeexample, X . Whatdowedo? Figure 19.1(a) shows h schematically asaregion:
13 r
everythinginsidetherectangleispartoftheextensionofh . Theexamplesthathaveactually
r
been seen so far are shown as “+” or “–”, and we see that h correctly categorizes all the
r
examples as positive or negative examples of WillWait. In Figure 19.1(b), a new example
(circled)isafalsenegative: thehypothesissaysitshouldbenegativebutitisactuallypositive.
Theextensionofthehypothesismustbeincreasedtoincludeit. Thisiscalledgeneralization;
GENERALIZATION
onepossiblegeneralization isshowninFigure19.1(c). TheninFigure19.1(d),weseeafalse
positive: the hypothesis says the new example (circled) should be positive, but it actually is
1 Theterms“false positive”and “falsenegative” areused inmedicine todescribe erroneous resultsfromlab
tests.Aresultisafalsepositiveifitindicatesthatthepatienthasthediseasewheninfactnodiseaseispresent.
Section19.1. ALogicalFormulationofLearning 771
– – – –
– – – – – – – – – –
– – – – –
– – – – –
+ + + + +
+ + + + +
+ – + – + – + – + –
– – – – –
+ + + + +
+ + + + + – + + – + +
++ – ++ – ++ – ++ – ++ –
– – – – – – – – – –
(a) (b) (c) (d) (e)
Figure19.1 (a)Aconsistenthypothesis. (b)Afalsenegative. (c)Thehypothesisisgen-
eralized.(d)Afalsepositive.(e)Thehypothesisisspecialized.
functionCURRENT-BEST-LEARNING(examples,h)returnsahypothesisorfail
ifexamples isemptythen
returnh
e←FIRST(examples)
ife isconsistentwithh then
returnCURRENT-BEST-LEARNING(REST(examples),h)
elseife isafalsepositiveforh then
foreachh(cid:5) in specializationsofh consistentwithexamples seensofardo
h(cid:5)(cid:5)←CURRENT-BEST-LEARNING(REST(examples),h(cid:5))
ifh(cid:5)(cid:5) (cid:7)= fail thenreturnh(cid:5)(cid:5)
elseife isafalsenegativeforh then
foreachh(cid:5) ingeneralizationsofh consistentwithexamples seensofardo
h(cid:5)(cid:5)←CURRENT-BEST-LEARNING(REST(examples),h(cid:5))
ifh(cid:5)(cid:5) (cid:7)= fail thenreturnh(cid:5)(cid:5)
returnfail
Figure 19.2 The current-best-hypothesis learning algorithm. It searches for a consis-
tent hypothesis that fits all the examples and backtracks when no consistent specializa-
tion/generalizationcan be found. To start the algorithm, any hypothesis can be passed in;
itwillbespecializedorgneralizedasneeded.
negative. Theextension ofthehypothesis mustbedecreased toexclude theexample. Thisis
calledspecialization; inFigure19.1(e)weseeonepossible specialization ofthe hypothesis.
SPECIALIZATION
The“moregeneral than” and“more specific than” relations between hypotheses provide the
logicalstructure onthehypothesis spacethatmakesefficientsearchpossible.
WecannowspecifytheCURRENT-BEST-LEARNING algorithm,showninFigure19.2.
Noticethateachtimeweconsidergeneralizingorspecializingthehypothesis,wemustcheck
forconsistency withtheotherexamples, because anarbitrary increase/decrease intheexten-
sionmightinclude/exclude previously seennegative/positive examples.
772 Chapter 19. KnowledgeinLearning
Wehave defined generalization and specialization as operations that change the exten-
sion of a hypothesis. Now we need to determine exactly how they can be implemented as
syntactic operations that change the candidate definition associated with the hypothesis, so
thataprogramcancarrythemout. Thisisdonebyfirstnotingthatgeneralizationandspecial-
ization are also logical relationships between hypotheses. If hypothesis h , with definition
1
C ,isageneralization ofhypothesis h withdefinitionC ,thenwemusthave
1 2 2
∀x C (x) ⇒ C (x).
2 1
Therefore in order to construct a generalization of h , we simply need to find a defini-
2
tion C that is logically implied by C . This is easily done. For example, if C (x) is
1 2 2
Alternate(x) ∧ Patrons(x,Some), then one possible generalization is given by C (x) ≡
1
DROPPING Patrons(x,Some). This is called dropping conditions. Intuitively, it generates a weaker
CONDITIONS
definitionandthereforeallowsalargersetofpositiveexamples. Thereareanumberofother
generalization operations, depending on the language being operated on. Similarly, we can
specialize ahypothesis byadding extra conditions toitscandidate definition orbyremoving
disjuncts from adisjunctive definition. Letusseehowthis worksontherestaurant example,
usingthedatainFigure18.3.
• Thefirstexample, X ,ispositive. Theattribute Alternate(X )istrue,solettheinitial
1 1
hypothesis be
h : ∀x WillWait(x) ⇔ Alternate(x).
1
• Thesecondexample,X ,isnegative. h predictsittobepositive,soitisafalsepositive.
2 1
Therefore,weneedtospecialize h . Thiscanbedonebyaddinganextraconditionthat
1
willruleoutX ,whilecontinuing toclassifyX aspositive. Onepossibility is
2 1
h : ∀x WillWait(x) ⇔ Alternate(x)∧Patrons(x,Some).
2
• Thethirdexample,X ,ispositive. h predictsittobenegative,soitisafalsenegative.
3 2
Therefore,weneedtogeneralize h . WedroptheAlternate condition, yielding
2
h : ∀x WillWait(x) ⇔ Patrons(x,Some).
3
• Thefourthexample,X ,ispositive. h predictsittobenegative,soitisafalsenegative.
4 3
We therefore need to generalize h . We cannot drop the Patrons condition, because
3
that would yield an all-inclusive hypothesis that would be inconsistent with X . One
2
possibility istoaddadisjunct:
h : ∀x WillWait(x) ⇔ Patrons(x,Some)
4
∨(Patrons(x,Full)∧Fri/Sat(x)).
Already,thehypothesis isstartingtolookreasonable. Obviously, thereareotherpossibilities
consistent withthefirstfourexamples;herearetwoofthem:
h (cid:2) : ∀x WillWait(x) ⇔ ¬WaitEstimate(x,30-60).
4
h (cid:2)(cid:2) : ∀x WillWait(x) ⇔ Patrons(x,Some)
4
∨(Patrons(x,Full)∧WaitEstimate(x,10-30)).
TheCURRENT-BEST-LEARNING algorithmisdescribednondeterministically, becauseatany
point,theremaybeseveralpossiblespecializationsorgeneralizationsthatcanbeapplied. The
Section19.1. ALogicalFormulationofLearning 773
functionVERSION-SPACE-LEARNING(examples)returnsaversionspace
localvariables: V,theversionspace:thesetofallhypotheses
V ←thesetofallhypotheses
foreachexamplee inexamples do
ifV isnotemptythenV ←VERSION-SPACE-UPDATE(V,e)
returnV
functionVERSION-SPACE-UPDATE(V,e)returnsanupdatedversionspace
V ←{h∈V : hisconsistentwithe}
Figure19.3 Theversionspacelearningalgorithm.ItfindsasubsetofV thatisconsistent
withalltheexamples.
choicesthataremadewillnotnecessarily leadtothesimplesthypothesis, andmayleadtoan
unrecoverable situation wherenosimplemodificationofthe hypothesis isconsistent withall
ofthedata. Insuchcases, theprogrammustbacktrack toaprevious choicepoint.
The CURRENT-BEST-LEARNING algorithm and its variants have been used in many
machine learning systems, starting with Patrick Winston’s (1970) “arch-learning” program.
Withalargenumberofexamplesandalargespace, however,somedifficulties arise:
1. Checkingallthepreviousexamplesoveragainforeachmodificationisveryexpensive.
2. Thesearchprocessmayinvolveagreatdealofbacktracking. AswesawinChapter18,
hypothesis spacecanbeadoublyexponentially largeplace.
19.1.3 Least-commitment search
Backtracking arises because the current-best-hypothesis approach has to choose a particular
hypothesis as its best guess even though it does not have enough data yet to be sure of the
choice. What we can do instead is to keep around all and only those hypotheses that are
consistent with all the data so far. Each new example will either have no effect or will get
rid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a
disjunctive sentence
h ∨h ∨h ...∨h .
1 2 3 n
Asvarioushypothesesarefoundtobeinconsistentwiththeexamples,thisdisjunctionshrinks,
retaining only those hypotheses not ruled out. Assuming that the original hypothesis space
does in fact contain the right answer, the reduced disjunction must still contain the right an-
swerbecauseonlyincorrecthypotheseshavebeenremoved. Thesetofhypothesesremaining
iscalledtheversionspace,andthelearningalgorithm (sketched inFigure19.3)iscalledthe
VERSIONSPACE
CANDIDATE versionspacelearningalgorithm (alsothe candidateelimination algorithm).
ELIMINATION
One important property of this approach is that it is incremental: one never has to
go back and reexamine the old examples. All remaining hypotheses are guaranteed to be
consistent with them already. But there is an obvious problem. We already said that the
774 Chapter 19. KnowledgeinLearning
This region all inconsistent
G 1 G 2 G 3 . . . G m
More general
More specific
S 1 S 2 . . . S n
This region all inconsistent
Figure19.4 Theversionspacecontainsallhypothesesconsistentwiththeexamples.
hypothesisspaceisenormous,sohowcanwepossiblywritedownthisenormousdisjunction?
The following simple analogy is very helpful. How do you represent all the real num-
bers between 1 and 2? After all, there are an infinite number of them! The answer is to use
anintervalrepresentation thatjustspecifiestheboundaries oftheset: [1,2]. Itworksbecause
wehaveanordering ontherealnumbers.
Wealsohaveanorderingonthehypothesisspace,namely,generalization/specialization.
This is a partial ordering, which means that each boundary will not be a point but rather a
set of hypotheses called a boundary set. The great thing is that we can represent the entire
BOUNDARYSET
version space using just twoboundary sets: amostgeneral boundary (the G-set) andamost
G-SET
specific boundary (the S-set). Everything in between isguaranteed to be consistent with the
S-SET
examples. Beforeweprovethis,letusrecap:
• The current version space is the set of hypotheses consistent with all the examples so
far. Itisrepresented bytheS-setandG-set,eachofwhichis asetofhypotheses.
• Every member of the S-set is consistent with all observations so far, and there are no
consistent hypotheses thataremorespecific.
• Every member of the G-set is consistent with all observations so far, and there are no
consistent hypotheses thataremoregeneral.
Wewanttheinitialversionspace(beforeanyexampleshavebeenseen)torepresentallpossi-
blehypotheses. WedothisbysettingtheG-settocontain True (thehypothesis thatcontains
everything), andtheS-settocontain False (thehypothesis whoseextension isempty).
Figure19.4showsthegeneralstructureoftheboundary-set representationoftheversion
space. Toshowthattherepresentation issufficient,weneed thefollowingtwoproperties:
Section19.1. ALogicalFormulationofLearning 775
1. Everyconsistenthypothesis(otherthanthoseintheboundarysets)ismorespecificthan
somememberoftheG-set,andmoregeneral thansomememberof theS-set. (Thatis,
there are no “stragglers” left outside.) This follows directly from the definitions of S
and G. If there were a straggler h, then it would have to be no more specific than any
member of G, in which case it belongs in G; or no more general than any member of
S,inwhichcaseitbelongsinS.
2. Everyhypothesis morespecific than somememberoftheG-setand moregeneral than
somememberoftheS-setisaconsistent hypothesis. (Thatis, there areno“holes” be-
tween the boundaries.) Any h between S and G must reject all the negative examples
rejectedbyeachmemberofG(becauseitismorespecific),andmustacceptallthepos-
itiveexamplesacceptedbyanymemberofS (becauseitismoregeneral). Thus,hmust
agree with all the examples, and therefore cannot be inconsistent. Figure 19.5 shows
the situation: there are no known examples outside S but inside G, so any hypothesis
inthegapmustbeconsistent.
We have therefore shown that if S and G are maintained according to their definitions, then
they provide a satisfactory representation ofthe version space. Theonly remaining problem
is how to update S and G for a new example (the job of the VERSION-SPACE-UPDATE
function). This may appear rather complicated at first, but from the definitions and with the
helpofFigure19.4,itisnottoohardtoreconstruct thealgorithm.
– –
– G
– 1
–
–
– – + + S G 2 –
+ + + 1 –
+
+
+ + +
–
–
– –
Figure 19.5 The extensions of the members of G and S. No known examples lie in
betweenthetwosetsofboundaries.
Weneedtoworryaboutthemembers S andG oftheS-andG-sets. Foreachone,the
i i
newexamplemaybeafalsepositiveorafalsenegative.
1. Falsepositive for S : Thismeans S istoo general, but there are no consistent special-
i i
izationsofS (bydefinition), sowethrowitoutoftheS-set.
i
2. FalsenegativeforS : ThismeansS istoospecific,sowereplaceitbyallitsimmediate
i i
generalizations, provided theyaremorespecificthansomememberofG.
3. FalsepositiveforG : ThismeansG istoogeneral,sowereplaceitbyallitsimmediate
i i
specializations, provided theyaremoregeneralthansomememberofS.
776 Chapter 19. KnowledgeinLearning
4. Falsenegative forG : ThismeansG istoospecific, butthere areno consistent gener-
i i
alizations ofG (bydefinition) sowethrowitoutoftheG-set.
i
Wecontinuetheseoperations foreachnewexampleuntiloneofthreethingshappens:
1. Wehave exactly one hypothesis left inthe version space, in which case wereturn itas
theunique hypothesis.
2. The version space collapses—either S or G becomes empty, indicating that there are
noconsistent hypotheses forthetraining set. Thisisthesamecaseasthefailure ofthe
simpleversionofthedecision treealgorithm.
3. We run out of examples and have several hypotheses remaining in the version space.
This means the version space represents a disjunction of hypotheses. For any new
example,ifallthedisjunctsagree,thenwecanreturntheirclassificationoftheexample.
Iftheydisagree, onepossibility istotakethemajorityvote.
Weleaveasanexercisetheapplication oftheVERSION-SPACE-LEARNING algorithm tothe
restaurant data.
Therearetwoprincipal drawbackstotheversion-space approach:
• Ifthedomaincontainsnoiseorinsufficientattributesforexactclassification,theversion
spacewillalwayscollapse.
• Ifweallowunlimiteddisjunction inthehypothesis space,theS-setwillalwayscontain
a single most-specific hypothesis, namely, the disjunction of the descriptions of the
positiveexamplesseentodate. Similarly,theG-setwillcontainjustthenegationofthe
disjunction ofthedescriptions ofthenegativeexamples.
• For some hypothesis spaces, the number of elements in the S-set or G-set may grow
exponentiallyinthenumberofattributes,eventhoughefficientlearningalgorithmsexist
forthosehypothesis spaces.
To date, no completely successful solution has been found for the problem of noise. The
problem ofdisjunction canbeaddressed byallowing onlylimitedformsofdisjunction orby
GENERALIZATION including a generalization hierarchy of more general predicates. For example, instead of
HIERARCHY
using thedisjunction WaitEstimate(x,30-60)∨WaitEstimate(x,>60), wemight use the
single literal LongWait(x). The set of generalization and specialization operations can be
easilyextendedtohandlethis.
The pure version space algorithm was first applied in the Meta-DENDRAL system,
which was designed to learn rules for predicting how molecules would break into pieces in
a mass spectrometer (Buchanan and Mitchell, 1978). Meta-DENDRAL was able to generate
rulesthatweresufficientlynoveltowarrantpublicationinajournalofanalyticalchemistry—
the first real scientific knowledge generated by a computer program. It was also used in the
elegantLEXsystem(Mitchelletal.,1983),whichwasabletolearntosolvesymbolicintegra-
tion problems by studying its own successes and failures. Although version space methods
are probably not practical in most real-world learning problems, mainly because of noise,
theyprovideagooddealofinsight intothelogicalstructure ofhypothesis space.
Section19.2. KnowledgeinLearning 777
Prior
knowledge
Knowledge-based
Observations Hypotheses Predictions
inductive learning
Figure 19.6 A cumulative learning process uses, and adds to, its stock of background
knowledgeovertime.
19.2 KNOWLEDGE IN LEARNING
Theprecedingsectiondescribedthesimplestsettingforinductivelearning. Tounderstandthe
role of prior knowledge, we need to talk about the logical relationships among hypotheses,
exampledescriptions, andclassifications. LetDescriptions denotetheconjunction ofallthe
exampledescriptions inthetrainingset,andletClassifications denotetheconjunction ofall
theexampleclassifications. ThenaHypothesis that“explains theobservations” mustsatisfy
thefollowingproperty (recallthat |=means“logically entails”):
Hypothesis ∧Descriptions |= Classifications . (19.3)
ENTAILMENT Wecall this kind of relationship an entailment constraint, inwhich Hypothesis is the“un-
CONSTRAINT
known.” Pure inductive learning means solving this constraint, where Hypothesis is drawn
from some predefined hypothesis space. For example, if we consider a decision tree as a
logicalformula(seeEquation(19.1)onpage769),thenadecisiontreethatisconsistentwith
all the examples will satisfy Equation (19.3). If weplace no restrictions on the logical form
ofthehypothesis, ofcourse,then Hypothesis = Classifications alsosatisfiestheconstraint.
Ockham’s razor tells us to prefer small, consistent hypotheses, so we try to do better than
simplymemorizingtheexamples.
Thissimpleknowledge-freepictureofinductivelearningpersisteduntiltheearly1980s.
Themodernapproachistodesignagentsthatalreadyknowsomethingandaretryingtolearn
somemore. Thismaynotsoundlikeaterrificallydeepinsight,butitmakesquiteadifference
to the way we design agents. It might also have some relevance to our theories about how
scienceitselfworks. Thegeneralideaisshownschematically inFigure19.6.
Anautonomous learning agent thatusesbackground knowledge mustsomehow obtain
the background knowledge in the first place, in order for it to be used in the new learning
episodes. This method must itself be a learning process. The agent’s life history will there-
fore be characterized by cumulative, or incremental, development. Presumably, the agent
could start out with nothing, performing inductions in vacuo like a good little pure induc-
tion program. But once it has eaten from the Tree of Knowledge, it can no longer pursue
such naive speculations and should use its background knowledge to learn more and more
effectively. Thequestion isthenhowtoactuallydothis.
778 Chapter 19. KnowledgeinLearning
19.2.1 Somesimpleexamples
Letusconsidersomecommonsenseexamplesoflearningwithbackgroundknowledge. Many
apparently rational cases of inferential behavior in the face of observations clearly do not
followthesimpleprinciples ofpureinduction.
• Sometimes one leaps to general conclusions after only one observation. Gary Larson
once drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard on
the end of a pointed stick. He is watched by an amazed crowd of his less intellectual
contemporaries,whohavebeenusingtheirbarehandstoholdtheirvictualsoverthefire.
Thisenlightening experience isenough toconvince thewatchers ofageneral principle
ofpainless cooking.
• OrconsiderthecaseofthetravelertoBrazilmeetingherfirstBrazilian. Onhearinghim
speak Portuguese, she immediately concludes that Brazilians speak Portuguese, yeton
discovering that his name is Fernando, she does not conclude that all Brazilians are
called Fernando. Similar examples appear in science. For example, when a freshman
physics student measures the density and conductance of a sample of copper at a par-
ticular temperature, she is quite confident in generalizing those values to all pieces of
copper. Yetwhenshemeasuresitsmass,shedoesnotevenconsiderthehypothesisthat
allpieces ofcopperhave that mass. Ontheotherhand, itwouldbequite reasonable to
makesuchageneralization overallpennies.
• Finally, consider the case of a pharmacologically ignorant but diagnostically sophisti-
cated medical student observing a consulting session between a patient and an expert
internist. After a series of questions and answers, the expert tells the patient to take a
course of a particular antibiotic. The medical student infers the general rule that that
particularantibiotic iseffectiveforaparticulartypeof infection.
These are all cases in which the use of background knowledge allows much faster learning
thanonemightexpectfromapureinduction program.
19.2.2 Somegeneral schemes
In each of the preceding examples, one can appeal to prior knowledge to try to justify the
generalizations chosen. Wewillnowlookatwhatkindsofentailment constraints areoperat-
ingineachcase. Theconstraints willinvolve the Background knowledge, inaddition tothe
Hypothesis andtheobserved Descriptions andClassifications.
In the case of lizard toasting, the cavemen generalize by explaining the success of the
pointed stick: it supports the lizard while keeping the hand away from the fire. From this
explanation,theycaninferageneralrule: thatanylong,rigid,sharpobjectcanbeusedtotoast
small, soft-bodied edibles. Thiskind ofgeneralization process hasbeencalled explanation-
EXPLANATION-
based learning, or EBL.Notice that the general rule follows logically from the background
BASED
LEARNING
knowledgepossessedbythecavemen. Hence,theentailmentconstraintssatisfiedbyEBLare
thefollowing:
Hypothesis ∧Descriptions |= Classifications
Background |= Hypothesis .
Section19.2. KnowledgeinLearning 779
Because EBL uses Equation (19.3), it was initially thought to be a way to learn from ex-
amples. But because it requires that the background knowledge be sufficient to explain the
Hypothesis, which in turn explains the observations, the agent does not actually learn any-
thing factually new from the example. Theagent could have derived the example from what
it already knew, although that might have required an unreasonable amount of computation.
EBL is now viewed as a method for converting first-principles theories into useful, special-
purpose knowledge. Wedescribealgorithms forEBLinSection19.3.
The situation of our traveler in Brazil is quite different, for she cannot necessarily ex-
plain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,
the same generalization would be forthcoming from a traveler entirely ignorant of colonial
history. The relevant prior knowledge in this case is that, within any given country, most
people tend to speak the same language; on the other hand, Fernando is not assumed to be
thenameofallBrazilians because thiskindofregularity doesnotholdfornames. Similarly,
thefreshman physics student alsowould behardputtoexplain theparticular values that she
discoversfortheconductance anddensityofcopper. Shedoesknow,however,thatthemate-
rial of which an object is composed and its temperature together determine its conductance.
Ineachcase,thepriorknowledge Background concernstherelevanceofasetoffeaturesto
RELEVANCE
thegoal predicate. Thisknowledge, together withtheobservations, allowstheagent toinfer
anew,generalrulethatexplains theobservations:
Hypothesis ∧Descriptions |= Classifications ,
(19.4)
Background ∧Descriptions ∧Classifications |= Hypothesis .
RELEVANCE-BASED Wecallthiskindofgeneralization relevance-based learning,orRBL(although thenameis
LEARNING
not standard). Notice that whereas RBLdoes make use ofthe content ofthe observations, it
doesnotproducehypothesesthatgobeyondthelogicalcontentofthebackgroundknowledge
and the observations. It is a deductive form of learning and cannot by itself account for the
creation ofnewknowledgestarting fromscratch.
In the case of the medical student watching the expert, we assume that the student’s
prior knowledge is sufficient to infer the patient’s disease D from the symptoms. This is
not, however, enough to explain the fact that the doctor prescribes aparticular medicine M.
The student needs to propose another rule, namely, that M generally is effective against D.
Giventhisruleandthestudent’spriorknowledge,thestudentcannowexplainwhytheexpert
prescribes M in this particular case. We can generalize this example to come up with the
entailment constraint
Background ∧Hypothesis ∧Descriptions |= Classifications . (19.5)
Thatis,thebackground knowledge andthenewhypothesis combinetoexplain theexamples.
Aswithpureinductivelearning,thelearningalgorithmshouldproposehypothesesthatareas
simple as possible, consistent with this constraint. Algorithms that satisfy constraint (19.5)
KNOWLEDGE-BASED
arecalled knowledge-basedinductivelearning,orKBIL,algorithms.
INDUCTIVE
LEARNING
KBIL algorithms, which are described in detail in Section 19.5, have been studied
INDUCTIVELOGIC mainly in the field of inductive logic programming, or ILP. In ILP systems, prior knowl-
PROGRAMMING
edgeplaystwokeyrolesinreducing thecomplexity oflearning:
780 Chapter 19. KnowledgeinLearning
1. Becauseanyhypothesis generated mustbeconsistent withthepriorknowledge aswell
as with the new observations, the effective hypothesis space size is reduced to include
onlythosetheoriesthatareconsistent withwhatisalready known.
2. For any given set of observations, the size of the hypothesis required to construct an
explanation for the observations can be much reduced, because the prior knowledge
will be available to help out the new rules in explaining the observations. The smaller
thehypothesis, theeasieritistofind.
In addition to allowing the use of prior knowledge in induction, ILP systems can formulate
hypotheses in general first-order logic, rather than in the restricted attribute-based language
ofChapter18. Thismeans thatthey canlearn inenvironments that cannot be understood by
simplersystems.
19.3 EXPLANATION-BASED LEARNING
Explanation-based learning is a method for extracting general rules from individual obser-
vations. As an example, consider the problem of differentiating and simplifying algebraic
expressions (Exercise 9.17). If we differentiate an expression such as X2 with respect to
X, we obtain 2X. (We use a capital letter for the arithmetic unknown X, to distinguish it
from the logical variable x.) In a logical reasoning system, the goal might be expressed as
ASK(Derivative(X2,X)=d, KB),withsolutiond = 2X.
Anyonewhoknowsdifferentialcalculuscanseethissolution“byinspection”asaresult
ofpracticeinsolvingsuchproblems. Astudentencounteringsuchproblemsforthefirsttime,
or a program with no experience, will have a much more difficult job. Application of the
standard rules of differentiation eventually yields the expression 1 × (2 × (X(2−1))), and
eventually this simplifies to 2X. In the authors’ logic programming implementation, this
takes 136 proof steps, of which 99 are on dead-end branches in the proof. After such an
experience, we would like the program to solve the same problem much more quickly the
nexttimeitarises.
The technique of memoization has long been used in computer science to speed up
MEMOIZATION
programs by saving the results of computation. The basic idea of memo functions is to
accumulate a database of input–output pairs; when the function is called, it first checks the
database to see whether it can avoid solving the problem from scratch. Explanation-based
learning takes this a good deal further, by creating general rules that cover an entire class
of cases. In the case of differentiation, memoization would remember that the derivative of
X2 withrespect toX is2X,butwouldleavetheagenttocalculate thederivativeofZ2 with
respect to Z from scratch. We would like to be able to extract the general rule that for any
arithmetic unknown u, the derivative of u2 with respect to u is 2u. (An even more general
rule for un can also be produced, but the current example suffices to make the point.) In
logicalterms,thisisexpressed bytherule
ArithmeticUnknown(u) ⇒ Derivative(u2,u)=2u.
Section19.3. Explanation-Based Learning 781
Ifthe knowledge base contains such arule, then anynew case that isaninstance ofthis rule
canbesolvedimmediately.
Thisis,ofcourse,merelyatrivialexampleofaverygeneral phenomenon. Oncesome-
thing is understood, it can be generalized and reused in other circumstances. It becomes an
“obvious” stepandcanthenbeusedasabuilding blockinsolving problems still morecom-
plex. Alfred North Whitehead (1911), co-author with Bertrand Russell of Principia Mathe-
matica, wrote “Civilization advances by extending the number of important operations that
wecandowithoutthinking aboutthem,”perhapshimselfapplyingEBLtohisunderstanding
of events such as Zog’s discovery. If you have understood the basic idea of the differenti-
ation example, then your brain is already busily trying to extract the general principles of
explanation-based learning fromit. Noticethatyouhadn’t already invented EBLbefore you
saw the example. Like the cavemen watching Zog, you (and we) needed an example before
we could generate the basic principles. This is because explaining why something is a good
ideaismucheasierthancomingupwiththeideainthefirstplace.
19.3.1 Extracting general rules from examples
Thebasic idea behind EBLisfirst toconstruct anexplanation ofthe observation using prior
knowledge, and then to establish adefinition of the class of cases forwhich thesame expla-
nation structure can be used. This definition provides the basis fora rule covering all of the
casesintheclass. The“explanation” canbealogicalproof, butmoregenerally itcanbeany
reasoning orproblem-solving process whose steps arewell defined. The keyis tobe able to
identify thenecessary conditions forthosesamestepstoapplytoanothercase.
We will use for our reasoning system the simple backward-chaining theorem prover
described inChapter9. Theprooftreefor Derivative(X2,X)=2X istoolargetouseasan
example, so we will use a simpler problem to illustrate the generalization method. Suppose
ourproblem istosimplify 1×(0+X). Theknowledgebaseincludes thefollowingrules:
Rewrite(u,v)∧Simplify(v,w) ⇒ Simplify(u,w).
Primitive(u) ⇒ Simplify(u,u).
ArithmeticUnknown(u) ⇒ Primitive(u).
Number(u) ⇒ Primitive(u).
Rewrite(1×u,u).
Rewrite(0+u,u).
.
.
.
The proof that the answer is X is shown in the top half of Figure 19.7. The EBL method
actually constructs twoprooftreessimultaneously. Thesecondprooftreeusesa variabilized
goal inwhich the constants from the original goal are replaced by variables. Asthe original
proof proceeds, the variabilized proof proceeds in step, using exactly the same rule applica-
tions. This could cause some of the variables to become instantiated. Forexample, in order
tousetheruleRewrite(1×u,u),thevariablexinthesubgoalRewrite(x×(y+z),v)must
(cid:2)
bebound to1. Similarly, y mustbebound to0inthesubgoal Rewrite(y+z,v )inorderto
usetherule Rewrite(0+u,u). Oncewehavethe generalized proof tree, wetake theleaves
782 Chapter 19. KnowledgeinLearning
Simplify(1 × (0+X),w)
Rewrite(1×(0+X),v) Simplify(0+X,w)
Yes,{v / 0+X}
Rewrite(0+X,v') Simplify(X,w)
Yes,{v' / X} {w / X}
Primitive(X)
ArithmeticUnknown(X)
Simplify(x ×(y+z),w) Yes,{ }
Rewrite(x ×(y+z),v) Simplify(y+z,w)
Yes,{x / 1, v / y+z}
Rewrite(y+z,v') Simplify(z,w)
Yes,{y / 0, v'/ z} {w / z}
Primitive(z)
ArithmeticUnknown(z)
Yes,{ }
Figure19.7 Prooftreesforthesimplificationproblem. Thefirsttreeshowstheprooffor
theoriginalprobleminstance,fromwhichwecanderive
ArithmeticUnknown(z) ⇒ Simplify(1×(0+z),z).
Thesecondtreeshowstheproofforaprobleminstancewithallconstantsreplacedbyvari-
ables,fromwhichwecanderiveavarietyofotherrules.
(withthenecessary bindings) andformageneral ruleforthe goalpredicate:
Rewrite(1×(0+z),0+z)∧Rewrite(0+z,z)∧ArithmeticUnknown(z)
⇒ Simplify(1×(0+z),z).
Noticethatthefirsttwoconditions ontheleft-hand sideare true regardless ofthevalue ofz.
Wecanthereforedropthemfromtherule,yielding
ArithmeticUnknown(z) ⇒ Simplify(1×(0+z),z).
Ingeneral, conditions canbedropped fromthefinalruleiftheyimposenoconstraints onthe
variables on the right-hand side of the rule, because the resulting rule will still be true and
will be more efficient. Notice that we cannot drop the condition ArithmeticUnknown(z),
because not all possible values of z are arithmetic unknowns. Values other than arithmetic
unknownsmightrequiredifferent formsofsimplification: forexample,if z were2×3,then
thecorrectsimplification of1×(0+(2×3))wouldbe6andnot2×3.
Torecap,thebasicEBLprocessworksasfollows:
1. Givenanexample,constructaproofthatthegoalpredicateappliestotheexampleusing
theavailable background knowledge.
Section19.3. Explanation-Based Learning 783
2. In parallel, construct a generalized proof tree for the variabilized goal using the same
inferencestepsasintheoriginalproof.
3. Construct a new rule whose left-hand side consists of the leaves of the proof tree and
whose right-hand side is the variabilized goal (after applying the necessary bindings
fromthegeneralized proof).
4. Dropanyconditions fromtheleft-hand sidethataretrueregardlessofthevaluesofthe
variablesinthegoal.
19.3.2 Improving efficiency
Thegeneralized prooftreeinFigure19.7actuallyyieldsmorethanonegeneralized rule. For
example, if we terminate, or prune, the growth of the right-hand branch in the proof tree
whenitreachesthePrimitive step,wegettherule
Primitive(z) ⇒ Simplify(1×(0+z),z).
Thisruleisasvalidas, but moregeneral than, therule using ArithmeticUnknown,because
itcoverscaseswhere z isanumber. Wecanextractastillmoregeneralrulebypruningafter
thestepSimplify(y+z,w),yielding therule
Simplify(y+z,w) ⇒ Simplify(1×(y+z),w).
Ingeneral,arulecanbeextractedfromanypartialsubtreeofthegeneralizedprooftree. Now
wehaveaproblem: whichoftheserulesdowechoose?
The choice of which rule to generate comes down to the question of efficiency. There
arethreefactorsinvolved intheanalysisofefficiencygainsfromEBL:
1. Adding large numbers of rules can slow down the reasoning process, because the in-
ferencemechanism muststillcheckthoserulesevenincases wheretheydonotyielda
solution. Inotherwords,itincreases the branchingfactorinthesearchspace.
2. Tocompensate for the slowdown in reasoning, the derived rules must offer significant
increases inspeedforthecasesthattheydocover. Theseincreases comeaboutmainly
because the derived rules avoid dead ends that would otherwise be taken, but also be-
causetheyshortentheproofitself.
3. Derivedrulesshouldbeasgeneralaspossible, sothattheyapplytothelargestpossible
setofcases.
Acommonapproachtoensuringthatderivedrulesareefficientistoinsistontheoperational-
ityofeachsubgoal intherule. Asubgoal isoperational ifitis“easy”tosolve. Forexample,
OPERATIONALITY
the subgoal Primitive(z) is easy to solve, requiring at most two steps, whereas the subgoal
Simplify(y +z,w) could lead to an arbitrary amount of inference, depending on the values
of y and z. If a test for operationality is carried out at each step in the construction of the
generalizedproof,thenwecanprunetherestofabranchassoonasanoperationalsubgoalis
found, keepingjusttheoperational subgoalasaconjunct ofthenewrule.
Unfortunately, there is usually a tradeoff between operationality and generality. More
specific subgoals are generally easier to solve but cover fewer cases. Also, operationality
is a matter of degree: one or two steps is definitely operational, but what about 10 or 100?
784 Chapter 19. KnowledgeinLearning
Finally, the cost of solving a given subgoal depends on what other rules are available in the
knowledge base. It can go up or down as more rules are added. Thus, EBL systems really
face a very complex optimization problem in trying to maximize the efficiency of a given
initialknowledge base. Itissometimespossible toderiveamathematicalmodeloftheeffect
on overall efficiency of adding a given rule and to use this model to select the best rule to
add. The analysis can become very complicated, however, especially when recursive rules
are involved. One promising approach is to address the problem of efficiency empirically,
simplybyaddingseveralrulesandseeingwhichonesareusefulandactuallyspeedthingsup.
Empirical analysis of efficiency is actually at the heart of EBL. What we have been
calling loosely the “efficiency of agiven knowledge base” isactually the average-case com-
plexity on a distribution of problems. By generalizing from past example problems, EBL
makes the knowledge base more efficient for the kind of problems that it is reasonable to
expect. This works as long as the distribution of past examples is roughly the same as for
future examples—the same assumption used for PAC-learning in Section 18.5. If the EBL
system is carefully engineered, it is possible to obtain significant speedups. For example, a
very large Prolog-based natural language system designed for speech-to-speech translation
between Swedish and English was able to achieve real-time performance only by the appli-
cationofEBLtotheparsingprocess (SamuelssonandRayner, 1991).
19.4 LEARNING USING RELEVANCE INFORMATION
OurtravelerinBrazilseemstobeabletomakeaconfidentgeneralizationconcerningthelan-
guagespokenbyotherBrazilians. Theinferenceissanctionedbyherbackgroundknowledge,
namely, that people in a given country (usually) speak the same language. We can express
thisinfirst-orderlogicasfollows:2
Nationality(x,n)∧Nationality(y,n)∧Language(x,l) ⇒ Language(y,l).(19.6)
(Literal translation: “If x and y have the samenationality n and x speaks language l, then y
alsospeaksit.”) Itisnotdifficulttoshowthat,fromthissentence andtheobservation that
Nationality(Fernando,Brazil)∧Language(Fernando,Portuguese),
thefollowingconclusion isentailed(seeExercise19.1):
Nationality(x,Brazil) ⇒ Language(x,Portuguese).
Sentencessuchas(19.6)express astrict formofrelevance: givennationality, language
isfullydetermined. (Putanotherway: languageisafunctionofnationality.) Thesesentences
FUNCTIONAL arecalled functionaldependenciesordeterminations. Theyoccursocommonly incertain
DEPENDENCY
kinds of applications (e.g., defining database designs) that a special syntax is used to write
DETERMINATION
them. Weadoptthenotation ofDavies(1985):
Nationality(x,n) ’ Language(x,l).
2 Weassumeforthesakeofsimplicitythatapersonspeaksonlyonelanguage. Clearly,therulewouldhaveto
beamendedforcountriessuchasSwitzerlandandIndia.
Section19.4. LearningUsingRelevanceInformation 785
As usual, this is simply a syntactic sugaring, but it makes it clear that the determination is
really a relationship between the predicates: nationality determines language. The relevant
properties determining conductance anddensity canbeexpressed similarly:
Material(x,m)∧Temperature(x,t) ’ Conductance(x,ρ);
Material(x,m)∧Temperature(x,t) ’ Density(x,d).
Thecorrespondinggeneralizationsfollowlogicallyfromthedeterminationsandobservations.
19.4.1 Determining the hypothesis space
Although the determinations sanction general conclusions concerning all Brazilians, or all
pieces of copper at a given temperature, they cannot, of course, yield a general predictive
theory for all nationalities, or for all temperatures and materials, from a single example.
Theirmaineffectcanbeseenaslimitingthespaceofhypothesesthatthelearningagentneed
consider. In predicting conductance, for example, one need consider only material and tem-
perature and can ignore mass, ownership, day of the week, the current president, and so on.
Hypotheses can certainly include terms that are in turn determined by material and temper-
ature, such as molecular structure, thermal energy, or free-electron density. Determinations
specifyasufficientbasisvocabularyfromwhichtoconstructhypothesesconcerningthetarget
predicate. This statement can be proven by showing that a given determination is logically
equivalent toastatement thatthecorrect definition ofthetarget predicate isoneofthesetof
alldefinitionsexpressible usingthepredicates ontheleft-hand sideofthedetermination.
Intuitively, it is clear that a reduction in the hypothesis space size should make it eas-
ier to learn the target predicate. Using the basic results of computational learning theory
(Section 18.5), we can quantify the possible gains. First, recall that for Boolean functions,
log(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the
size of the hypothesis space. If the learner has n Boolean features with which to construct
hypotheses, then, in the absence of further restrictions, |H| = O(22n ), so the number of ex-
amples isO(2n). Ifthe determination contains dpredicates intheleft-hand side, the learner
willrequireonly O(2d)examples,areduction ofO(2n−d).
19.4.2 Learning and using relevanceinformation
As we stated in the introduction to this chapter, prior knowledge is useful in learning; but
it too has to be learned. In order to provide a complete story of relevance-based learning,
we must therefore provide a learning algorithm for determinations. The learning algorithm
wenowpresentisbasedonastraightforward attempttofindthesimplestdetermination con-
sistent with the observations. A determination P ’ Q says that if any examples match on
P, then they must also match on Q. A determination is therefore consistent with a set of
examples if every pair that matches on the predicates on the left-hand side also matches on
the goal predicate. For example, suppose we have the following examples of conductance
measurements onmaterialsamples:
786 Chapter 19. KnowledgeinLearning
functionMINIMAL-CONSISTENT-DET(E,A)returnsasetofattributes
inputs:E,asetofexamples
A,asetofattributes,ofsizen
fori =0ton do
foreachsubsetAiofAofsizei do
ifCONSISTENT-DET?(Ai,E)thenreturnAi
functionCONSISTENT-DET?(A,E)returnsatruthvalue
inputs:A,asetofattributes
E,asetofexamples
localvariables: H,ahashtable
foreachexamplee inE do
ifsomeexampleinH hasthesamevaluesase fortheattributesA
butadifferentclassificationthenreturnfalse
storetheclassofe inH,indexedbythevaluesforattributesAoftheexamplee
returntrue
Figure19.8 Analgorithmforfindingaminimalconsistentdetermination.
Sample Mass Temperature Material Size Conductance
S1 12 26 Copper 3 0.59
S1 12 100 Copper 3 0.57
S2 24 26 Copper 6 0.59
S3 12 26 Lead 2 0.05
S3 12 100 Lead 2 0.04
S4 24 26 Lead 4 0.05
The minimal consistent determination is Material ∧Temperature ’ Conductance. There
is a nonminimal but consistent determination, namely, Mass ∧ Size ∧ Temperature ’
Conductance. Thisisconsistentwiththeexamplesbecausemassandsizedeterminedensity
and, inourdata set, wedonot have twodifferent materials withthe samedensity. Asusual,
wewouldneedalargersamplesetinordertoeliminateanearlycorrecthypothesis.
There are several possible algorithms for finding minimal consistent determinations.
Themostobviousapproachistoconductasearchthroughthespaceofdeterminations,check-
ingalldeterminations withonepredicate, twopredicates, andsoon, until aconsistent deter-
mination isfound. Wewillassumeasimple attribute-based representation, like thatusedfor
decision tree learning in Chapter 18. A determination d will be represented by the set of
attributesontheleft-handside,becausethetargetpredicateisassumedtobefixed. Thebasic
algorithm isoutlinedinFigure19.8.
The time complexity of this algorithm depends on the size of the smallest consistent
determination. Suppose thisdetermination has pattributes outofthe ntotalattr(cid:20)ib(cid:21)utes. Then
thealgorithmwillnotfindituntilsearchingthesubsetsofAofsizep. Thereare n = O(np)
p
Section19.4. LearningUsingRelevanceInformation 787
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20 40 60 80 100 120 140
tes
tset
no
tcerroc
noitroporP
RBDTL
DTL
Training set size
Figure 19.9 A performance comparison between DECISION-TREE-LEARNING and
RBDTL on randomly generated data for a target function that depends on only 5 of 16
attributes.
such subsets; hence the algorithm is exponential inthe size ofthe minimal determination. It
turns out that the problem is NP-complete, so we cannot expect to do better in the general
case. Inmostdomains, however, there willbesufficient localstructure (seeChapter14fora
definitionoflocallystructured domains)that pwillbesmall.
Givenanalgorithm forlearningdeterminations, alearning agenthasawaytoconstruct
aminimalhypothesiswithinwhichtolearnthetargetpredicate. Forexample,wecancombine
MINIMAL-CONSISTENT-DET withthe DECISION-TREE-LEARNING algorithm. Thisyields
a relevance-based decision-tree learning algorithm RBDTL that first identifies a minimal
set of relevant attributes and then passes this set to the decision tree algorithm for learning.
Unlike DECISION-TREE-LEARNING, RBDTL simultaneously learns andusesrelevance in-
formationinordertominimizeitshypothesisspace. WeexpectthatRBDTLwilllearnfaster
thanDECISION-TREE-LEARNING,andthisisinfactthecase. Figure19.9showsthelearning
performance for the two algorithms on randomly generated data for a function that depends
ononly 5of16 attributes. Obviously, incases whereall the available attributes are relevant,
RBDTL willshownoadvantage.
Thissectionhasonlyscratched thesurfaceofthefieldof declarative bias,whichaims
DECLARATIVEBIAS
tounderstand how priorknowledge can beused to identify the appropriate hypothesis space
withinwhichtosearchforthecorrecttargetdefinition. Therearemanyunansweredquestions:
• Howcanthealgorithms beextendedtohandlenoise?
• Canwehandle continuous-valued variables?
• Howcanotherkindsofpriorknowledgebeused,besidesdeterminations?
• How can the algorithms be generalized to cover any first-order theory, rather than just
anattribute-based representation?
Someofthesequestions areaddressed inthenextsection.
788 Chapter 19. KnowledgeinLearning
19.5 INDUCTIVE LOGIC PROGRAMMING
Inductivelogicprogramming(ILP)combinesinductivemethodswiththepoweroffirst-order
representations, concentrating in particular on the representation of hypotheses as logic pro-
grams.3 It has gained popularity for three reasons. First, ILP offers a rigorous approach to
the general knowledge-based inductive learning problem. Second, it offers complete algo-
rithms for inducing general, first-order theories from examples, which can therefore learn
successfully in domains where attribute-based algorithms are hard to apply. An example is
in learning how protein structures fold (Figure 19.10). The three-dimensional configuration
of a protein molecule cannot be represented reasonably by a set of attributes, because the
configuration inherently refers to relationships between objects, not to attributes of a single
object. First-order logic is an appropriate language for describing the relationships. Third,
inductive logic programming produces hypotheses that are (relatively) easy for humans to
read. For example, the English translation in Figure 19.10 can be scrutinized and criticized
byworkingbiologists. Thismeansthatinductive logicprogrammingsystemscanparticipate
inthescientificcycleofexperimentation, hypothesisgeneration,debate,andrefutation. Such
participation wouldnotbepossible forsystemsthatgenerate “black-box” classifiers, suchas
neuralnetworks.
19.5.1 Anexample
RecallfromEquation(19.5)thatthegeneralknowledge-basedinductionproblemisto“solve”
theentailmentconstraint
Background ∧Hypothesis ∧Descriptions |= Classifications
fortheunknown Hypothesis, given the Background knowledge and examples described by
Descriptions and Classifications. To illustrate this, we will use the problem of learning
family relationships from examples. The descriptions will consist of an extended family
tree, described in terms of Mother, Father, and Married relations and Male and Female
properties. As an example, we will use the family tree from Exercise 8.14, shown here in
Figure19.11. Thecorresponding descriptions areasfollows:
Father(Philip,Charles) Father(Philip,Anne) ...
Mother(Mum,Margaret) Mother(Mum,Elizabeth) ...
Married(Diana,Charles) Married(Elizabeth,Philip) ...
Male(Philip) Male(Charles) ...
Female(Beatrice) Female(Margaret) ...
ThesentencesinClassifications dependonthetargetconceptbeinglearned. Wemightwant
to learn Grandparent, BrotherInLaw, or Ancestor, for example. For Grandparent, the
3 ItmightbeappropriateatthispointforthereadertorefertoChapter7forsomeoftheunderlyingconcepts,
includingHornclauses,conjunctivenormalform,unification,andresolution.
Section19.5. Inductive LogicProgramming 789
completesetofClassifications contains 20×20=400conjuncts oftheform
Grandparent(Mum,Charles) Grandparent(Elizabeth,Beatrice) ...
¬Grandparent(Mum,Harry) ¬Grandparent(Spencer,Peter) ...
Wecouldofcourselearnfromasubsetofthiscompleteset.
The object of an inductive learning program is to come up with a set of sentences for
theHypothesis suchthattheentailmentconstraintissatisfied. Suppose,forthemoment,that
the agent hasno background knowledge: Background isempty. Thenonepossible solution
H:5[111-113]
H:1[19-37] H:6[79-88]
H:3[71-84]
H:4[61-64]
H:1[8-17]
H:5[66-70]
H:2[26-33]
E:2[96-98]
E:1[57-59]
H:4[93-108]
H:2[41-64]
H:7[99-106] H:3[40-50]
2mhr - Four-helical up-and-down bundle 1omd - EF-Hand
(a) (b)
Figure 19.10 (a) and (b) show positive and negative examples, respectively, of the
“four-helical up-and-down bundle” concept in the domain of protein folding. Each
example structure is coded into a logical expression of about 100 conjuncts such as
TotalLength(D2mhr,118)∧NumberHelices(D2mhr,6)∧....Fromthesedescriptionsand
from classifications such as Fold(FOUR-HELICAL-UP-AND-DOWN-BUNDLE,D2mhr),
theILPsystemPROGOL(Muggleton,1995)learnedthefollowingrule:
Fold(FOUR-HELICAL-UP-AND-DOWN-BUNDLE,p)⇐
Helix(p,h
1
)∧Length(h
1
,HIGH)∧Position(p,h
1
,n)
∧(1≤n≤3)∧Adjacent(p,h ,h )∧Helix(p,h ).
1 2 2
Thiskindofrulecouldnotbelearned,orevenrepresented,byanattribute-basedmechanism
suchaswesaw inpreviouschapters. TherulecanbetranslatedintoEnglishas“ Proteinp
hasfoldclass“Four-helicalup-and-down-bundle”ifitcontainsalonghelixh atasecondary
1
structurepositionbetween1and3andh isnexttoasecondhelix.”
1
790 Chapter 19. KnowledgeinLearning
forHypothesis isthefollowing:
Grandparent(x,y) ⇔ [∃z Mother(x,z)∧Mother(z,y)]
∨ [∃z Mother(x,z)∧Father(z,y)]
∨ [∃z Father(x,z)∧Mother(z,y)]
∨ [∃z Father(x,z)∧Father(z,y)].
Noticethatanattribute-based learningalgorithm,suchas DECISION-TREE-LEARNING,will
getnowhere insolving thisproblem. Inordertoexpress Grandparent asanattribute (i.e., a
unarypredicate), wewouldneedtomake pairsofpeopleintoobjects:
Grandparent((cid:16)Mum,Charles(cid:17))...
Thenwegetstuckintryingtorepresenttheexampledescriptions. Theonlypossibleattributes
arehorriblethingssuchas
FirstElementIsMotherOfElizabeth((cid:16)Mum,Charles(cid:17)).
The definition of Grandparent in terms of these attributes simply becomes a large disjunc-
tionofspecificcasesthatdoesnotgeneralizetonewexamplesatall. Attribute-basedlearning
algorithmsareincapable oflearningrelational predicates. Thus,oneoftheprincipal advan-
tages of ILP algorithms is their applicability to a much wider range of problems, including
relational problems.
Thereader will certainly have noticed that a little bit ofbackground knowledge would
help in the representation of the Grandparent definition. For example, if Background in-
cludedthesentence
Parent(x,y) ⇔ [Mother(x,y)∨Father(x,y)],
thenthedefinitionofGrandparent wouldbereducedto
Grandparent(x,y) ⇔ [∃z Parent(x,z)∧Parent(z,y)].
This shows how background knowledge can dramatically reduce the size of hypotheses re-
quiredtoexplaintheobservations.
It is also possible for ILP algorithms to create new predicates in order to facilitate the
expression of explanatory hypotheses. Given the example data shown earlier, it is entirely
reasonable for the ILP program to propose an additional predicate, which we would call
George Mum
Spencer Kydd Elizabeth Philip Margaret
Diana Charles Anne Mark Andrew Sarah Edward Sophie
William Harry Peter Zara Beatrice Eugenie Louise James
Figure19.11 Atypicalfamilytree.
Section19.5. Inductive LogicProgramming 791
“Parent,” in order to simplify the definitions of the target predicates. Algorithms that can
CONSTRUCTIVE generate new predicates are called constructive inductionalgorithms. Clearly, constructive
INDUCTION
induction is a necessary part of the picture of cumulative learning. It has been one of the
hardestproblemsinmachinelearning,butsomeILPtechniquesprovideeffectivemechanisms
forachieving it.
In the rest of this chapter, we will study the two principal approaches to ILP. The first
uses a generalization of decision tree methods, and the second uses techniques based on
inverting aresolution proof.
19.5.2 Top-down inductive learningmethods
ThefirstapproachtoILPworksbystartingwithaverygeneral ruleandgraduallyspecializing
it so that it fits the data. This is essentially what happens in decision-tree learning, where a
decision tree is gradually grown until it is consistent with the observations. To do ILP we
use first-order literals instead ofattributes, and the hypothesis isaset ofclauses instead of a
decision tree. Thissection describes FOIL (Quinlan, 1990),oneofthefirstILPprograms.
Suppose we are trying to learn a definition of the Grandfather(x,y) predicate, using
the same family data as before. As with decision-tree learning, we can divide the examples
intopositiveandnegativeexamples. Positiveexamplesare
(cid:16)George,Anne(cid:17), (cid:16)Philip,Peter(cid:17), (cid:16)Spencer,Harry(cid:17), ...
andnegativeexamplesare
(cid:16)George,Elizabeth(cid:17), (cid:16)Harry,Zara(cid:17), (cid:16)Charles,Philip(cid:17), ...
Noticethateachexample isapair ofobjects, because Grandfather isabinary predicate. In
all,thereare12positiveexamplesinthefamilytreeand388 negativeexamples(alltheother
pairsofpeople).
FOILconstructsasetofclauses,eachwithGrandfather(x,y)asthehead. Theclauses
must classify the 12 positive examples as instances of the Grandfather(x,y) relationship,
whilerulingoutthe388negativeexamples. TheclausesareHornclauses,withtheextension
thatnegatedliterals areallowedinthebody ofaclause andareinterpreted usingnegation as
failure, asinProlog. Theinitialclausehasanemptybody:
⇒ Grandfather(x,y).
Thisclause classifies every example as positive, so itneeds to bespecialized. Wedo this by
addingliteralsoneatatimetotheleft-hand side. Herearethreepotential additions:
Father(x,y) ⇒ Grandfather(x,y).
Parent(x,z) ⇒ Grandfather(x,y).
Father(x,z) ⇒ Grandfather(x,y).
(Noticethatweareassumingthataclausedefining Parent isalreadypartofthebackground
knowledge.) Thefirstofthesethreeclausesincorrectly classifiesallofthe12positiveexam-
ples as negative and can thus beignored. Thesecond and third agree withallof the positive
examples, butthesecondisincorrect onalargerfraction of thenegativeexamples—twiceas
many,because itallowsmothersaswellasfathers. Hence,wepreferthethirdclause.
792 Chapter 19. KnowledgeinLearning
Now we need to specialize this clause further, to rule out the cases in which x is the
fatherofsomez,butz isnotaparentofy. Addingthesingleliteral Parent(z,y)gives
Father(x,z)∧Parent(z,y) ⇒ Grandfather(x,y),
which correctly classifies all the examples. FOIL will find and choose this literal, thereby
solving the learning task. In general, the solution is a set of Horn clauses, each of which
implies the target predicate. For example, if we didn’t have the Parent predicate in our
vocabulary, thenthesolution mightbe
Father(x,z)∧Father(z,y) ⇒ Grandfather(x,y)
Father(x,z)∧Mother(z,y) ⇒ Grandfather(x,y).
Notethateachoftheseclausescoverssomeofthepositiveexamples,thattogethertheycover
all the positive examples, and that NEW-CLAUSE is designed in such a way that no clause
willincorrectly coveranegativeexample. Ingeneral FOIL willhavetosearchthrough many
unsuccessful clauses beforefindingacorrectsolution.
Thisexample isavery simpleillustration ofhow FOIL operates. Asketch of thecom-
plete algorithm is shown in Figure 19.12. Essentially, the algorithm repeatedly constructs a
clause,literalbyliteral,untilitagreeswithsomesubset ofthepositiveexamplesandnoneof
the negative examples. Then the positive examples covered by the clause are removed from
the training set, and the process continues until no positive examples remain. The two main
subroutinestobeexplainedareNEW-LITERALS,whichconstructsallpossiblenewliteralsto
addtotheclause, and CHOOSE-LITERAL,whichselectsaliteraltoadd.
NEW-LITERALS takes a clause and constructs all possible “useful” literals that could
beaddedtotheclause. Letususeasanexampletheclause
Father(x,z) ⇒ Grandfather(x,y).
Therearethreekindsofliteralsthatcanbeadded:
1. Literalsusingpredicates: theliteralcanbenegatedorunnegated,anyexistingpredicate
(includingthegoalpredicate)canbeused,andtheargumentsmustallbevariables. Any
variablecanbeusedforanyargumentofthepredicate,withonerestriction: eachliteral
mustincludeatleastonevariablefromanearlierliteralorfromtheheadoftheclause.
Literals such as Mother(z,u), Married(z,z), ¬Male(y), and Grandfather(v,x) are
allowed, whereas Married(u,v) is not. Notice that the use of the predicate from the
headoftheclauseallows FOIL tolearnrecursive definitions.
2. Equality and inequality literals: these relate variables already appearing in the clause.
Forexample, we might add z (cid:7)= x. These literals can also include user-specified con-
stants. Forlearning arithmetic wemightuse0and1,andforlearning listfunctions we
mightusetheemptylist[].
3. Arithmetic comparisons: when dealing with functions of continuous variables, literals
such as x > y and y ≤ z can be added. As in decision-tree learning, a constant
threshold valuecanbechosen tomaximizethediscriminatory powerofthetest.
Theresultingbranchingfactorinthissearchspaceisverylarge(seeExercise19.6),butFOIL
can also use type information to reduce it. Forexample, if the domain included numbers as
Section19.5. Inductive LogicProgramming 793
functionFOIL(examples,target)returnsasetofHornclauses
inputs:examples,setofexamples
target,aliteralforthegoalpredicate
localvariables: clauses,setofclauses,initiallyempty
whileexamples containspositiveexamplesdo
clause←NEW-CLAUSE(examples,target)
removepositiveexamplescoveredbyclause fromexamples
addclause toclauses
returnclauses
functionNEW-CLAUSE(examples,target)returnsaHornclause
localvariables: clause,aclausewithtarget asheadandanemptybody
l,aliteraltobeaddedtotheclause
extended examples,asetofexampleswithvaluesfornewvariables
extended examples←examples
whileextended examples containsnegativeexamplesdo
l←CHOOSE-LITERAL(NEW-LITERALS(clause),extended examples)
appendl tothebodyofclause
extended examples←setofexamplescreatedbyapplyingEXTEND-EXAMPLE
toeachexampleinextended examples
returnclause
functionEXTEND-EXAMPLE(example,literal)returnsasetofexamples
ifexample satisfiesliteral
thenreturnthesetofexamplescreatedbyextendingexample with
eachpossibleconstantvalueforeachnewvariableinliteral
elsereturntheemptyset
Figure 19.12 Sketch of the FOIL algorithm forlearning sets of first-orderHorn clauses
fromexamples. NEW-LITERALSandCHOOSE-LITERALareexplainedinthetext.
wellaspeople,typerestrictionswouldpreventNEW-LITERALS fromgeneratingliteralssuch
asParent(x,n),wherexisapersonandnisanumber.
CHOOSE-LITERALusesaheuristicsomewhatsimilartoinformationgain(seepage704)
to decide which literal to add. The exact details are not important here, and a number of
different variations have been tried. One interesting additional feature of FOIL is the use of
Ockham’srazortoeliminatesomehypotheses. Ifaclausebecomeslonger(accordingtosome
metric) than the total length of the positive examples that the clause explains, that clause is
notconsideredasapotentialhypothesis. Thistechniqueprovidesawaytoavoidovercomplex
clausesthatfitnoiseinthedata.
FOIL andits relatives havebeen used tolearn awidevariety ofdefinitions. Oneofthe
mostimpressivedemonstrations (QuinlanandCameron-Jones,1993)involvedsolvingalong
sequence of exercises on list-processing functions from Bratko’s (1986) Prolog textbook. In
794 Chapter 19. KnowledgeinLearning
each case, the program wasable tolearn acorrect definition ofthefunction from asmallset
ofexamples, usingthepreviously learnedfunctions asbackground knowledge.
19.5.3 Inductive learningwithinverse deduction
The second major approach to ILP involves inverting the normal deductive proof process.
INVERSE Inverse resolution is based on the observation that if the example Classifications follow
RESOLUTION
fromBackground ∧Hypothesis ∧Descriptions,thenonemustbeabletoprovethisfactby
resolution (becauseresolutioniscomplete). Ifwecan“run theproofbackward,”thenwecan
find aHypothesis such that the proof goes through. The key, then, is to find away to invert
theresolution process.
Wewillshowabackwardproofprocessforinverseresolution thatconsistsofindividual
backward steps. Recall that an ordinary resolution step takes two clauses C and C and
1 2
resolves them to produce the resolvent C. An inverse resolution step takes a resolvent C
and produces two clauses C and C , such that C is the result of resolving C and C .
1 2 1 2
Alternatively, it may take a resolvent C and clause C and produce a clause C such that C
1 2
istheresultofresolving C andC .
1 2
The early steps in an inverse resolution process are shown in Figure 19.13, where we
focus on the positive example Grandparent(George,Anne). Theprocess begins at the end
of the proof (shown at the bottom of the figure). We take the resolvent C to be empty
clause(i.e. acontradiction) andC tobe¬Grandparent(George,Anne),whichisthenega-
2
tion of the goal example. The first inverse step takes C and C and generates the clause
2
Grandparent(George,Anne) for C . The next step takes this clause as C and the clause
1
Parent(Elizabeth,Anne)asC ,andgenerates theclause
2
¬Parent(Elizabeth,y)∨Grandparent(George,y)
asC . Thefinalstep treats this clause as the resolvent. With Parent(George,Elizabeth)as
1
C ,onepossible clauseC isthehypothesis
2 1
Parent(x,z)∧Parent(z,y) ⇒ Grandparent(x,y).
Nowwehavearesolutionproofthatthehypothesis,descriptions,andbackgroundknowledge
entailtheclassification Grandparent(George,Anne).
Clearly, inverse resolution involves a search. Each inverse resolution step is nonde-
terministic, because for any C, there can be many or even an infinite number of clauses
C and C that resolve to C. For example, instead of choosing ¬Parent(Elizabeth,y) ∨
1 2
Grandparent(George,y) for C in the last step of Figure 19.13, the inverse resolution step
1
mighthavechosenanyofthefollowingsentences:
¬Parent(Elizabeth,Anne)∨Grandparent(George,Anne).
¬Parent(z,Anne)∨Grandparent(George,Anne).
¬Parent(z,y)∨Grandparent(George,y).
.
.
.
(See Exercises 19.4 and 19.5.) Furthermore, the clauses that participate in each step can be
chosenfromtheBackground knowledge,fromtheexampleDescriptions,fromthenegated
Section19.5. Inductive LogicProgramming 795
Classifications,orfromhypothesizedclausesthathavealreadybeengeneratedintheinverse
resolution tree. Thelarge number ofpossibilities means alarge branching factor (and there-
foreaninefficientsearch)withoutadditionalcontrols. Anumberofapproaches totamingthe
searchhavebeentriedinimplemented ILPsystems:
1. Redundant choices can be eliminated—for example, by generating only the most spe-
cifichypothesespossibleandbyrequiringthatallthehypothesizedclausesbeconsistent
witheachother, andwiththeobservations. Thislastcriterionwouldruleouttheclause
¬Parent(z,y)∨Grandparent(George,y),listedbefore.
2. The proof strategy can be restricted. For example, we saw in Chapter 9 that linear
resolutionisacomplete,restrictedstrategy. Linearresolutionproducesprooftreesthat
have a linear branching structure—the whole tree follows one line, with only single
clausesbranching offthatline(asinFigure19.13).
3. Therepresentationlanguagecanberestricted,forexamplebyeliminatingfunctionsym-
bols or by allowing only Horn clauses. For instance, PROGOL operates with Horn
INVERSE clausesusinginverseentailment. Theideaistochangetheentailmentconstraint
ENTAILMENT
Background ∧Hypothesis ∧Descriptions |= Classifications
tothelogically equivalent form
Background ∧Descriptions ∧¬Classifications |= ¬Hypothesis.
From this, one can use a process similar to the normal Prolog Horn-clause deduction,
withnegation-as-failure toderive Hypothesis. Becauseitisrestricted toHornclauses,
thisisanincomplete method, but itcanbemoreefficientthan full resolution. Itisalso
possibletoapplycompleteinference withinverseentailment (Inoue,2001).
4. Inferencecanbedonewithmodelcheckingratherthantheoremproving. ThePROGOL
system (Muggleton, 1995) uses a form of model checking to limit the search. That
Parent(Elizabeth,y) Grandparent(George,y) Parent(Elizabeth,Anne)
{y/Anne}
Grandparent(George,Anne) Grandparent(George,Anne)
>
Parent(George,Elizabeth)
{x/George, z/Elizabeth}
> Parent(z,y) Grandparent(x,y) > ¬Parent(x,z) ¬
¬
¬
Figure 19.13 Early steps in an inverse resolution process. The shaded clauses are
generated by inverse resolution steps from the clause to the right and the clause below.
The unshaded clauses are from the Descriptions and Classifications (including negated
Classifications).
796 Chapter 19. KnowledgeinLearning
is, like answer set programming, it generates possible values for logical variables, and
checksforconsistency.
5. Inferencecanbedonewithgroundpropositional clausesratherthaninfirst-orderlogic.
TheLINUSsystem(LavraucandDuzeroski,1994)worksbytranslatingfirst-orderthe-
ories into propositional logic, solving them with a propositional learning system, and
then translating back. Working with propositional formulas can be more efficient on
someproblems, aswesawwithSATPLAN inChapter10.
19.5.4 Making discoveries withinductive logicprogramming
Aninverse resolution procedure that inverts a complete resolution strategy is, in principle, a
complete algorithm for learning first-order theories. That is, if some unknown Hypothesis
generates a set of examples, then an inverse resolution procedure can generate Hypothesis
from the examples. This observation suggests an interesting possibility: Suppose that the
available examples include avariety oftrajectories offalling bodies. Wouldaninverse reso-
lutionprogrambetheoretically capableofinferringthelawofgravity? Theanswerisclearly
yes,becausethelawofgravityallowsonetoexplaintheexamples,givensuitablebackground
mathematics. Similarly,onecanimaginethatelectromagnetism,quantummechanics,andthe
theoryofrelativityarealsowithinthescopeofILPprograms. Ofcourse,theyarealsowithin
the scope of a monkey with a typewriter; we still need better heuristics and new ways to
structure thesearchspace.
Onethingthatinverseresolution systems willdoforyouisinventnewpredicates. This
abilityisoftenseenassomewhatmagical,becausecomputersareoftenthoughtofas“merely
working with what they are given.” In fact, new predicates fall directly out of the inverse
resolution step. Thesimplestcasearises inhypothesizing twonewclauses C andC ,given
1 2
aclauseC. TheresolutionofC andC eliminatesaliteralthatthetwoclausesshare;hence,
1 2
itisquite possible thattheeliminated literalcontained apredicate thatdoes notappearin C.
Thus, whenworking backward, one possibility is togenerate anewpredicate from which to
reconstruct themissingliteral.
Figure19.14showsanexampleinwhichthenewpredicateP isgeneratedintheprocess
oflearning adefinition forAncestor. Oncegenerated, P canbeusedinlaterinverseresolu-
tionsteps. Forexample,alaterstepmighthypothesizethatMother(x,y) ⇒ P(x,y). Thus,
thenewpredicate P hasitsmeaningconstrainedbythegenerationofhypotheses thatinvolve
it. Another example might lead to the constraint Father(x,y) ⇒ P(x,y). In other words,
the predicate P is what we usually think of as the Parent relationship. As we mentioned
earlier, the invention of new predicates can significantly reduce the size of the definition of
thegoalpredicate. Hence,byincludingtheabilitytoinventnewpredicates,inverseresolution
systemscanoftensolvelearning problemsthatareinfeasible withothertechniques.
Someof the deepest revolutions in science come from the invention of new predicates
and functions—for example, Galileo’s invention of acceleration orJoule’s invention of ther-
mal energy. Once these terms are available, the discovery of new laws becomes (relatively)
easy. The difficult part lies in realizing that some new entity, with a specific relationship
to existing entities, will allow an entire body of observations to be explained with a much
Section19.6. Summary 797
Father(x,y) P(x,y)
{x/George}
>
Father(George,y) Ancestor(George,y) >
P(George,y) Ancestor(George,y) > ¬ ¬
Figure19.14 AninverseresolutionstepthatgeneratesanewpredicateP.
simplerandmoreeleganttheorythanpreviously existed.
Asyet,ILPsystemshavenotmadediscoveriesonthelevelofGalileoorJoule,buttheir
discoveries have been deemed publishable in the scientific literature. For example, in the
JournalofMolecularBiology,Turcotteetal.(2001)describetheautomateddiscoveryofrules
forprotein folding by the ILP program PROGOL. Many of the rules discovered by PROGOL
could havebeenderivedfrom knownprinciples, butmosthadnotbeenpreviously published
as part of a standard biological database. (See Figure 19.10 for an example.). In related
work, Srinivasan et al. (1994) dealt with the problem of discovering molecular-structure-
basedrulesforthemutagenicityofnitroaromaticcompounds. Thesecompoundsarefoundin
automobileexhaustfumes. For80%ofthecompoundsinastandarddatabase,itispossibleto
identify fourimportantfeatures, andlinearregression on thesefeaturesoutperforms ILP.For
theremaining20%,thefeaturesalonearenotpredictive, andILPidentifiesrelationships that
allow it to outperform linear regression, neural nets, and decision trees. Most impressively,
Kingetal.(2009)endowedarobotwiththeabilitytoperformmolecularbiologyexperiments
and extended ILP techniques to include experiment design, thereby creating an autonomous
scientistthatactuallydiscoverednewknowledgeaboutthefunctionalgenomicsofyeast. For
alltheseexamplesitappearsthattheabilitybothtorepresentrelationsandtousebackground
knowledgecontribute toILP’shighperformance. ThefactthattherulesfoundbyILPcanbe
interpreted by humans contributes to the acceptance of these techniques in biology journals
ratherthanjustcomputersciencejournals.
ILPhas made contributions to other sciences besides biology. One of the most impor-
tant is natural language processing, where ILP has been used to extract complex relational
information fromtext. TheseresultsaresummarizedinChapter23.
19.6 SUMMARY
This chapter has investigated various ways in which prior knowledge can help an agent to
learn from new experiences. Because much prior knowledge is expressed in terms of rela-
tional models rather than attribute-based models, we have also covered systems that allow
learning ofrelational models. Theimportantpointsare:
• The use of prior knowledge in learning leads to a picture of cumulative learning, in
whichlearning agentsimprovetheirlearning abilityastheyacquiremoreknowledge.
• Priorknowledge helps learning byeliminating otherwise consistent hypotheses andby
798 Chapter 19. KnowledgeinLearning
“fillingin”theexplanationofexamples,therebyallowingforshorterhypotheses. These
contributions oftenresultinfasterlearningfromfewerexamples.
• Understanding the different logical roles played by prior knowledge, as expressed by
entailmentconstraints, helpstodefineavarietyoflearningtechniques.
• Explanation-basedlearning(EBL)extractsgeneralrulesfromsingleexamplesby ex-
plainingtheexamplesandgeneralizingtheexplanation. Itprovides adeductivemethod
forturning first-principles knowledgeintouseful, efficient,special-purpose expertise.
• Relevance-based learning(RBL)uses priorknowledge in the form of determinations
to identify the relevant attributes, thereby generating a reduced hypothesis space and
speedinguplearning. RBLalsoallowsdeductivegeneralizationsfromsingleexamples.
• Knowledge-based inductive learning (KBIL)finds inductive hypotheses that explain
setsofobservations withthehelpofbackground knowledge.
• Inductive logic programming (ILP) techniques perform KBIL on knowledge that is
expressed in first-order logic. ILP methods can learn relational knowledge that is not
expressible inattribute-based systems.
• ILPcanbedone withatop-down approach ofrefining averygeneral rule orthrough a
bottom-upapproach ofinverting thedeductive process.
• ILPmethodsnaturallygeneratenewpredicates withwhichconcisenewtheoriescanbe
expressedandshowpromiseasgeneral-purpose scientifictheoryformationsystems.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Althoughtheuseofpriorknowledgeinlearningwouldseemtobeanaturaltopicforphiloso-
phersofscience,littleformalworkwasdoneuntilquiterecently. Fact,Fiction,andForecast,
by the philosopher Nelson Goodman (1954), refuted the earlier supposition that induction
was simply a matter of seeing enough examples of some universally quantified proposition
andthenadoptingitasahypothesis. Consider, forexample, thehypothesis“Allemeraldsare
grue,” where grue means “green if observed before time t, but blue if observed thereafter.”
At any time up to t, we might have observed millions of instances confirming the rule that
emeraldsaregrue,andnodisconfirminginstances,andyetweareunwillingtoadopttherule.
Thiscanbeexplained onlybyappealtotheroleofrelevantpriorknowledgeintheinduction
process. Goodmanproposesavarietyofdifferentkindsofpriorknowledgethatmightbeuse-
ful,includingaversionofdeterminationscalledoverhypotheses. Unfortunately, Goodman’s
ideaswereneverpursuedinmachinelearning.
Thecurrent-best-hypothesisapproachisanoldideainphilosophy(Mill,1843). Early
work in cognitive psychology also suggested that it is a natural form of concept learning in
humans (Bruner et al., 1957). In AI, the approach is most closely associated with the work
of Patrick Winston, whose Ph.D. thesis (Winston, 1970) addressed the problem of learning
descriptions of complex objects. The version space method (Mitchell, 1977, 1982) takes
a different approach, maintaining the set of all consistent hypotheses and eliminating those
found tobeinconsistent withnewexamples. Theapproach wasused intheMeta-DENDRAL
Bibliographical andHistorical Notes 799
expert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchell’s (1983)
LEX system, which learns to solve calculus problems. Athird influential thread wasformed
by the work ofMichalski and colleagues on the AQseries of algorithms, which learned sets
oflogical rules(Michalski, 1969;Michalski etal.,1986).
EBL had its roots in the techniques used by the STRIPS planner (Fikes et al., 1972).
When a plan was constructed, a generalized version of it was saved in a plan library and
used in later planning as a macro-operator. Similar ideas appeared in Anderson’s ACT*
architecture, under the heading of knowledge compilation (Anderson, 1983), and in the
SOAR architecture, as chunking (Laird et al., 1986). Schema acquisition (DeJong, 1981),
analytical generalization (Mitchell, 1982), and constraint-based generalization (Minton,
1984) were immediate precursors of the rapid growth of interest in EBL stimulated by the
papers of Mitchell et al. (1986) and DeJong and Mooney (1986). Hirsh (1987) introduced
theEBLalgorithmdescribedinthetext,showinghowitcould beincorporated directlyintoa
logicprogrammingsystem. VanHarmelenandBundy(1988)explainEBLasavariantofthe
partialevaluation methodusedinprogram analysis systems(Jones etal.,1993).
Initial enthusiasm for EBLwas tempered by Minton’s finding (1988) that, without ex-
tensiveextrawork,EBLcouldeasilyslowdownaprogramsignificantly. Formalprobabilistic
analysisoftheexpectedpayoffofEBLcanbefoundin Greiner(1989)andSubramanianand
Feldman(1990). Anexcellent surveyofearlyworkonEBLappearsin Dietterich(1990).
Insteadofusingexamplesasfociforgeneralization, onecanusethemdirectly tosolve
ANALOGICAL new problems, in a process known as analogical reasoning. This form of reasoning ranges
REASONING
from a form of plausible reasoning based on degree of similarity (Gentner, 1983), through
a form of deductive inference based on determinations but requiring the participation of the
example (Davies and Russell, 1987), to a form of “lazy” EBL that tailors the direction of
generalization of the old example to fit the needs of the new problem. This latter form of
analogical reasoning is found most commonly in case-based reasoning (Kolodner, 1993)
andderivational analogy(VelosoandCarbonell, 1993).
Relevance information in the form of functional dependencies was first developed in
the database community, where it is used to structure large sets of attributes into manage-
able subsets. Functional dependencies were used for analogical reasoning by Carbonell
and Collins (1973) and rediscovered and given a full logical analysis by Davies and Rus-
sell (Davies, 1985; Davies and Russell, 1987). Their role as prior knowledge in inductive
learning was explored by Russell and Grosof (1987). The equivalence of determinations to
arestricted-vocabulary hypothesis space wasproved inRussell (1988). Learning algorithms
for determinations and the improved performance obtained by RBDTL were first shown in
the FOCUS algorithm, due toAlmuallim andDietterich (1991). Tadepalli (1993) describes a
veryingenious algorithm forlearning withdeterminations thatshowslarge improvements in
learning speed.
The idea that inductive learning can be performed by inverse deduction can be traced
to W. S. Jevons (1874), who wrote, “The study both of Formal Logic and of the Theory of
Probabilities has led meto adopt theopinion that there isno such thing as adistinct method
of induction as contrasted with deduction, but that induction is simply an inverse employ-
mentofdeduction.” Computational investigations beganwiththeremarkablePh.D.thesisby
800 Chapter 19. KnowledgeinLearning
GordonPlotkin(1971) atEdinburgh. AlthoughPlotkindeveloped manyofthetheorems and
methodsthatareincurrentuseinILP,hewasdiscouragedbysomeundecidability resultsfor
certainsubproblems ininduction. MIS(Shapiro, 1981)reintroduced theproblem oflearning
logic programs, but was seen mainly as a contribution to the theory of automated debug-
ging. Workonruleinduction, suchasthe ID3 (Quinlan, 1986) and CN2 (ClarkandNiblett,
1989)systems,ledtoFOIL (Quinlan,1990), whichforthefirsttimeallowedpractical induc-
tion of relational rules. The field of relational learning was reinvigorated by Muggleton and
Buntine(1988),whoseCIGOLprogramincorporated aslightlyincompleteversionofinverse
resolution andwascapableofgenerating newpredicates. Theinverseresolution methodalso
appears in(Russell, 1986), withasimple algorithm given in afootnote. Thenextmajorsys-
tem was GOLEM (Muggleton and Feng, 1990), which uses a covering algorithm based on
Plotkin’s concept of relative least general generalization. ITOU (Rouveirol and Puget, 1989)
and CLINT (DeRaedt, 1992) wereothersystemsofthatera. Morerecently, PROGOL (Mug-
gleton, 1995) has taken a hybrid (top-down and bottom-up) approach to inverse entailment
and has been applied to a number of practical problems, particularly in biology and natural
language processing. Muggleton (2000) describes anextension of PROGOL tohandle uncer-
taintyintheformofstochastic logicprograms.
A formal analysis of ILP methods appears in Muggleton (1991), a large collection of
papers in Muggleton (1992), and a collection of techniques and applications in the book
byLavraucandDuzeroski(1994). PageandSrinivasan(2002)giveamorerecentoverviewof
thefield’shistory andchallenges forthefuture. Earlycomplexity results byHaussler(1989)
suggestedthatlearningfirst-ordersentenceswasintractible. However,withbetterunderstand-
ingoftheimportance ofsyntactic restrictions onclauses, positiveresults havebeenobtained
even for clauses with recursion (Duzeroski et al., 1992). Learnability results for ILP are
surveyedbyKietzandDuzeroski(1994)andCohenandPage(1995).
Although ILPnowseemstobethedominant approach toconstructive induction, ithas
not been the only approach taken. So-called discovery systems aim to model the process
DISCOVERYSYSTEM
of scientific discovery of new concepts, usually by a direct search in the space of concept
definitions. Doug Lenat’s Automated Mathematician, or AM (Davis and Lenat, 1982), used
discovery heuristics expressed as expert system rules to guide its search for concepts and
conjectures in elementary number theory. Unlike most systems designed for mathematical
reasoning, AM lacked a concept of proof and could only make conjectures. It rediscovered
Goldbach’s conjecture and the Unique Prime Factorization theorem. AM’sarchitecture was
generalized inthe EURISKOsystem(Lenat,1983)byaddingamechanismcapable ofrewrit-
ingthe system’s owndiscovery heuristics. EURISKO wasapplied inanumberofareas other
thanmathematicaldiscovery, althoughwithlesssuccessthan AM. Themethodology ofAM
and EURISKOhasbeencontroversial (RitchieandHanna,1984;LenatandBrown,1984).
Anotherclassofdiscoverysystemsaimstooperate withreal scientificdatatofindnew
laws. The systems DALTON, GLAUBER, and STAHL (Langley et al., 1987) are rule-based
systems that look for quantitative relationships in experimental data from physical systems;
in each case, the system has been able to recapitulate a well-known discovery from the his-
tory of science. Discovery systems based on probabilistic techniques—especially clustering
algorithms thatdiscovernewcategories—are discussed inChapter20.
Exercises 801
EXERCISES
19.1 Show, by translating into conjunctive normal form and applying resolution, that the
conclusion drawnonpage784concerning Braziliansissound.
19.2 For each of the following determinations, write down the logical representation and
explainwhythedetermination istrue(ifitis):
a. Designanddenomination determinethemassofacoin.
b. Foragivenprogram, inputdetermines output.
c. Climate,foodintake, exercise, andmetabolism determine weightgainandloss.
d. Baldnessisdetermined bythebaldness (orlackthereof)of one’smaternalgrandfather.
19.3 Wouldaprobabilistic versionofdeterminations beuseful? Suggestadefinition.
19.4 Fill in the missing values for the clauses C or C (or both) in the following sets of
1 2
clauses, giventhatC istheresolventofC andC :
1 2
a. C = True ⇒ P(A,B),C = P(x,y) ⇒ Q(x,y),C =??.
1 2
b. C = True ⇒ P(A,B),C =??,C =??.
1 2
c. C = P(x,y) ⇒ P(x,f(y)),C =??,C =??.
1 2
Ifthereismorethanonepossiblesolution,provideoneexampleofeachdifferentkind.
19.5 Suppose one writes a logic program that carries out a resolution inference step. That
is, let Resolve(c ,c ,c) succeed if c is the result of resolving c and c . Normally, Resolve
1 2 1 2
would be used as part of a theorem prover by calling it with c and c instantiated to par-
1 2
ticular clauses, thereby generating the resolvent c. Now suppose instead that we call it with
c instantiated and c and c uninstantiated. Will this succeed in generating the appropriate
1 2
results of an inverse resolution step? Would you need any special modifications to the logic
programming systemforthistowork?
19.6 Suppose that FOIL is considering adding a literal to a clause using a binary predicate
P andthatpreviousliterals(including theheadoftheclause)containfivedifferentvariables.
a. Howmanyfunctionallydifferentliteralscanbegenerated? Twoliteralsarefunctionally
identicaliftheydifferonlyinthenamesofthenewvariablesthattheycontain.
b. Can you find a general formula for the number of different literals with a predicate of
arityrwhenthereare nvariables previously used?
c. Whydoes FOIL notallowliteralsthatcontainnopreviously usedvariables?
19.7 UsingthedatafromthefamilytreeinFigure19.11,orasubsetthereof,applytheFOIL
algorithm tolearnadefinition forthe Ancestor predicate.
20
LEARNING
PROBABILISTIC MODELS
Inwhichweviewlearning asaformofuncertain reasoning fromobservations.
Chapter13pointedouttheprevalenceofuncertaintyinrealenvironments. Agentscanhandle
uncertainty byusingthemethodsofprobability anddecision theory, butfirsttheymustlearn
their probabilistic theories of the world from experience. This chapter explains how they
can do that, by formulating the learning task itself as a process of probabilistic inference
(Section20.1). WewillseethataBayesianviewoflearningisextremelypowerful,providing
general solutions to the problems of noise, overfitting, and optimal prediction. It also takes
intoaccountthefactthataless-than-omniscientagentcanneverbecertainaboutwhichtheory
oftheworldiscorrect,yetmuststillmakedecisions byusingsometheoryoftheworld.
Wedescribemethodsforlearningprobability models—primarily Bayesiannetworks—
in Sections 20.2 and 20.3. Some of the material in this chapter is fairly mathematical, al-
thoughthegenerallessonscanbeunderstoodwithoutplungingintothedetails. Itmaybenefit
thereadertoreviewChapters13and14andpeekatAppendixA.
20.1 STATISTICAL LEARNING
The key concepts in this chapter, just as in Chapter 18, are data and hypotheses. Here, the
dataareevidence—thatis,instantiationsofsomeoralloftherandomvariablesdescribingthe
domain. The hypotheses in this chapter are probabilistic theories of how the domain works,
including logicaltheories asaspecial case.
Consider a simple example. Our favorite Surprise candy comes in two flavors: cherry
(yum)andlime(ugh). Themanufacturerhasapeculiarsenseofhumorandwrapseachpiece
of candy in the same opaque wrapper, regardless of flavor. The candy is sold in very large
bags,ofwhichthereareknowntobefivekinds—again, indistinguishable fromtheoutside:
h : 100%cherry,
1
h : 75%cherry+25%lime,
2
h : 50%cherry+50%lime,
3
h : 25%cherry+75%lime,
4
h : 100%lime .
5
802
Section20.1. StatisticalLearning 803
Given a new bag of candy, the random variable H (for hypothesis) denotes the type of the
bag, with possible values h through h . H is not directly observable, of course. As the
1 5
pieces of candy are opened and inspected, data are revealed—D , D , ..., D , where each
1 2 N
D is a random variable with possible values cherry and lime. The basic task faced by the
i
agent is to predict the flavor of the next piece of candy.1 Despite its apparent triviality, this
scenario serves to introduce many of the major issues. The agent really does need to infer a
theoryofitsworld,albeitaverysimpleone.
Bayesianlearningsimplycalculatestheprobability ofeachhypothesis,giventhedata,
BAYESIANLEARNING
and makes predictions on that basis. That is, the predictions are made by using all the hy-
potheses,weightedbytheirprobabilities, ratherthanbyusingjustasingle“best”hypothesis.
In this way, learning is reduced to probabilistic inference. Let D represent all the data, with
observed valued;thentheprobability ofeachhypothesis isobtainedbyBayes’rule:
P(h |d) = αP(d|h )P(h ). (20.1)
i i i
Now,suppose wewanttomakeaprediction aboutanunknownquantityX. Thenwehave
(cid:12) (cid:12)
P(X|d)= P(X|d,h )P(h |d) = P(X|h )P(h |d), (20.2)
i i i i
i i
where we have assumed that each hypothesis determines a probability distribution over X.
This equation shows that predictions are weighted averages overthe predictions of the indi-
vidual hypotheses. The hypotheses themselves are essentially “intermediaries” between the
rawdataandthepredictions. ThekeyquantitiesintheBayesianapproacharethe hypothesis
prior,P(h ),andthelikelihoodofthedataundereachhypothesis, P(d|h ).
HYPOTHESISPRIOR i i
For our candy example, we will assume for the time being that the prior distribution
LIKELIHOOD
over h ,...,h is given by (cid:16)0.1,0.2,0.4,0.2,0.1(cid:17), as advertised by the manufacturer. The
1 5
likelihood of the data is calculated under the assumption that the observations are i.i.d. (see
page708),sothat
(cid:25)
P(d|h ) = P(d |h ). (20.3)
i j i
j
For example, suppose the bag is really an all-lime bag (h ) and the first 10 candies are all
5
lime;thenP(d|h )is0.510,because halfthecandies inanh bagarelime.2 Figure20.1(a)
3 3
shows how the posterior probabilities of the five hypotheses change as the sequence of 10
lime candies is observed. Notice that the probabilities start out at their prior values, so h
3
is initially the most likely choice and remains so after 1 lime candy is unwrapped. After 2
limecandies areunwrapped, h ismostlikely; after3ormore, h (thedreadedall-limebag)
4 5
is the most likely. After 10 in a row, we are fairly certain of our fate. Figure 20.1(b) shows
thepredicted probability thatthenextcandyislime,based onEquation(20.2). Aswewould
expect,itincreases monotonically toward1.
1 Statisticallysophisticatedreaderswillrecognizethisscenarioasavariantoftheurn-and-ballsetup. Wefind
urnsandballslesscompellingthancandy;furthermore,candylendsitselftoothertasks,suchasdecidingwhether
totradethebagwithafriend—seeExercise20.2.
2 Westatedearlierthatthebagsofcandyareverylarge;otherwise,thei.i.d.assumptionfailstohold.Technically,
itismorecorrect(butlesshygienic)torewrapeachcandyafterinspectionandreturnittothebag.
804 Chapter 20. LearningProbabilistic Models
1
0.8
0.6
0.4
0.2
0
0 2 4 6 8 10
sisehtopyh
fo
ytilibaborp
roiretsoP
1
P(h | d)
1
P(h | d)
P(h 2 | d) 0.9 3
P(h | d)
P(h 4 | d) 0.8
5
0.7
0.6
0.5
0.4
0 2 4 6 8 10
Number of observations in d
emil
si ydnac
txen
taht
ytilibaborP
Number of observations in d
(a) (b)
Figure 20.1 (a) Posterior probabilities P(hi |d
1
,...,dN) from Equation (20.1). The
number of observations N ranges from 1 to 10, and each observation is of a lime candy.
(b)BayesianpredictionP(dN+1 =lime|d
1
,...,dN)fromEquation(20.2).
The example shows that the Bayesian prediction eventually agrees with the true hy-
pothesis. This is characteristic of Bayesian learning. For any fixed prior that does not rule
out the true hypothesis, the posterior probability of any false hypothesis will, under certain
technical conditions, eventually vanish. Thishappens simplybecause theprobability ofgen-
erating “uncharacteristic” data indefinitely is vanishingly small. (This point is analogous to
one made in the discussion of PAC learning in Chapter 18.) More important, the Bayesian
prediction is optimal, whetherthe data setbesmallorlarge. Giventhe hypothesis prior, any
otherprediction isexpected tobecorrectlessoften.
The optimality of Bayesian learning comes at a price, of course. For real learning
problems, the hypothesis space is usually very large orinfinite, as wesaw in Chapter 18. In
somecases, thesummationinEquation (20.2)(orintegration, inthecontinuous case)canbe
carriedouttractably, butinmostcaseswemustresorttoapproximate orsimplifiedmethods.
Averycommonapproximation—onethatisusuallyadoptedinscience—istomakepre-
dictionsbasedonasinglemostprobablehypothesis—that is,anh thatmaximizesP(h |d).
i i
MAXIMUMA ThisisoftencalledamaximumaposterioriorMAP(pronounced“em-ay-pee”)hypothesis.
POSTERIORI
Predictions madeaccording toanMAPhypothesis h areapproximately Bayesian tothe
MAP
extentthatP(X|d)≈ P(X|h ). Inourcandyexample,h =h afterthreelimecan-
MAP MAP 5
diesinarow,sotheMAPlearnerthenpredicts thatthefourth candyislimewithprobability
1.0—a much more dangerous prediction than the Bayesian prediction of 0.8 shown in Fig-
ure20.1(b). Asmoredataarrive,theMAPandBayesian predictions becomecloser, because
thecompetitors totheMAPhypothesis becomelessandlessprobable.
Although our example doesn’t show it, finding MAP hypotheses is often much easier
thanBayesianlearning,becauseitrequiressolvinganoptimizationprobleminsteadofalarge
summation(orintegration) problem. Wewillseeexamplesof thislaterinthechapter.
Section20.1. StatisticalLearning 805
In both Bayesian learning and MAP learning, the hypothesis prior P(h ) plays an im-
i
portant role. We saw in Chapter 18 that overfitting can occur when the hypothesis space
is too expressive, so that it contains many hypotheses that fit the data set well. Rather than
placing an arbitrary limit on the hypotheses to be considered, Bayesian and MAP learning
methods use the prior to penalize complexity. Typically, more complex hypotheses have a
lower prior probability—in part because there are usually many more complex hypotheses
than simple hypotheses. Onthe otherhand, more complex hypotheses have agreater capac-
ity to fit the data. (In the extreme case, a lookup table can reproduce the data exactly with
probability 1.) Hence, the hypothesis priorembodies atradeoff between thecomplexity ofa
hypothesis anditsdegreeoffittothedata.
Wecanseetheeffectofthistradeoffmostclearlyinthelogicalcase,whereH contains
onlydeterministic hypotheses. Inthatcase,P(d|h )is1ifh isconsistent and 0otherwise.
i i
Looking at Equation (20.1), we see that h will then be the simplest logical theory that
MAP
is consistent with the data. Therefore, maximum a posteriori learning provides a natural
embodimentofOckham’srazor.
Another insight into the tradeoff between complexity and degree of fit is obtained by
taking the logarithm of Equation (20.1). Choosing h to maximize P(d|h )P(h ) is
MAP i i
equivalent tominimizing
−log P(d|h )−log P(h ).
2 i 2 i
Using the connection between information encoding and probability that we introduced in
Chapter18.3.4,weseethatthe− log P(h )termequalsthenumberofbitsrequiredtospec-
2 i
ifythehypothesish . Furthermore, − log P(d|h )istheadditionalnumberofbitsrequired
i 2 i
to specify the data, given the hypothesis. (To see this, consider that no bits are required
if the hypothesis predicts the data exactly—as with h and the string of lime candies—and
5
log 1=0.) Hence, MAP learning is choosing the hypothesis that provides maximum com-
2
pression ofthedata. Thesametaskisaddressed moredirectly bythe minimumdescription
length,orMDL,learningmethod. WhereasMAPlearningexpressessimplicitybyassigning
higherprobabilities tosimplerhypotheses, MDLexpresses itdirectly bycounting thebitsin
abinaryencoding ofthehypotheses anddata.
A final simplification is provided by assuming a uniform prior over the space of hy-
potheses. In that case, MAP learning reduces to choosing an h that maximizes P(d|h ).
i i
MAXIMUM- Thisiscalledamaximum-likelihood(ML)hypothesis, h . Maximum-likelihood learning
LIKELIHOOD ML
is very common in statistics, a discipline in which many researchers distrust the subjective
natureofhypothesis priors. Itisareasonable approach whenthereisnoreasontopreferone
hypothesis overanother apriori—for example, whenallhypotheses are equally complex. It
provides a good approximation to Bayesian and MAP learning when the data set is large,
because the data swamps the prior distribution over hypotheses, but it has problems (as we
shallsee)withsmalldatasets.
806 Chapter 20. LearningProbabilistic Models
20.2 LEARNING WITH COMPLETE DATA
Thegeneraltaskoflearningaprobability model,givendata thatareassumedtobegenerated
from that model, is called density estimation. (The term applied originally to probability
DENSITYESTIMATION
densityfunctions forcontinuous variables, butisusednow fordiscretedistributions too.)
This section covers the simplest case, where we have complete data. Data are com-
COMPLETEDATA
plete when each data point contains values forevery variable in the probability model being
PARAMETER learned. We focus on parameter learning—finding the numerical parameters for a proba-
LEARNING
bility model whose structure is fixed. For example, we might be interested in learning the
conditional probabilities in a Bayesian network with a given structure. We will also look
brieflyattheproblem oflearning structure andatnonparametric densityestimation.
20.2.1 Maximum-likelihoodparameter learning: Discrete models
Supposewebuyabagoflimeandcherrycandyfromanewmanufacturerwhoselime–cherry
proportions are completely unknown; the fraction could be anywhere between 0 and 1. In
that case, we have a continuum of hypotheses. The parameter in this case, which we call
θ, is the proportion of cherry candies, and the hypothesis is h . (The proportion of limes is
θ
just 1−θ.) If we assume that all proportions are equally likely a priori, then a maximum-
likelihood approach is reasonable. If we model the situation with a Bayesian network, we
needjustonerandomvariable, Flavor (theflavorofarandomlychosencandyfromthebag).
Ithasvaluescherry andlime,wheretheprobabilityofcherry isθ(seeFigure20.2(a)). Now
suppose weunwrap N candies, ofwhich care cherries and (cid:3)=N −care limes. According
toEquation(20.3), thelikelihood ofthisparticular datasetis
(cid:25)N
P(d|h ) = P(d |h ) = θc·(1−θ)(cid:3) .
θ j θ
j=1
The maximum-likelihood hypothesis is given by the value of θ that maximizes this expres-
sion. Thesamevalueisobtained bymaximizingtheloglikelihood,
LOGLIKELIHOOD
(cid:12)N
L(d|h ) = logP(d|h ) = logP(d |h )= clogθ+(cid:3)log(1−θ).
θ θ j θ
j=1
(Bytaking logarithms, wereduce theproduct toasum overthe data, whichisusually easier
tomaximize.) Tofindthemaximum-likelihood valueofθ,wedifferentiate Lwithrespect to
θ andsettheresulting expression tozero:
dL(d|h ) c (cid:3) c c
θ = − = 0 ⇒ θ = = .
dθ θ 1−θ c+(cid:3) N
In English, then, the maximum-likelihood hypothesis h asserts that the actual proportion
ML
ofcherries inthebagisequaltotheobserved proportion inthecandiesunwrapped sofar!
It appears that we have done a lot of work to discover the obvious. In fact, though,
wehavelaidoutonestandardmethodformaximum-likelihood parameterlearning,amethod
withbroadapplicability:
Section20.2. LearningwithCompleteData 807
P(F=cherry)
θ
P(F=cherry) Flavor
θ
F P(W=red| F)
θ
Flavor cherry 1
θ
lime 2
Wrapper
(a) (b)
Figure20.2 (a)Bayesiannetworkmodelforthecaseofcandieswithanunknownpropor-
tionofcherriesandlimes. (b)Modelforthecasewherethewrappercolordepends(proba-
bilistically)onthecandyflavor.
1. Writedownanexpressionforthelikelihoodofthedataasafunctionoftheparameter(s).
2. Writedownthederivativeoftheloglikelihood withrespecttoeachparameter.
3. Findtheparametervaluessuchthatthederivativesarezero.
The trickiest step is usually the last. In our example, it was trivial, but we will see that in
manycasesweneedtoresorttoiterativesolutionalgorithmsorothernumericaloptimization
techniques, as described in Chapter 4. The example also illustrates a significant problem
with maximum-likelihood learning in general: when the data set is small enough that some
eventshavenotyetbeenobserved—forinstance,nocherrycandies—themaximum-likelihood
hypothesis assigns zero probability to those events. Various tricks are used to avoid this
problem,suchasinitializing thecountsforeacheventto1insteadof0.
Letus look atanother example. Suppose this new candy manufacturer wants togive a
littlehinttotheconsumerandusescandywrapperscoloredredandgreen. TheWrapper for
eachcandyisselectedprobabilistically, accordingtosomeunknownconditionaldistribution,
depending on the flavor. The corresponding probability model is shown in Figure 20.2(b).
Notice that it has three parameters: θ, θ , and θ . With these parameters, the likelihood of
1 2
seeing, say, a cherry candy in a green wrapper can be obtained from the standard semantics
forBayesiannetworks(page513):
P(Flavor =cherry,Wrapper =green|h )
θ,θ1,θ2
= P(Flavor =cherry|h )P(Wrapper =green|Flavor =cherry,h )
θ,θ1,θ2 θ,θ1,θ2
= θ·(1−θ ).
1
Nowweunwrap N candies, ofwhichcarecherries and (cid:3)arelimes. Thewrappercounts are
asfollows: r ofthecherrieshaveredwrappersand g havegreen,whiler ofthelimeshave
c c (cid:3)
redandg havegreen. Thelikelihood ofthedataisgivenby
(cid:3)
P(d|h ) = θc(1−θ)(cid:3)·θrc(1−θ )gc ·θr(cid:3)(1−θ )g(cid:3) .
θ,θ1,θ2 1 1 2 2
808 Chapter 20. LearningProbabilistic Models
Thislooksprettyhorrible, buttakinglogarithmshelps:
L=[clogθ+(cid:3)log(1−θ)]+[rclogθ
1
+gclog(1−θ
1
)]+[r(cid:3)logθ
2
+g(cid:3)log(1−θ
2
)].
Thebenefitoftakinglogsisclear: theloglikelihoodisthesumofthreeterms,eachofwhich
containsasingleparameter. Whenwetakederivativeswithrespecttoeachparameterandset
themtozero,wegetthreeindependent equations, eachcontaining justoneparameter:
∂L = c − (cid:3) = 0 ⇒ θ = c
∂θ θ 1−θ c+(cid:3)
∂L = rc − gc = 0 ⇒ θ = rc
∂θ1 θ1 1−θ1 1 rc+gc
∂L = r(cid:3) − g(cid:3) = 0 ⇒ θ = r(cid:3) .
∂θ2 θ2 1−θ2 2 r(cid:3)+g(cid:3)
The solution for θ is the same as before. The solution for θ , the probability that a cherry
1
candy has a red wrapper, is the observed fraction of cherry candies with red wrappers, and
similarlyforθ .
2
Theseresultsareverycomforting, anditiseasytoseethattheycanbeextended toany
Bayesiannetworkwhoseconditionalprobabilitiesarerepresentedastables. Themostimpor-
tant point is that, with complete data, the maximum-likelihood parameter learning problem
foraBayesiannetworkdecomposesintoseparatelearningproblems,oneforeachparameter.
(SeeExercise20.6forthenontabulatedcase,whereeachparameteraffectsseveralconditional
probabilities.) Thesecond point isthat theparametervalues foravariable, given itsparents,
are just the observed frequencies of the variable values for each setting of the parent values.
Asbefore,wemustbecarefultoavoidzeroeswhenthedataset issmall.
20.2.2 NaiveBayes models
Probably the most common Bayesian network model used in machine learning is the naive
Bayesmodelfirstintroduced onpage499. Inthis model, the“class” variable C (which isto
bepredicted) istherootandthe“attribute” variables X aretheleaves. Themodelis“naive”
i
because it assumes that the attributes are conditionally independent of each other, given the
class. (The model in Figure 20.2(b) is a naive Bayes model with class Flavor and just one
attribute, Wrapper.) AssumingBooleanvariables, theparameters are
θ=P(C=true),θ =P(X =true|C=true),θ =P(X =true|C=false).
i1 i i2 i
The maximum-likelihood parameter values are found in exactly the same way as for Fig-
ure20.2(b). Oncethemodelhasbeentrainedinthisway,itcanbeusedtoclassifynewexam-
plesforwhichtheclassvariable C isunobserved. Withobservedattributevalues x ,...,x ,
1 n
theprobability ofeachclassisgivenby
(cid:25)
P(C|x ,...,x )= α P(C) P(x |C).
1 n i
i
A deterministic prediction can be obtained by choosing the most likely class. Figure 20.3
shows the learning curve for this method when it is applied to the restaurant problem from
Chapter 18. The method learns fairly well but not as well as decision-tree learning; this is
presumably because the true hypothesis—which is a decision tree—is not representable ex-
actly using anaiveBayesmodel. NaiveBayeslearning turnsout todosurprisingly wellina
wide range of applications; the boosted version (Exercise 20.4) is one of the most effective
Section20.2. LearningwithCompleteData 809
1
0.9
0.8
0.7
0.6
0.5
0.4
0 20 40 60 80 100
tes
tset
no
tcerroc
noitroporP
Decision tree
Naive Bayes
Training set size
Figure20.3 ThelearningcurvefornaiveBayeslearningappliedtotherestaurantproblem
fromChapter18;thelearningcurvefordecision-treelearningisshownforcomparison.
general-purpose learning algorithms. Naive Bayes learning scales well to very large prob-
lems: withn Boolean attributes, there are just 2n+1parameters, and nosearch is required
tofindh ,the maximum-likelihood naive Bayeshypothesis. Finally, naiveBayeslearning
ML
systems have no difficulty with noisy or missing data and can give probabilistic predictions
whenappropriate.
20.2.3 Maximum-likelihoodparameter learning: Continuous models
Continuous probability models such as the linear Gaussian model were introduced in Sec-
tion14.3. Becausecontinuousvariablesareubiquitousinreal-worldapplications, itisimpor-
tanttoknowhowtolearntheparametersofcontinuous models fromdata. Theprinciples for
maximum-likelihood learningareidentical inthecontinuous anddiscretecases.
Let us begin with a very simple case: learning the parameters of a Gaussian density
function onasinglevariable. Thatis,thedataaregenerated asfollows:
P(x) = √
1
e
−(x
2
−
σ
μ
2
)2
.
2πσ
The parameters of this model are the mean μ and the standard deviation σ. (Notice that the
normalizing “constant” depends on σ, so we cannot ignore it.) Let the observed values be
x ,...,x . Thentheloglikelihood is
1 N
L = (cid:12)N log √ 1 e − (xj 2 − σ2 μ)2 = N(−log √ 2π−logσ)− (cid:12)N (x j −μ)2 .
2πσ 2σ2
j=1 j=1
Settingthederivatives tozeroasusual,weobtain
(cid:2) P
∂L = − 1 N (x −μ) = 0 ⇒ μ = j xj
∂μ σ2 j=1
(cid:2)
j (cid:9)NP
(20.4)
∂L = −N + 1 N (x −μ)2 = 0 ⇒ σ = j (xj −μ)2 .
∂σ σ σ3 j=1 j N
Thatis,themaximum-likelihood valueofthemeanisthesampleaverageandthemaximum-
likelihood value of the standard deviation is the square root of the sample variance. Again,
thesearecomforting resultsthatconfirm“commonsense” practice.
810 Chapter 20. LearningProbabilistic Models
1
0.8
P(y |x)
4 0.6
3.5
3
2.5
0.4
2
1.5
1
0.5 0.8 1 0.2
0 0.6
0 0.2 0 x .4 0.6 0.8 1 0 0.2 0.4 y 0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
y
x
(a) (b)
Figure20.4 (a)AlinearGaussianmodeldescribedas y=θ x+θ plusGaussiannoise
1 2
withfixedvariance.(b)Asetof50datapointsgeneratedfromthismodel.
NowconsideralinearGaussianmodelwithonecontinuous parentX andacontinuous
child Y. As explained on page 520, Y has a Gaussian distribution whose mean depends
linearly on the value of X and whose standard deviation is fixed. To learn the conditional
distribution P(Y |X),wecanmaximizetheconditional likelihood
P(y|x) = √
1
e
−(y−(θ1
2σ
x
2
+θ2))2
. (20.5)
2πσ
Here,theparametersareθ ,θ ,andσ. Thedataareacollectionof(x ,y )pairs,asillustrated
1 2 j j
inFigure20.4. Usingtheusualmethods(Exercise20.5),wecanfindthemaximum-likelihood
values of the parameters. The point here is different. If we consider just the parameters θ
1
and θ that define thelinear relationship between x and y, it becomes clearthat maximizing
2
the log likelihood with respect to these parameters is the same as minimizing the numerator
(y − (θ x + θ ))2 in the exponent of Equation (20.5). This is the L loss, the squared er-
1 2 2
ror between the actual value y and the prediction θ x+θ . This is the quantity minimized
1 2
by the standard linear regression procedure described in Section 18.6. Now we can under-
standwhy: minimizingthesumofsquarederrorsgivesthemaximum-likelihood straight-line
model,provided thatthedataaregenerated withGaussiannoiseoffixedvariance.
20.2.4 Bayesianparameter learning
Maximum-likelihood learning gives rise to some very simple procedures, but it has some
serious deficiencies with small data sets. For example, after seeing one cherry candy, the
maximum-likelihood hypothesis is that the bag is 100% cherry (i.e., θ=1.0). Unless one’s
hypothesis prior is that bags must be either all cherry or all lime, this is not a reasonable
conclusion. It is more likely that the bag is a mixture of lime and cherry. The Bayesian
approach to parameter learning starts by defining a prior probability distribution over the
possible hypotheses. We call this the hypothesis prior. Then, as data arrives, the posterior
HYPOTHESISPRIOR
probability distribution isupdated.
Section20.2. LearningwithCompleteData 811
2.5
2
1.5
1
0.5
0
0 0.2 0.4 0.6 0.8 1
)θ
=
Θ(P
6
[5,5]
5
[2,2] 4
3
[1,1]
2
1
0
0 0.2 0.4 0.6 0.8 1
Parameterθ
)θ
=
Θ(P
[30,10]
[6,2]
[3,1]
Parameterθ
(a) (b)
Figure20.5 Examplesofthebeta[a,b]distributionfordifferentvaluesof[a,b].
The candy example in Figure 20.2(a) has one parameter, θ: the probability that a ran-
domly selected piece of candy is cherry-flavored. In the Bayesian view, θ is the (unknown)
value of a random variable Θ that defines the hypothesis space; the hypothesis prior is just
thepriordistribution P(Θ). Thus,P(Θ=θ)isthepriorprobabilitythatthebaghasafraction
θ ofcherrycandies.
If the parameter θ can be any value between 0 and 1, then P(Θ) must be a continuous
distributionthatisnonzeroonlybetween0and1andthatintegratesto1. Theuniformdensity
P(θ) = Uniform[0,1](θ) is one candidate. (See Chapter 13.) It turns out that the uniform
density isamemberofthefamilyofbetadistributions. Eachbetadistribution isdefinedby
BETADISTRIBUTION
twohyperparameters3 aandbsuchthat
HYPERPARAMETER
beta[a,b](θ) =
αθa−1(1−θ)b−1
, (20.6)
forθintherange[0,1]. Thenormalizationconstantα,whichmakesthedistributionintegrate
to1,depends onaandb. (SeeExercise 20.7.) Figure20.5showswhatthedistribution looks
like forvarious values of aand b. The meanvalue of the distribution is a/(a+b), so larger
values of a suggest a belief that Θ is closer to 1 than to 0. Larger values of a+b make the
distribution more peaked, suggesting greater certainty about the value of Θ. Thus, the beta
familyprovides ausefulrangeofpossibilities forthehypothesis prior.
Besides its flexibility, the beta family has another wonderful property: if Θ has aprior
beta[a,b], then, after a data point is observed, the posterior distribution for Θ is also a beta
distribution. In other words, beta is closed under update. The beta family is called the
conjugate prior for the family of distributions for a Boolean variable.4 Let’s see how this
CONJUGATEPRIOR
works. Supposeweobserveacherrycandy;thenwehave
3 Theyarecalledhyperparametersbecausetheyparameterizeadistributionoverθ,whichisitselfaparameter.
4 Otherconjugatepriorsincludethe Dirichletfamilyfortheparametersofadiscretemultivalueddistribution
andtheNormal–WishartfamilyfortheparametersofaGaussiandistribution.SeeBernardoandSmith(1994).
812 Chapter 20. LearningProbabilistic Models
Θ
Flavor Flavor Flavor
1 2 3
Wrapper Wrapper Wrapper
1 2 3
Θ Θ
1 2
Figure20.6 ABayesiannetworkthatcorrespondstoaBayesianlearningprocess. Poste-
riordistributionsfortheparametervariables Θ,Θ ,andΘ canbeinferredfromtheirprior
1 2
distributionsandtheevidenceintheFlavoriandWrapperi variables.
P(θ|D =cherry) = αP(D =cherry|θ)P(θ)
1 1
= α (cid:2) θ·beta[a,b](θ) = α (cid:2) θ·θa−1(1−θ)b−1
= α
(cid:2) θa(1−θ)b−1
= beta[a+1,b](θ).
Thus, after seeing acherry candy, wesimply increment the aparameter toget the posterior;
similarly, after seeing a lime candy, weincrement the b parameter. Thus, we can view the a
and bhyperparameters as virtual counts, inthesense thataprior beta[a,b] behaves exactly
VIRTUALCOUNTS
asifwehad started outwithauniform prior beta[1,1] andseen a−1actual cherry candies
andb−1actuallimecandies.
Byexaminingasequenceofbetadistributions forincreasingvaluesofaandb,keeping
the proportions fixed, we can see vividly how the posterior distribution over the parameter
Θ changes as data arrive. Forexample, suppose the actual bag ofcandy is75% cherry. Fig-
ure 20.5(b) shows the sequence beta[3,1], beta[6,2], beta[30,10]. Clearly, the distribution
isconvergingtoanarrowpeakaroundthetruevalueof Θ. Forlargedatasets,then,Bayesian
learning(atleastinthiscase)convergestothesameanswerasmaximum-likelihoodlearning.
Nowletusconsideramorecomplicated case. Thenetwork inFigure 20.2(b)hasthree
parameters, θ,θ ,andθ ,whereθ istheprobability ofaredwrapperonacherrycandy and
1 2 1
θ is the probability of a red wrapper on a lime candy. The Bayesian hypothesis prior must
2
cover all three parameters—that is, we need to specify P(Θ,Θ ,Θ ). Usually, we assume
1 2
PARAMETER parameterindependence:
INDEPENDENCE
P(Θ,Θ ,Θ ) = P(Θ)P(Θ )P(Θ ).
1 2 1 2
Section20.2. LearningwithCompleteData 813
With this assumption, each parameter can have its ownbeta distribution that is updated sep-
arately as data arrive. Figure 20.6 shows how we can incorporate the hypothesis prior and
any data into one Bayesian network. The nodes Θ,Θ ,Θ have no parents. But each time
1 2
wemakeanobservation ofawrapperandcorresponding flavorofapieceofcandy, weadda
nodeFlavor ,whichisdependent ontheflavorparameter Θ:
i
P(Flavor =cherry|Θ=θ)= θ .
i
WealsoaddanodeWrapper ,whichisdependent onΘ andΘ :
i 1 2
P(Wrapper =red |Flavor =cherry,Θ =θ ) = θ
i i 1 1 1
P(Wrapper =red |Flavor =lime,Θ =θ )= θ .
i i 2 2 2
Now, the entire Bayesian learning process can be formulated as an inference problem. We
add new evidence nodes, then query the unknown nodes (in this case, Θ,Θ ,Θ ). This for-
1 2
mulation of learning and prediction makes it clear that Bayesian learning requires no extra
“principles oflearning.” Furthermore, thereis, inessence, justonelearning algorithm —the
inference algorithm forBayesian networks. Ofcourse, thenatureofthesenetworks issome-
whatdifferent from those of Chapter14because ofthe potentially huge numberofevidence
variables representing the training set and the prevalence of continuous-valued parameter
variables.
20.2.5 Learning Bayes net structures
Sofar, wehaveassumed thatthestructure oftheBayesnetisgivenandwearejusttrying to
learn the parameters. The structure of the network represents basic causal knowledge about
the domain that is often easy for an expert, or even a naive user, to supply. In some cases,
however, the causal model may be unavailable or subject to dispute—for example, certain
corporations have long claimed that smoking does not cause cancer—so it is important to
understand how the structure of a Bayes net can be learned from data. This section gives a
briefsketchofthemainideas.
The most obvious approach is to search for a good model. We can start with a model
containing no links and begin adding parents for each node, fitting the parameters with the
methods we have just covered and measuring the accuracy of the resulting model. Alterna-
tively, we can start with an initial guess at the structure and use hill-climbing or simulated
annealing search to make modifications, retuning the parameters after each change in the
structure. Modifications can include reversing, adding, or deleting links. We must not in-
troduce cycles in the process, so many algorithms assume that an ordering is given for the
variables, and that a node can have parents only among those nodes that come earlier in the
ordering (just asintheconstruction process inChapter14). Forfullgenerality, wealsoneed
tosearchoverpossible orderings.
There are two alternative methods fordeciding when a good structure has been found.
Thefirstistotestwhethertheconditionalindependenceassertionsimplicitinthestructureare
actually satisfied inthe data. Forexample, the useof anaive Bayesmodel fortherestaurant
problem assumesthat
P(Fri/Sat,Bar |WillWait) = P(Fri/Sat|WillWait)P(Bar |WillWait)
814 Chapter 20. LearningProbabilistic Models
andwecancheckinthedatathatthesameequation holdsbetweenthecorresponding condi-
tional frequencies. But even if the structure describes the true causal nature of the domain,
statistical fluctuations in the data set mean that the equation will never be satisfied exactly,
so we need to perform a suitable statistical test to see if there is sufficient evidence that the
independence hypothesis is violated. The complexity of the resulting network will depend
on the threshold used forthis test—the stricter the independence test, the more links will be
addedandthegreaterthedangerofoverfitting.
An approach more consistent with the ideas in this chapter is to assess the degree to
which the proposed model explains the data (in a probabilistic sense). We must be careful
how we measure this, however. If we just try to find the maximum-likelihood hypothesis,
wewill end up with afully connected network, because adding more parents to anode can-
not decrease the likelihood (Exercise 20.8). We are forced to penalize model complexity in
some way. The MAP (or MDL) approach simply subtracts a penalty from the likelihood of
each structure (after parameter tuning) before comparing different structures. The Bayesian
approach places ajoint prioroverstructures and parameters. There are usually fartoo many
structures to sum over (superexponential in the number of variables), so most practitioners
useMCMCtosampleoverstructures.
Penalizingcomplexity(whetherbyMAPorBayesianmethods)introducesanimportant
connection between the optimal structure and the nature of the representation for the condi-
tional distributions in the network. With tabular distributions, the complexity penalty for a
node’s distribution growsexponentially with thenumber of parents, but with, say, noisy-OR
distributions, itgrows only linearly. This means that learning with noisy-OR (orother com-
pactlyparameterized)modelstendstoproducelearnedstructureswithmoreparentsthandoes
learning withtabulardistributions.
20.2.6 Density estimationwithnonparametric models
Itispossibletolearnaprobabilitymodelwithoutmakinganyassumptionsaboutitsstructure
and parameterization by adopting the nonparametric methods of Section 18.8. The task of
NONPARAMETRIC nonparametric density estimation is typically done in continuous domains, such as that
DENSITYESTIMATION
shown inFigure 20.7(a). Thefigure shows aprobability density function on aspace defined
by two continuous variables. In Figure 20.7(b) we see a sample of data points from this
densityfunction. Thequestion is,canwerecoverthemodelfromthesamples?
First we will consider k-nearest-neighbors models. (In Chapter 18 we saw nearest-
neighbor models for classification and regression; here we see them for density estimation.)
Givenasampleofdatapoints,toestimatetheunknownprobability densityataquerypointx
wecansimplymeasurethedensityofthedatapointsintheneighborhoodofx. Figure20.7(b)
shows two query points (small squares). For each query point we have drawn the smallest
circle that encloses 10neighbors—the 10-nearest-neighborhood. Wecansee thatthe central
circle is large, meaning there is a low density there, and the circle on the right is small,
meaningthereisahighdensitythere. InFigure20.8weshowthreeplotsofdensityestimation
using k-nearest-neighbors, for different values of k. It seems clear that (b) is about right,
while(a)istoospiky(k istoosmall)and(c)istoosmooth(k istoobig).
Section20.2. LearningwithCompleteData 815
Density
18
16 1
14
0.9
12
10 0.8
8
6 0.7
4
1 0.6
2 0.8
0 0 0.6 0.5
0.2 0.4
0.4 0.6 0.2 0.4
0.8 0
1
0.3
0 0.2 0.4 0.6 0.8 1
(a) (b)
Figure20.7 (a)A3DplotofthemixtureofGaussiansfromFigure20.11(a). (b)A128-
pointsampleofpointsfromthemixture,togetherwithtwoquerypoints(smallsquares)and
their10-nearest-neighborhoods(mediumandlargecircles).
Density Density Density
1 1 1
0.8 0.8 0.8
0.6 0.6 0.6
0 0.20.40.60.8
0
0.2 0.4 0 0.20.40.60.8
0
0.2 0.4 0 0.20.40.60.8
0
0.2 0.4
(a) (b) (c)
Figure 20.8 Density estimation using k-nearest-neighbors, applied to the data in Fig-
ure 20.7(b), for k=3, 10, and 40 respectively. k = 3 is too spiky, 40 is too smooth, and
10isjustaboutright.Thebestvalueforkcanbechosenbycross-validation.
Density Density Density
1 1 1
0.8 0.8 0.8
0.6 0.6 0.6
0 0.20.40.60.8
0
0.2 0.4 0 0.20.40.60.8
0
0.2 0.4 0 0.20.40.60.8
0
0.2 0.4
(a) (b) (c)
Figure20.9 KerneldensityestimationforthedatainFigure20.7(b),usingGaussianker-
nelswithw=0.02,0.07,and0.20respectively.w=0.07isaboutright.
816 Chapter 20. LearningProbabilistic Models
Another possibility is to use kernel functions, as we did for locally weighted regres-
sion. Toapplyakernelmodeltodensityestimation,assumethateachdatapointgeneratesits
ownlittledensityfunction, usingaGaussiankernel. Theestimateddensityataquerypointx
isthentheaverage densityasgivenbyeachkernelfunction:
(cid:12)N
1
P(x) = K(x,x ).
j
N
j=1
Wewillassumespherical Gaussianswithstandard deviation w alongeachaxis:
K(x,x j ) = √ 1 e − D( 2 x w ,x 2 j)2 ,
(w2 2π)d
where d is the number of dimensions in x and D is the Euclidean distance function. We
still have the problem of choosing a suitable value for kernel width w; Figure 20.9 shows
valuesthataretoosmall,justright, andtoolarge. Agoodvalueofwcanbechosenbyusing
cross-validation.
20.3 LEARNING WITH HIDDEN VARIABLES: THE EM ALGORITHM
The preceding section dealt with the fully observable case. Many real-world problems have
hiddenvariables (sometimes called latent variables), which are not observable in the data
LATENTVARIABLE
that are available for learning. For example, medical records often include the observed
symptoms, the physician’s diagnosis, the treatment applied, and perhaps the outcome of the
treatment, but they seldom contain a direct observation of the disease itself! (Note that the
diagnosisisnotthedisease;itisacausalconsequenceoftheobservedsymptoms,whicharein
turncausedbythedisease.) Onemightask,“Ifthediseaseisnotobserved,whynotconstruct
a model without it?” The answer appears in Figure 20.10, which shows a small, fictitious
diagnostic modelforheartdisease. Therearethreeobservablepredisposing factorsandthree
observable symptoms (which are too depressing to name). Assume that each variable has
three possible values (e.g., none, moderate, and severe). Removing the hidden variable
from the network in (a) yields the network in (b); the total number of parameters increases
from 78 to 708. Thus, latent variables can dramatically reduce the number of parameters
required tospecify aBayesiannetwork. This,inturn,candramatically reduce theamountof
dataneededtolearntheparameters.
Hidden variables are important, but they do complicate the learning problem. In Fig-
ure 20.10(a), for example, it is not obvious how to learn the conditional distribution for
HeartDisease,givenitsparents, becausewedonotknowthevalueofHeartDisease ineach
case; the same problem arises in learning the distributions for the symptoms. This section
EXPECTATION– describes an algorithm called expectation–maximization, or EM, that solves this problem
MAXIMIZATION
in avery general way. Wewill show three examples and then provide a general description.
The algorithm seems like magic at first, but once the intuition has been developed, one can
findapplications forEMinahugerangeoflearning problems.
Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 817
2 2 2 2 2 2
Smoking Diet Exercise Smoking Diet Exercise
54 HeartDisease
6 6 6 54 162 486
Symptom Symptom Symptom Symptom Symptom Symptom
1 2 3 1 2 3
(a) (b)
Figure20.10 (a)Asimple diagnosticnetworkforheartdisease, whichisassumedtobe
a hidden variable. Each variable has three possible values and is labeled with the number
of independent parameters in its conditional distribution; the total number is 78. (b) The
equivalent network with HeartDisease removed. Note that the symptom variables are no
longerconditionallyindependentgiventheirparents.Thisnetworkrequires708parameters.
20.3.1 Unsupervised clustering: Learning mixtures ofGaussians
UNSUPERVISED Unsupervised clustering is the problem of discerning multiple categories in a collection of
CLUSTERING
objects. Theproblemisunsupervisedbecausethecategorylabelsarenotgiven. Forexample,
suppose we record the spectra of a hundred thousand stars; are there different types of stars
revealed by the spectra, and, if so, how many types and what are their characteristics? We
are all familiar with terms such as “red giant” and “white dwarf,” but the stars do not carry
these labels on their hats—astronomers had to perform unsupervised clustering to identify
these categories. Other examples include the identification of species, genera, orders, and
so on in the Linnæan taxonomy and the creation of natural kinds for ordinary objects (see
Chapter12).
Unsupervised clustering beginswithdata. Figure20.11(b) shows500datapoints,each
ofwhichspecifies thevalues oftwocontinuous attributes. Thedatapoints mightcorrespond
to stars, and the attributes might correspond to spectral intensities at two particular frequen-
cies. Next,weneedtounderstandwhatkindofprobability distribution mighthavegenerated
MIXTURE the data. Clustering presumes that the data are generated from a mixture distribution, P.
DISTRIBUTION
Such a distribution has k components, each of which is a distribution in its own right. A
COMPONENT
datapointisgeneratedbyfirstchoosingacomponentandthengenerating asamplefromthat
component. Lettherandom variable C denotethecomponent, withvalues1,...,k;thenthe
mixturedistribution isgivenby
(cid:12)k
P(x) = P(C=i) P(x|C=i),
i=1
where x refers to the values of the attributes for a data point. Forcontinuous data, a natural
choiceforthecomponentdistributionsisthemultivariate Gaussian,whichgivestheso-called
MIXTUREOF mixtureofGaussiansfamilyofdistributions. Theparameters ofamixtureofGaussians are
GAUSSIANS
818 Chapter 20. LearningProbabilistic Models
1 1 1
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0 0 0
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
(a) (b) (c)
Figure20.11 (a)AGaussianmixturemodelwiththreecomponents;theweights(left-to-
right)are0.2,0.3,and0.5.(b)500datapointssampledfromthemodelin(a).(c)Themodel
reconstructedbyEMfromthedatain(b).
w =P(C=i) (the weight of each component), μ (the mean of each component), and Σ
i i i
(the covariance of each component). Figure 20.11(a) shows a mixture of three Gaussians;
this mixture is in fact the source of the data in (b) as well as being the model shown in
Figure20.7(a)onpage815.
The unsupervised clustering problem, then, is to recover a mixture model like the one
inFigure20.11(a)fromrawdatalikethatinFigure20.11(b). Clearly,ifweknewwhichcom-
ponentgenerated eachdatapoint,thenitwouldbeeasytorecoverthecomponentGaussians:
wecouldjustselectallthedatapointsfromagivencomponentandthenapply(amultivariate
versionof)Equation(20.4)(page809)forfittingtheparametersofaGaussiantoasetofdata.
On the other hand, if we knew the parameters of each component, then we could, at least in
a probabilistic sense, assign each data point to a component. The problem is that we know
neithertheassignments northeparameters.
The basic idea of EM in this context is to pretend that we know the parameters of the
modelandthentoinfertheprobabilitythateachdatapointbelongstoeachcomponent. After
that,werefitthecomponentstothedata,whereeachcomponentisfittedtotheentiredataset
with each point weighted by the probability that it belongs to that component. The process
iterates until convergence. Essentially, weare“completing” thedata byinferring probability
distributionsoverthehiddenvariables—whichcomponenteachdatapointbelongsto—based
onthecurrentmodel. ForthemixtureofGaussians, weinitialize themixture-model parame-
tersarbitrarily andtheniteratethefollowingtwosteps:
1. E-step: Compute the probabilities p =P(C=i|x ), the probability that datum x
ij j j
wasgeneratedbycomponenti. ByBayes’rule,wehavep =αP(x |C=i)P(C=i).
ij j
The term P(x |C=i) is just the probability at x of the ith Gaussian, and the term
j j (cid:2)
P(C=i) is just the weight parameter for the ith Gaussian. Define n = p , the
i j ij
effectivenumberofdatapointscurrently assigned tocomponenti.
2. M-step: Computethenewmean,covariance, andcomponentweightsusingthefollow-
ingstepsinsequence:
Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 819
(cid:12)
μ ← p x /n
i ij j i
j
(cid:12)
Σ ← p (x −μ )(x −μ ) (cid:12) /n
i ij j i j i i
j
w ← n /N
i i
where N is the total number of data points. The E-step, or expectation step, can be viewed
ascomputingtheexpectedvaluesp ofthehiddenindicatorvariablesZ ,whereZ is1if
INDICATORVARIABLE ij ij ij
datumx wasgeneratedbytheithcomponentand0otherwise. TheM-step,ormaximization
j
step, finds the new values of the parameters that maximize the log likelihood of the data,
giventheexpectedvaluesofthehidden indicatorvariables.
ThefinalmodelthatEMlearnswhenitisappliedtothedatainFigure20.11(a)isshown
in Figure 20.11(c); it is virtually indistinguishable from the original model from which the
data were generated. Figure 20.12(a) plots the log likelihood of the data according to the
currentmodelasEMprogresses.
There are two points to notice. First, the log likelihood for the final learned model
slightly exceeds that of the original model, from which the data were generated. This might
seem surprising, but it simply reflects the fact that the data were generated randomly and
might not provide an exact reflection of the underlying model. The second point is that EM
increases theloglikelihood ofthedataateveryiteration. Thisfactcanbeprovedingeneral.
Furthermore, under certain conditions (that hold in ost cases), EM can be proven to reach
a local maximum in likelihood. (In rare cases, it could reach a saddle point or even a local
minimum.) Inthis sense, EMresembles agradient-based hill-climbing algorithm, butnotice
thatithasno“stepsize”parameter.
700
600
500
400
300
200
100
0
-100
-200
0 5 10 15 20
Ldoohilekil-goL
-1975
-1980
-1985
-1990
-1995
-2000
-2005
-2010
-2015
-2020
-2025
0 20 40 60 80 100 120
Iteration number
Ldoohilekil-goL
Iteration number
(a) (b)
Figure20.12 Graphsshowingtheloglikelihoodofthedata, L, asafunctionoftheEM
iteration.Thehorizontallineshowstheloglikelihoodaccordingtothetruemodel.(a)Graph
for the Gaussian mixture model in Figure 20.11. (b) Graph for the Bayesian network in
Figure20.13(a).
820 Chapter 20. LearningProbabilistic Models
P(Bag=1)
θ
Bag C
Bag P(F=cherry| B)
θ
1 F1
θ
2 F2
Flavor Wrapper Hole X
(a) (b)
Figure20.13 (a)Amixturemodelforcandy. Theproportionsofdifferentflavors,wrap-
pers,presenceofholesdependonthebag,whichisnotobserved. (b)Bayesiannetworkfor
aGaussianmixture. ThemeanandcovarianceoftheobservablevariablesXdependonthe
componentC.
Things do not always go as well as Figure 20.12(a) might suggest. It can happen, for
example,thatoneGaussiancomponentshrinkssothatitcoversjustasingledatapoint. Then
its variance will go to zero and its likelihood will go to infinity! Another problem is that
twocomponents can“merge,”acquiringidenticalmeansandvariancesandsharingtheirdata
points. These kinds of degenerate local maxima are serious problems, especially in high
dimensions. One solution is to place priors on the model parameters and to apply the MAP
version of EM. Another is to restart a component with new random parameters if it gets too
smallortooclosetoanothercomponent. Sensibleinitialization alsohelps.
20.3.2 Learning Bayesiannetworks withhidden variables
To learn a Bayesian network with hidden variables, we apply the same insights that worked
formixturesofGaussians. Figure20.13represents asituation inwhichtherearetwobagsof
candies that have been mixed together. Candies are described by three features: in addition
to the Flavor and the Wrapper, some candies have a Hole in the middle and some do not.
The distribution of candies in each bag is described by a naive Bayes model: the features
are independent, given the bag, but the conditional probability distribution for each feature
depends on the bag. The parameters are as follows: θ is the prior probability that a candy
comes from Bag 1; θ and θ are the probabilities that the flavor is cherry, given that the
F1 F2
candy comes from Bag 1orBag 2 respectively; θ and θ give the probabilities that the
W1 W2
wrapperisred;and θ andθ givetheprobabilities thatthecandy hasahole. Noticethat
H1 H2
the overall model is a mixture model. (In fact, we can also model the mixture of Gaussians
as a Bayesian network, as shown in Figure 20.13(b).) In the figure, the bag is a hidden
variable because, once thecandies have been mixedtogether, weno longer know which bag
each candy came from. In such a case, can we recover the descriptions of the two bags by
Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 821
observingcandiesfromthemixture? LetusworkthroughaniterationofEMforthisproblem.
First,let’slookatthedata. Wegenerated 1000samplesfrom amodelwhosetrueparameters
areasfollows:
θ=0.5, θ =θ =θ =0.8, θ =θ =θ =0.3. (20.7)
F1 W1 H1 F2 W2 H2
That is, the candies are equally likely to come from either bag; the first is mostly cherries
with red wrappers and holes; the second is mostly limes with green wrappers and no holes.
Thecountsfortheeightpossible kindsofcandyareasfollows:
W =red W =green
H=1 H=0 H=1 H=0
F =cherry 273 93 104 90
F =lime 79 100 94 167
Westartbyinitializing theparameters. Fornumerical simplicity, wearbitrarily choose5
θ(0)=0.6, θ (0) =θ (0) =θ (0) =0.6, θ (0) =θ (0) =θ (0) =0.4. (20.8)
F1 W1 H1 F2 W2 H2
First, let us work on the θ parameter. In the fully observable case, we would estimate this
directly fromthe observed counts ofcandies frombags1and2. Because thebagisahidden
variable, we calculate the expected counts instead. The expected count Nˆ(Bag=1) is the
sum,overallcandies, oftheprobability thatthecandycame frombag1:
(cid:12)N
θ(1) = Nˆ(Bag=1)/N = P(Bag=1|flavor ,wrapper ,holes )/N .
j j j
j=1
Theseprobabilities canbecomputed byanyinference algorithm forBayesian networks. For
a naive Bayes model such as the one in our example, we can do the inference “by hand,”
usingBayes’ruleandapplying conditional independence:
(cid:12)N
θ(1) =
1
(cid:2)
P(flavorj |Bag=1)P(wrapperj |Bag=1)P(holesj |Bag=1)P(Bag=1)
.
N
j=1 i
P(flavorj |Bag=i)P(wrapperj |Bag=i)P(holesj |Bag=i)P(Bag=i)
Applying thisformula to, say, the273 red-wrapped cherry candies withholes, weget acon-
tribution of
273 θ (0) θ (0) θ (0) θ(0)
· F1 W1 H1 ≈ 0.22797.
1000 θ (0) θ (0) θ (0) θ(0)+θ (0) θ (0) θ (0) (1−θ(0))
F1 W1 H1 F2 W2 H2
Continuingwiththeothersevenkindsofcandyinthetableofcounts,weobtainθ(1)=0.6124.
Nowletusconsidertheotherparameters, suchas θ . Inthefullyobservable case,we
F1
wouldestimatethisdirectlyfromtheobservedcountsofcherryandlimecandiesfrombag1.
Theexpected countofcherrycandiesfrombag1isgivenby
(cid:12)
P(Bag=1|Flavor =cherry,wrapper ,holes ).
j j j
j:Flavorj=cherry
5 Itisbetterinpracticetochoosethemrandomly,toavoidlocalmaximaduetosymmetry.
822 Chapter 20. LearningProbabilistic Models
Again, these probabilities can be calculated by any Bayes net algorithm. Completing this
process, weobtainthenewvaluesofalltheparameters:
θ(1)=0.6124, θ (1) =0.6684, θ (1) =0.6483, θ (1) =0.6558,
F1 W1 H1 (20.9)
(1) (1) (1)
θ =0.3887,θ =0.3817, θ =0.3827.
F2 W2 H2
The log likelihood of the data increases from about −2044 initially to about −2021 after
the first iteration, as shown in Figure 20.12(b). That is, the update improves the likelihood
itself by a factor of about e23 ≈ 1010. By the tenth iteration, the learned model is a better
fitthantheoriginal model(L= −1982.214). Thereafter, progress becomes veryslow. This
is not uncommon with EM, and many practical systems combine EM with a gradient-based
algorithm suchasNewton–Raphson (seeChapter4)forthelastphaseoflearning.
The general lesson from this example is that the parameter updates for Bayesian net-
work learning with hidden variables are directly available from the results of inference on
each example. Moreover, only local posterior probabilities are needed for each parame-
ter. Here, “local” means that the CPT for each variable X can be learned from posterior
i
probabilities involving just X and its parents U . Defining θ to be the CPT parameter
i i ijk
P(X =x |U =u ),theupdateisgivenbythenormalizedexpected countsasfollows:
i ij i ik
θ ← Nˆ(X =x ,U =u )/Nˆ(U =u ).
ijk i ij i ik i ik
Theexpectedcountsareobtainedbysummingovertheexamples,computingtheprobabilities
P(X =x ,U =u ) for each by using any Bayes net inference algorithm. For the exact
i ij i ik
algorithms—including variableelimination—alltheseprobabilities areobtainabledirectlyas
aby-product ofstandard inference, withnoneed forextracomputations specific tolearning.
Moreover, theinformation needed forlearning isavailable locallyforeachparameter.
20.3.3 Learning hidden Markovmodels
Our final application of EM involves learning the transition probabilities in hidden Markov
models (HMMs). Recall from Section 15.3 that a hidden Markov model can be represented
by a dynamic Bayes net with a single discrete state variable, as illustrated in Figure 20.14.
Each data point consists of an observation sequence of finite length, so the problem is to
learn the transition probabilities from a set of observation sequences (or from just one long
sequence).
We have already worked out how to learn Bayes nets, but there is one complication:
in Bayes nets, each parameter is distinct; in a hidden Markov model, on the other hand, the
individualtransitionprobabilitiesfromstateitostatejattimet,θ =P(X =j|X =i),
ijt t+1 t
are repeated across time—that is, θ =θ for all t. To estimate the transition probability
ijt ij
from state i to state j, we simply calculate the expected proportion of times that the system
undergoes atransition tostate j wheninstatei:
(cid:12) (cid:12)
θ ← Nˆ(X =j,X =i)/ Nˆ(X =i).
ij t+1 t t
t t
TheexpectedcountsarecomputedbyanHMMinferencealgorithm. Theforward–backward
algorithm shown in Figure 15.4 can be modified very easily to compute the necessary prob-
abilities. One important point is that the probabilities required are obtained by smoothing
Section20.3. LearningwithHiddenVariables: TheEMAlgorithm 823
P(R 0) R t 0 P 0 (R .7 1 ) P(R 0) R t 0 P 0 (R .7 1) R t 1 P 0 (R .7 2) R t 2 P 0 (R .7 3 ) R t 3 P 0 (R .7 4 )
0.7 f 0.3 0.7 f 0.3 f 0.3 f 0.3 f 0.3
Rain Rain Rain Rain Rain Rain Rain
0 1 0 1 2 3 4
Umbrella 1 Umbrella 1 Umbrella 2 Umbrella 3 Umbrella 4
R 1 P(U 1 ) R 1 P(U 1 ) R 2 P(U 2 ) R 3 P(U 3 ) R 4 P(U 4 )
t 0.9 t 0.9 t 0.9 t 0.9 t 0.9
f 0.2 f 0.2 f 0.2 f 0.2 f 0.2
Figure 20.14 An unrolled dynamic Bayesian network that represents a hidden Markov
model(repeatofFigure15.16).
rather than filtering; that is, we need to pay attention to subsequent evidence in estimating
theprobability thataparticulartransition occurred. The evidence inamurdercaseisusually
obtained afterthecrime(i.e.,thetransition fromstate itostatej)hastakenplace.
20.3.4 The general form oftheEM algorithm
We have seen several instances of the EM algorithm. Each involves computing expected
values ofhidden variables foreach example and then recomputing the parameters, using the
expected values as if they were observed values. Let x be all the observed values in all the
examples, let Z denote all the hidden variables for all the examples, and let θ be all the
parameters fortheprobability model. ThentheEMalgorithm is
(cid:12)
θ(i+1) = argmax P(Z=z|x,θ(i))L(x,Z=z|θ).
θ
z
ThisequationistheEMalgorithminanutshell. TheE-stepisthecomputationofthesumma-
tion,whichistheexpectationoftheloglikelihoodofthe“completed”datawithrespecttothe
distributionP(Z=z|x,θ(i)),whichistheposterioroverthehiddenvariables,giventhedata.
The M-step is the maximization of this expected log likelihood with respect to the parame-
ters. FormixturesofGaussians,thehiddenvariablesaretheZ s,whereZ is1ifexamplej
ij ij
wasgeneratedbycomponent i. ForBayesnets,Z isthevalueofunobserved variable X in
ij i
examplej. ForHMMs,Z isthestateofthesequence inexamplej attimet. Startingfrom
jt
the general form, itis possible toderive anEM algorithm for aspecific application once the
appropriate hiddenvariables havebeenidentified.
As soon as we understand the general idea of EM, it becomes easy to derive all sorts
of variants and improvements. For example, in many cases the E-step—the computation of
posteriors over the hidden variables—is intractable, as in large Bayes nets. It turns out that
one can use an approximate E-step and still obtain an effective learning algorithm. With a
samplingalgorithm suchasMCMC(seeSection14.5),thelearning processisveryintuitive:
each state (configuration of hidden and observed variables) visited by MCMC is treated ex-
actlyasifitwereacompleteobservation. Thus,theparameters canbeupdated directly after
eachMCMCtransition. Otherformsofapproximate inference, suchasvariational andloopy
methods, havealsoprovedeffectiveforlearning verylarge networks.
824 Chapter 20. LearningProbabilistic Models
20.3.5 Learning Bayes net structures withhidden variables
In Section 20.2.5, we discussed the problem of learning Bayes net structures with complete
data. When unobserved variables may be influencing the data that are observed, things get
moredifficult. Inthesimplestcase,ahumanexpertmighttellthelearningalgorithmthatcer-
tainhidden variables exist, leaving ittothealgorithm tofindaplacefortheminthenetwork
structure. Forexample,analgorithmmighttrytolearnthestructureshowninFigure20.10(a)
onpage817,giventheinformationthatHeartDisease (athree-valuedvariable)shouldbein-
cludedinthemodel. Asinthecomplete-datacase,theoverallalgorithmhasanouterloopthat
searchesoverstructuresandaninnerloopthatfitsthenetworkparametersgiventhestructure.
If the learning algorithm is not told which hidden variables exist, then there are two
choices: either pretend that the data is really complete—which may force the algorithm to
learn aparameter-intensive model such astheoneinFigure 20.10(b)—or invent newhidden
variablesinordertosimplifythemodel. Thelatterapproachcanbeimplementedbyincluding
newmodificationchoicesinthestructuresearch: inadditiontomodifyinglinks,thealgorithm
canaddordeleteahiddenvariableorchangeitsarity. Ofcourse,thealgorithmwillnotknow
that the new variable it has invented is called HeartDisease; nor will it have meaningful
namesforthevalues. Fortunately, newlyinventedhiddenvariableswillusuallybeconnected
topreexistingvariables,soahumanexpertcanofteninspectthelocalconditionaldistributions
involving thenewvariableandascertain itsmeaning.
Asinthecomplete-datacase,puremaximum-likelihoodstructurelearningwillresultin
acompletely connected network (moreover, one withno hidden variables), so some form of
complexity penaltyisrequired. WecanalsoapplyMCMCtosamplemanypossible network
structures, thereby approximating Bayesian learning. For example, wecanlearnmixtures of
Gaussianswithanunknownnumberofcomponentsbysamplingoverthenumber;theapprox-
imateposteriordistributionforthenumberofGaussiansis givenbythesamplingfrequencies
oftheMCMCprocess.
For the complete-data case, the inner loop to learn the parameters is very fast—just a
matter of extracting conditional frequencies from the data set. When there are hidden vari-
ables, the inner loop may involve many iterations of EM ora gradient-based algorithm, and
eachiterationinvolvesthecalculationofposteriorsinaBayesnet,whichisitselfanNP-hard
problem. To date, this approach has proved impractical for learning complex models. One
possibleimprovementistheso-called structuralEMalgorithm, whichoperatesinmuchthe
STRUCTURALEM
same way as ordinary (parametric) EM except that the algorithm can update the structure
as well as the parameters. Just as ordinary EM uses the current parameters to compute the
expected counts in the E-step and then applies those counts in the M-step to choose new
parameters,structuralEMusesthecurrentstructuretocomputeexpectedcountsandthenap-
pliesthose counts intheM-steptoevaluate thelikelihood forpotential newstructures. (This
contrasts with the outer-loop/inner-loop method, which computes new expected counts for
each potential structure.) In this way, structural EM may make several structural alterations
tothenetworkwithoutoncerecomputingtheexpectedcounts,andiscapableoflearningnon-
trivial Bayes net structures. Nonetheless, much work remains to be done before we can say
thatthestructure-learning problem issolved.
Section20.4. Summary 825
20.4 SUMMARY
Statistical learning methods rangefromsimple calculation ofaverages totheconstruction of
complex models such as Bayesian networks. They have applications throughout computer
science, engineering, computational biology, neuroscience, psychology, and physics. This
chapter has presented some of the basic ideas and given a flavor of the mathematical under-
pinnings. Themainpointsareasfollows:
• Bayesian learning methods formulate learning as a form of probabilistic inference,
using the observations to update a prior distribution over hypotheses. This approach
providesagoodwaytoimplementOckham’srazor,butquickly becomesintractablefor
complexhypothesis spaces.
• Maximum a posteriori (MAP) learning selects a single most likely hypothesis given
the data. Thehypothesis prior isstill used and the method is often moretractable than
fullBayesianlearning.
• Maximum-likelihoodlearningsimplyselectsthehypothesisthatmaximizesthelikeli-
hoodofthedata;itisequivalent toMAPlearningwithauniformprior. Insimplecases
suchaslinearregressionandfullyobservableBayesiannetworks,maximum-likelihood
solutions can be found easily in closed form. Naive Bayes learning is a particularly
effectivetechnique thatscaleswell.
• When some variables are hidden, local maximum likelihood solutions can be found
using the EM algorithm. Applications include clustering using mixtures of Gaussians,
learningBayesiannetworks, andlearning hiddenMarkovmodels.
• Learning the structure of Bayesian networks is an example of model selection. This
usually involves a discrete search in the space of structures. Some method is required
fortrading offmodelcomplexityagainstdegreeoffit.
• Nonparametric models represent a distribution using the collection of data points.
Thus,thenumberofparametersgrowswiththetrainingset. Nearest-neighborsmethods
look at the examples nearest to the point in question, whereas kernel methods form a
distance-weighted combination ofalltheexamples.
Statisticallearningcontinuestobeaveryactiveareaofresearch. Enormousstrideshavebeen
madeinboth theory andpractice, tothepoint whereitispossible tolearn almost anymodel
forwhichexactorapproximate inference isfeasible.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The application of statistical learning techniques in AI was an active area of research in the
early years (see Duda and Hart, 1973) but became separated from mainstream AI as the
latterfieldconcentratedonsymbolicmethods. Aresurgence ofinterestoccurredshortlyafter
the introduction of Bayesian network models in the late 1980s; at roughly the same time,
826 Chapter 20. LearningProbabilistic Models
a statistical view of neural network learning began to emerge. In the late 1990s, there was
a noticeable convergence of interests in machine learning, statistics, and neural networks,
centered onmethodsforcreatinglargeprobabilistic modelsfromdata.
The naive Bayes model is one of the oldest and simplest forms of Bayesian network,
dating backtothe 1950s. Itsorigins werementioned inChapter13. Itssurprising success is
partially explained by Domingos and Pazzani (1997). Aboosted form ofnaive Bayes learn-
ingwonthefirstKDDCupdataminingcompetition (Elkan,1997). Heckerman(1998)gives
an excellent introduction to the general problem of Bayes net learning. Bayesian parame-
terlearningwithDirichletpriorsforBayesiannetworkswasdiscussedbySpiegelhalter etal.
(1993). TheBUGSsoftwarepackage(Gilksetal.,1994)incorporatesmanyoftheseideasand
provides averypowerfultoolforformulating andlearning complexprobability models. The
firstalgorithms forlearning Bayesnet structures usedconditional independence tests (Pearl,
1988; Pearl and Verma, 1991). Spirtes et al. (1993) developed a comprehensive approach
embodied in the TETRAD package forBayes net learning. Algorithmic improvements since
then led to a clear victory in the 2001 KDD Cup data mining competition for a Bayes net
learning method (Cheng et al., 2002). (The specific task here was a bioinformatics prob-
lem with 139,351 features!) A structure-learning approach based on maximizing likelihood
wasdeveloped by Cooperand Herskovits (1992) and improved byHeckerman et al. (1994).
Several algorithmic advances since that time have led to quite respectable performance in
the complete-data case (Moore and Wong, 2003; Teyssier and Koller, 2005). Oneimportant
component is an efficient data structure, the AD-tree, for caching counts over all possible
combinations of variables and values (Moore and Lee, 1997). Friedman and Goldszmidt
(1996)pointedouttheinfluenceoftherepresentation oflocalconditionaldistributions onthe
learnedstructure.
The general problem of learning probability models with hidden variables and miss-
ing data wasaddressed by Hartley (1958), who described the general idea of what was later
called EM and gave several examples. Further impetus came from the Baum–Welch algo-
rithm forHMMlearning (BaumandPetrie, 1966), whichisaspecial caseofEM.Thepaper
by Dempster, Laird, and Rubin (1977), which presented the EM algorithm in general form
and analyzed its convergence, is one of the most cited papers in both computer science and
statistics. (Dempster himself views EM as a schema rather than an algorithm, since a good
deal of mathematical work may be required before it can be applied to a new family of dis-
tributions.) McLachlan and Krishnan (1997) devote an entire book to the algorithm and its
properties. The specific problem of learning mixture models, including mixtures of Gaus-
sians,iscoveredbyTitterington etal.(1985). WithinAI,thefirstsuccessfulsystemthatused
EMformixture modeling was AUTOCLASS (Cheeseman etal.,1988; Cheeseman andStutz,
1996). AUTOCLASShasbeenappliedtoanumberofreal-worldscientificclassificationtasks,
includingthediscoveryofnewtypesofstarsfromspectraldata(Goebeletal.,1989)andnew
classesofproteinsandintronsinDNA/proteinsequencedatabases(HunterandStates,1992).
For maximum-likelihood parameter learning in Bayes nets with hidden variables, EM
andgradient-basedmethodswereintroducedaroundthesametimebyLauritzen(1995),Rus-
sell et al. (1995), and Binder et al. (1997a). The structural EM algorithm was developed by
Friedman (1998) and applied to maximum-likelihood learning of Bayes net structures with
Exercises 827
latentvariables. FriedmanandKoller(2003). describeBayesianstructure learning.
TheabilitytolearnthestructureofBayesiannetworksiscloselyconnected totheissue
of recovering causal information from data. That is, is it possible to learn Bayes nets in
such a way that the recovered network structure indicates real causal influences? For many
years,statisticiansavoidedthisquestion,believingthatobservationaldata(asopposedtodata
generatedfromexperimentaltrials)couldyieldonlycorrelational information—after all,any
two variables that appear related might in fact be influenced by a third, unknown causal
factor rather than influencing each other directly. Pearl (2000) has presented convincing
arguments to the contrary, showing that there are in fact many cases where causality can be
ascertained and developing the causal network formalism to express causes and the effects
CAUSALNETWORK
ofintervention aswellasordinary conditional probabilities.
Nonparametric densityestimation, alsocalled Parzenwindowdensity estimation, was
investigated initially byRosenblatt (1956) and Parzen (1962). Since thattime, ahuge litera-
turehasdevelopedinvestigating theproperties ofvarious estimators. Devroye(1987)givesa
thorough introduction. There isalsoarapidly growing literature onnonparametric Bayesian
methods, originating with the seminal work of Ferguson (1973) on the Dirichlet process,
DIRICHLETPROCESS
whichcanbethoughtofasadistribution overDirichletdistributions. Thesemethodsarepar-
ticularlyusefulformixtureswithunknownnumbersofcomponents. Ghahramani(2005)and
Jordan (2005) provide useful tutorials on the many applications of these ideas to statistical
learning. The text by Rasmussen and Williams (2006) covers the Gaussian process, which
GAUSSIANPROCESS
givesawayofdefiningpriordistributions overthespaceofcontinuous functions.
Thematerialinthischapterbringstogetherworkfromthefieldsofstatisticsandpattern
recognition, so the story has been told many times in many ways. Good texts on Bayesian
statistics include thosebyDeGroot(1970), Berger(1985), andGelmanetal.(1995). Bishop
(2007)andHastieetal.(2009)provideanexcellentintroduction tostatistical machinelearn-
ing. Forpatternclassification, theclassictextformanyyearshasbeenDudaandHart(1973),
now updated (Duda etal., 2001). Theannual NIPS(Neural Information Processing Confer-
ence)conference,whoseproceedingsarepublishedastheseriesAdvancesinNeuralInforma-
tionProcessingSystems,isnowdominatedbyBayesianpapers. PapersonlearningBayesian
networksalsoappearintheUncertaintyinAIandMachineLearningconferences andinsev-
eralstatistics conferences. Journalsspecifictoneuralnetworksinclude NeuralComputation,
Neural Networks, and the IEEE Transactions on Neural Networks. Specifically Bayesian
venues include the Valencia International Meetings on Bayesian Statistics and the journal
BayesianAnalysis.
EXERCISES
20.1 The data used for Figure 20.1 on page 804 can be viewed as being generated by h .
5
For each of the other four hypotheses, generate a data set of length 100 and plot the cor-
responding graphs for P(h |d ,...,d ) and P(D =lime|d ,...,d ). Comment on
i 1 N N+1 1 N
yourresults.
828 Chapter 20. LearningProbabilistic Models
20.2 SupposethatAnn’sutilitiesforcherryandlimecandiesare c and(cid:3) ,whereasBob’s
A A
utilities are c and (cid:3) . (But once Ann has unwrapped a piece of candy, Bob won’t buy
B B
it.) Presumably, if Bob likes lime candies much more than Ann, it would be wise for Ann
tosellherbagofcandies oncesheissufficiently sureofitslimecontent. Ontheotherhand,
if Ann unwraps too many candies in the process, the bag will be worth less. Discuss the
problem of determining the optimal point at which to sell the bag. Determine the expected
utilityoftheoptimalprocedure, giventhepriordistribution fromSection20.1.
20.3 Two statisticians go to the doctor and are both given the same prognosis: A 40%
chance that the problem is the deadly disease A, and a 60% chance of the fatal disease B.
Fortunately, thereareanti-A andanti-B drugs thatareinexpensive, 100% effective, and free
of side-effects. The statisticians have the choice of taking one drug, both, or neither. What
willthefirststatistician(anavidBayesian)do? Howaboutthesecondstatistician,whoalways
usesthemaximumlikelihood hypothesis?
The doctor does some research and discovers that disease B actually comes in two
versions, dextro-B and levo-B, which are equally likely and equally treatable by the anti-B
drug. Nowthattherearethreehypotheses, whatwillthetwostatisticians do?
20.4 ExplainhowtoapplytheboostingmethodofChapter18tonaiveBayeslearning. Test
theperformance oftheresulting algorithm ontherestaurant learning problem.
20.5 ConsiderN datapoints(x ,y ),wherethey saregeneratedfromthex saccordingto
j j j j
thelinearGaussianmodelinEquation(20.5). Findthevaluesofθ ,θ ,andσ thatmaximize
1 2
theconditional loglikelihood ofthedata.
20.6 Consider the noisy-OR model for fever described in Section 14.3. Explain how to
applymaximum-likelihoodlearningtofittheparametersofsuchamodeltoasetofcomplete
data. (Hint: usethechainruleforpartialderivatives.)
20.7 ThisexerciseinvestigatespropertiesoftheBetadistributiondefinedinEquation(20.6).
a. By integrating over the range [0,1], show that the normalization constant for the dis-
tribution beta[a,b] is given by α = Γ(a + b)/Γ(a)Γ(b) where Γ(x) is the Gamma
function,definedbyΓ(x+1)=x·Γ(x)andΓ(1)=1. (Forinteger x,Γ(x+1)=x!.)
GAMMAFUNCTION
b. Showthatthemeanisa/(a+b).
c. Findthemode(s)(themostlikelyvalue(s)ofθ).
d. Describethedistributionbeta[(cid:2),(cid:2)]forverysmall(cid:2). Whathappensassuchadistribution
isupdated?
20.8 Consideranarbitrary Bayesian network, acomplete dataset forthatnetwork, andthe
likelihood for the data set according to the network. Give a simple proof that the likelihood
ofthedatacannotdecreaseifweaddanewlinktothenetworkandrecomputethemaximum-
likelihood parametervalues.
20.9 Consider a single Boolean random variable Y (the “classification”). Let the prior
probability P(Y =true)beπ. Let’strytofindπ,givenatrainingsetD=(y ,...,y )with
1 N
N independent samples ofY. Furthermore, suppose p of the N are positive and n of the N
arenegative.
Exercises 829
a. Write down an expression for the likelihood of D (i.e., the probability of seeing this
particularsequence ofexamples,givenafixedvalueof π)intermsofπ,p,andn.
b. BydifferentiatingtheloglikelihoodL,findthevalueofπthatmaximizesthelikelihood.
c. Nowsuppose weaddink Booleanrandomvariables X ,X ,...,X (the“attributes”)
1 2 k
that describe each sample, and suppose weassume that the attributes are conditionally
independent ofeach other given the goal Y. Draw theBayes net corresponding to this
assumption.
d. Write down the likelihood for the data including the attributes, using the following
additional notation:
• α isP(X =true|Y =true).
i i
• β isP(X =true|Y =false).
i i
• p+ isthecountofsamplesforwhichX =trueandY =true.
i i
• n+ isthecountofsamplesforwhichX =falseandY =true.
i i
• p − isthecountofsamplesforwhichX =trueandY =false.
i i
• n − isthecountofsamplesforwhichX =falseandY =false.
i i
[Hint: considerfirsttheprobability ofseeingasingleexamplewithspecifiedvaluesfor
X ,X ,...,X andY.]
1 2 k
e. Bydifferentiating theloglikelihood L,findthevaluesofα andβ (intermsofthevar-
i i
iouscounts)thatmaximizethelikelihoodandsayinwordswhatthesevaluesrepresent.
f. Letk = 2,andconsideradatasetwith4allfourpossibleexamplesoftheXORfunction.
Computethemaximumlikelihood estimates ofπ,α ,α ,β ,andβ .
1 2 1 2
g. Given these estimates of π, α , α , β , and β , what are the posterior probabilities
1 2 1 2
P(Y =true|x ,x )foreachexample?
1 2
20.10 Consider the application of EM to learn the parameters for the network in Fig-
ure20.13(a), giventhetrueparametersinEquation(20.7).
a. Explain why the EM algorithm would not work if there were just two attributes in the
modelratherthanthree.
b. Showthecalculations forthefirstiterationofEMstarting fromEquation(20.8).
c. What happens if we start with all the parameters set to the same value p? (Hint: you
mayfindithelpful toinvestigate thisempirically beforederivingthegeneral result.)
d. Writeoutanexpressionfortheloglikelihoodofthetabulatedcandydataonpage821in
termsoftheparameters,calculatethepartialderivatives withrespecttoeachparameter,
andinvestigate thenatureofthefixedpointreachedinpart(c).
21
REINFORCEMENT
LEARNING
In which we examine how an agent can learn from success and failure, from re-
wardandpunishment.
21.1 INTRODUCTION
Chapters18,19,and20coveredmethodsthatlearnfunctions,logicaltheories,andprobability
modelsfrom examples. Inthischapter, wewillstudyhowagents canlearn whattodointhe
absenceoflabeledexamplesofwhattodo.
Consider, for example, the problem of learning to play chess. A supervised learning
agent needs tobe told the correct moveforeach position it encounters, but such feedback is
seldom available. In the absence of feedback from a teacher, an agent can learn a transition
modelforitsownmovesandcan perhaps learn topredict theopponent’s moves, butwithout
somefeedbackaboutwhatisgoodandwhatisbad,theagentwillhavenogroundsfordecid-
ing which moveto make. Theagent needs toknow that something good has happened when
it (accidentally) checkmates the opponent, and that something bad has happened when it is
checkmated—or vice versa, if the game is suicide chess. This kind of feedback is called a
reward,orreinforcement. Ingameslikechess,thereinforcementisreceivedonlyattheend
REINFORCEMENT
of the game. In other environments, the rewards come more frequently. In ping-pong, each
point scored can be considered a reward; when learning to crawl, any forward motion is an
achievement. Our framework for agents regards the reward as part of the input percept, but
the agent must be “hardwired” to recognize that part as a reward rather than as just another
sensory input. Thus, animalsseemtobehardwired torecognize painandhungerasnegative
rewards and pleasure andfood intake aspositive rewards. Reinforcement has been carefully
studiedbyanimalpsychologists forover60years.
Rewards were introduced in Chapter 17, where they served to define optimal policies
in Markov decision processes (MDPs). An optimal policy is a policy that maximizes the
expectedtotalreward. Thetaskofreinforcementlearningistouseobservedrewardstolearn
an optimal (ornearly optimal) policy for the environment. Whereas in Chapter 17 the agent
hasacompletemodeloftheenvironment andknowstherewardfunction,hereweassumeno
830
Section21.1. Introduction 831
priorknowledge ofeither. Imagineplaying anew gamewhoserules youdon’t know; aftera
hundred orso moves, your opponent announces, “You lose.” This is reinforcement learning
inanutshell.
In many complex domains, reinforcement learning is the only feasible way to train a
program toperform athighlevels. Forexample,ingameplaying, itisveryhardforahuman
toprovideaccurateandconsistentevaluationsoflargenumbersofpositions, whichwouldbe
needed to train an evaluation function directly from examples. Instead, the program can be
told when it has won or lost, and it can use this information to learn an evaluation function
thatgivesreasonablyaccurateestimatesoftheprobabilityofwinningfromanygivenposition.
Similarly,itisextremelydifficulttoprogramanagenttoflyahelicopter;yetgivenappropriate
negativerewardsforcrashing, wobbling,ordeviatingfrom asetcourse,anagentcanlearnto
flybyitself.
Reinforcementlearningmightbeconsidered toencompassallofAI:anagentisplaced
in an environment and must learn to behave successfully therein. To keep the chapter man-
ageable, wewillconcentrate onsimpleenvironments andsimpleagentdesigns. Forthemost
part, we will assume a fully observable environment, so that the current state is supplied by
each percept. On the other hand, we will assume that the agent does not know how the en-
vironment worksorwhatitsactions do,andwewillallow forprobabilistic action outcomes.
Thus, the agent faces an unknown Markov decision process. We will consider three of the
agentdesignsfirstintroduced inChapter2:
• Autility-basedagentlearnsautilityfunctiononstatesandusesittoselectactionsthat
maximizetheexpectedoutcomeutility.
• A Q-learning agent learns an action-utility function, or Q-function, giving the ex-
Q-LEARNING
pectedutilityoftakingagivenactioninagivenstate.
Q-FUNCTION
• Areflexagentlearnsapolicythatmapsdirectly fromstatestoactions.
Autility-based agentmustalsohaveamodel oftheenvironment inordertomakedecisions,
because itmustknowthestates towhichitsactions willlead. Forexample, inordertomake
useofabackgammonevaluationfunction, abackgammonprogrammustknowwhatitslegal
moves are and how they affect the board position. Only in this way can it apply the utility
function to the outcome states. A Q-learning agent, on the other hand, can compare the
expectedutilitiesforitsavailable choiceswithoutneedingtoknowtheiroutcomes, soitdoes
not need a model of the environment. On the other hand, because they do not know where
theiractionslead,Q-learningagentscannotlookahead;thiscanseriouslyrestricttheirability
tolearn, asweshallsee.
Webegin in Section 21.2 with passive learning, where the agent’s policy is fixed and
PASSIVELEARNING
thetaskistolearntheutilitiesofstates(orstate–action pairs);thiscouldalsoinvolvelearning
amodeloftheenvironment. Section21.3covers active learning, wheretheagent mustalso
ACTIVELEARNING
learn what to do. The principal issue is exploration: an agent must experience as much as
EXPLORATION
possible ofitsenvironment inordertolearn howtobehave in it. Section 21.4discusses how
an agent can use inductive learning to learn much faster from its experiences. Section 21.5
covers methods forlearning direct policy representations in reflexagents. Anunderstanding
ofMarkovdecisionprocesses (Chapter17)isessentialforthischapter.
832 Chapter 21. Reinforcement Learning
21.2 PASSIVE REINFORCEMENT LEARNING
Tokeep things simple, westart with the case of a passive learning agent using a state-based
representation in a fully observable environment. In passive learning, the agent’s policy π
is fixed: in state s, it always executes the action π(s). Its goal is simply to learn how good
the policy is—that is, to learn the utility function Uπ(s). We will use as our example the
4×3 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the
corresponding utilities. Clearly, the passive learning task is similar to the policy evaluation
task, part of the policy iteration algorithm described in Section 17.3. The main difference
is that the passive learning agent does not know the transition model P(s
(cid:2)|s,a),
which
(cid:2)
specifies the probability of reaching state s from state s after doing action a; nor does it
knowtherewardfunctionR(s),whichspecifiestherewardforeachstate.
3 +1
3 0.812 0.868 0.918 + 1
2 –1 2 0.762 0.660 –1
1 1 0.705 0.655 0.611 0.388
1 2 3 4 1 2 3 4
(a) (b)
Figure 21.1 (a) A policy π for the 4×3 world; this policy happens to be optimal with
rewardsof R(s)= −0.04inthenonterminalstatesandnodiscounting. (b)Theutilitiesof
thestatesinthe4×3world,givenpolicyπ.
Theagentexecutesasetoftrialsintheenvironmentusingitspolicyπ. Ineachtrial,the
TRIAL
agent starts in state (1,1) and experiences a sequence of state transitions until it reaches one
oftheterminal states, (4,2)or(4,3). Itspercepts supply boththecurrent stateandthereward
receivedinthatstate. Typicaltrialsmightlooklikethis:
(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(4,3)
-.04 -.04 -.04 -.04 -.04 -.04 -.04 +1
(1,1) (cid:2)(1,2) (cid:2)(1,3) (cid:2)(2,3) (cid:2)(3,3) (cid:2)(3,2) (cid:2)(3,3) (cid:2)(4,3)
-.04 -.04 -.04 -.04 -.04 -.04 -.04 +1
(1,1) (cid:2)(2,1) (cid:2)(3,1) (cid:2)(3,2) (cid:2)(4,2) .
-.04 -.04 -.04 -.04 -1
Notethat each state percept is subscripted withthe reward received. Theobject istouse the
informationaboutrewardstolearntheexpectedutility Uπ(s)associatedwitheachnontermi-
nal state s. Theutility is defined to be the expected sum of (discounted) rewards obtained if
Section21.2. PassiveReinforcement Learning 833
policyπ isfollowed. AsinEquation(17.2)onpage650,wewrite
" #
(cid:12)∞
Uπ(s) = E γtR(S ) (21.1)
t
t=0
whereR(s)istherewardforastate, S (arandomvariable)isthestatereachedattimetwhen
t
executing policy π, and S =s. Wewillinclude a discountfactor γ inall of ourequations,
0
butforthe4×3worldwewillsetγ=1.
21.2.1 Directutility estimation
DIRECTUTILITY A simple method for direct utility estimation was invented in the late 1950s in the area of
ESTIMATION
ADAPTIVECONTROL adaptive control theory by Widrow and Hoff (1960). The idea is that the utility of a state
THEORY
is the expected total reward from that state onward (called the expected reward-to-go), and
REWARD-TO-GO
eachtrialprovides a sampleofthisquantity foreachstatevisited. Forexample, thefirsttrial
in the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two
samples of0.76 and 0.84 for(1,2), twosamples of0.80 and 0.88 for(1,3), and so on. Thus,
attheendofeachsequence,thealgorithmcalculatestheobservedreward-to-goforeachstate
andupdatestheestimatedutilityforthatstateaccordingly, justbykeeping arunningaverage
foreachstateinatable. Inthelimitofinfinitelymanytrials,thesampleaveragewillconverge
tothetrueexpectation inEquation(21.1).
It is clear that direct utility estimation is just an instance of supervised learning where
each example has the state as input and the observed reward-to-go as output. This means
that we have reduced reinforcement learning to a standard inductive learning problem, as
discussed inChapter18. Section 21.4discusses theuseofmorepowerful kinds ofrepresen-
tations for the utility function. Learning techniques for those representations can be applied
directlytotheobserveddata.
Direct utility estimation succeeds in reducing the reinforcement learning problem to
an inductive learning problem, about which much is known. Unfortunately, it misses a very
important source of information, namely, the fact that the utilities of states are not indepen-
dent! Theutility ofeachstateequals itsownrewardplus theexpected utility ofitssuccessor
states. That is, the utility values obey the Bellman equations for a fixed policy (see also
Equation(17.10)):
(cid:12)
Uπ(s) = R(s)+γ P(s (cid:2)|s,π(s))Uπ(s (cid:2) ). (21.2)
s(cid:3)
Byignoringtheconnections betweenstates,directutility estimation missesopportunities for
learning. For example, the second of the three trials given earlier reaches the state (3,2),
which has not previously been visited. The next transition reaches (3,3), which is known
from the first trial to have a high utility. The Bellman equation suggests immediately that
(3,2)isalsolikelytohaveahighutility, because itleadsto(3,3),butdirectutilityestimation
learns nothing until the end of the trial. More broadly, we can view direct utility estimation
as searching for U in a hypothesis space that is much larger than it needs to be, in that it
includes many functions that violate the Bellman equations. For this reason, the algorithm
oftenconverges veryslowly.
834 Chapter 21. Reinforcement Learning
functionPASSIVE-ADP-AGENT(percept)returnsanaction
inputs:percept,aperceptindicatingthecurrentstates(cid:5)andrewardsignalr(cid:5)
persistent: π,afixedpolicy
mdp,anMDPwithmodelP,rewardsR,discountγ
U,atableofutilities,initiallyempty
Nsa,atableoffrequenciesforstate–actionpairs,initiallyzero
Ns(cid:3)|sa,atableofoutcomefrequenciesgivenstate–actionpairs,initiallyzero
s,a,thepreviousstateandaction,initiallynull
ifs(cid:5)isnewthenU[s(cid:5)]←r(cid:5);R[s(cid:5)]←r(cid:5)
ifs isnotnullthen
incrementNsa[s,a]andNs(cid:3)|sa[s(cid:5),s,a]
foreacht suchthatNs(cid:3)|sa[t,s,a]isnonzerodo
P(t|s,a)←Ns(cid:3)|sa[t,s,a]/Nsa[s,a]
U ←POLICY-EVALUATION(π,U,mdp)
ifs(cid:5).TERMINAL?thens,a←nullelses,a←s(cid:5),π[s(cid:5)]
returna
Figure21.2 Apassivereinforcementlearningagentbasedonadaptivedynamicprogram-
ming. The POLICY-EVALUATION function solves the fixed-policy Bellman equations, as
describedonpage657.
21.2.2 Adaptivedynamicprogramming
ADAPTIVEDYNAMIC An adaptive dynamic programming (or ADP) agent takes advantage of the constraints
PROGRAMMING
among the utilities of states by learning the transition model that connects them and solv-
ing the corresponding Markov decision process using a dynamic programming method. For
apassivelearningagent,thismeanspluggingthelearnedtransitionmodelP(s
(cid:2)|s,π(s))and
the observed rewards R(s) into the Bellman equations (21.2) to calculate the utilities of the
states. As we remarked in our discussion of policy iteration in Chapter 17, these equations
are linear (no maximization involved) so they can be solved using any linear algebra pack-
age. Alternatively, we can adopt the approach of modified policy iteration (see page 657),
using a simplified value iteration process to update the utility estimates after each change to
the learned model. Because the model usually changes only slightly with each observation,
the value iteration process can use the previous utility estimates as initial values and should
converge quitequickly.
The process of learning the model itself is easy, because the environment is fully ob-
servable. Thismeansthatwehaveasupervisedlearningtaskwheretheinputisastate–action
pair and the output is the resulting state. In the simplest case, we can represent the tran-
sition model as a table of probabilities. We keep track of how often each action outcome
occurs and estimate the transition probability P(s
(cid:2)|s,a)
from the frequency with which s
(cid:2)
is reached when executing a in s. Forexample, in the three trials given on page 832, Right
is executed three times in (1,3) and two out of three times the resulting state is (2,3), so
P((2,3)|(1,3),Right)isestimatedtobe2/3.
Section21.2. PassiveReinforcement Learning 835
1
0.8
0.6
0.4
0.2
0
0 20 40 60 80 100
setamitse
ytilitU
0.6
(4,3)
(3,3) 0.5
(1,3)
0.4
(1,1)
(3,2)
0.3
0.2
0.1
0
0 20 40 60 80 100
Number of trials
ytilitu
ni
rorre
SMR
Number of trials
(a) (b)
Figure21.3 ThepassiveADPlearningcurvesforthe4×3world,giventheoptimalpolicy
showninFigure21.1. (a)Theutilityestimatesforaselectedsubsetofstates, asa function
ofthenumberoftrials. Noticethelargechangesoccurringaroundthe78thtrial—thisisthe
first time that the agent falls into the −1 terminal state at (4,2). (b) The root-mean-square
error(seeAppendixA)intheestimateforU(1,1),averagedover20runsof100trialseach.
The full agent program for a passive ADP agent is shown in Figure 21.2. Its perfor-
mance on the 4×3 world is shown in Figure 21.3. In terms of how quickly its value es-
timates improve, the ADP agent is limited only by its ability to learn the transition model.
In this sense, it provides a standard against which to measure other reinforcement learning
algorithms. Itis,however, intractable forlarge statespaces. Inbackgammon, forexample, it
wouldinvolve solvingroughly 1050 equations in1050 unknowns.
AreaderfamiliarwiththeBayesianlearning ideasofChapter20willhavenoticed that
the algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition
model; moreover, by choosing a policy based solely on the estimated model it is acting as
if the model were correct. This is not necessarily a good idea! For example, a taxi agent
that didn’t know about how traffic lights might ignore a red light once or twice without no
ill effects and then formulate a policy to ignore red lights from then on. Instead, it might
be a good idea to choose a policy that, while not optimal for the model estimated by maxi-
mumlikelihood, worksreasonablywellforthewholerangeof modelsthathaveareasonable
chanceofbeingthetruemodel. Therearetwomathematicalapproaches thathavethisflavor.
BAYESIAN
The first approach, Bayesian reinforcement learning, assumes a prior probability
REINFORCEMENT
LEARNING P(h)foreachhypothesis haboutwhatthetruemodelis;theposteriorprobability P(h|e)is
obtainedintheusualwaybyBayes’rulegiventheobservationstodate. Then,iftheagenthas
decided tostop learning, the optimal policy istheone that gives thehighest expected utility.
Let uπ be the expected utility, averaged over all possible start states, obtained by executing
h
policyπ inmodelh. Thenwehave
(cid:12)
π ∗ = argmax P(h|e)uπ .
h
π
h
836 Chapter 21. Reinforcement Learning
In somespecial cases, this policy can even be computed! If the agent willcontinue learning
in the future, however, then finding an optimal policy becomes considerably more difficult,
because the agent must consider the effects of future observations on its beliefs about the
transition model. Theproblem becomes aPOMDPwhosebelief states aredistributions over
models. This concept provides an analytical foundation for understanding the exploration
problem described inSection21.3.
ROBUSTCONTROL Thesecond approach, derived from robustcontroltheory, allowsforasetofpossible
THEORY
modelsHanddefinesanoptimalrobustpolicyasonethatgivesthebestoutcomeintheworst
caseoverH:
π ∗ = argmaxminuπ .
h
π h
Often, thesetH willbethesetofmodels thatexceedsomelikelihood threshold onP(h|e),
so the robust and Bayesian approaches are related. Sometimes, the robust solution can be
computed efficiently. There are, moreover, reinforcement learning algorithms that tend to
produce robustsolutions, althoughwedonotcoverthemhere.
21.2.3 Temporal-difference learning
Solving the underlying MDP as in the preceding section is not the only way to bring the
Bellman equations to bear on the learning problem. Another way is to use the observed
transitions to adjust the utilities of the observed states so that they agree with the constraint
equations. Consider, for example, the transition from (1,3) to (2,3) in the second trial on
page 832. Suppose that, as a result of the firsttrial, the utility estimates are Uπ(1,3)=0.84
andUπ(2,3)=0.92. Now,ifthis transition occurred allthe time,wewould expect theutili-
tiestoobeytheequation
Uπ(1,3) = −0.04+Uπ(2,3),
soUπ(1,3)wouldbe0.88. Thus,itscurrentestimateof0.84mightbealittlelowandshould
(cid:2)
be increased. More generally, when a transition occurs from state s to state s, weapply the
followingupdatetoUπ(s):
Uπ(s) ← Uπ(s)+α(R(s)+γUπ(s (cid:2) )−Uπ(s)). (21.3)
Here,αisthelearningrateparameter. Becausethisupdateruleusesthedifferenceinutilities
TEMPORAL- betweensuccessivestates, itisoftencalledthetemporal-difference, orTD,equation.
DIFFERENCE
All temporal-difference methods work by adjusting the utility estimates towards the
idealequilibrium thatholdslocally whentheutility estimates arecorrect. Inthecaseofpas-
sivelearning, theequilibrium isgiven byEquation (21.2). NowEquation (21.3) does in fact
cause theagenttoreachtheequilibrium givenbyEquation (21.2), butthere issomesubtlety
(cid:2)
involved. First, notice that the update involves only the observed successor s, whereas the
actualequilibriumconditionsinvolveallpossiblenextstates. Onemightthinkthatthiscauses
animproperly largechangein Uπ(s)whenaveryraretransition occurs;but,infact,because
rare transitions occur only rarely, the average value of Uπ(s) will converge to the correct
value. Furthermore, if we change α from a fixed parameter to a function that decreases as
thenumberoftimesastate hasbeen visited increases, then Uπ(s)itself willconverge tothe
Section21.2. PassiveReinforcement Learning 837
functionPASSIVE-TD-AGENT(percept)returnsanaction
inputs:percept,aperceptindicatingthecurrentstates(cid:5)andrewardsignalr(cid:5)
persistent: π,afixedpolicy
U,atableofutilities,initiallyempty
Ns,atableoffrequenciesforstates,initiallyzero
s,a,r,thepreviousstate,action,andreward,initiallynull
ifs(cid:5)isnewthenU[s(cid:5)]←r(cid:5)
ifs isnotnullthen
incrementNs[s]
U[s]←U[s] + α(Ns[s])(r + γU[s(cid:5)] − U[s])
ifs(cid:5).TERMINAL?thens,a,r←nullelses,a,r←s(cid:5),π[s(cid:5)],r(cid:5)
returna
Figure21.4 Apassivereinforcementlearningagentthatlearnsutilityestimatesusingtem-
poraldifferences.Thestep-sizefunctionα(n)ischosentoensureconvergence,asdescribed
inthetext.
correct value.1 Thisgivesustheagentprogram showninFigure21.4. Figure21.5illustrates
theperformance ofthepassiveTDagentonthe 4×3world. Itdoesnotlearnquiteasfastas
the ADPagent and shows muchhigher variability, but itismuch simplerand requires much
lesscomputationperobservation. NoticethatTDdoesnotneedatransitionmodeltoperform
itsupdates. Theenvironment suppliestheconnection betweenneighboring statesintheform
ofobserved transitions.
TheADPapproach andtheTDapproach areactually closely related. Bothtrytomake
local adjustments tothe utility estimates inordertomakeeach state “agree” withitssucces-
sors. One difference is that TD adjusts a state to agree with its observed successor (Equa-
tion (21.3)), whereas ADP adjusts the state to agree with all of the successors that might
occur, weighted by their probabilities (Equation (21.2)). This difference disappears when
the effects of TD adjustments are averaged over a large number of transitions, because the
frequencyofeachsuccessorinthesetoftransitionsisapproximatelyproportionaltoitsprob-
ability. A more important difference is that whereas TD makes a single adjustment per ob-
served transition, ADP makes as many as it needs to restore consistency between the utility
estimates U and the environment model P. Although the observed transition makes only a
local change in P, its effects might need to be propagated throughout U. Thus, TD can be
viewedasacrudebutefficientfirstapproximation toADP.
Each adjustment made by ADP could be seen, from the TD point of view, as a re-
sult of a “pseudoexperience” generated by simulating the current environment model. It
is possible to extend the TD approach to use an environment model to generate several
pseudoexperiences—transitionsthattheTDagentcanimaginemighthappen,givenitscurrent
model. Foreachobservedtransition, theTDagentcangenerate alargenumberofimaginary
1 Thetechnical conditionsaregiven onpage725. InFigure21.5wehaveusedα(n)=60/(59+n), which
satisfiestheconditions.
838 Chapter 21. Reinforcement Learning
1
0.8
0.6
0.4
0.2
0
0 100 200 300 400 500
setamitse
ytilitU
0.6
(4,3)
(3,3) 0.5
(1,3)
(1,1) 0.4
(2,1)
0.3
0.2
0.1
0
0 20 40 60 80 100
Number of trials
ytilitu
ni
rorre
SMR
Number of trials
(a) (b)
Figure21.5 TheTD learningcurvesforthe 4×3 world. (a) Theutility estimates fora
selectedsubsetofstates,asafunctionofthenumberoftrials. (b)Theroot-mean-squareerror
intheestimateforU(1,1),averagedover20runsof500trialseach. Onlythefirst100trials
areshowntoenablecomparisonwithFigure21.3.
transitions. Inthisway,theresultingutilityestimateswillapproximatemoreandmoreclosely
thoseofADP—ofcourse, attheexpenseofincreased computation time.
Ina similar vein, wecan generate more efficient versions of ADPby directly approxi-
mating the algorithms forvalue iteration orpolicy iteration. Even though the value iteration
algorithm is efficient, it is intractable if we have, say, 10100 states. However, many of the
necessary adjustments to the state values on each iteration will be extremely tiny. One pos-
sible approach to generating reasonably good answers quickly is to bound the number of
adjustmentsmadeaftereachobservedtransition. Onecanalsouseaheuristictorankthepos-
PRIORITIZED sibleadjustmentssoastocarryoutonlythemostsignificant ones. Theprioritizedsweeping
SWEEPING
heuristic preferstomakeadjustments tostateswhose likelysuccessors havejustundergone a
large adjustment in their own utility estimates. Using heuristics like this, approximate ADP
algorithmsusuallycanlearnroughlyasfastasfullADP,intermsofthenumberoftrainingse-
quences, butcanbeseveralordersofmagnitudemoreefficientintermsofcomputation. (See
Exercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.
Approximate ADPalgorithms haveanadditional advantage: intheearlystages oflearning a
new environment, the environment model P often will be far from correct, so there is little
pointincalculatinganexactutilityfunctiontomatchit. Anapproximationalgorithmcanuse
aminimumadjustmentsizethatdecreasesastheenvironment modelbecomesmoreaccurate.
This eliminates the very long value iterations that can occur early in learning due to large
changes inthemodel.
Section21.3. ActiveReinforcement Learning 839
21.3 ACTIVE REINFORCEMENT LEARNING
Apassivelearningagenthasafixedpolicythatdeterminesitsbehavior. Anactiveagentmust
decide whatactions totake. Letusbeginwiththeadaptive dynamicprogramming agent and
considerhowitmustbemodifiedtohandlethisnewfreedom.
First, the agent will need to learn a complete model with outcome probabilities for all
actions, ratherthanjust themodelforthefixedpolicy. Thesimplelearning mechanism used
by PASSIVE-ADP-AGENT will do just fine for this. Next, we need to take into account the
fact thatthe agent hasachoice ofactions. Theutilities itneeds tolearn arethose defined by
theoptimalpolicy;theyobeytheBellmanequationsgivenonpage652,whichwerepeathere
forconvenience:
(cid:12)
U(s) = R(s)+γ max P(s
(cid:2)|s,a)U(s (cid:2)
). (21.4)
a
s(cid:3)
These equations can be solved to obtain the utility function U using the value iteration or
policyiterationalgorithmsfromChapter17. Thefinalissueiswhattodoateachstep. Having
obtained a utility function U that is optimal for the learned model, the agent can extract an
optimal action by one-step look-ahead to maximize the expected utility; alternatively, if it
uses policy iteration, the optimal policy is already available, so it should simply execute the
actiontheoptimalpolicyrecommends. Orshouldit?
21.3.1 Exploration
Figure 21.6 shows the results of one sequence of trials for an ADP agent that follows the
recommendation of the optimal policy for the learned model at each step. The agent does
not learn the true utilities or the true optimal policy! What happens instead is that, in the
39th trial, it finds a policy that reaches the +1 reward along the lower route via (2,1), (3,1),
(3,2), and (3,3). (See Figure 21.6(b).) After experimenting with minor variations, from the
276th trial onward it sticks to that policy, never learning the utilities of the other states and
neverfindingtheoptimalroutevia(1,2),(1,3),and(2,3). Wecallthisagentthegreedyagent.
GREEDYAGENT
Repeatedexperimentsshowthatthegreedyagentveryseldomconvergestotheoptimalpolicy
forthisenvironment andsometimesconverges toreallyhorrendous policies.
Howcanitbethatchoosingtheoptimalactionleadstosuboptimalresults? Theanswer
is that the learned model is not the same as the true environment; what is optimal in the
learned modelcantherefore besuboptimal inthetrueenvironment. Unfortunately, theagent
does not know what the true environment is, so it cannot compute the optimal action forthe
trueenvironment. What,then,istobedone?
What the greedy agent has overlooked is that actions do more than provide rewards
according tothecurrentlearnedmodel;theyalsocontribute tolearning thetruemodelbyaf-
fectingthepercepts thatarereceived. Byimprovingthemodel,theagentwillreceivegreater
rewards in the future.2 An agent therefore must make a tradeoff between exploitation to
EXPLOITATION
maximize its reward—as reflected inits current utility estimates—and exploration to maxi-
EXPLORATION
2 NoticethedirectanalogytothetheoryofinformationvalueinChapter16.
840 Chapter 21. Reinforcement Learning
2
1.5
1
0.5
0
0 50 100 150 200 250 300 350 400 450 500
ssol
ycilop
,rorre
SMR
3 +1
RMS error
Policy loss
2 –1
1
1 2 3 4
Number of trials
(a) (b)
Figure21.6 Performanceofa greedyADP agentthatexecutesthe actionrecommended
bytheoptimalpolicyforthelearnedmodel. (a)RMSerrorintheutilityestimatesaveraged
over the nine nonterminal squares. (b) The suboptimal policy to which the greedy agent
convergesinthisparticularsequenceoftrials.
mizeitslong-term well-being. Pureexploitation risksgettingstuckinarut. Pureexploration
toimproveone’sknowledgeisofnouseifoneneverputsthatknowledgeintopractice. Inthe
real world, one constantly has to decide between continuing in a comfortable existence and
striking outintotheunknown inthehopes ofdiscovering anewandbetterlife. With greater
understanding, lessexploration isnecessary.
Canwebe alittle more precise than this? Isthere an optimal exploration policy? This
questionhasbeenstudiedindepthinthesubfieldofstatisticaldecisiontheorythatdealswith
so-called banditproblems. (Seesidebar.)
BANDITPROBLEM
Although bandit problems are extremely difficult to solve exactly to obtain an optimal
exploration method, it is nonetheless possible to come up with a reasonable scheme that
will eventually lead to optimal behavior by the agent. Technically, any such scheme needs
to be greedy in the limit of infinite exploration, or GLIE. A GLIE scheme must try each
GLIE
action in each state an unbounded number of times to avoid having a finite probability that
an optimal action ismissed because of an unusually bad series of outcomes. AnADPagent
usingsuchaschemewilleventuallylearnthetrueenvironment model. AGLIEschememust
alsoeventuallybecomegreedy,sothattheagent’sactionsbecomeoptimalwithrespecttothe
learned(andhencethetrue)model.
ThereareseveralGLIEschemes;oneofthesimplest istohave theagentchoose aran-
dom action a fraction 1/t of the time and to follow the greedy policy otherwise. While this
does eventually converge to an optimal policy, it can be extremely slow. A more sensible
approach would give some weight to actions that the agent has not tried very often, while
tending to avoid actions that are believed to be of low utility. This can be implemented by
altering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively
Section21.3. ActiveReinforcement Learning 841
EXPLORATION AND BANDITS
In Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin,
pullthelever, and collect thewinnings (ifany). Ann-armedbandithasnlevers.
The gambler must choose which lever to play on each successive coin—the one
thathaspaidoffbest,ormaybeonethathasnotbeentried?
Then-armedbandit problem isaformal modelforreal problems inmanyvi-
tally important areas, such as deciding on the annual budget for AI research and
development. Each arm corresponds to an action (such as allocating $20 million
forthedevelopmentofnewAItextbooks),andthepayofffrompullingthearmcor-
responds to the benefits obtained from taking the action (immense). Exploration,
whether it is exploration of a new research field orexploration of a new shopping
mall,isrisky, isexpensive, andhasuncertain payoffs; ontheotherhand, failureto
exploreatallmeansthatoneneverdiscovers anyactionsthatareworthwhile.
Toformulateabanditproblemproperly,onemustdefineexactlywhatismeant
by optimal behavior. Most definitions in the literature assume that the aim is to
maximizetheexpectedtotalrewardobtained overtheagent’slifetime. Thesedefi-
nitionsrequirethattheexpectationbetakenoverthepossibleworldsthattheagent
couldbein,aswellasoverthepossibleresultsofeachactionsequenceinanygiven
world. Here, a“world” isdefinedbythetransition model P(s
(cid:2)|s,a).
Thus, inor-
derto act optimally, the agent needs aprior distribution over the possible models.
Theresulting optimization problemsareusuallywildlyintractable.
Insomecases—forexample,whenthepayoffofeachmachineisindependent
and discounted rewards are used—it is possible to calculate a Gittins index for
each slot machine (Gittins, 1989). The index is a function only of the number of
timestheslotmachinehasbeenplayedandhowmuchithaspaidoff. Theindexfor
eachmachineindicateshowworthwhileitistoinvestmore;generallyspeaking,the
higher the expected return and the higher the uncertainty in the utility of a given
choice, the better. Choosing the machine with the highest index value gives an
optimalexplorationpolicy. Unfortunately,nowayhasbeenfoundtoextendGittins
indicestosequential decision problems.
One can use the theory of n-armed bandits to argue for the reasonableness
of the selection strategy in genetic algorithms. (See Chapter 4.) If you consider
each arm in an n-armed bandit problem to be a possible string of genes, and the
investment of a coin in one arm to be the reproduction of those genes, then it can
beproventhatgeneticalgorithmsallocatecoinsoptimally,givenanappropriateset
ofindependence assumptions.
842 Chapter 21. Reinforcement Learning
unexplored state–action pairs. Essentially, thisamounts toanoptimisticprioroverthepossi-
bleenvironments andcauses theagenttobehave initially as ifthere werewonderful rewards
scattered alloverthe place. Letus use U+(s)todenote the optimistic estimate of the utility
(i.e.,theexpected reward-to-go) ofthestate s,andletN(s,a)bethenumberoftimesaction
a has been tried in state s. Suppose we are using value iteration in an ADP learning agent;
thenweneedtorewritetheupdateequation (Equation(17.6) onpage652)toincorporate the
optimisticestimate. Thefollowingequation doesthis:
(cid:13) (cid:14)
(cid:2)
U+(s) ← R(s)+γ max f P(s (cid:2)|s,a)U+(s (cid:2) ), N(s,a) . (21.5)
a s(cid:3)
EXPLORATION Here, f(u,n) is called the exploration function. It determines how greed (preference for
FUNCTION
high values of u) is traded off against curiosity (preference for actions that have not been
tried often and have low n). The function f(u,n) should be increasing in u and decreasing
inn. Obviously, therearemanypossible functions thatfitthese conditions. Oneparticularly
simpledefinitionis
(cid:24)
R+ ifn< N
f(u,n) = e
u otherwise
whereR+ isanoptimisticestimateofthebestpossiblerewardobtainableinanystateandN
e
is a fixed parameter. This will have the effect of making the agent try each action–state pair
atleastN times.
e
The fact that U+ rather than U appears on the right-hand side of Equation (21.5) is
veryimportant. Asexploration proceeds,thestatesandactionsnearthestartstatemightwell
be tried a large number of times. If we used U, the more pessimistic utility estimate, then
the agent would soon become disinclined to explore further afield. The use of U+ means
that the benefits of exploration are propagated back from the edges of unexplored regions,
so that actions that lead toward unexplored regions are weighted more highly, rather than
just actions that are themselves unfamiliar. Theeffect of this exploration policy can be seen
clearlyinFigure21.7,whichshowsarapidconvergence towardoptimalperformance, unlike
thatofthegreedyapproach. Averynearlyoptimalpolicyisfoundafterjust18trials. Notice
that the utility estimates themselves do not converge as quickly. This is because the agent
stops exploring the unrewarding parts of the state space fairly soon, visiting them only “by
accident”thereafter. However,itmakesperfectsensefortheagentnottocareabouttheexact
utilities ofstatesthatitknowsareundesirable andcanbeavoided.
21.3.2 Learning anaction-utility function
Nowthatwehavean activeADPagent, letusconsider how toconstruct anactivetemporal-
difference learning agent. The most obvious change from the passive case is that the agent
is no longer equipped with a fixed policy, so, if it learns a utility function U, it will need to
learn a model in order to be able to choose an action based on U via one-step look-ahead.
Themodelacquisition problemfortheTDagentisidenticaltothatfortheADPagent. What
oftheTDupdateruleitself? Perhapssurprisingly, theupdaterule(21.3)remainsunchanged.
Thismightseem odd, forthefollowing reason: Supposetheagent takesastep thatnormally
Section21.3. ActiveReinforcement Learning 843
2.2
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0 20 40 60 80 100
setamitse
ytilitU
(1,1) 1.4
(1,2)
(1,3) 1.2
(2,3)
(3,2) 1
(3,3)
(4,3) 0.8
0.6
0.4
0.2
0
0 20 40 60 80 100
Number of trials
ssol
ycilop
,rorre
SMR
RMS error
Policy loss
Number of trials
(a) (b)
Figure21.7 PerformanceoftheexploratoryADPagent. usingR+ = 2andNe = 5. (a)
Utility estimates for selected states overtime. (b) The RMS error in utility values and the
associatedpolicyloss.
leadstoagooddestination,butbecauseofnondeterminism intheenvironmenttheagentends
upinacatastrophicstate. TheTDupdaterulewilltakethisasseriouslyasiftheoutcomehad
been the normal result of the action, whereas one might suppose that, because the outcome
was a fluke, the agent should not worry about it too much. In fact, of course, the unlikely
outcome will occur only infrequently in a large set of training sequences; hence in the long
run its effects will be weighted proportionally to its probability, as we would hope. Once
again,itcanbeshownthattheTDalgorithm willconverge tothesamevaluesasADPasthe
numberoftrainingsequences tendstoinfinity.
There is an alternative TD method, called Q-learning, which learns an action-utility
representation instead of learning utilities. We will use the notation Q(s,a) to denote the
valueofdoingactionainstates. Q-valuesaredirectlyrelated toutilityvaluesasfollows:
U(s) = maxQ(s,a). (21.6)
a
Q-functions may seem like just another way of storing utility information, but they have a
very important property: a TD agent that learns a Q-function does not need a model of the
form P(s
(cid:2)|s,a),
either for learning or for action selection. For this reason, Q-learning is
called a model-free method. As with utilities, we can write a constraint equation that must
holdatequilibrium whentheQ-valuesarecorrect:
MODEL-FREE
(cid:12)
Q(s,a) = R(s)+γ P(s
(cid:2)|s,a)maxQ(s (cid:2)
,a
(cid:2)
). (21.7)
a(cid:3)
s(cid:3)
As in the ADP learning agent, we can use this equation directly as an update equation for
an iteration process that calculates exact Q-values, given an estimated model. This does,
however, require that a model also be learned, because the equation uses P(s
(cid:2)|s,a).
The
temporal-difference approach, on the other hand, requires no model of state transitions—all
844 Chapter 21. Reinforcement Learning
functionQ-LEARNING-AGENT(percept)returnsanaction
inputs:percept,aperceptindicatingthecurrentstates(cid:5)andrewardsignalr(cid:5)
persistent: Q,atableofactionvaluesindexedbystateandaction,initiallyzero
Nsa,atableoffrequenciesforstate–actionpairs,initiallyzero
s,a,r,thepreviousstate,action,andreward,initiallynull
ifTERMINAL?(s)thenQ[s,None]←r(cid:5)
ifs isnotnullthen
incrementNsa[s,a]
Q[s,a]←Q[s,a] + α(Nsa[s,a])(r + γ maxa(cid:3) Q[s(cid:5),a(cid:5)] − Q[s,a])
s,a,r←s(cid:5),argmax
a(cid:3)
f(Q[s(cid:5),a(cid:5)],Nsa[s(cid:5),a(cid:5)]),r(cid:5)
returna
Figure21.8 AnexploratoryQ-learningagent. Itisanactivelearnerthatlearnsthevalue
Q(s,a)of each actionin each situation. Ituses the same explorationfunctionf as the ex-
ploratoryADPagent,butavoidshavingtolearnthetransitionmodelbecausetheQ-valueof
astatecanberelateddirectlytothoseofitsneighbors.
itneedsaretheQvalues. Theupdateequation forTDQ-learningis
Q(s,a) ← Q(s,a)+α(R(s)+γ maxQ(s (cid:2) ,a (cid:2) )−Q(s,a)), (21.8)
a(cid:3)
(cid:2)
whichiscalculated wheneveraction aisexecutedinstatesleading tostates.
The complete agent design for an exploratory Q-learning agent using TD is shown in
Figure 21.8. Notice that it uses exactly the same exploration function f as that used by the
exploratory ADP agent—hence the need to keep statistics on actions taken (the table N). If
asimplerexploration policy isused—say, acting randomly onsome fraction ofsteps, where
thefractiondecreases overtime—thenwecandispense withthestatistics.
Q-learninghasacloserelativecalled SARSA(forState-Action-Reward-State-Action).
SARSA
TheupdateruleforSARSAisverysimilartoEquation(21.8):
Q(s,a) ← Q(s,a)+α(R(s)+γ Q(s (cid:2) ,a (cid:2) )−Q(s,a)), (21.9)
(cid:2) (cid:2)
where a is the action actually taken in state s. The rule is applied at the end of each
(cid:2) (cid:2)
s, a, r, s, a quintuplet—hence the name. The difference from Q-learning is quite subtle:
whereas Q-learning backs up the best Q-value from the state reached in the observed transi-
tion, SARSAwaitsuntilanaction isactually takenand backsuptheQ-valueforthataction.
Now, for a greedy agent that always takes the action with best Q-value, the two algorithms
are identical. When exploration is happening, however, they differ significantly. Because
Q-learning usesthebestQ-value, itpaysnoattention tothe actual policybeing followed—it
isanoff-policylearningalgorithm,whereasSARSAisanon-policyalgorithm. Q-learningis
OFF-POLICY
moreflexiblethanSARSA,inthesensethataQ-learningagentcanlearnhowtobehavewell
ON-POLICY
evenwhenguidedbyarandomoradversarial exploration policy. Ontheotherhand,SARSA
ismorerealistic: forexample,iftheoverallpolicyisevenpartlycontrolledbyotheragents,it
isbettertolearnaQ-functionforwhatwillactuallyhappen ratherthanwhattheagentwould
liketohappen.
Section21.4. Generalization inReinforcement Learning 845
Both Q-learning and SARSA learn the optimal policy for the 4×3 world, but do so
at a much slower rate than the ADP agent. This is because the local updates do not enforce
consistencyamongalltheQ-valuesviathemodel. Thecomparisonraisesageneralquestion:
is it better to learn a model and a utility function or to learn an action-utility function with
no model? In other words, what is the best way to represent the agent function? This is
an issue at the foundations of artificial intelligence. As we stated in Chapter 1, one of the
key historical characteristics of much of AI research is its (often unstated) adherence to the
knowledge-based approach. This amounts to an assumption that the best way to represent
the agent function is to build a representation of some aspects of the environment in which
theagentissituated.
Some researchers, both inside and outside AI, have claimed that the availability of
model-free methods such asQ-learning means thatthe knowledge-based approach isunnec-
essary. Thereis,however,littletogoonbutintuition. Ourintuition,forwhatit’sworth,isthat
as the environment becomes more complex, the advantages of a knowledge-based approach
become more apparent. This is borne out even in games such as chess, checkers (draughts),
and backgammon (see next section), where efforts to learn an evaluation function by means
ofamodelhavemetwithmoresuccessthanQ-learning methods.
21.4 GENERALIZATION IN REINFORCEMENT LEARNING
Sofar, wehave assumed that the utility functions and Q-functions learned by the agents are
represented in tabular form with one output value for each input tuple. Such an approach
worksreasonably wellforsmall statespaces, but thetimeto convergence and(forADP)the
timeperiterationincreaserapidlyasthespacegetslarger. Withcarefullycontrolled, approx-
imate ADP methods, it might be possible to handle 10,000 states or more. This suffices for
two-dimensional maze-like environments, but more realistic worlds are out of the question.
Backgammon and chess are tiny subsets of the real world, yet their state spaces contain on
the order of 1020 and 1040 states, respectively. It would be absurd to suppose that one must
visitallthesestatesmanytimesinordertolearnhowtoplay thegame!
FUNCTION One way to handle such problems is to use function approximation, which simply
APPROXIMATION
means using any sort of representation for the Q-function other than a lookup table. The
representation isviewed asapproximate because itmight not bethe case that the true utility
function orQ-function canberepresented inthechosen form. Forexample, inChapter5we
described an evaluation function for chess that is represented as aweighted linear function
ofasetoffeatures(orbasisfunctions)f ,...,f :
BASISFUNCTION 1 n
Uˆ (s) = θ f (s)+θ f (s)+···+θ f (s).
θ 1 1 2 2 n n
A reinforcement learning algorithm can learn values for the parameters θ=θ ,...,θ such
1 n
that the evaluation function Uˆ approximates the true utility function. Instead of, say, 1040
θ
values in a table, this function approximator is characterized by, say, n=20 parameters—
an enormous compression. Although no one knows the true utility function for chess, no
one believes that it can be represented exactly in 20 numbers. If the approximation is good
846 Chapter 21. Reinforcement Learning
enough, however, theagentmightstillplayexcellent chess.3 Function approximation makes
itpracticaltorepresentutilityfunctionsforverylargestatespaces,butthatisnotitsprincipal
benefit. The compression achieved by a function approximator allows the learning agent to
generalize from states it has visited to states it has not visited. That is, the most important
aspectoffunctionapproximationisnotthatitrequireslessspace,butthatitallowsforinduc-
tive generalization over input states. To give you some idea of the power of this effect: by
examiningonlyoneinevery1012 ofthepossiblebackgammonstates,itispossibletolearna
utilityfunction thatallowsaprogramtoplayaswellasanyhuman(Tesauro,1992).
Ontheflip side, of course, there isthe problem that there could fail tobe any function
in the chosen hypothesis space that approximates the true utility function sufficiently well.
As in all inductive learning, there is a tradeoff between the size of the hypothesis space and
thetimeittakestolearnthefunction. Alargerhypothesis spaceincreases thelikelihood that
agoodapproximation canbefound, butalsomeansthatconvergence islikelytobedelayed.
Letusbeginwiththesimplestcase,whichisdirectutilityestimation. (SeeSection21.2.)
With function approximation, this is an instance of supervised learning. Forexample, sup-
posewerepresenttheutilitiesforthe4×3worldusingasimplelinearfunction. Thefeatures
ofthesquares arejusttheir xandy coordinates, sowehave
Uˆ (x,y) =θ +θ x+θ y . (21.10)
θ 0 1 2
Thus,if(θ ,θ ,θ )=(0.5,0.2,0.1),thenUˆ (1,1)=0.8. Givenacollection oftrials, weob-
0 1 2 θ
tainasetofsamplevaluesofUˆ (x,y),andwecanfindthebestfit,inthesenseofminimizing
θ
thesquarederror, usingstandard linearregression. (SeeChapter18.)
For reinforcement learning, it makes more sense to use an online learning algorithm
that updates the parameters after each trial. Suppose we run a trial and the total reward
obtained starting at (1,1) is 0.4. This suggests that Uˆ (1,1), currently 0.8, is too large and
θ
must be reduced. How should the parameters be adjusted to achieve this? As with neural-
network learning, we write an error function and compute its gradient with respect to the
parameters. If u (s) is the observed total reward from state s onward in the jth trial, then
j
theerrorisdefined as(half) thesquared difference ofthepredicted totalandtheactual total:
E (s) = (Uˆ (s)−u (s))2/2. Therateofchangeoftheerrorwithrespecttoeachparameter
j θ j
θ is∂E /∂θ ,sotomovetheparameterinthedirection ofdecreasing theerror,wewant
i j i
∂E (s) ∂Uˆ (s)
θ ← θ −α j = θ +α(u (s)−Uˆ (s)) θ . (21.11)
i i i j θ
∂θ ∂θ
i i
This is called the Widrow–Hoff rule, or the delta rule, for online least-squares. For the
WIDROW–HOFFRULE
linearfunctionapproximator Uˆ (s)inEquation(21.10),wegetthreesimpleupdaterules:
DELTARULE θ
θ ← θ +α(u (s)−Uˆ (s)),
0 0 j θ
θ ← θ +α(u (s)−Uˆ (s))x,
1 1 j θ
θ ← θ +α(u (s)−Uˆ (s))y .
2 2 j θ
3 WedoknowthattheexactutilityfunctioncanberepresentedinapageortwoofLisp,Java,orC++. Thatis,
itcanberepresentedbyaprogramthatsolvesthegameexactlyeverytimeitiscalled. Weareinterestedonlyin
functionapproximatorsthatuseareasonable amountofcomputation. Itmightinfactbebettertolearnavery
simplefunctionapproximatorandcombineitwithacertainamountoflook-aheadsearch.Thetradeoffsinvolved
arecurrentlynotwellunderstood.
Section21.4. Generalization inReinforcement Learning 847
We can apply these rules to the example where Uˆ (1,1) is 0.8 and u (1,1) is 0.4. θ , θ ,
θ j 0 1
andθ arealldecreased by 0.4α,whichreduces theerrorfor(1,1). Noticethat changing the
2
parametersθinresponsetoanobservedtransitionbetweentwostatesalsochangesthevalues
of Uˆ for every other state! This is what we mean by saying that function approximation
θ
allowsareinforcement learnertogeneralize fromitsexperiences.
We expect that the agent will learn faster if it uses a function approximator, provided
that the hypothesis space is not too large, but includes some functions that are a reasonably
good fit to the true utility function. Exercise 21.5 asks you to evaluate the performance of
directutilityestimation, bothwithandwithoutfunction approximation. Theimprovementin
the4×3worldisnoticeablebutnotdramatic,becausethisisaverysmallstatespacetobegin
with. Theimprovementismuchgreaterina 10×10worldwitha+1rewardat(10,10). This
world is well suited for a linear utility function because the true utility function is smooth
and nearly linear. (See Exercise 21.8.) If we put the +1 reward at (5,5), the true utility is
more like a pyramid and the function approximator in Equation (21.10) will fail miserably.
All is not lost, however! Remember that what matters for linear function approximation
is that the function be linear in the parameters—the features themselves can be arbitrary
no(cid:10)nlinearfunctionsofthestatevariables. Hence,wecanincludeatermsuchasθ
3
f
3
(x,y) =
θ (x−x )2+(y−y )2 thatmeasuresthedistancetothegoal.
3 g g
Wecanapplytheseideasequallywelltotemporal-differencelearners. Allweneeddois
adjusttheparameters totrytoreduce thetemporaldifference betweensuccessive states. The
new versions of the TD and Q-learning equations (21.3 on page 836 and 21.8 on page 844)
aregivenby
∂Uˆ (s)
θ ← θ +α[R(s)+γUˆ (s (cid:2) )−Uˆ (s)] θ (21.12)
i i θ θ
∂θ
i
forutilitiesand
∂Qˆ (s,a)
θ ← θ +α[R(s)+γ maxQˆ (s (cid:2) ,a (cid:2) )−Qˆ (s,a)] θ (21.13)
i i θ θ
a(cid:3) ∂θ
i
forQ-values. ForpassiveTDlearning,theupdaterulecanbeshowntoconvergetotheclosest
possible4 approximation tothetruefunction whenthefunction approximator is linear inthe
parameters. With active learning and nonlinear functions such as neural networks, all bets
are off: There are some very simple cases in which the parameters can go off to infinity
even though there are good solutions in the hypothesis space. There are more sophisticated
algorithms thatcan avoid these problems, but atpresent reinforcement learning withgeneral
function approximators remainsadelicate art.
Function approximation can also be very helpful for learning a model of the environ-
ment. Rememberthatlearning amodelforan observable environment isasupervised learn-
ingproblem,becausethenextperceptgivestheoutcomestate. Anyofthesupervisedlearning
methodsinChapter18canbeused,withsuitableadjustmentsforthefactthatweneedtopre-
dictacompletestatedescriptionratherthanjustaBooleanclassificationorasinglerealvalue.
For a partially observable environment, the learning problem is much more difficult. If we
knowwhatthehiddenvariablesareandhowtheyarecausallyrelatedtoeachotherandtothe
4 Thedefinitionofdistancebetweenutilityfunctionsisrathertechnical;seeTsitsiklisandVanRoy(1997).
848 Chapter 21. Reinforcement Learning
observablevariables,thenwecanfixthestructureofadynamicBayesiannetworkandusethe
EMalgorithm tolearn the parameters, as wasdescribed in Chapter20. Inventing thehidden
variables and learning the model structure are still open problems. Somepractical examples
aredescribed inSection21.6.
21.5 POLICY SEARCH
The final approach we will consider for reinforcement learning problems is called policy
search. In some ways, policy search is the simplest of all the methods in this chapter: the
POLICYSEARCH
ideaistokeeptwiddling thepolicyaslongasitsperformance improves,thenstop.
Letus begin withthe policies themselves. Rememberthat apolicy π isafunction that
mapsstatestoactions. Weareinterestedprimarilyin parameterized representations ofπ that
have far fewer parameters than there are states in the state space (just as in the preceding
section). For example, we could represent π by a collection of parameterized Q-functions,
oneforeachaction, andtaketheactionwiththehighestpredicted value:
π(s) = maxQˆ (s,a). (21.14)
θ
a
Each Q-function could be a linear function of the parameters θ, as in Equation (21.10),
or it could be a nonlinear function such as a neural network. Policy search will then ad-
just the parameters θ to improve the policy. Notice that if the policy is represented by Q-
functions, then policy search results in a process that learns Q-functions. This process is
not the same asQ-learning! In Q-learning with function approximation, the algorithm finds
a value of θ such that Qˆ is “close” to Q ∗ , the optimal Q-function. Policy search, on the
θ
other hand, finds a value of θ that results in good performance; the values found by the two
methods may differ very substantially. (For example, the approximate Q-function defined
by Qˆ (s,a)=Q ∗ (s,a)/10 gives optimal performance, even though it is not at all close to
θ
∗
Q .) Anotherclearinstance ofthe difference isthecase where π(s)iscalculated using, say,
depth-10 look-ahead search with anapproximate utility function Uˆ . Avalue of θ that gives
θ
goodresultsmaybealongwayfrommaking
Uˆ
resemblethetrueutilityfunction.
θ
One problem with policy representations of the kind given in Equation (21.14) is that
thepolicy isadiscontinuous function oftheparameters whentheactions arediscrete. (Fora
continuousactionspace,thepolicycanbeasmoothfunctionoftheparameters.) Thatis,there
willbevaluesofθsuchthataninfinitesimalchangeinθcausesthepolicytoswitchfromone
action to another. This means that the value of the policy may also change discontinuously,
whichmakesgradient-basedsearchdifficult. Forthisreason,policysearchmethodsoftenuse
astochasticpolicyrepresentation π (s,a),whichspecifiestheprobabilityofselectingaction
STOCHASTICPOLICY θ
ainstates. Onepopularrepresentation isthe softmaxfunction:
SOFTMAXFUNCTION
(cid:12)
π (s,a) = eQˆ θ(s,a)/ eQˆ θ(s,a(cid:3)) .
θ
a(cid:3)
Softmax becomes nearly deterministic if one action is much better than the others, but it
always gives adifferentiable function of θ; hence, the value of the policy (which depends in
Section21.5. PolicySearch 849
a continuous fashion on the action selection probabilities) is a differentiable function of θ.
Softmaxisageneralization ofthelogistic function(page725)tomultiplevariables.
Nowletuslookatmethodsforimprovingthepolicy. Westartwiththesimplestcase: a
deterministic policy and a deterministic environment. Let ρ(θ) be the policy value, i.e., the
POLICYVALUE
expectedreward-to-gowhenπ isexecuted. Ifwecanderiveanexpressionforρ(θ)inclosed
θ
form,thenwehaveastandardoptimizationproblem,asdescribedinChapter4. Wecanfollow
the policy gradient vector ∇ ρ(θ) provided ρ(θ) is differentiable. Alternatively, if ρ(θ) is
POLICYGRADIENT θ
not available in closed form, we can evaluate π simply by executing it and observing the
θ
accumulatedreward. Wecanfollowtheempiricalgradientbyhillclimbing—i.e.,evaluating
the change in policy value for small increments in each parameter. With the usual caveats,
thisprocesswillconverge toalocaloptimuminpolicyspace.
When the environment (or the policy) is stochastic, things get more difficult. Suppose
we are trying to do hill climbing, which requires comparing ρ(θ) and ρ(θ +Δθ) for some
small Δθ. The problem is that the total reward on each trial may vary widely, so estimates
of the policy value from a small number of trials will be quite unreliable; trying to compare
twosuch estimates willbeeven moreunreliable. Onesolution issimply to runlots oftrials,
measuring the sample variance and using it to determine that enough trials have been run
to get a reliable indication of the direction of improvement for ρ(θ). Unfortunately, this is
impractical formanyrealproblemswhereeachtrialmaybeexpensive, time-consuming, and
perhapsevendangerous.
Forthecaseofastochastic policyπ (s,a),itispossibletoobtainanunbiasedestimate
θ
of the gradient at θ, ∇ ρ(θ), directly from the results of trials executed at θ. Forsimplicity,
θ
wewillderive thisestimate forthesimple caseofanonsequential environment inwhich the
reward R(a) is obtained immediately after doing action a in the start state s . In this case,
0
thepolicyvalueisjusttheexpected valueofthereward,and wehave
(cid:12) (cid:12)
∇ ρ(θ)= ∇ π (s ,a)R(a) = (∇ π (s ,a))R(a).
θ θ θ 0 θ θ 0
a a
Now we perform a simple trick so that this summation can be approximated by samples
generated from the probability distribution defined by π (s ,a). Suppose that we have N
θ 0
trialsinallandtheactiontakenonthejthtrialisa . Then
j
(cid:12) (∇ π (s ,a))R(a) 1 (cid:12)N (∇ π (s ,a ))R(a )
∇ ρ(θ)= π (s ,a)· θ θ 0 ≈ θ θ 0 j j .
θ θ 0
π (s ,a) N π (s ,a )
θ 0 θ 0 j
a j=1
Thus, the true gradient of the policy value is approximated by a sum of terms involving
the gradient of the action-selection probability in each trial. For the sequential case, this
generalizes to
1 (cid:12)N (∇ π (s,a ))R (s)
∇ ρ(θ)≈ θ θ j j
θ
N π (s,a )
θ j
j=1
for each state s visited, where a is executed in s on the jth trial and R (s) is the total
j j
reward received from state s onwards in the jth trial. The resulting algorithm is called
REINFORCE (Williams, 1992); it is usually much more effective than hill climbing using
lotsoftrialsateachvalueofθ. Itisstillmuchslowerthannecessary, however.
850 Chapter 21. Reinforcement Learning
Consider the following task: given two blackjack5 programs, determine which is best.
One way to do this is to have each play against a standard “dealer” for a certain number of
handsandthentomeasuretheirrespectivewinnings. Theproblemwiththis,aswehaveseen,
isthatthewinningsofeachprogram fluctuatewidelydepending onwhetheritreceivesgood
or bad cards. An obvious solution is to generate a certain number of hands in advance and
have each program play the same set of hands. In this way, we eliminate the measurement
CORRELATED error due to differences in the cards received. This idea, called correlated sampling, un-
SAMPLING
derlies a policy-search algorithm called PEGASUS (Ng and Jordan, 2000). The algorithm is
applicable to domains for which a simulator is available so that the “random” outcomes of
actions canberepeated. Thealgorithm worksbygenerating inadvance N sequences ofran-
domnumbers, eachofwhichcanbeusedtorunatrialofanypolicy. Policysearchiscarried
outbyevaluatingeachcandidatepolicyusingthesamesetofrandomsequencestodetermine
theactionoutcomes. Itcanbeshownthatthenumberofrandomsequencesrequiredtoensure
thatthevalueofeverypolicyiswellestimated depends onlyonthecomplexity ofthepolicy
space,andnotatallonthecomplexityoftheunderlying domain.
21.6 APPLICATIONS OF REINFORCEMENT LEARNING
Wenowturntoexamplesoflarge-scale applications ofreinforcement learning. Weconsider
applicationsingameplaying,wherethetransitionmodelisknownandthegoalistolearnthe
utilityfunction, andinrobotics, wherethemodelisusually unknown.
21.6.1 Applicationsto gameplaying
Thefirstsignificant application ofreinforcement learning wasalsothefirstsignificant learn-
ing program of any kind—the checkers program written by Arthur Samuel (1959, 1967).
Samuel first used a weighted linear function for the evaluation of positions, using up to 16
termsatanyonetime. HeappliedaversionofEquation(21.12)toupdatetheweights. There
weresomesignificantdifferences,however,betweenhisprogramandcurrentmethods. First,
heupdatedtheweightsusingthedifferencebetweenthecurrentstateandthebacked-upvalue
generated byfulllook-ahead inthesearchtree. Thisworksfine,because itamountstoview-
ing the state space at a different granularity. A second difference was that the program did
notuseanyobservedrewards! Thatis,thevaluesofterminalstatesreachedinself-playwere
ignored. Thismeansthatitistheoretically possibleforSamuel’sprogramnottoconverge, or
to converge on a strategy designed to lose rather than to win. Hemanaged to avoid this fate
by insisting that the weight for material advantage should always be positive. Remarkably,
this was sufficient to direct the program into areas of weight space corresponding to good
checkers play.
GerryTesauro’s backgammon program TD-GAMMON (1992) forcefully illustrates the
potential of reinforcement learning techniques. In earlier work (Tesauro and Sejnowski,
1989), Tesauro tried learning a neural network representation of Q(s,a) directly from ex-
5 Alsoknownastwenty-oneorpontoon.
Section21.6. Applications ofReinforcement Learning 851
θ
x
Figure21.9 Setupfortheproblemofbalancingalongpoleontopofamovingcart. The
cartcanbejerkedleftorrightbyacontrollerthatobserves x,θ,x˙,andθ˙.
amples of moves labeled with relative values by a human expert. This approach proved
extremely tedious forthe expert. Itresulted inaprogram, called NEUROGAMMON,thatwas
strong by computer standards, but not competitive with human experts. The TD-GAMMON
project was an attempt to learn from self-play alone. The only reward signal was given at
the end of each game. The evaluation function was represented by a fully connected neural
network with a single hidden layer containing 40 nodes. Simply by repeated application of
Equation (21.12), TD-GAMMON learned toplayconsiderably betterthan NEUROGAMMON,
eventhoughtheinputrepresentation contained justtheraw boardpositionwithnocomputed
features. Thistookabout200,000traininggamesandtwoweeksofcomputertime. Although
that may seem like a lot of games, it is only a vanishingly small fraction of the state space.
Whenprecomputedfeatureswereaddedtotheinputrepresentation,anetworkwith80hidden
nodes wasable, after300,000 training games, toreach astandard ofplay comparable tothat
of the top three human players worldwide. Kit Woolsey, a top player and analyst, said that
“Thereisnoquestion inmymindthatitspositional judgment isfarbetterthanmine.”
21.6.2 Applicationto robot control
The setup for the famous cart–pole balancing problem, also known as the inverted pendu-
CART–POLE
INVERTED lum, is shown in Figure 21.9. The problem is to control the position x of the cart so that
PENDULUM
the pole stays roughly upright (θ ≈ π/2), while staying within the limits of the cart track
as shown. Several thousand papers in reinforcement learning and control theory have been
published on this seemingly simple problem. The cart–pole problem differs from the prob-
lemsdescribedearlierinthatthestatevariables
x,θ,x˙,andθ˙
arecontinuous. Theactionsare
BANG-BANG usuallydiscrete: jerkleftorjerkright,theso-called bang-bangcontrolregime.
CONTROL
The earliest work on learning for this problem was carried out by Michie and Cham-
bers(1968). TheirBOXES algorithm wasabletobalancethepoleforoveranhourafteronly
about30trials. Moreover,unlikemanysubsequentsystems, BOXESwasimplementedwitha
852 Chapter 21. Reinforcement Learning
realcartandpole,notasimulation. Thealgorithmfirstdiscretized thefour-dimensional state
spaceintoboxes—hence thename. Itthenrantrialsuntilthe polefelloverorthecarthitthe
endofthetrack. Negativereinforcement wasassociated withthefinalactioninthefinalbox
and then propagated back through the sequence. It was found that the discretization caused
some problems when the apparatus was initialized in a position different from those used in
training, suggesting that generalization was not perfect. Improved generalization and faster
learning canbeobtained usinganalgorithm that adaptively partitions thestatespaceaccord-
ingtotheobservedvariationinthereward,orbyusingacontinuous-state, nonlinearfunction
approximatorsuchasaneuralnetwork. Nowadays,balancing atripleinvertedpendulum isa
commonexercise—afeatfarbeyondthecapabilities ofmosthumans.
Still more impressive is the application of reinforcement learning to helicopter flight
(Figure 21.10). This work has generally used policy search (Bagnell and Schneider, 2001)
as well as the PEGASUS algorithm with simulation based on a learned transition model (Ng
etal.,2004). FurtherdetailsaregiveninChapter25.
Figure21.10 Superimposedtime-lapse imagesof an autonomoushelicopterperforming
a very difficult “nose-in circle” maneuver. The helicopter is under the control of a policy
developedbythe PEGASUS policy-searchalgorithm. A simulatormodelwas developedby
observingtheeffectsofvariouscontrolmanipulationsontherealhelicopter;thenthealgo-
rithmwasrunonthesimulatormodelovernight.Avarietyofcontrollersweredevelopedfor
differentmaneuvers. In all cases, performance far exceeded that of an expert human pilot
usingremotecontrol.(ImagecourtesyofAndrewNg.)
Section21.7. Summary 853
21.7 SUMMARY
This chapter has examined the reinforcement learning problem: how an agent can become
proficientinanunknownenvironment, givenonlyitsperceptsandoccasional rewards. Rein-
forcement learning canbeviewedasamicrocosm fortheentireAIproblem, butitisstudied
inanumberofsimplifiedsettingstofacilitate progress. Themajorpointsare:
• The overall agent design dictates the kind of information that must be learned. The
three main designs we covered were the model-based design, using a model P and a
utility function U; the model-free design, using an action-utility function Q; and the
reflexdesign, usingapolicyπ.
• Utilitiescanbelearnedusingthreeapproaches:
1. Direct utilityestimation usesthetotal observed reward-to-go foragivenstateas
directevidenceforlearning itsutility.
2. Adaptive dynamic programming (ADP) learns a model and a reward function
from observations and then uses value or policy iteration to obtain the utilities or
an optimal policy. ADP makes optimal use of the local constraints on utilities of
statesimposedthroughtheneighborhood structure oftheenvironment.
3. Temporal-difference(TD)methodsupdateutilityestimatestomatchthoseofsuc-
cessorstates. Theycanbeviewedassimpleapproximations totheADPapproach
thatcanlearnwithoutrequiring atransition model. Usinga learned modeltogen-
eratepseudoexperiences can,however,resultinfasterlearning.
• Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD
approach. With TD, Q-learning requires no model in either the learning or action-
selectionphase. Thissimplifiesthelearningproblembutpotentiallyrestrictstheability
to learn in complex environments, because the agent cannot simulate the results of
possiblecourses ofaction.
• When the learning agent is responsible for selecting actions while it learns, it must
trade off the estimated value of those actions against the potential for learning useful
new information. Anexact solution of the exploration problem is infeasible, but some
simpleheuristics doareasonable job.
• Inlargestatespaces, reinforcement learning algorithmsmustuseanapproximate func-
tional representation in order to generalize over states. The temporal-difference signal
canbeuseddirectly toupdateparameters inrepresentations suchasneuralnetworks.
• Policy-search methods operate directly on a representation of the policy, attempting
to improve it based on observed performance. The variation in the performance in a
stochasticdomainisaseriousproblem;forsimulateddomainsthiscanbeovercomeby
fixingtherandomness inadvance.
Becauseofitspotentialforeliminatinghandcodingofcontrolstrategies,reinforcementlearn-
ing continues to be one of the most active areas of machine learning research. Applications
in robotics promise tobe particularly valuable; these will require methods forhandling con-
854 Chapter 21. Reinforcement Learning
tinuous, high-dimensional, partially observable environments in which successful behaviors
mayconsist ofthousands orevenmillionsofprimitiveactions.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Turing(1948,1950)proposedthereinforcement-learning approach,althoughhewasnotcon-
vincedofitseffectiveness, writing,“theuseofpunishments andrewardscanatbestbeapart
of the teaching process.” Arthur Samuel’s work (1959) was probably the earliest successful
machine learning research. Although this work was informal and had a number of flaws,
it contained most of the modern ideas in reinforcement learning, including temporal differ-
encing and function approximation. Around the same time, researchers in adaptive control
theory(WidrowandHoff,1960),buildingonworkbyHebb(1949),weretrainingsimplenet-
worksusingthedeltarule. (Thisearlyconnectionbetweenneuralnetworksandreinforcement
learning may have led to the persistent misperception that the latter is a subfield of the for-
mer.) Thecart–poleworkofMichieandChambers(1968)canalsobeseenasareinforcement
learningmethodwithafunctionapproximator. Thepsychologicalliteratureonreinforcement
learningismucholder;HilgardandBower(1975)provideagoodsurvey. Directevidencefor
the operation of reinforcement learning in animals has been provided by investigations into
theforagingbehaviorofbees;thereisaclearneuralcorrelateoftherewardsignalintheform
ofalarge neuron mapping from thenectar intake sensors directly tothe motorcortex (Mon-
tague et al., 1995). Research using single-cell recording suggests that the dopamine system
in primate brains implements something resembling value function learning (Schultz et al.,
1997). The neuroscience text by Dayan and Abbott (2001) describes possible neural imple-
mentations of temporal-difference learning, while Dayan and Niv (2008) survey the latest
evidence fromneuroscientific andbehavioral experiments.
The connection between reinforcement learning and Markov decision processes was
first made by Werbos (1977), but the development of reinforcement learning in AI stems
from work at the University of Massachusetts in the early 1980s (Barto et al., 1981). The
paper by Sutton (1988) provides a good historical overview. Equation (21.3) in this chapter
is a special case for λ=0 of Sutton’s general TD(λ) algorithm. TD(λ) updates the utility
valuesofallstatesinasequence leading uptoeachtransition byanamountthatdropsoffas
λt for states t steps in the past. TD(1) is identical to the Widrow–Hoff ordelta rule. Boyan
(2002), building on work by Bradtke and Barto (1996), argues that TD(λ) and related algo-
rithmsmake inefficient useofexperiences; essentially, theyareonline regression algorithms
that converge much more slowly than offline regression. His LSTD (least-squares temporal
differencing) algorithm is an online algorithm for passive reinforcement learning that gives
the same results as offline regression. Least-squares policy iteration, or LSPI (Lagoudakis
and Parr, 2003), combines this idea with the policy iteration algorithm, yielding a robust,
statistically efficient,model-free algorithm forlearningpolicies.
The combination of temporal-difference learning with the model-based generation of
simulated experiences wasproposed inSutton’s DYNAarchitecture (Sutton, 1990). Theidea
of prioritized sweeping was introduced independently by Moore and Atkeson (1993) and
Bibliographical andHistorical Notes 855
PengandWilliams(1993). Q-learningwasdevelopedinWatkins’sPh.D.thesis(1989),while
SARSAappeared inatechnical reportbyRummeryandNiranjan (1994).
Banditproblems, whichmodeltheproblem ofexploration for nonsequential decisions,
areanalyzedindepthbyBerryandFristedt(1985). Optimalexplorationstrategiesforseveral
settings are obtainable using the technique called Gittins indices (Gittins, 1989). A vari-
ety of exploration methods for sequential decision problems are discussed by Barto et al.
(1995). Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithms
thatexploreunknown environments andareguaranteed toconverge onnear-optimal policies
in polynomial time. Bayesian reinforcement learning (Dearden et al., 1998, 1999) provides
anotherangleonbothmodeluncertainty andexploration.
Function approximation in reinforcement learning goes back to the work of Samuel,
whousedbothlinearandnonlinearevaluationfunctionsandalsousedfeature-selectionmeth-
odstoreduce thefeature space. Latermethods include the CMAC(Cerebellar ModelArtic-
CMAC
ulation Controller) (Albus, 1975), which is essentially a sum of overlapping local kernel
functions, and the associative neural networks of Barto et al. (1983). Neural networks are
currently the most popular form of function approximator. The best-known application is
TD-Gammon (Tesauro, 1992, 1995), which was discussed in the chapter. One significant
problem exhibitedbyneural-network-based TDlearners isthattheytendtoforgetearlierex-
periences, especially those in parts of the state space that are avoided once competence is
achieved. Thiscanresultincatastrophic failureifsuchcircumstances reappear. Functionap-
proximation based on instance-based learning can avoid this problem (Ormoneit and Sen,
2002;Forbes,2002).
Theconvergence ofreinforcement learningalgorithmsusingfunction approximation is
an extremely technical subject. Results for TD learning have been progressively strength-
enedforthecaseoflinearfunctionapproximators (Sutton, 1988;Dayan,1992;Tsitsiklisand
VanRoy, 1997), but several examples ofdivergence have been presented fornonlinear func-
tions (see Tsitsiklis and Van Roy, 1997, for a discussion). Papavassiliou and Russell (1999)
describe a new type of reinforcement learning that converges with any form of function ap-
proximator, providedthatabest-fitapproximation canbefoundfortheobserveddata.
PolicysearchmethodswerebroughttotheforebyWilliams(1992),whodevelopedthe
REINFORCE familyofalgorithms. LaterworkbyMarbachandTsitsiklis(1998),Suttonetal.
(2000), and Baxterand Bartlett(2000) strengthened and generalized theconvergence results
forpolicysearch. Themethodofcorrelated sampling forcomparing different configurations
of a system was described formally by Kahn and Marshall (1953), but seems to have been
known long before that. Its use in reinforcement learning is due to Van Roy (1998) and Ng
and Jordan (2000); the latter paper also introduced the PEGASUS algorithm and proved its
formalproperties.
As we mentioned in the chapter, the performance of a stochastic policy is a continu-
ous function of its parameters, which helps with gradient-based search methods. This is not
the only benefit: Jaakkola et al. (1995) argue that stochastic policies actually work better
than deterministic policies in partially observable environments, if both are limited to act-
ing based on the current percept. (One reason is that the stochastic policy is less likely to
get “stuck” because of some unseen hindrance.) Now, in Chapter 17 we pointed out that
856 Chapter 21. Reinforcement Learning
optimal policies in partially observable MDPs are deterministic functions of the belief state
ratherthanthecurrentpercept, sowewouldexpectstillbetterresultsbykeepingtrackofthe
belief state using the filtering methods of Chapter 15. Unfortunately, belief-state space is
high-dimensional and continuous, and effective algorithms have not yet been developed for
reinforcement learningwithbeliefstates.
Real-world environments also exhibit enormous complexity in terms of the number
of primitive actions required to achieve significant reward. For example, a robot playing
soccer might make a hundred thousand individual leg motions before scoring a goal. One
commonmethod,usedoriginallyinanimaltraining,iscalledrewardshaping. Thisinvolves
REWARDSHAPING
supplying the agent with additional rewards, called pseudorewards, for“making progress.”
PSEUDOREWARD
For example, in soccer the real reward is for scoring a goal, but pseudorewards might be
given for making contact with the ball or for kicking it toward the goal. Such rewards can
speed up learning enormously and are simple to provide, but there is a risk that the agent
willlearntomaximizethepseudorewards ratherthanthetruerewards;forexample,standing
next to the ball and “vibrating” causes many contacts with the ball. Ng et al. (1999) show
(cid:2)
that the agent will still learn the optimal policy provided that the pseudoreward F(s,a,s)
satisfies F(s,a,s (cid:2) )=γΦ(s (cid:2) )−Φ(s),where Φisanarbitrary function ofthe state. Φcanbe
constructed to reflect any desirable aspects of the state, such as achievement of subgoals or
distance toagoalstate.
Thegeneration ofcomplexbehaviorscanalsobefacilitated byhierarchicalreinforce-
HIERARCHICAL
mentlearningmethods, whichattempttosolveproblemsatmultiplelevels ofabstraction—
REINFORCEMENT
LEARNING
much like the HTNplanningmethods of Chapter11. Forexample, “scoring agoal” can be
broken down into “obtain possession,” “dribble towards the goal,” and “shoot;” and each of
these can be broken down further into lower-level motor behaviors. The fundamental result
in this area is due to Forestier and Varaiya (1978), who proved that lower-level behaviors
of arbitrary complexity can be treated just like primitive actions (albeit ones that can take
varying amounts of time) from the point of view of the higher-level behavior that invokes
them. Current approaches (Parr and Russell, 1998; Dietterich, 2000; Sutton et al., 2000;
Andre and Russell, 2002) build on this result to develop methods for supplying an agent
withapartial program thatconstrains theagent’s behavior tohaveaparticular hierarchical
PARTIALPROGRAM
structure. The partial-programming language for agent programs extends an ordinary pro-
gramming language by adding primitives for unspecified choices that must be filled in by
learning. Reinforcement learning is then applied to learn the best behavior consistent with
the partial program. The combination of function approximation, shaping, and hierarchical
reinforcement learning hasbeenshowntosolvelarge-scale problems—forexample,policies
thatexecutefor104stepsinstatespacesof10100stateswithbranchingfactorsof1030(Marthi
et al., 2005). One key result (Dietterich, 2000) is that the hierarchical structure provides a
natural additive decomposition oftheoverallutility function intotermsthatdepend onsmall
subsetsofthevariablesdefiningthestatespace. Thisissomewhatanalogoustotherepresen-
tationtheoremsunderlying theconciseness ofBayesnets(Chapter14).
Thetopicofdistributedandmultiagentreinforcementlearningwasnottoucheduponin
thechapterbutisofgreatcurrent interest. Indistributed RL,theaimistodevisemethodsby
whichmultiple,coordinatedagentslearntooptimizeacommonutilityfunction. Forexample,
Bibliographical andHistorical Notes 857
can wedevise methods whereby separate subagents forrobot navigation and robot obstacle
SUBAGENT
avoidance could cooperatively achieve a combined control system that is globally optimal?
Some basic results in this direction have been obtained (Guestrin et al., 2002; Russell and
Zimdars, 2003). The basic idea is that each subagent learns its own Q-function from its
ownstream of rewards. Forexample, arobot-navigation component can receive rewards for
makingprogresstowardsthegoal,whiletheobstacle-avoidance componentreceivesnegative
rewards foreverycollision. Eachglobal decision maximizes thesumofQ-functions andthe
wholeprocessconverges toglobally optimalsolutions.
Multiagent RL is distinguished from distributed RL by the presence of agents who
cannot coordinate their actions (except by explicit communicative acts) and who may not
share the same utility function. Thus, multiagent RL deals with sequential game-theoretic
problems or Markovgames, asdefined inChapter17. Theconsequent requirement forran-
domized policies isnot asignificant complication, aswesawonpage 848. Whatdoes cause
problems is the fact that, while an agent is learning to defeat its opponent’s policy, the op-
ponent is changing its policy to defeat the agent. Thus, the environment is nonstationary
(seepage568). Littman(1994)notedthisdifficultywhenintroducing thefirstRLalgorithms
for zero-sum Markov games. Hu and Wellman (2003) present a Q-learning algorithm for
general-sumgamesthatconvergeswhentheNashequilibrium isunique;whentherearemul-
tipleequilibria, thenotionofconvergence isnotsoeasyto define(Shohametal.,2004).
Sometimestherewardfunction isnoteasytodefine. Consider thetaskofdrivingacar.
There are extreme states (such as crashing the car) that clearly should have a large penalty.
But beyond that, it is difficult to be precise about the reward function. However, it is easy
enough forahumantodriveforawhileandthentellarobot“do itlikethat.” Therobotthen
APPRENTICESHIP has the task of apprenticeship learning; learning from an example of the task done right,
LEARNING
without explicit rewards. Ng etal. (2004) and Coates etal. (2009) show how this technique
works for learning to fly a helicopter; see Figure 25.25 on page 1002 for an example of the
acrobatics the resulting policy is capable of. Russell (1998) describes the task of inverse
INVERSE
reinforcement learning—figuring out what the reward function must be from an example
REINFORCEMENT
LEARNING
path through that state space. Thisis useful as a part of apprenticeship learning, oras a part
ofdoingscience—wecanunderstand ananimalorrobotbyworkingbackwardsfromwhatit
doestowhatitsrewardfunction mustbe.
Thischapterhasdealt onlywithatomicstates—all theagent knowsabout astate isthe
set of available actions and the utilities of the resulting states (or of state-action pairs). But
it is also possible to apply reinforcement learning to structured representations rather than
RELATIONAL
atomicones;thisiscalledrelational reinforcementlearning(Tadepallietal.,2004).
REINFORCEMENT
LEARNING
ThesurveybyKaelblingetal.(1996)providesagoodentrypointtotheliterature. The
textbySuttonandBarto(1998),twoofthefield’spioneers,focusesonarchitecturesandalgo-
rithms,showinghowreinforcement learning weavestogethertheideasoflearning, planning,
and acting. The somewhat more technical work by Bertsekas and Tsitsiklis (1996) gives a
rigorous grounding in the theory of dynamic programming and stochastic convergence. Re-
inforcement learning papers arepublished frequently in MachineLearning, intheJournalof
MachineLearning Research, andintheInternational Conferences onMachine Learning and
theNeuralInformation Processing Systemsmeetings.
858 Chapter 21. Reinforcement Learning
EXERCISES
21.1 Implement apassive learning agent inasimpleenvironment, suchasthe4×3world.
Forthe case of an initially unknown environment model, compare the learning performance
ofthedirect utility estimation, TD,andADPalgorithms. Dothecomparison fortheoptimal
policy and for several random policies. For which do the utility estimates converge faster?
What happens when the size of the environment is increased? (Try environments with and
withoutobstacles.)
21.2 Chapter 17 defined a proper policy for an MDP as one that is guaranteed to reach a
terminal state. Show that it is possible for a passive ADP agent to learn a transition model
for which its policy π is improper even if π is proper for the true MDP; with such models,
the POLICY-EVALUATION step may fail if γ=1. Show that this problem cannot arise if
POLICY-EVALUATION isapplied tothelearnedmodelonlyattheendofatrial.
21.3 Starting withthepassive ADPagent, modifyittouseanapproximate ADPalgorithm
asdiscussed inthetext. Dothisintwosteps:
a. Implementapriorityqueueforadjustments totheutilityestimates. Wheneverastateis
adjusted, all of its predecessors also become candidates for adjustment and should be
added to the queue. Thequeue is initialized with the state from which the most recent
transition tookplace. Allowonlyafixednumberofadjustments.
b. Experiment with various heuristics forordering thepriority queue, examining theiref-
fectonlearning ratesandcomputation time.
21.4 Writeouttheparameterupdateequations forTDlearningwith
(cid:9)
Uˆ(x,y) = θ +θ x+θ y+θ (x−x )2+(y−y )2 .
0 1 2 3 g g
21.5 Implement an exploring reinforcement learning agent that uses direct utility estima-
tion. Make two versions—one with a tabular representation and one using the function ap-
proximatorinEquation(21.10). Comparetheirperformance inthreeenvironments:
a. The4×3worlddescribed inthechapter.
b. A10×10worldwithnoobstacles anda+1rewardat(10,10).
c. A10×10worldwithnoobstacles anda+1rewardat(5,5).
21.6 Devisesuitable features forreinforcement learning instochastic gridworlds(general-
izations ofthe 4×3 world) that contain multiple obstacles and multiple terminal states with
rewardsof +1or−1.
21.7 Extend the standard game-playing environment (Chapter 5) to incorporate a reward
signal. Put two reinforcement learning agents into the environment (they may, of course,
share the agent program) and have them play against each other. Apply the generalized TD
updaterule(Equation(21.12))toupdatetheevaluationfunction. Youmightwishtostartwith
asimplelinearweightedevaluation function andasimplegame,suchastic-tac-toe.
Exercises 859
21.8 Compute the true utility function and the best linear approximation in x and y (as in
Equation(21.10))forthefollowingenvironments:
a. A10×10worldwithasingle +1terminalstateat(10,10).
b. Asin(a),butadda−1terminalstateat(10,1).
c. Asin(b),butaddobstacles in10randomly selectedsquares.
d. Asin(b),butplaceawallstretching from(5,2)to(5,9).
e. Asin(a),butwiththeterminalstateat(5,5).
Theactions aredeterministic movesinthefourdirections. Ineach case, compare the results
using three-dimensional plots. Foreach environment, propose additional features (besides x
andy)thatwouldimprovetheapproximation andshowtheresults.
21.9 Implement the REINFORCE and PEGASUS algorithms and apply them to the 4×3
world,usingapolicyfamilyofyourownchoosing. Commentontheresults.
21.10 Isreinforcementlearninganappropriateabstractmodelforevolution? Whatconnec-
tionexists,ifany,betweenhardwiredrewardsignalsandevolutionary fitness?
22
NATURAL LANGUAGE
PROCESSING
In which we see how to make use of the copious knowledge that is expressed in
naturallanguage.
Homosapiensissetapartfromotherspeciesbythecapacityforlanguage. Somewherearound
100,000yearsago,humanslearnedhowtospeak,andabout7,000yearsagolearnedtowrite.
Althoughchimpanzees, dolphins, andotheranimalshaveshownvocabularies ofhundreds of
signs,onlyhumanscanreliablycommunicateanunboundednumberofqualitativelydifferent
messagesonanytopicusingdiscretesigns.
Of course, there are other attributes that are uniquely human: no other species wears
clothes, creates representational art, or watches three hours of television a day. But when
Alan Turing proposed his Test (see Section 1.1.1), he based it on language, not art or TV.
There are two main reasons why we want our computer agents to be able to process natural
languages: first,tocommunicatewithhumans,atopicwetakeupinChapter23,andsecond,
toacquire information fromwrittenlanguage, thefocusofthischapter.
There are over a trillion pages of information on the Web, almost all of it in natural
KNOWLEDGE language. An agent that wants to do knowledge acquisition needs to understand (at least
ACQUISITION
partially) the ambiguous, messy languages that humans use. We examine the problem from
the point of view of specific information-seeking tasks: text classification, information re-
trieval,andinformationextraction. Onecommonfactorinaddressingthesetasksistheuseof
languagemodels: modelsthatpredicttheprobability distribution oflanguage expressions.
LANGUAGEMODEL
22.1 LANGUAGE MODELS
Formallanguages,suchastheprogramminglanguagesJavaorPython,havepreciselydefined
LANGUAGE language models. A language can be defined as a set of strings; “print(2 + 2)” is a
legal program inthe language Python, whereas “2)+(2 print”isnot. Since there are an
infinitenumberoflegalprograms,theycannotbeenumerated; insteadtheyarespecifiedbya
set of rules called a grammar. Formallanguages also have rules that define the meaning or
GRAMMAR
SEMANTICS semanticsofaprogram;forexample,therulessaythatthe“meaning” of “2 + 2”is4,and
themeaningof“1/0”isthatanerrorissignaled.
860
Section22.1. LanguageModels 861
Natural languages, such as English or Spanish, cannot be characterized as a definitive
set of sentences. Everyone agrees that “Not to be invited is sad” is a sentence of English,
butpeople disagree onthegrammaticality of“Tobenotinvited issad.” Therefore, itismore
fruitful todefineanatural language model asaprobability distribution oversentences rather
than a definitive set. That is, rather than asking if a string of words is or is not a memberof
thesetdefiningthelanguage, weinsteadaskforP(S=words)—whatistheprobability that
arandom sentencewouldbewords.
Naturallanguagesarealsoambiguous. “Hesawherduck”canmeaneitherthathesaw
AMBIGUITY
a waterfowl belonging to her, or that he saw her move to evade something. Thus, again, we
cannot speak ofasingle meaning forasentence, but rather of aprobability distribution over
possible meanings.
Finally, natural languages are difficult to deal with because they are very large, and
constantly changing. Thus, our language models are, at best, an approximation. We start
withthesimplestpossible approximations andmoveupfromthere.
22.1.1 N-gram character models
Ultimately,awrittentextiscomposedofcharacters—letters, digits,punctuation, andspaces
CHARACTERS
in English (and more exotic characters in some other languages). Thus, one of the simplest
languagemodelsisaprobability distribution oversequencesofcharacters. AsinChapter15,
we write P(c ) for the probability of a sequence of N characters, c through c . In one
1:N 1 N
Webcollection,P(“the”)=0.027andP(“zgq”)=0.000000002. Asequenceofwrittensym-
bolsoflength niscalledann-gram(from theGreekrootforwritingorletters), withspecial
case “unigram” for1-gram, “bigram” for2-gram, and “trigram” for3-gram. A model ofthe
N probability distribution of n-lettersequences is thus called an n-gram model. (Butbe care-
-GRAMMODEL
ful: we can have n-gram models over sequences of words, syllables, or other units; not just
overcharacters.)
Ann-gram modelisdefinedasaMarkovchainoforder n−1. Recallfrompage 568
that in a Markov chain the probability of character c depends only on the immediately pre-
i
ceding characters, not on any other characters. So in a trigram model (Markov chain of
order2)wehave
P(c i |c 1:i−1 ) = P(c i |c i−2:i−1 ).
We can define the probability of a sequence of characters P(c ) under the trigram model
1:N
byfirstfactoring withthechainruleandthenusingtheMarkovassumption:
(cid:25)N (cid:25)N
P(c 1:N ) = P(c i |c 1:i−1 )= P(c i |c i−2:i−1 ).
i=1 i=1
Foratrigramcharactermodelinalanguagewith100characters,P(C i |C i−2:i−1 )hasamillion
entries, andcanbeaccurately estimated bycounting charactersequences inabodyoftextof
10 million characters or more. We call a body of text a corpus (plural corpora), from the
CORPUS
Latinwordforbody.
862 Chapter 22. NaturalLanguageProcessing
Whatcanwedowithn-gramcharactermodels? Onetaskforwhichtheyarewellsuited
LANGUAGE islanguageidentification: givenatext,determinewhatnaturallanguageitiswrittenin. This
IDENTIFICATION
is arelatively easy task; evenwith short texts such as “Hello, world” or“Wie geht es dir,” it
iseasy toidentify thefirstasEnglish andthesecond asGerman. Computersystems identify
languages with greater than 99% accuracy; occasionally, closely related languages, such as
SwedishandNorwegian,areconfused.
One approach to language identification is to first build a trigram character model of
each candidate language, P(c i |c i−2:i−1 ,(cid:3)), wherethe variable (cid:3)ranges overlanguages. For
each (cid:3) the model is built by counting trigrams in acorpus of that language. (About 100,000
characters of each language are needed.) That gives us a model of P(Text|Language), but
wewanttoselectthemostprobablelanguagegiventhetext,soweapplyBayes’rulefollowed
bytheMarkovassumption togetthemostprobable language:
(cid:3) ∗ = argmax P((cid:3)|c )
1:N
(cid:3)
= argmax P((cid:3))P(c |(cid:3))
1:N
(cid:3)
(cid:25)N
= argmax P((cid:3)) P(c i |c i−2:i−1 ,(cid:3))
(cid:3)
i=1
Thetrigram model canbe learned from acorpus, but whatabout thepriorprobability P((cid:3))?
Wemayhave someestimate ofthese values; forexample, ifweareselecting arandom Web
pageweknowthatEnglishisthemostlikelylanguageandthattheprobabilityofMacedonian
will be less than 1%. The exact number weselect forthese priors is not critical because the
trigrammodelusuallyselectsonelanguagethatisseveralordersofmagnitudemoreprobable
thananyother.
Other tasks for character models include spelling correction, genre classification, and
named-entity recognition. Genre classification means deciding if a text is a news story, a
legal document, a scientific article, etc. While many features help make this classification,
counts of punctuation and other character n-gram features go a long way (Kessler et al.,
1997). Named-entity recognition is the task of finding names of things in a document and
deciding whatclasstheybelong to. Forexample, inthetext“Mr. Sopersteen wasprescribed
aciphex,”weshouldrecognizethat“Mr. Sopersteen”isthenameofapersonand“aciphex”is
thenameofadrug. Character-level modelsaregoodforthistaskbecause theycanassociate
thecharactersequence “ex ”(“ex”followedbyaspace)withadrugnameand“steen ”with
apersonname,andtherebyidentifywordsthattheyhaveneverseenbefore.
22.1.2 Smoothing n-gram models
The major complication of n-gram models is that the training corpus provides only an esti-
mateofthetrueprobability distribution. Forcommoncharacter sequences suchas“ th”any
Englishcorpuswillgiveagoodestimate: about1.5%ofalltrigrams. Ontheotherhand,“ ht”
is very uncommon—no dictionary words start with ht. It is likely that the sequence would
have acount ofzero inatraining corpus ofstandard English. Does that meanweshould as-
signP(“ th”)=0? Ifwedid,thenthetext“Theprogram issuesanhttprequest” wouldhave
Section22.1. LanguageModels 863
anEnglishprobabilityofzero,whichseemswrong. Wehaveaproblemingeneralization: we
want ourlanguage models to generalize well totexts they haven’t seen yet. Just because we
haveneverseen“ http”before doesnotmeanthatourmodelshould claimthatit isimpossi-
ble. Thus, we will adjust our language model so that sequences that have a count of zero in
thetraining corpuswillbeassignedasmallnonzeroprobability (andtheothercountswillbe
adjusted downward slightly sothat theprobability still sumsto1). Theprocess od adjusting
theprobability oflow-frequency countsiscalled smoothing.
SMOOTHING
ThesimplesttypeofsmoothingwassuggestedbyPierre-SimonLaplaceinthe18thcen-
tury: hesaidthat,inthelackoffurtherinformation,ifarandomBooleanvariableX hasbeen
falseinallnobservationssofarthentheestimateforP(X=true)shouldbe1/(n+2). That
is,heassumesthatwithtwomoretrials, onemightbetrueand onefalse. Laplacesmoothing
(alsocalledadd-onesmoothing)isastepintherightdirection,butperformsrelativelypoorly.
Abetterapproachisabackoffmodel,inwhichwestartbyestimatingn-gramcounts,butfor
BACKOFFMODEL
anyparticularsequencethathasalow(orzero)count,webackoffto(n−1)-grams. Linear
LINEAR
interpolation smoothing is a backoff model that combines trigram, bigram, and unigram
INTERPOLATION
SMOOTHING
modelsbylinearinterpolation. Itdefinestheprobability estimateas
P * (c i |c i−2:i−1 )= λ 3 P(c i |c i−2:i−1 )+λ 2 P(c i |c i−1 )+λ 1 P(c i ),
where λ +λ +λ =1. The parameter values λ can be fixed, or they can be trained with
3 2 1 i
an expectation–maximization algorithm. It is also possible to have the values of λ depend
i
on the counts: if we have a high count of trigrams, then we weigh them relatively more; if
onlyalowcount,thenweputmoreweightonthebigramandunigrammodels. Onecampof
researchers has developed evermore sophisticated smoothing models, while the other camp
suggestsgatheringalargercorpussothatevensimplesmoothingmodelsworkwell. Bothare
gettingatthesamegoal: reducing thevarianceinthelanguage model.
Onecomplication: note that the expression P(c i |c i−2:i−1 ) asks for P(c 1 |c-1:0 ) when
i = 1, but there are no characters before c . We can introduce artificial characters, for
1
example, defining c to be a space character or a special “begin text” character. Or we can
0
fall back on lower-order Markov models, in effect defining c-1:0 to be the empty sequence
andthusP(c
1
|c-1:0 )=P(c
1
).
22.1.3 Model evaluation
With so many possible n-gram models—unigram, bigram, trigram, interpolated smoothing
withdifferent values of λ,etc.—howdoweknow whatmodeltochoose? Wecanevaluate a
model with cross-validation. Split the corpus into a training corpus and a validation corpus.
Determine the parameters of the model from the training data. Then evaluate the model on
thevalidation corpus.
The evaluation can be a task-specific metric, such as measuring accuracy on language
identification. Alternatively wecanhave atask-independent model oflanguage quality: cal-
culate the probability assigned to the validation corpus by the model; the higher the proba-
bility the better. This metric is inconvenient because the probability of a large corpus will
be a very small number, and floating-point underflow becomes an issue. A different way of
describing theprobability ofasequenceiswithameasurecalledperplexity,definedas
PERPLEXITY
864 Chapter 22. NaturalLanguageProcessing
−1
Perplexity(c 1:N )= P(c 1:N ) N .
Perplexity canbethought ofasthereciprocal ofprobability, normalized bysequence length.
Itcanalsobethoughtofastheweightedaveragebranching factorofamodel. Supposethere
are 100 characters in our language, and our model says they are all equally likely. Then for
asequence ofanylength, theperplexity willbe100. Ifsomecharacters aremorelikely than
others, andthemodelreflectsthat,thenthemodelwillhaveaperplexity lessthan100.
22.1.4 N-gram wordmodels
Now weturn to n-gram models overwords rather than characters. Allthe same mechanism
applies equally towordandcharacter models. Themaindifference isthatthevocabulary—
VOCABULARY
the set of symbols that make up the corpus and the model—is larger. There are only about
100 characters in most languages, and sometimes we build character models that are even
more restrictive, for example by treating “A” and “a” as the same symbol or by treating all
punctuation asthesamesymbol. Butwithwordmodelswehaveatleasttensofthousands of
symbols,andsometimesmillions. Thewiderangeisbecauseitisnotclearwhatconstitutesa
word. InEnglishasequenceofletterssurroundedbyspacesisaword,butinsomelanguages,
likeChinese,wordsarenotseparatedbyspaces,andeveninEnglishmanydecisionsmustbe
madetohaveaclearpolicyonwordboundaries: howmanywordsarein“ne’er-do-well”? Or
in“(Tel:1-800-960-5660x123)”?
OUTOF Wordn-grammodelsneedtodealwithoutofvocabularywords. Withcharactermod-
VOCABULARY
els, we didn’t have to worry about someone inventing a new letter of the alphabet.1 But
withwordmodels thereisalwaysthechance ofanewwordthatwasnotseen inthetraining
corpus, so we need to model that explicitly in our language model. This can be done by
adding just one new word to the vocabulary: <UNK>, standing for the unknown word. We
can estimate n-gram counts for <UNK> by this trick: go through the training corpus, and
the first time any individual word appears it is previously unknown, so replace it with the
symbol <UNK>. Allsubsequent appearances of the word remain unchanged. Then compute
n-gram counts forthe corpus as usual, treating <UNK>just like anyother word. Then when
anunknown wordappears inatestset, welook upitsprobability under <UNK>. Sometimes
multiple unknown-word symbols are used, for different classes. For example, any string of
digitsmightbereplaced with<NUM>,oranyemailaddress with<EMAIL>.
To get a feeling for what word models can do, we built unigram, bigram, and trigram
modelsoverthewordsinthisbookandthenrandomlysampledsequences ofwordsfromthe
models. Theresultsare
Unigram: logical areasareconfusion amayrighttriesagentgoalthewas...
Bigram: systemsareverysimilarcomputational approach wouldberepresented ...
Trigram: planning andscheduling areintegrated thesuccess ofnaive bayesmodelis...
Evenwiththissmallsample,itshouldbeclearthattheunigrammodelisapoorapproximation
ofeitherEnglishorthecontentofanAItextbook,andthatthebigramandtrigrammodelsare
1 WiththepossibleexceptionofthegroundbreakingworkofT.Geisel(1955).
Section22.2. TextClassification 865
muchbetter. Themodelsagreewiththisassessment: theperplexity was891fortheunigram
model,142forthebigram modeland91forthetrigrammodel.
With the basics of n-gram models—both character- and word-based—established, we
canturnnowtosomelanguage tasks.
22.2 TEXT CLASSIFICATION
TEXT Wenowconsiderindepththetaskoftextclassification,alsoknownascategorization: given
CLASSIFICATION
atextofsomekind,decidewhichofapredefinedsetofclassesitbelongsto. Languageiden-
tification and genre classification areexamples oftextclassification, asissentiment analysis
(classifying amovieorproductreviewaspositiveornegative)andspamdetection(classify-
SPAMDETECTION
inganemailmessageasspamornot-spam). Since“not-spam” isawkward,researchers have
coined the term ham fornot-spam. We can treat spam detection as a problem in supervised
learning. A training set is readily available: the positive (spam) examples are in my spam
folder, thenegative(ham)examplesareinmyinbox. Hereisanexcerpt:
Spam:WholesaleFashionWatches-57%today.Designerwatchesforcheap...
Spam:YoucanbuyViagraFr$1.85AllMedicationsatunbeatableprices! ...
Spam:WECANTREATANYTHINGYOUSUFFERFROMJUSTTRUSTUS...
Spam:Sta.rtearn*ingthesalaryyo,ud-eservebyo’btainingtheprope,rcrede’ntials!
Ham:Thepracticalsignificanceofhypertreewidthinidentifyingmore...
Ham:Abstract:Wewillmotivatetheproblemofsocialidentityclustering:...
Ham:Goodtoseeyoumyfriend.HeyPeter,Itwasgoodtohearfromyou....
Ham:PDSimpliesconvexityoftheresultingoptimizationproblem(KernelRidge...
From this excerpt we can start to get an idea of what might be good features to include in
the supervised learning model. Word n-grams such as “forcheap” and “You can buy” seem
to be indicators of spam (although they would have a nonzero probability in ham as well).
Character-level features also seem important: spam ismore likely tobe alluppercase and to
havepunctuationembeddedinwords. Apparentlythespammersthoughtthatthewordbigram
“you deserve” would be too indicative of spam, and thus wrote “yo,u d-eserve” instead. A
character model should detect this. We could either create a full character n-gram model
of spam and ham, or we could handcraft features such as “number of punctuation marks
embeddedinwords.”
Note that we have two complementary ways of talking about classification. In the
language-modeling approach, wedefineonen-gramlanguagemodelforP(Message|spam)
bytrainingonthespamfolder,andonemodelforP(Message|ham)bytrainingontheinbox.
Thenwecanclassify anewmessagewithanapplication ofBayes’rule:
argmax P(c|message) = argmax P(message|c)P(c).
c∈{spam,ham} c∈{spam,ham}
where P(c)isestimated justby counting thetotal numberofspam and ham messages. This
approach workswellforspamdetection, justasitdidforlanguage identification.
866 Chapter 22. NaturalLanguageProcessing
In the machine-learning approach we represent the message as a set of feature/value
pairs and apply a classification algorithm h to the feature vector X. We can make the
language-modeling andmachine-learning approachescompatiblebythinkingofthen-grams
as features. This is easiest to see with a unigram model. The features are the words in the
vocabulary: “a,” “aardvark,” ..., and the values are the number of times each word appears
inthemessage. Thatmakesthefeaturevectorlargeandsparse. Ifthereare100,000wordsin
thelanguagemodel,thenthefeaturevectorhaslength100,000,butforashortemailmessage
almostallthefeatures willhavecount zero. Thisunigram representation hasbeen called the
bagofwordsmodel. Youcanthink ofthemodelasputting thewordsofthetraining corpus
BAGOFWORDS
in a bag and then selecting words one at a time. The notion of order of the words is lost; a
unigrammodelgivesthesameprobability toanypermutation ofatext. Higher-order n-gram
modelsmaintainsomelocalnotionofwordorder.
Withbigramsandtrigramsthenumberoffeatures issquared orcubed, andwecanadd
in other, non-n-gram features: the time the message was sent, whether a URL or an image
is part of the message, an ID number for the sender of the message, the sender’s number of
previousspamandhammessages,andsoon. Thechoiceoffeaturesisthemostimportantpart
ofcreatingagoodspamdetector—moreimportantthanthechoiceofalgorithmforprocessing
the features. In part this is because there is a lot of training data, so if we can propose a
feature, the data can accurately determine if it is good or not. It is necessary to constantly
update features, because spam detection is an adversarial task; the spammers modify their
spaminresponse tothespamdetector’s changes.
Itcan be expensive to run algorithms on a very large feature vector, so often aprocess
offeatureselectionisusedtokeeponlythefeaturesthatbestdiscriminatebetweenspamand
FEATURESELECTION
ham. Forexample,thebigram“ofthe”isfrequentinEnglish, andmaybeequallyfrequentin
spam and ham, sothere isno sense incounting it. Often the top hundred orsofeatures do a
goodjobofdiscriminating betweenclasses.
Once we have chosen a set of features, we can apply any of the supervised learning
techniques we have seen; popular ones for text categorization include k-nearest-neighbors,
support vector machines, decision trees, naive Bayes, and logistic regression. All of these
have been applied to spam detection, usually with accuracy in the 98%–99% range. With a
carefully designed featureset,accuracy canexceed99.9%.
22.2.1 Classificationby data compression
Another way to think about classification is as a problem in data compression. A lossless
DATACOMPRESSION
compressionalgorithmtakesasequenceofsymbols,detectsrepeatedpatternsinit,andwrites
a description of the sequence that is more compact than the original. For example, the text
“0.142857142857142857”mightbecompressedto“0.[142857]*3.” Compressionalgorithms
workbybuilding dictionaries ofsubsequences ofthetext,andthenreferring toentries inthe
dictionary. Theexampleherehadonlyonedictionary entry, “142857.”
In effect, compression algorithms are creating a language model. The LZWalgorithm
inparticulardirectlymodelsamaximum-entropyprobabilitydistribution. Todoclassification
bycompression, wefirstlumptogetherallthespamtrainingmessagesandcompressthemas
Section22.3. Information Retrieval 867
aunit. Wedothesamefortheham. Thenwhengiven anewmessage toclassify, weappend
ittothespammessagesandcompresstheresult. Wealsoappend ittothehamandcompress
that. Whichever class compresses better—adds the fewernumber of additional bytes forthe
new message—is the predicted class. The idea is that a spam message will tend to share
dictionaryentrieswithotherspammessagesandthuswillcompressbetterwhenappendedto
acollection thatalready containsthespamdictionary.
Experimentswithcompression-based classificationonsomeofthestandardcorporafor
textclassification—the 20-Newsgroups dataset,theReuters-10Corpora, theIndustry Sector
corpora—indicatethatwhereasrunningoff-the-shelfcompressionalgorithmslikegzip,RAR,
and LZW can be quite slow, their accuracy is comparable to traditional classification algo-
rithms. This is interesting in its own right, and also serves to point out that there is promise
foralgorithmsthatusecharacter n-gramsdirectlywithnopreprocessing ofthetextorfeature
selection: theyseemtobecaptiring somerealpatterns.
22.3 INFORMATION RETRIEVAL
INFORMATION Information retrieval is the task of finding documents that are relevant to a user’s need for
RETRIEVAL
information. The best-known examples of information retrieval systems are search engines
ontheWorldWideWeb. AWebusercantypeaquerysuchas[AIbook]2 intoasearchengine
and see a list of relevant pages. In this section, we will see how such systems are built. An
information retrieval(henceforth IR)systemcanbecharacterized by
IR
1. Acorpusofdocuments. Eachsystemmustdecidewhatitwantstotreatasadocument:
aparagraph, apage,oramultipage text.
2. Queries posed in a query language. A query specifies what the user wants to know.
QUERYLANGUAGE
The query language can be just a list of words, such as [AI book]; or it can specify
a phrase of words that must be adjacent, as in [“AI book”]; it can contain Boolean
operatorsasin[AIANDbook];itcanincludenon-Booleanoperatorssuchas[AINEAR
book]or[AIbooksite:www.aaai.org].
3. Aresultset. ThisisthesubsetofdocumentsthattheIRsystemjudgestoberelevantto
RESULTSET
thequery. Byrelevant, wemeanlikely tobeofusetotheperson whoposed thequery,
RELEVANT
fortheparticularinformation needexpressedinthequery.
4. A presentation of the result set. This can be as simple as a ranked list of document
PRESENTATION
titles or as complex as a rotating color map of the result set projected onto a three-
dimensional space,rendered asatwo-dimensional display.
BOOLEANKEYWORD The earliest IR systems worked on a Boolean keyword model. Each word in the document
MODEL
collection istreated as aBoolean feature that is true ofadocument ifthe word occurs in the
document and false if it does not. So the feature “retrieval” is true for the current chapter
but false for Chapter 15. The query language is the language of Boolean expressions over
2 Wedenote asearch query as[query]. Squarebrackets areused ratherthan quotation markssothat wecan
distinguishthequery[“twowords”]from[twowords].
868 Chapter 22. NaturalLanguageProcessing
features. A document is relevant only if the expression evaluates to true. For example, the
query[information ANDretrieval]istrueforthecurrentchapterandfalseforChapter15.
This model has the advantage of being simple to explain and implement. However,
it has some disadvantages. First, the degree of relevance of a document is a single bit, so
there is no guidance as to how to order the relevant documents for presentation. Second,
Boolean expressions are unfamiliar to users who are not programmers or logicians. Users
find it unintuitive that when they want to know about farming in the states of Kansas and
Nebraska they need to issue the query [farming (Kansas OR Nebraska)]. Third, it can be
hard to formulate anappropriate query, even foraskilled user. Suppose wetry[information
AND retrieval AND models AND optimization] and get an empty result set. We could try
[information ORretrieval ORmodels ORoptimization], butifthat returns too manyresults,
itisdifficulttoknowwhattotrynext.
22.3.1 IR scoring functions
MostIRsystemshaveabandonedtheBooleanmodelandusemodelsbasedonthestatisticsof
BM25SCORING wordcounts. Wedescribethe BM25scoringfunction,whichcomesfromtheOkapiproject
FUNCTION
of Stephen Robertson and Karen Sparck Jones at London’s City College, and has been used
insearchengines suchastheopen-source Luceneproject.
Ascoringfunctiontakesadocumentandaqueryandreturnsanumericscore;themost
relevant documents have the highest scores. In the BM25 function, the score is a linear
weighted combination ofscores foreach ofthe words that makeup thequery. Threefactors
affect the weight of a query term: First, the frequency with which a query term appears in
a document (also known as TF for term frequency). For the query [farming in Kansas],
documents that mention “farming” frequently will have higher scores. Second, the inverse
document frequency oftheterm, or IDF. Theword“in” appears inalmostevery document,
soithasahigh document frequency, andthus alowinverse document frequency, and thusit
isnotasimportanttothequeryas“farming”or“Kansas.” Third,thelengthofthedocument.
Amillion-worddocumentwillprobablymentionallthequery words,butmaynotactuallybe
aboutthequery. Ashortdocument thatmentionsallthewords isamuchbettercandidate.
The BM25 function takes all three of these into account. We assume we have created
an index of the N documents in the corpus so that we can look up TF(q ,d ), the count of
i j
the number of times word q appears in document d . We also assume a table of document
i j
frequency counts, DF(q ), that gives the number of documents that contain the word q .
i i
Then,givenadocument d andaqueryconsisting ofthewords q ,wehave
j 1:N
(cid:12)N TF(q ,d )·(k+1)
BM25(d ,q ) = IDF(q )· i j ,
j 1:N i TF(q ,d )+k·(1−b+b· |dj | )
i=1 i j L
where |d | is the length of document d in words, and L is the average document length
j (cid:2) j
in the corpus: L = |d |/N. We have two parameters, k and b, that can be tuned by
i i
cross-validation; typical values are k = 2.0 and b = 0.75. IDF(q )is the inverse document
i
Section22.3. Information Retrieval 869
frequency ofword q ,givenby
i
N −DF(q )+0.5
i
IDF(q )= log .
i
DF(q )+0.5
i
Of course, it would be impractical to apply the BM25 scoring function to every document
in the corpus. Instead, systems create an index ahead of time that lists, for each vocabulary
INDEX
word,thedocumentsthatcontaintheword. Thisiscalledthehitlistfortheword. Thenwhen
HITLIST
given a query, we intersect the hit lists of the query words and only score the documents in
theintersection.
22.3.2 IR system evaluation
Howdoweknow whetheranIRsystem isperforming well? Weundertake anexperiment in
whichthesystemisgivenasetofqueriesandtheresultsetsarescoredwithrespecttohuman
relevance judgments. Traditionally, therehavebeentwomeasures usedinthescoring: recall
and precision. Weexplain them withthehelp ofanexample. Imagine thatan IRsystem has
returned aresult setforasingle query, forwhich weknow which documents areandare not
relevant, outofacorpusof100documents. Thedocument countsineachcategory aregiven
inthefollowingtable:
Inresultset Notinresultset
Relevant 30 20
Notrelevant 10 40
Precision measures the proportion of documents in the result set that are actually relevant.
PRECISION
Inourexample,theprecision is 30/(30+10)=.75. Thefalsepositive rateis1−.75=.25.
Recall measures the proportion of all the relevant documents in the collection that are in
RECALL
the result set. In our example, recall is 30/(30 +20)=.60. The false negative rate is 1−
.60=.40. Inaverylargedocumentcollection,suchastheWorldWideWeb,recallisdifficult
to compute, because there is no easy way to examine every page on the Web for relevance.
Allwecandoiseitherestimaterecallbysamplingorignorerecallcompletelyandjustjudge
precision. In the case of a Web search engine, there may be thousands of documents in the
result set, so it makes more sense to measure precision for several different sizes, such as
“P@10” (precision in the top 10 results) or“P@50,” rather than to estimate precision in the
entireresultset.
It is possible to trade off precision against recall by varying the size of the result set
returned. Intheextreme, asystem thatreturns everydocument inthedocument collection is
guaranteed arecall of 100%, but willhave low precision. Alternately, asystem could return
a single document and have low recall, but a decent chance at 100% precision. A summary
ofbothmeasuresistheF score,asinglenumberthatistheharmonicmeanofprecision and
1
recall, 2PR/(P +R).
22.3.3 IR refinements
There are many possible refinements to the system described here, and indeed Web search
enginesarecontinually updatingtheiralgorithmsastheydiscovernewapproachesandasthe
Webgrowsandchanges.
870 Chapter 22. NaturalLanguageProcessing
Onecommonrefinementisabettermodeloftheeffectofdocumentlengthonrelevance.
Singhal et al. (1996) observed that simple document length normalization schemes tend to
favor short documents too much and long documents not enough. They propose a pivoted
document length normalization scheme; the idea is that the pivot is the document length at
which the old-style normalization is correct; documents shorter than that get a boost and
longeronesgetapenalty.
The BM25 scoring function uses a word model that treats all words as completely in-
dependent, but we know that some words are correlated: “couch” is closely related to both
“couches” and“sofa.” ManyIRsystemsattempttoaccountforthesecorrelations.
Forexample, ifthequeryis[couch], itwouldbeashametoexcludefromtheresultset
those documents that mention “COUCH” or “couches” but not “couch.” Most IR systems
do case folding of “COUCH” to “couch,” and some use a stemming algorithm to reduce
CASEFOLDING
“couches” to the stem form “couch,” both in the query and the documents. This typically
STEMMING
yields a small increase in recall (on the order of 2% for English). However, it can harm
precision. For example, stemming “stocking” to “stock” will tend to decrease precision for
queries about eitherfootcoverings orfinancial instruments, although itcould improverecall
for queries about warehousing. Stemming algorithms based on rules (e.g., remove “-ing”)
cannot avoid this problem, but algorithms based on dictionaries (don’t remove “-ing” if the
word is already listed in the dictionary) can. While stemming has a small effect in English,
it is more important in other languages. In German, for example, it is not uncommon to
see words like “Lebensversicherungsgesellschaftsangestellter” (life insurance company em-
ployee). Languages suchasFinnish, Turkish, Inuit, andYupikhaverecursive morphological
rulesthatinprinciple generate wordsofunbounded length.
Thenextstepistorecognizesynonyms,suchas“sofa”for“couch.” Aswithstemming,
SYNONYM
this has the potential for small gains in recall, but can hurt precision. A user who gives the
query [Tim Couch] wants to see results about the football player, not sofas. The problem is
that“languagesabhorabsolutesynonymsjustasnatureabhorsavacuum”(Cruse,1986). That
is, anytime there aretwowords thatmean thesamething, speakers ofthelanguage conspire
to evolve the meanings to remove the confusion. Related words that are not synonyms also
play an important role in ranking—terms like “leather”, “wooden,” or “modern” can serve
to confirm that the document really is about “couch.” Synonyms and related words can be
found in dictionaries or by looking for correlations in documents or in queries—if we find
that many users who ask the query [new sofa] follow it up with the query [new couch], we
caninthefuturealter[newsofa]tobe[newsofaORnewcouch].
As a final refinement, IR can be improved by considering metadata—data outside of
METADATA
thetext ofthe document. Examples include human-supplied keywords andpublication data.
OntheWeb,hypertext linksbetweendocuments areacrucialsourceofinformation.
LINKS
22.3.4 The PageRankalgorithm
PageRank3 wasoneofthetwooriginal ideas thatsetGoogle’s search apart from otherWeb
PAGERANK
search engines whenitwasintroduced in1997. (Theotherinnovation wastheuseofanchor
3 ThenamestandsbothforWebpagesandforcoinventorLarryPage(BrinandPage,1998).
Section22.3. Information Retrieval 871
functionHITS(query)returnspages withhubandauthoritynumbers
pages←EXPAND-PAGES(RELEVANT-PAGES(query))
foreachp inpages do
p.AUTHORITY ←1
p.HUB ←1
repeatuntilconvergencedo
foreachp inpages d(cid:2)o
p.AUTHOR(cid:2)ITY ←
i
INLINKi(p).HUB
p.HUB ←
i
OUTLINKi(p).AUTHORITY
NORMALIZE(pages)
returnpages
Figure 22.1 The HITS algorithm for computing hubs and authorities with respect to a
query. RELEVANT-PAGESfetchesthepagesthatmatchthequery,andEXPAND-PAGESadds
ineverypagethatlinkstoorislinkedfromoneoftherelevantpages. NORMALIZEdivides
each page’s score by the sum of the squares of all pages’ scores (separately for both the
authorityandhubsscores).
text—theunderlinedtextinahyperlink—toindexapage,eventhoughtheanchortextwason
adifferent pagethantheonebeingindexed.) PageRankwasinvented tosolvetheproblemof
thetyrannyofTF scores: ifthequeryis[IBM],howdowemakesurethatIBM’shomepage,
ibm.com,isthefirstresult,evenifanotherpagementionstheterm“IBM”morefrequently?
Theideaisthatibm.comhasmanyin-links(linkstothepage),soitshouldberankedhigher:
each in-link is a vote for the quality of the linked-to page. But if we only counted in-links,
thenitwouldbepossible foraWebspammertocreateanetwork ofpages andhavethemall
point to a page of his choosing, increasing the score of that page. Therefore, the PageRank
algorithm is designed to weight links from high-quality sites more heavily. What is a high-
quality site? Onethatislinked tobyotherhigh-quality sites. Thedefinition isrecursive, but
wewillseethattherecursion bottomsoutproperly. ThePageRankforapagepisdefinedas:
(cid:12)
1−d PR(in )
i
PR(p) = +d ,
N C(in )
i
i
where PR(p) is the PageRank of page p, N is the total number of pages in the corpus, in
i
are the pages that link in to p, and C(in ) is the count of the total number of out-links on
i
page in . The constant d is a damping factor. It can be understood through the random
i
RANDOMSURFER surfer model: imagine a Web surfer who starts at some random page and begins exploring.
MODEL
With probability d (we’ll assume d=0.85) the surfer clicks on one of the links on the page
(choosing uniformly among them), and with probability 1−d she gets bored with the page
and restarts on a random page anywhere on the Web. The PageRank of page p is then the
probability that the random surfer will be at page p at any point in time. PageRank can be
computed by an iterative procedure: start with all pages having PR(p)=1, and iterate the
algorithm, updating ranksuntiltheyconverge.
872 Chapter 22. NaturalLanguageProcessing
22.3.5 The HITSalgorithm
The Hyperlink-Induced Topic Search algorithm, also known as “Hubs and Authorities” or
HITS, is another influential link-analysis algorithm (see Figure 22.1). HITS differs from
PageRankinseveralways. First,itisaquery-dependent measure: itratespages withrespect
to a query. That means that it must be computed anew for each query—a computational
burden that most search engines have elected not totake on. Given aquery, HITS firstfinds
a set of pages that are relevant to the query. It does that by intersecting hit lists of query
words, andthen adding pages inthelink neighborhood ofthese pages—pages thatlink toor
arelinkedfromoneofthepagesintheoriginalrelevantset.
Each page in this set is considered an authority on the query to the degree that other
AUTHORITY
pages in the relevant set point to it. A page is considered a hub to the degree that it points
HUB
to other authoritative pages in the relevant set. Just as with PageRank, we don’t want to
merely count the number of links; we want to give more value to the high-quality hubs and
authorities. Thus, as with PageRank, weiterate a process that updates the authority score of
a page to be the sum of the hub scores of the pages that point to it, and the hub score to be
thesum oftheauthority scores ofthe pages itpoints to. Ifwe then normalize thescores and
repeat k times,theprocesswillconverge.
Both PageRank and HITS played important roles in developing our understanding of
Webinformationretrieval. Thesealgorithmsandtheirextensionsareusedinrankingbillions
ofqueries dailyassearch engines steadily develop betterwaysofextracting yetfinersignals
ofsearchrelevance.
22.3.6 Questionanswering
Information retrieval is the task of finding documents that are relevant to a query, where the
QUESTION querymaybeaquestion, orjustatopicareaorconcept. Questionansweringisasomewhat
ANSWERING
different task, in which the query really is a question, and the answer is not a ranked list
of documents but rather a short response—a sentence, or even just a phrase. There have
been question-answering NLP (natural language processing) systems since the 1960s, but
onlysince2001havesuchsystemsusedWebinformation retrievaltoradically increase their
breadthofcoverage.
The ASKMSR system (Bankoetal., 2002) isatypical Web-based question-answering
system. It is based on the intuition that most questions will be answered many times on the
Web, so question answering should be thought of as a problem in precision, not recall. We
don’t have to deal with all the different ways that an answer might be phrased—we only
have to find one of them. For example, consider the query [Who killed Abraham Lincoln?]
Suppose a system had to answer that question with access only to a single encyclopedia,
whoseentryonLincolnsaid
John Wilkes Booth altered history with a bullet. He will foreverbe known as the man
whoendedAbrahamLincoln’slife.
Tousethispassagetoanswerthequestion, thesystem wouldhavetoknowthatendingalife
canbeakilling,that“He”referstoBooth,andseveralother linguistic andsemanticfacts.
Section22.4. Information Extraction 873
ASKMSRdoesnotattemptthiskindofsophistication—itknowsnothingaboutpronoun
reference,oraboutkilling,oranyotherverb. Itdoesknow15differentkindsofquestions,and
howthey canberewritten asqueries toasearch engine. Itknowsthat[Who killed Abraham
Lincoln] canberewritten asthequery[*killed Abraham Lincoln]andas[Abraham Lincoln
waskilled by*]. Itissues theserewritten queries andexamines theresults thatcomeback—
not the full Web pages, just the short summaries of text that appear near the query terms.
Theresultsarebrokeninto1-,2-,and3-gramsandtalliedforfrequencyintheresultsetsand
for weight: an n-gram that came back from a very specific query rewrite (such as the exact
phrase match query [“Abraham Lincoln was killed by *”]) would get more weight than one
from a general query rewrite, such as [Abraham OR Lincoln OR killed]. We would expect
that“JohnWilkesBooth”wouldbeamongthehighlyrankedn-gramsretrieved,butsowould
“AbrahamLincoln”and“theassassination of”and“Ford’sTheatre.”
Once the n-grams are scored, they are filtered by expected type. If the original query
startswith“who,”thenwefilteronnamesofpeople;for“howmany”wefilteronnumbers,for
“when,”onadateortime. Thereisalsoafilterthatsaystheanswershouldnotbepartofthe
question; together these should allow us to return “John Wilkes Booth” (and not “Abraham
Lincoln”)asthehighest-scoring response.
In some cases the answer will be longer than three words; since the components re-
sponses only go up to 3-grams, a longer response would have to be pieced together from
shorter pieces. For example, in a system that used only bigrams, the answer “John Wilkes
Booth”couldbepiecedtogetherfromhigh-scoringpieces“JohnWilkes”and“WilkesBooth.”
At the Text Retrieval Evaluation Conference (TREC), ASKMSR was rated as one of
the top systems, beating out competitors with the ability to do far more complex language
understanding. ASKMSR relies upon the breadth of the content on the Web rather than on
its own depth of understanding. It won’t be able to handle complex inference patterns like
associating “whokilled” with“ended thelifeof.” ButitknowsthattheWebissovastthatit
canaffordtoignorepassages likethatandwaitforasimplepassage itcanhandle.
22.4 INFORMATION EXTRACTION
INFORMATION Informationextractionistheprocessofacquiringknowledgebyskimmingatextandlook-
EXTRACTION
ing for occurrences of a particular class of object and for relationships among objects. A
typical task is to extract instances of addresses from Web pages, with database fields for
street, city, state, and zip code; or instances of storms from weather reports, with fields for
temperature, wind speed, and precipitation. In a limited domain, this can be done with high
accuracy. Asthedomaingetsmoregeneral, morecomplexlinguistic modelsandmorecom-
plex learning techniques are necessary. We will see in Chapter 23 how to define complex
language models of the phrase structure (noun phrases and verb phrases) of English. But so
far there are no complete models of this kind, so for the limited needs of information ex-
traction, we define limited models that approximate the full English model, and concentrate
on just the parts that are needed for the task at hand. The models we describe in this sec-
874 Chapter 22. NaturalLanguageProcessing
tionareapproximations inthesamewaythatthesimple1-CNFlogical modelinFigure7.21
(page271)isanapproximations ofthefull,wiggly,logical model.
In this section we describe six different approaches to information extraction, in order
ofincreasing complexity onseveraldimensions: deterministic tostochastic, domain-specific
togeneral, hand-crafted tolearned, andsmall-scale tolarge-scale.
22.4.1 Finite-stateautomata forinformationextraction
ATTRIBUTE-BASED Thesimplesttypeofinformation extraction system isan attribute-based extraction system
EXTRACTION
thatassumesthattheentiretextreferstoasingleobjectandthetaskistoextractattributes of
that object. For example, we mentioned in Section 12.7 the problem of extracting from the
text “IBM ThinkBook 970. Our price: $399.00” the set of attributes {Manufacturer=IBM,
Model=ThinkBook970, Price=$399.00}. We can address this problem by defining a tem-
plate (also known as a pattern) for each attribute we would like to extract. The template is
TEMPLATE
REGULAR definedbyafinitestateautomaton,thesimplestexampleofwhichistheregularexpression,
EXPRESSION
or regex. Regular expressions are used in Unix commands such as grep, in programming
languages such as Perl, and in word processors such as Microsoft Word. The details vary
slightly from one tool to another and so are best learned from the appropriate manual, but
hereweshowhowtobuilduparegularexpression templatefor pricesindollars:
[0-9] matchesanydigitfrom0to9
[0-9]+ matchesoneormoredigits
[.][0-9][0-9] matchesaperiodfollowedbytwodigits
([.][0-9][0-9])? matchesaperiodfollowedbytwodigits,ornothing
[$][0-9]+([.][0-9][0-9])? matches$249.99or$1.23or$1000000 or...
Templatesareoftendefinedwiththreeparts: aprefixregex,atargetregex,andapostfixregex.
Forprices,thetargetregexisasabove,theprefixwouldlook forstringssuchas“price:” and
the postfix could be empty. The idea is that some clues about an attribute come from the
attribute valueitselfandsomecomefromthesurrounding text.
If a regular expression for an attribute matches the text exactly once, then we can pull
outtheportion ofthetextthatisthevalueoftheattribute. Ifthereisnomatch,allwecando
isgiveadefaultvalueorleavetheattributemissing;butifthereareseveralmatches,weneed
aprocesstochooseamongthem. Onestrategyistohaveseveraltemplatesforeachattribute,
ordered by priority. So, for example, the top-priority template for price might look for the
prefix“ourprice:”;ifthatisnotfound,welookfortheprefix “price:” andifthatisnotfound,
the empty prefix. Another strategy is to take all the matches and find some way to choose
among them. Forexample, we could take the lowest price that is within 50% of the highest
price. Thatwillselect$78.00asthetargetfromthetext“Listprice$99.00, specialsaleprice
$78.00,shipping $3.00.”
RELATIONAL Onestepupfromattribute-based extractionsystemsare relationalextractionsystems,
EXTRACTION
which deal with multiple objects and the relations among them. Thus, when these systems
seethetext“$249.99,”theyneedtodeterminenotjustthatitisaprice,butalsowhichobject
hasthatprice. Atypicalrelational-based extractionsystemisFASTUS,whichhandlesnews
storiesaboutcorporate mergersandacquisitions. Itcanreadthestory
Section22.4. Information Extraction 875
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local
concernandaJapanesetradinghousetoproducegolfclubstobeshippedtoJapan.
andextracttherelations:
e∈JointVentures ∧Product(e,“golf clubs”)∧Date(e,“Friday”)
∧Member(e,“Bridgestone SportsCo”)∧Member(e,“a local concern”)
∧Member(e,“a Japanesetrading house”).
CASCADED
A relational extraction system can be built asa series of cascaded finite-state transducers.
FINITE-STATE
TRANSDUCERS
Thatis,thesystem consists ofaseries ofsmall,efficientfinite-state automata(FSAs),where
each automaton receives text as input, transduces the text into adifferent format, and passes
italongtothenextautomaton. FASTUS consistsoffivestages:
1. Tokenization
2. Complex-wordhandling
3. Basic-group handling
4. Complex-phrase handling
5. Structuremerging
FASTUS’s first stage is tokenization, which segments the stream of characters into tokens
(words, numbers, and punctuation). ForEnglish, tokenization can be fairly simple; just sep-
aratingcharacters atwhitespaceorpunctuation doesafairlygoodjob. Sometokenizers also
dealwithmarkuplanguages suchasHTML,SGML,andXML.
Thesecond stage handles complexwords, including collocations suchas“setup” and
“joint venture,” as well as proper names such as “Bridgestone Sports Co.” These are rec-
ognized by a combination of lexical entries and finite-state grammar rules. For example, a
companynamemightberecognized bytherule
CapitalizedWord+(“Company”|“Co”|“Inc”|“Ltd”)
The third stage handles basic groups, meaning noun groups and verb groups. The idea is
to chunk these into units that will be managed by the later stages. We will see how to write
a complex description of noun and verb phrases in Chapter 23, but here we have simple
rules that only approximate the complexity of English, but have the advantage of being rep-
resentable by finite state automata. The example sentence would emerge from this stage as
thefollowingsequence oftaggedgroups:
1 NG: Bridgestone Sports Co. 10 NG: a local concern
2 VG: said 11 CJ: and
3 NG: Friday 12 NG: a Japanese trading house
4 NG: it 13 VG: to produce
5 VG: had set up 14 NG: golf clubs
6 NG: a joint venture 15 VG: to be shipped
7 PR: in 16 PR: to
8 NG: Taiwan 17 NG: Japan
9 PR: with
HereNGmeansnoungroup,VGisverbgroup, PRispreposition, andCJisconjunction.
876 Chapter 22. NaturalLanguageProcessing
The fourth stage combines the basic groups into complex phrases. Again, the aim
is to have rules that are finite-state and thus can be processed quickly, and that result in
unambiguous (or nearly unambiguous) output phrases. One type of combination rule deals
withdomain-specific events. Forexample,therule
Company+SetUpJointVenture(“with”Company+)?
captures one way to describe the formation of a joint venture. This stage is the first one in
thecascadewheretheoutputisplacedintoadatabasetemplateaswellasbeingplacedinthe
output stream. The final stage merges structures that were built up in the previous step. If
thenextsentence says“Thejointventurewillstartproduction inJanuary,”thenthisstepwill
notice that there are two references to a joint venture, and that they should be merged into
one. Thisisaninstance oftheidentityuncertaintyproblemdiscussed inSection14.6.3.
Ingeneral,finite-statetemplate-basedinformationextractionworkswellforarestricted
domaininwhichitispossible topredetermine whatsubjects willbediscussed, andhowthey
will be mentioned. The cascaded transducer model helps modularize the necessary knowl-
edge, easing construction of the system. These systems work especially well when they are
reverse-engineering textthathasbeengenerated byaprogram. Forexample, ashopping site
ontheWebisgenerated byaprogram thattakesdatabase entriesandformatsthemintoWeb
pages; a template-based extractor then recovers the original database. Finite-state informa-
tionextraction islesssuccessful atrecovering information inhighly variable format, suchas
textwrittenbyhumansonavarietyofsubjects.
22.4.2 Probabilisticmodels forinformationextraction
Wheninformationextractionmustbeattemptedfromnoisyorvariedinput,simplefinite-state
approaches fare poorly. It is too hard to get all the rules and their priorities right; it is better
touseaprobabilistic modelratherthanarule-based model. Thesimplestprobabilistic model
forsequences withhidden stateisthehiddenMarkovmodel,orHMM.
Recall from Section 15.3 that an HMM models a progression through a sequence of
hidden states, x , with an observation e at each step. To apply HMMs to information ex-
t t
traction, we can either build one big HMM for all the attributes or build a separate HMM
for each attribute. We’ll do the second. The observations are the words of the text, and the
hiddenstatesarewhetherweareinthetarget, prefix,orpostfixpartoftheattribute template,
orin the background (not part of a template). Forexample, here is a brief text and the most
probable (Viterbi)pathforthattextfortwoHMMs,onetrainedtorecognize thespeakerina
talkannouncement, andonetrainedtorecognizedates. The“-”indicatesabackground state:
Text: There will be a seminar by Dr. Andrew McCallum on Friday
Speaker: - - - - PRE PRE TARGET TARGET TARGET POST -
Date: - - - - - - - - - PRE TARGET
HMMshavetwobigadvantagesoverFSAsforextraction. First,HMMsareprobabilistic,and
thus tolerant to noise. In a regular expression, if a single expected character is missing, the
regexfailstomatch;withHMMsthereisgracefuldegradationwithmissingcharacters/words,
andwegetaprobabilityindicatingthedegreeofmatch,notjustaBooleanmatch/fail. Second,
Section22.4. Information Extraction 877
dr
who : professor
speaker with 0.99 robert
speak 1.0 ; michael
5409 about mr
appointment how
will
0.99
(
0.76 received
has
w 0.56 is
cavalier
stevens
seminar that christel
reminder 1.0 by l
theater speakers 0.24
artist /
additionally here
0.44
Prefix Target Postfix
Figure 22.2 Hidden Markov model for the speaker of a talk announcement. The two
square states are the target (note the second target state has a self-loop, so the target can
match a string of any length), the fourcircles to the left are the prefix, and the one on the
rightisthepostfix. Foreachstate,onlyafewofthehigh-probabilitywordsareshown.From
FreitagandMcCallum(2000).
HMMscan be trained from data; they don’t require laborious engineering of templates, and
thustheycanmoreeasilybekeptuptodateastextchangesovertime.
Notethatwehaveassumed acertain levelofstructure inourHMMtemplates: theyall
consist of one or more target states, and any prefix states must precede the targets, postfix
states most follow the targets, and other states must be background. This structure makes
it easier to learn HMMs from examples. With a partially specified structure, the forward–
backward algorithm can be used to learn both the transition probabilities P(X t |X t−1 ) be-
tween states and the observation model, P(E |X ), which says how likely each word is in
t t
each state. For example, the word “Friday” would have high probability in one or more of
thetargetstatesofthedateHMM,andlowerprobability elsewhere.
Withsufficienttrainingdata,theHMMautomaticallylearnsastructureofdatesthatwe
findintuitive: thedateHMMmighthaveonetargetstateinwhichthehigh-probability words
are “Monday,” “Tuesday,” etc., and which has a high-probability transition to a target state
with words “Jan”, “January,” “Feb,” etc. Figure 22.2 shows the HMM for the speaker of a
talk announcement, as learned from data. The prefix covers expressions such as “Speaker:”
and “seminar by,” and the target has one state that covers titles and first names and another
statethatcoversinitialsandlastnames.
Once the HMMs have been learned, we can apply them to a text, using the Viterbi
algorithm to find the most likely path through the HMM states. One approach is to apply
each attribute HMM separately; in this case you would expect most of the HMMs to spend
most of their time in background states. This is appropriate when the extraction is sparse—
whenthenumberofextracted wordsissmallcomparedtothelengthofthetext.
878 Chapter 22. NaturalLanguageProcessing
TheotherapproachistocombinealltheindividualattributesintoonebigHMM,which
wouldthenfindapaththatwandersthrough different target attributes, firstfindingaspeaker
target, then a date target, etc. Separate HMMs are better when we expect just one of each
attribute in a text and one big HMM is better when the texts are more free-form and dense
with attributes. With either approach, in the end we have a collection of target attribute
observations, and have to decide what to do with them. If every expected attribute has one
target filler then the decision is easy: we have an instance of the desired relation. If there
aremultiple fillers,weneedtodecide whichtochoose, aswediscussed withtemplate-based
systems. HMMs have the advantage of supplying probability numbers that can help make
thechoice. Ifsometargets aremissing,weneedtodecideifthisisaninstance ofthedesired
relationatall,orifthetargetsfoundarefalsepositives. Amachinelearningalgorithmcanbe
trainedtomakethischoice.
22.4.3 Conditionalrandom fields forinformationextraction
One issue with HMMs for the information extraction task is that they model a lot of prob-
abilities that we don’t really need. An HMM is a generative model; it models the full joint
probability ofobservations andhiddenstates,andthuscanbeusedtogeneratesamples. That
is, we can use the HMM model not only to parse a text and recover the speaker and date,
butalsotogenerate arandom instance ofatextcontaining aspeakerandadate. Sincewe’re
not interested in that task, it is natural to ask whether we might be better off with a model
that doesn’t bother modeling that possibility. All we need in order to understand a text is a
discriminative model, one that models the conditional probability of the hidden attributes
given the observations (the text). Given a text e , the conditional model finds the hidden
1:N
statesequence X thatmaximizesP(X |e ).
1:N 1:N 1:N
Modeling this directly gives us some freedom. We don’t need the independence as-
sumptions of the Markov model—we can have an x that is dependent on x . A framework
t 1
CONDITIONAL forthistype of modelis the conditional random field,orCRF,which models aconditional
RANDOMFIELD
probability distribution of a set of target variables given a set of observed variables. Like
Bayesiannetworks,CRFscanrepresentmanydifferentstructuresofdependenciesamongthe
LINEAR-CHAIN
variables. One common structure is the linear-chain conditional random field for repre-
CONDITIONAL
RANDOMFIELD
sentingMarkovdependencies amongvariables inatemporalsequence. Thus,HMMsarethe
temporal version of naive Bayes models, and linear-chain CRFs are the temporal version of
logistic regression, where thepredicted target isan entire state sequence ratherthan asingle
binaryvariable.
Lete be the observations (e.g., words in a document), and x be the sequence of
1:N 1:N
hidden states (e.g., the prefix, target, and postfix states). A linear-chain conditional random
fielddefinesaconditional probability distribution:
P
P(x |e ) = αe [ N i=1 F(x i−1,x i,e,i)] ,
1:N 1:N
whereαisanormalizationfactor(tomakesuretheprobabilitiessumto1),andF isafeature
function definedastheweightedsumofacollection ofk component featurefunctions:
(cid:12)
F(x i−1 ,x i ,e,i) = λ k f k (x i−1 ,x i ,e,i).
k
Section22.4. Information Extraction 879
The λ parameter values are learned with aMAP (maximum a posteriori) estimation proce-
k
durethatmaximizestheconditional likelihood ofthetraining data. Thefeaturefunctions are
thekeycomponentsofaCRF.Thefunctionf k hasaccesstoapairofadjacentstates,x i−1 and
x ,butalsotheentireobservation(word)sequence e,andthecurrentpositioninthetemporal
i
sequence, i. This gives us a lot of flexibility in defining features. We can define a simple
feature function, forexample one that produces avalue of1ifthe current wordis ANDREW
andthecurrentstateis SPE(cid:24)AKER:
f 1 (x i−1 ,x i ,e,i) = 0 1 o if th x e i r = wis S e PEAKER ande i = ANDREW
Howarefeaturesliketheseused? Itdependsontheircorresponding weights. Ifλ > 0,then
1
whenever f is true, it increases the probability of the hidden state sequence x . This is
1 1:N
another way of saying “the CRFmodel should prefer the target state SPEAKER for the word
ANDREW.” If on the other hand λ
1
< 0, the CRF model will try to avoid this association,
andifλ = 0,thisfeatureisignored. Parametervaluescanbesetmanuallyorcanbelearned
1
fromdata. Nowconsiderasecondfeaturefunction:
(cid:24)
f 2 (x i−1 ,x i ,e,i) = 0 1 o if th x e i r = wis S e PEAKER ande i+1 = SAID
This feature is true if the current state is SPEAKER and the next word is “said.” One would
therefore expect apositive λ valuetogowiththefeature. Moreinterestingly, notethatboth
2
f and f can hold at the same time for a sentence like “Andrew said ....” In this case, the
1 2
two features overlap each other and both boost the belief in x
1
= SPEAKER. Because of the
independence assumption, HMMscannot useoverlapping features; CRFscan. Furthermore,
a feature in a CRFcan use any part of the sequence e . Features can also be defined over
1:N
transitionsbetweenstates. Thefeatureswedefinedherewerebinary,butingeneral,afeature
functioncanbeanyreal-valuedfunction. Fordomainswherewehavesomeknowledgeabout
the types of features we would like to include, the CRF formalism gives us a great deal of
flexibility in defining them. This flexibility can lead to accuracies that are higher than with
lessflexiblemodelssuchasHMMs.
22.4.4 Ontologyextractionfrom largecorpora
So far we have thought of information extraction as finding a specific set of relations (e.g.,
speaker, time, location) in a specific text (e.g., a talk announcement). A different applica-
tion of extraction technology is building a large knowledge base or ontology of facts from
a corpus. This is different in three ways: First it is open-ended—we want to acquire facts
about all types of domains, not just one specific domain. Second, with a large corpus, this
taskisdominatedbyprecision, notrecall—justaswithquestionansweringontheWeb(Sec-
tion 22.3.6). Third, the results can be statistical aggregates gathered from multiple sources,
ratherthanbeingextractedfromonespecifictext.
For example, Hearst (1992) looked at the problem of learning an ontology of concept
categories and subcategories from a large corpus. (In 1992, a large corpus was a 1000-page
encyclopedia; todayitwouldbea100-million-page Webcorpus.) Theworkconcentrated on
templates that are very general (not tied to a specific domain) and have high precision (are
880 Chapter 22. NaturalLanguageProcessing
almostalwayscorrectwhentheymatch)butlowrecall(donot alwaysmatch). Hereisoneof
themostproductive templates:
NP suchasNP (,NP)*(,)? ((and|or)NP)? .
Here the bold words and commas must appear literally in the text, but the parentheses are
for grouping, the asterisk means repetition of zero or more, and the question mark means
optional. NP is avariable standing fora noun phrase; Chapter 23 describes how to identify
nounphrases;fornowjustassumethatweknowsomewordsarenounsandotherwords(such
as verbs) that we can reliably assume are not part of a simple noun phrase. This template
matches the texts “diseases such asrabies affect your dog” and “supports network protocols
such as DNS,” concluding that rabies is a disease and DNS is a network protocol. Similar
templatescanbeconstructed withthekeywords“including,”“especially,”and“orother.” Of
course these templates will fail to match many relevant passages, like “Rabies is a disease.”
Thatisintentional. The“NP isaNP”templatedoesindeedsometimesdenoteasubcategory
relation, but it often means something else, as in “There is a God” or “She is a little tired.”
Withalargecorpuswecanaffordtobepicky;touseonlythehigh-precision templates. We’ll
miss many statements of a subcategory relationship, but most likely we’ll find a paraphrase
ofthestatementsomewhereelseinthecorpusinaformwecanuse.
22.4.5 Automated templateconstruction
Thesubcategory relationissofundamentalthatisworthwhiletohandcraftafewtemplatesto
helpidentify instances ofitoccurring innaturallanguage text. Butwhataboutthethousands
of other relations in the world? There aren’t enough AI grad students in the world to create
anddebug templates forallofthem. Fortunately, itispossible tolearntemplates fromafew
examples,thenusethetemplatestolearnmoreexamples,fromwhichmoretemplatescanbe
learned,andsoon. Inoneofthefirstexperimentsofthiskind,Brin(1999)startedwithadata
setofjustfiveexamples:
(“IsaacAsimov”,“TheRobotsofDawn”)
(“DavidBrin”,“StartideRising”)
(“JamesGleick”,“Chaos—MakingaNewScience”)
(“CharlesDickens”,“GreatExpectations”)
(“WilliamShakespeare”,“TheComedyofErrors”)
Clearlytheseareexamplesoftheauthor–title relation,butthelearningsystemhadnoknowl-
edge of authors or titles. The words in these examples were used in a search over a Web
corpus, resulting in199matches. Eachmatchisdefinedasatupleofsevenstrings,
(Author, Title,Order,Prefix,Middle,Postfix,URL),
where Order is true if the author came first and false if the title came first, Middle is the
characters between theauthorandtitle, Prefixisthe 10characters before the match, Suffix is
the10characters afterthematch,and URListheWebaddresswherethematchwasmade.
Given a set of matches, a simple template-generation scheme can find templates to
explainthematches. Thelanguage oftemplateswasdesigned tohaveaclosemappingtothe
matches themselves, tobeamenable toautomated learning, andtoemphasize high precision
Section22.4. Information Extraction 881
(possibly at the risk of lower recall). Each template has the same seven components as a
match. The Author and Title are regexes consisting of any characters (but beginning and
ending in letters) and constrained to have a length from half the minimum length of the
examples to twice the maximum length. The prefix, middle, and postfix are restricted to
literal strings, not regexes. The middle is the easiest to learn: each distinct middle string in
the set of matches is a distinct candidate template. For each such candidate, the template’s
Prefixisthendefinedasthelongestcommonsuffixofalltheprefixesinthematches,andthe
Postfixisdefinedasthelongestcommonprefixofallthepostfixesinthematches. Ifeitherof
these is of length zero, then the template is rejected. The URL of the template is defined as
thelongestprefixoftheURLsinthematches.
In the experiment run by Brin, the first 199 matches generated three templates. The
mostproductivetemplatewas
<LI><B>Title</B> byAuthor(
URL:www.sff.net/locus/c
Thethreetemplateswerethenusedtoretrieve4047more(author,title)examples. Theexam-
ples were then used to generate more templates, and so on, eventually yielding over 15,000
titles. Givenagood setoftemplates, thesystem can collect agood setofexamples. Givena
goodsetofexamples,thesystemcanbuildagoodsetoftemplates.
The biggest weakness in this approach is the sensitivity to noise. If one of the first
few templates isincorrect, errors can propagate quickly. One waytolimit this problem isto
not accept a new example unless it is verified by multiple templates, and not accept a new
templateunlessitdiscoversmultipleexamplesthatarealsofoundbyothertemplates.
22.4.6 Machinereading
Automatedtemplateconstructionisabigstepupfromhandcraftedtemplateconstruction, but
itstill requires ahandful oflabeled examples ofeachrelation toget started. Tobuild alarge
ontology withmanythousands ofrelations, eventhatamount ofworkwould beonerous; we
wouldliketohaveanextractionsystemwithnohumaninputofanykind—asystemthatcould
readonitsownandbuildupitsowndatabase. Suchasystemwouldberelation-independent;
would work for any relation. In practice, these systems work on all relations in parallel,
becauseoftheI/Odemandsoflargecorpora. Theybehaveless likeatraditionalinformation-
extraction system thatistargeted atafewrelations andmorelikeahumanreaderwholearns
fromthetextitself;becauseofthisthefieldhasbeencalled machinereading.
MACHINEREADING
Arepresentative machine-reading systemisTEXTRUNNER (BankoandEtzioni,2008).
TEXTRUNNER uses cotraining toboost itsperformance, but itneeds something to bootstrap
from. InthecaseofHearst(1992),specificpatterns(e.g.,suchas)providedthebootstrap,and
forBrin (1998), itwasaset offiveauthor–title pairs. For TEXTRUNNER, the original inspi-
ration wasataxonomy ofeight verygeneral syntactic templates, asshowninFigure 22.3. It
wasfeltthatasmallnumberoftemplateslikethiscouldcovermostofthewaysthatrelation-
shipsareexpressedinEnglish. Theactualbootsrappingstartsfromasetoflabelledexamples
that are extracted from the PennTreebank, acorpus of parsed sentences. Forexample, from
theparseofthesentence “Einstein received theNobelPrize in1921,” TEXTRUNNER isable
882 Chapter 22. NaturalLanguageProcessing
toextracttherelation(“Einstein,” “received,”“NobelPrize”).
Given a set of labeled examples of this type, TEXTRUNNER trains a linear-chain CRF
to extract further examples from unlabeled text. The features in the CRF include function
words like “to” and “of” and “the,” but not nouns and verbs (and not noun phrases or verb
phrases). Because TEXTRUNNER is domain-independent, it cannot rely on predefined lists
ofnounsandverbs.
Type Template Example Frequency
Verb NP Verb NP XestablishedY 38%
1 2
Noun–Prep NP NP Prep NP XsettlementwithY 23%
1 2
Verb–Prep NP Verb Prep NP XmovedtoY 16%
1 2
Infinitive NP toVerb NP XplanstoacquireY 9%
1 2
Modifier NP Verb NP Noun XisYwinner 5%
1 2
Noun-Coordinate NP (,|and|-|:)NP NP X-Ydeal 2%
1 2
Verb-Coordinate NP (,|and)NP Verb X,Ymerge 1%
1 2
Appositive NP NP (:|,)? NP Xhometown: Y 1%
1 2
Figure22.3 Eightgeneraltemplatesthatcoverabout95%ofthewaysthat relationsare
expressedinEnglish.
TEXTRUNNER achieves a precision of 88% and recall of 45% (F
1
of 60%) on a large
Web corpus. TEXTRUNNER has extracted hundreds of millions of facts from a corpus of a
half-billion Web pages. Forexample, even though it has no predefined medical knowledge,
ithasextracted over2000answerstothequery[whatkillsbacteria]; correct answersinclude
antibiotics, ozone, chlorine, Cipro, andbroccoli sprouts. Questionable answersinclude “wa-
ter,” whichcamefrom thesentence “Boiling waterforatleast 10minutes willkillbacteria.”
Itwouldbebettertoattribute thisto“boilingwater”ratherthanjust“water.”
Withthetechniques outlined inthischapterandcontinual newinventions, wearestart-
ingtogetclosertothegoalofmachinereading.
22.5 SUMMARY
Themainpointsofthischapterareasfollows:
• Probabilistic language models based on n-grams recover a surprising amount of infor-
mation about a language. They can perform well on such diverse tasks as language
identification, spellingcorrection, genreclassification, andnamed-entity recognition.
• These language models can have millions of features, so feature selection and prepro-
cessingofthedatatoreducenoiseisimportant.
• Text classification can be done with naive Bayes n-gram models or with any of the
classification algorithms wehave previously discussed. Classification canalso beseen
asaproblem indatacompression.
Bibliographical andHistorical Notes 883
• Information retrieval systems use a very simple language model based on bags of
words, yet still manage to perform well in terms of recall and precision on very large
corporaoftext. OnWebcorpora, link-analysis algorithmsimproveperformance.
• Questionansweringcanbehandledbyanapproachbasedoninformationretrieval,for
questions that have multiple answers in the corpus. When more answers are available
inthecorpus, wecanusetechniques thatemphasizeprecision ratherthanrecall.
• Information-extraction systems use a more complex model that includes limited no-
tions of syntax and semantics in the form of templates. They can be built from finite-
stateautomata,HMMs,orconditionalrandomfields,andcanbelearnedfromexamples.
• Inbuildingastatisticallanguagesystem,itisbesttodeviseamodelthatcanmakegood
useofavailable data,evenifthemodelseemsoverlysimplistic.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
N-gram letter models for language modeling were proposed by Markov (1913). Claude
Shannon (Shannon and Weaver, 1949) was the first to generate n-gram word models of En-
glish. Chomsky(1956,1957)pointedoutthelimitationsoffinite-statemodelscomparedwith
context-free models, concluding, “Probabilistic models give no particular insight into some
ofthebasicproblemsofsyntacticstructure.” Thisistrue, butprobabilisticmodelsdoprovide
insight into some other basic problems—problems that context-free models ignore. Chom-
sky’sremarkshadtheunfortunateeffectofscaringmanypeopleawayfromstatisticalmodels
fortwodecades, untilthesemodelsreemerged foruseinspeechrecognition (Jelinek, 1976).
Kessleretal.(1997)showhowtoapplycharactern-grammodelstogenreclassification,
and Kleinet al. (2003) describe named-entity recognition with character models. Franz and
Brants(2006) describe theGoogle n-gram corpusof13millionunique wordsfrom atrillion
wordsofWebtext;itisnowpublicly available. Thebagofwordsmodelgetsitsnamefrom
a passage from linguist Zellig Harris (1954), “language is not merely a bag of words but
a tool with particular properties.” Norvig (2009) gives some examples of tasks that can be
accomplished withn-grammodels.
Add-onesmoothing,firstsuggestedbyPierre-SimonLaplace(1816),wasformalizedby
Jeffreys (1948), and interpolation smoothing is due to Jelinek and Mercer (1980), who used
it for speech recognition. Other techniques include Witten–Bell smoothing (1991), Good–
Turing smoothing (Church and Gale, 1991) and Kneser–Ney smoothing (1995). Chen and
Goodman(1996)andGoodman(2001)surveysmoothing techniques.
Simple n-gram letter and word models are not the only possible probabilistic models.
Blei et al. (2001) describe a probabilistic text model called latent Dirichlet allocation that
viewsadocumentasamixtureoftopics,eachwithitsowndistribution ofwords. Thismodel
can be seen as an extension and rationalization of the latent semantic indexing model of
(Deerwester et al., 1990) (see also Papadimitriou et al. (1998)) and is also related to the
multiple-cause mixturemodelof(Sahamietal.,1996).
884 Chapter 22. NaturalLanguageProcessing
ManningandSchu¨tze(1999)andSebastiani(2002)surveytext-classificationtechniques.
Joachims (2001) uses statistical learning theory and support vector machines to give a theo-
reticalanalysisofwhenclassificationwillbesuccessful. Apte´etal.(1994)reportanaccuracy
of96% inclassifying Reutersnewsarticles intothe“Earnings” category. KollerandSahami
(1997)reportaccuracyupto95%withanaiveBayesclassifier,andupto98.6%withaBayes
classifier that accounts for some dependencies among features. Lewis (1998) surveys forty
years of application of naive Bayes techniques to text classification and retrieval. Schapire
and Singer (2000) show that simple linear classifiers can often achieve accuracy almost as
good as more complex models and are more efficient to evaluate. Nigamet al. (2000) show
how to use the EM algorithm to label unlabeled documents, thus learning a better classifi-
cation model. Witten et al. (1999) describe compression algorithms for classification, and
show the deep connection between the LZW compression algorithm and maximum-entropy
language models.
Many of the n-gram model techniques are also used in bioinformatics problems. Bio-
statisticsandprobabilisticNLParecomingclosertogether,aseachdealswithlong,structured
sequences chosenfromanalphabet ofconstituents.
The field of information retrieval is experiencing a regrowth in interest, sparked by
the wide usage of Internet searching. Robertson (1977) gives an early overview and intro-
duces the probability ranking principle. Croft et al. (2009) and Manning et al. (2008) are
the firsttextbooks to coverWeb-based search as wellas traditional IR.Hearst (2009) covers
user interfaces for Web search. The TREC conference, organized by the U.S. government’s
National Institute of Standards and Technology (NIST), hosts an annual competition for IR
systems and publishes proceedings with results. In the first seven years of the competition,
performance roughly doubled.
ThemostpopularmodelforIRisthevectorspacemodel(Saltonetal.,1975). Salton’s
work dominated the early years of the field. There are two alternative probabilistic models,
one due to Ponte and Croft (1998) and one by Maron and Kuhns (1960) and Robertson and
Sparck Jones (1976). Lafferty and Zhai (2001) show that the models are based on the same
joint probability distribution, but that the choice of model has implications for training the
parameters. Craswelletal.(2005)describetheBM25scoringfunctionandSvoreandBurges
(2009)describehowBM25canbeimprovedwithamachinelearning approachthatincorpo-
ratesclickdata—examples ofpastsearchqueiesandtheresultsthatwereclickedon.
Brin and Page (1998) describe the PageRank algorithm and the implementation of a
Websearchengine. Kleinberg (1999)describes theHITSalgorithm. Silverstein etal.(1998)
investigate a log of a billion Web searches. The journal Information Retrieval and the pro-
ceedings oftheannualSIGIRconference coverrecentdevelopments inthefield.
Earlyinformation extraction programs include GUS (Bobrow etal.,1977) and FRUMP
(DeJong, 1982). Recentinformation extraction hasbeenpushed forwardbytheannual Mes-
sage Understand Conferences (MUC), sponsored by the U.S. government. The FASTUS
finite-state system was done by Hobbs et al. (1997). It was based in part on the idea from
Pereira and Wright (1991) of using FSAs as approximations to phrase-structure grammars.
Surveys of template-based systems are given by Roche and Schabes (1997), Appelt (1999),
Exercises 885
and Muslea (1999). Large databases of facts were extracted by Craven et al. (2000), Pasca
etal.(2006),Mitchell(2007), andDurmeandPasca(2008).
Freitag and McCallum (2000) discuss HMMs for Information Extraction. CRFs were
introduced by Lafferty et al. (2001); an example of their use for information extraction is
describedin(McCallum,2003)andatutorialwithpractical guidanceisgivenby(Suttonand
McCallum,2007). Sarawagi(2007)givesacomprehensive survey.
Banko et al. (2002) present the ASKMSR question-answering system; a similar sys-
tem is due to Kwok et al. (2001). Pasca and Harabagiu (2001) discuss a contest-winning
question-answering system. Twoearly influential approaches toautomated knowledge engi-
neeringwerebyRiloff(1993),whoshowedthatanautomatically constructed dictionaryper-
formedalmostaswellasacarefullyhandcrafteddomain-specificdictionary,andbyYarowsky
(1995),whoshowedthatthetaskofwordsenseclassification(seepage756)couldbeaccom-
plishedthroughunsupervised trainingonacorpusofunlabeledtextwithaccuracyasgoodas
supervised methods.
Theideaofsimultaneouslyextractingtemplatesandexamplesfromahandfuloflabeled
examples was developed independently and simultaneously by Blum and Mitchell (1998),
who called it cotraining and by Brin (1998), who called it DIPRE (Dual Iterative Pattern
Relation Extraction). You can see why the term cotraining has stuck. Similar early work,
underthenameofbootstrapping, wasdonebyJonesetal.(1999). Themethodwasadvanced
by the QXTRACT (Agichtein and Gravano, 2003) and KNOWITALL (Etzioni et al., 2005)
systems. MachinereadingwasintroducedbyMitchell(2005)andEtzionietal.(2006)andis
thefocusofthe TEXTRUNNER project(Banko etal.,2007;BankoandEtzioni,2008).
Thischapterhasfocused onnaturallanguage text,butitisalsopossible todoinforma-
tion extraction based on the physical structure or layout of text rather than on the linguistic
structure. HTML lists and tables in both HTML and relational databases are home to data
thatcanbeextractedandconsolidated(Hurst,2000;Pintoetal.,2003;Cafarellaetal.,2008).
The Association for Computational Linguistics (ACL) holds regular conferences and
publishes the journal Computational Linguistics. There is also an International Conference
onComputationalLinguistics(COLING).ThetextbookbyManningandSchu¨tze(1999)cov-
ers statistical language processing, while Jurafsky and Martin (2008) give a comprehensive
introduction tospeechandnaturallanguage processing.
EXERCISES
22.1 This exercise explores the quality of the n-gram model of language. Find or create a
monolingual corpus of100,000 wordsormore. Segmentitinto words, andcompute the fre-
quencyofeachword. Howmanydistinctwordsarethere? Alsocountfrequenciesofbigrams
(twoconsecutive words)and trigrams(three consecutive words). Nowusethose frequencies
togeneratelanguage: fromtheunigram,bigram,andtrigram models,inturn,generatea100-
word text by making random choices according to the frequency counts. Compare the three
generated textswithactuallanguage. Finally, calculatetheperplexity ofeachmodel.
886 Chapter 22. NaturalLanguageProcessing
22.2 Write a program to do segmentation of words without spaces. Given a string, such
astheURL“thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,” returna
list ofcomponent words: [“the,” “longest,” “list,” ...]. Thistask isuseful forparsing URLs,
for spelling correction when words runtogether, and for languages such as Chinese that do
not have spaces between words. It can be solved with aunigram orbigram word model and
adynamicprogramming algorithm similartotheViterbialgorithm.
22.3 (AdaptedfromJurafskyandMartin(2000).) Inthisexercise youwilldevelopaclassi-
fierforauthorship: given atext, the classifier predicts which oftwocandidate authors wrote
the text. Obtain samples of text from two different authors. Separate them into training and
test sets. Now train a language model on the training set. You can choose what features to
use; n-grams of words orletters are the easiest, but you can add additional features that you
think may help. Then compute the probability of the text under each language model and
chose the most probable model. Assess the accuracy of this technique. How does accuracy
change as you alter the set of features? This subfield of linguistics is called stylometry; its
STYLOMETRY
successesincludetheidentificationoftheauthorofthedisputedFederalistPapers(Mosteller
and Wallace, 1964) and some disputed works of Shakespeare (Hope, 1994). Khmelev and
Tweedie(2001) producegoodresultswithasimpleletterbigrammodel.
22.4 Thisexerciseconcernstheclassificationofspamemail. Createacorpusofspamemail
andoneofnon-spammail. Examineeachcorpusanddecidewhatfeaturesappeartobeuseful
for classification: unigram words? bigrams? message length, sender, time of arrival? Then
trainaclassificationalgorithm(decisiontree,naiveBayes,SVM,logisticregression, orsome
otheralgorithmofyourchoosing)onatrainingsetandreportitsaccuracyonatestset.
22.5 Create a test set of ten queries, and pose them to three major Web search engines.
Evaluate each one for precision at 1, 3, and 10 documents. Can you explain the differences
betweenengines?
22.6 Trytoascertain whichofthesearchenginesfromthepreviousexerciseareusingcase
folding, stemming,synonyms, andspelling correction.
22.7 Write aregular expression orashort program to extract company names. Testit on a
corpusofbusiness newsarticles. Reportyourrecallandprecision.
22.8 Consider the problem of trying to evaluate the quality of an IR system that returns a
ranked list of answers (like most Web search engines). The appropriate measure of quality
depends on the presumed model of what the searcher is trying to achieve, and whatstrategy
sheemploys. Foreachofthefollowingmodels,propose acorresponding numericmeasure.
a. Thesearcherwilllookatthefirsttwentyanswersreturned, withtheobjectiveofgetting
asmuchrelevantinformation aspossible.
b. Thesearcherneedsonlyonerelevantdocument,andwillgodownthelistuntilshefinds
thefirstone.
c. Thesearcherhasafairlynarrowqueryandisabletoexamine alltheanswersretrieved.
She wants to be sure that she has seen everything in the document collection that is
Exercises 887
relevant to her query. (E.g., a lawyer wants to be sure that she has found all relevant
precedents, andiswillingtospendconsiderable resources onthat.)
d. The searcher needs just one document relevant to the query, and can afford to pay a
researchassistantforanhour’sworklookingthroughtheresults. Theassistantcanlook
through 100 retrieved documents inan hour. Theassistant will charge the searcher for
thefullhourregardlessofwhetherhefindsitimmediatelyor attheendofthehour.
e. The searcher will look through all the answers. Examining a document has cost $A;
finding arelevant document has value $B; failing to finda relevant document has cost
$C foreachrelevantdocumentnotfound.
f. Thesearcherwantstocollectasmanyrelevantdocumentsaspossible, butneedssteady
encouragement. She looks through the documents in order. If the documents she has
lookedatsofararemostlygood, shewillcontinue; otherwise, shewillstop.
23
NATURAL LANGUAGE
FOR COMMUNICATION
Inwhichweseehowhumanscommunicatewithoneanotherinnaturallanguage,
andhowcomputeragentsmightjoinintheconversation.
Communicationistheintentional exchange ofinformation broughtaboutby theproduction
COMMUNICATION
andperceptionofsignsdrawnfromasharedsystemofconventional signs. Mostanimalsuse
SIGN
signs torepresent important messages: foodhere, predator nearby, approach, withdraw, let’s
mate. Inapartially observable world, communication canhelpagents besuccessful because
theycanlearninformationthatisobservedorinferredbyothers. Humansarethemostchatty
of all species, and if computer agents are to be helpful, they’ll need to learn to speak the
language. In this chapter we look at language models for communication. Models aimed at
deep understanding of a conversation necessarily need to be more complex than the simple
models aimed at, say, spam classification. We start with grammatical models of the phrase
structure of sentences, add semantics to the model, and then apply it to machine translation
andspeechrecognition.
23.1 PHRASE STRUCTURE GRAMMARS
The n-gram language models of Chapter 22 were based on sequences of words. The big
issueforthesemodelsisdatasparsity—withavocabulary of,say,105 words,thereare1015
trigram probabilities to estimate, and so a corpus of even a trillion words will not be able to
supply reliable estimates for all of them. We can address the problem of sparsity through
generalization. Fromthefactthat“black dog”ismorefrequent than“dogblack”andsimilar
observations, we can form the generalization that adjectives tend to come before nouns in
English (whereas they tend to follow nouns in French: “chien noir” is more frequent). Of
coursetherearealwaysexceptions;“galore”isanadjectivethatfollowsthenounitmodifies.
Despitetheexceptions,thenotionofalexicalcategory(alsoknownasapartofspeech)such
LEXICALCATEGORY
asnounoradjective isausefulgeneralization—useful initsownright,butmore sowhenwe
SYNTACTIC string together lexical categories to form syntactic categories such as noun phrase or verb
CATEGORIES
phrase, and combine these syntactic categories into trees representing the phrase structure
PHRASESTRUCTURE
ofsentences: nestedphrases, eachmarkedwithacategory.
888
Section23.1. PhraseStructureGrammars 889
GENERATIVE CAPACITY
Grammaticalformalismscanbeclassifiedbytheirgenerativecapacity: thesetof
languages theycanrepresent. Chomsky(1957)describesfourclassesofgrammat-
ical formalisms that differ only in the form of the rewrite rules. The classes can
be arranged in a hierarchy, where each class can be used to describe all the lan-
guages that can be described by a less powerful class, as well as some additional
languages. Herewelistthehierarchy, mostpowerfulclassfirst:
Recursively enumerable grammars use unrestricted rules: both sides of the
rewrite rules can have any number of terminal and nonterminal symbols, asin the
ruleAB C → D E. Thesegrammarsareequivalent toTuringmachinesintheir
expressivepower.
Context-sensitive grammars are restricted only in that the right-hand side
must contain at least as many symbols as the left-hand side. The name “context-
sensitive” comes from the fact that a rule such as A X B → A Y B says that
an X can be rewritten as a Y in the context of a preceding A and a following B.
Context-sensitive grammars can represent languages such as anbncn (a sequence
ofncopiesofafollowedbythesamenumberofbsandthencs).
In context-free grammars (or CFGs), the left-hand side consists of a sin-
gle nonterminal symbol. Thus, each rule licenses rewriting the nonterminal as
the right-hand side in any context. CFGs are popular for natural-language and
programming-language grammars, although itisnowwidelyaccepted thatatleast
somenaturallanguageshaveconstructionsthatarenotcontext-free(Pullum,1991).
Context-free grammarscanrepresent anbn,butnotanbncn.
Regulargrammars are the most restricted class. Every rule has asingle non-
terminalontheleft-handsideandaterminalsymboloptionallyfollowedbyanon-
terminalontheright-handside. Regulargrammarsareequivalentinpowertofinite-
state machines. They are poorly suited for programming languages, because they
cannot represent constructs such as balanced opening and closing parentheses (a
variation ofthe anbn language). Theclosest theycancomeisrepresenting a ∗ b ∗ ,a
sequence ofanynumberofasfollowedbyanynumberofbs.
The grammars higher up in the hierarchy have more expressive power, but
the algorithms for dealing with them are less efficient. Up to the 1980s, linguists
focusedoncontext-freeandcontext-sensitivelanguages. Sincethen,therehasbeen
renewed interest in regular grammars, brought about by the need to process and
learn from gigabytes or terabytes of online text very quickly, even at the cost of
a less complete analysis. As Fernando Pereira put it, “The older I get, the further
down the Chomsky hierarchy I go.” To see what he means, compare Pereira and
Warren (1980) with Mohri, Pereira, and Riley (2002) (and note that these three
authorsallnowworkonlargetextcorporaatGoogle).
890 Chapter 23. NaturalLanguageforCommunication
There have been many competing language models based on the idea of phrase struc-
PROBABILISTIC
ture; we will describe a popular model called the probabilistic context-free grammar, or
CONTEXT-FREE
GRAMMAR PCFG.1 A grammar is a collection of rules that defines a language as a set of allowable
GRAMMAR
strings ofwords. “Context-free” isdescribed inthesidebar onpage 889, and “probabilistic”
LANGUAGE
meansthatthegrammarassignsaprobability toeverystring. HereisaPCFGrule:
VP → Verb [0.70]
| VP NP [0.30].
NON-TERMINAL Here VP (verb phrase) and NP (noun phrase) are non-terminal symbols. The grammar
SYMBOLS
alsorefers toactual words, whicharecalled terminalsymbols. Thisruleissaying thatwith
TERMINALSYMBOL
probability 0.70 averb phrase consists solely ofa verb, and with probability 0.30 itis a VP
followedbyanNP. Appendix Bdescribes non-probabilistic context-free grammars.
Wenow defineagrammarforatiny fragment ofEnglish thatissuitable forcommuni-
cationbetweenagentsexploringthewumpusworld. WecallthislanguageE . Latersections
0
improve on E to make it slightly closer to real English. We are unlikely ever to devise a
0
completegrammarforEnglish, ifonlybecausenotwopersons wouldagreeentirelyonwhat
constitutes validEnglish.
23.1.1 The lexiconofE
0
Firstwedefinethelexicon,orlistofallowablewords. Thewordsaregroupedintothelexical
LEXICON
categories familiar to dictionary users: nouns, pronouns, and names to denote things; verbs
to denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:
articles (such as the), prepositions (in), and conjunctions (and). Figure 23.1 shows a small
lexiconforthelanguage E .
0
Eachofthecategories endsin...toindicate thatthereareotherwordsinthecategory.
For nouns, names, verbs, adjectives, and adverbs, it is infeasible even in principle to list all
the words. Not only are there tens of thousands of members in each class, but new ones–
like iPod or biodiesel—are being added constantly. These five categories are called open
classes. Forthecategoriesofpronoun, relativepronoun, article, preposition, andconjunction
OPENCLASS
we could have listed all the words with a little more work. These are called closed classes;
CLOSEDCLASS
they have a small number of words (a dozen or so). Closed classes change over the course
ofcenturies, notmonths. Forexample, “thee” and “thou” werecommonly used pronouns in
the17thcentury, wereonthedeclineinthe19th,andareseen todayonlyinpoetryandsome
regionaldialects.
23.1.2 The GrammarofE
0
The next step is to combine the words into phrases. Figure 23.2 shows a grammar for E ,
0
with rules for each of the six syntactic categories and an example for each rewrite rule.2
Figure 23.3 shows a parse tree for the sentence “Every wumpus smells.” The parse tree
PARSETREE
1 PCFGsarealsoknownasstochasticcontext-freegrammars,orSCFGs.
2 A relative clause follows and modifies a noun phrase. It consists of a relative pronoun (such as “who” or
“that”)followedbyaverbphrase. Anexampleofarelativeclauseisthatstinksin“Thewumpusthatstinksisin
22.”Anotherkindofrelativeclausehasnorelativepronoun,e.g.,Iknowin“themanIknow.”
Section23.1. PhraseStructureGrammars 891
Noun → stench [0.05] | breeze [0.10] | wumpus[0.15]| pits [0.05]| ...
Verb → is [0.10] | feel [0.10]| smells [0.10]| stinks [0.05] | ...
Adjective → right [0.10]| dead [0.05]| smelly [0.02]| breezy [0.02]...
Adverb → here [0.05]| ahead [0.05] | nearby [0.02] | ...
Pronoun → me [0.10]| you [0.03] | I[0.10] | it [0.10]| ...
RelPro → that [0.40] | which [0.15]| who[0.20]| whom [0.02]∨...
Name → John [0.01] | Mary [0.01]| Boston [0.01]| ...
Article → the [0.40]| a [0.30]| an [0.10] | every [0.05] | ...
Prep → to [0.20] | in [0.10]| on [0.05]| near [0.10] | ...
Conj → and [0.50] | or [0.10] | but[0.20] | yet [0.02]∨...
Digit → 0 [0.20] | 1 [0.20]| 2 [0.20]| 3 [0.20]| 4 [0.20] | ...
Figure23.1 ThelexiconforE .RelProisshortforrelativepronoun,Prepforpreposition,
0
andConj forconjunction.Thesumoftheprobabilitiesforeachcategoryis1.
E : S → NP VP [0.90] I+feelabreeze
0
| S Conj S [0.10] Ifeelabreeze+and+Itstinks
NP → Pronoun [0.30] I
| Name [0.10] John
| Noun [0.10] pits
| Article Noun [0.25] the+wumpus
| Article Adjs Noun [0.05] the+smellydead+wumpus
| Digit Digit [0.05] 34
| NP PP [0.10] thewumpus+in13
| NP RelClause [0.05] thewumpus+thatissmelly
VP → Verb [0.40] stinks
| VP NP [0.35] feel+abreeze
| VP Adjective [0.05] smells+dead
| VP PP [0.10] is+in13
| VP Adverb [0.10] go+ahead
Adjs → Adjective [0.80] smelly
| Adjective Adjs [0.20] smelly+dead
PP → Prep NP [1.00] to+theeast
RelClause → RelPro VP [1.00] that+issmelly
Figure23.2 ThegrammarforE ,withexamplephrasesforeachrule. Thesyntacticcat-
0
egories are sentence (S), noun phrase (NP), verb phrase (VP), list of adjectives (Adjs),
prepositionalphrase(PP),andrelativeclause(RelClause).
892 Chapter 23. NaturalLanguageforCommunication
S
0.90
NP VP
0.25 0.40
Article Noun Verb
0.05 0.15 0.10
Every wumpus smells
Figure23.3 Parsetreeforthesentence“Everywumpussmells”accordingtothegrammar
E . Eachinteriornodeofthetreeislabeledwithitsprobability. Theprobabilityofthetree
0
asawholeis0.9×0.25×0.05×0.15×0.40×0.10=0.0000675. Sincethistreeistheonly
parseofthesentence,thatnumberisalsotheprobabilityof thesentence. Thetreecanalso
bewritteninlinearformas[S [NP [Article every][Noun wumpus]][VP [Verb smells]]].
givesaconstructive proofthatthestring ofwordsisindeed asentence according totherules
ofE . TheE grammargenerates awiderangeofEnglishsentences suchasthefollowing:
0 0
Johnisinthepit
Thewumpusthatstinksisin22
MaryisinBostonandthewumpusisnear32
Unfortunately, thegrammar overgenerates: thatis,itgenerates sentences that arenotgram-
OVERGENERATION
matical, such as “Me go Boston” and “I smell pits wumpus John.” It also undergenerates:
UNDERGENERATION
there are many sentences of English that it rejects, such as “I think the wumpus is smelly.”
Wewillseehow to learn abetter grammarlater; fornow weconcentrate onwhat wecan do
withthegrammarwehave.
23.2 SYNTACTIC ANALYSIS (PARSING)
Parsingistheprocessofanalyzingastringofwordstouncoveritsphrasestructure,according
PARSING
totherulesofagrammar. Figure23.4showsthatwecanstartwiththeS symbol andsearch
topdownforatreethathasthewordsasitsleaves,orwecanstartwiththewordsandsearch
bottom up for a tree that culminates in an S. Both top-down and bottom-up parsing can be
inefficient,however,becausetheycanenduprepeatingeffortinareasofthesearchspacethat
leadtodeadends. Considerthefollowingtwosentences:
Havethestudentsinsection2ofComputerScience101taketheexam.
Havethestudentsinsection2ofComputerScience101takentheexam?
Eventhoughtheysharethefirst10words,thesesentenceshaveverydifferentparses,because
the first is a command and the second is a question. A left-to-right parsing algorithm would
have toguess whetherthe firstwordispart of acommand oraquestion andwillnot beable
to tell if the guess is correct until at least the eleventh word, take or taken. If the algorithm
guesseswrong,itwillhavetobacktrackallthewaytothefirstwordandreanalyze thewhole
sentence undertheotherinterpretation.
Section23.2. SyntacticAnalysis(Parsing) 893
Listofitems Rule
S
NP VP S → NP VP
NP VP Adjective VP → VP Adjective
NP Verb Adjective VP → Verb
NP Verb dead Adjective → dead
NP isdead Verb → is
Article Noun isdead NP → Article Noun
Article wumpusisdead Noun → wumpus
thewumpusisdead Article → the
Figure23.4 Traceoftheprocessoffindingaparseforthestring“Thewumpusisdead”
asasentence,accordingtothegrammarE . Viewedasatop-downparse,westartwiththe
0
listofitemsbeingS and,oneachstep,matchanitemX witharuleoftheform(X →...)
andreplaceX inthelistofitemswith(...). Viewedasabottom-upparse,westartwiththe
listofitemsbeingthewordsofthesentence,and,oneachstep,matchastringoftokens(...)
inthelistagainstaruleoftheform(X →...) andreplace(...) withX.
To avoid this source of inefficiency we can use dynamic programming: every time we
analyze a substring, store the results so we won’t have to reanalyze it later. For example,
oncewediscoverthat“thestudentsinsection2ofComputerScience101”isanNP,wecan
recordthatresultinadatastructureknownasachart. Algorithmsthatdothisarecalledchart
CHART
parsers. Because we are dealing with context-free grammars, any phrase that was found in
thecontextofonebranchofthesearchspacecanworkjustaswellinanyotherbranchofthe
search space. Therearemanytypes ofchartparsers; wedescribe abottom-up version called
theCYKalgorithm,afteritsinventors, JohnCocke,DanielYounger, andTadeo Kasami.
CYKALGORITHM
The CYK algorithm is shown in Figure 23.5. Note that it requires a grammar with all
rulesinoneoftwoveryspecificformats: lexicalrulesofthe formX → word,andsyntactic
CHOMSKYNORMAL rules of the form X → Y Z. This grammar format, called Chomsky Normal Form, may
FORM
seem restrictive, but it is not: any context-free grammar can be automatically transformed
intoChomskyNormalForm. Exercise23.8leadsyouthroughtheprocess.
The CYK algorithm uses space of O(n2m) for the P table, where n is the number of
wordsinthesentence,andmisthenumberofnonterminalsymbolsinthegrammar,andtakes
timeO(n3m). (Sincemisconstant foraparticular grammar, this iscommonly described as
O(n3).) No algorithm can do better for general context-free grammars, although there are
fasteralgorithms onmorerestricted grammars. Infact, itisquite atrick forthealgorithm to
completeinO(n3)time,giventhatitispossibleforasentencetohaveanexponentialnumber
ofparsetrees. Considerthesentence
Fallleavesfallandspringleavesspring.
It is ambiguous because each word (except “and”) can be either a noun ora verb, and “fall”
and “spring” can be adjectives as well. (For example, one meaning of “Fall leaves fall” is
894 Chapter 23. NaturalLanguageforCommunication
functionCYK-PARSE(words,grammar)returnsP,atableofprobabilities
N ←LENGTH(words)
M ←thenumberofnonterminalsymbolsingrammar
P←anarrayofsize[M,N,N],initiallyall0
/*Insertlexicalrulesforeachword*/
fori =1toN do
foreachruleofform(X → wordsi[p])do
P[X,i,1]←p
/*Combinefirstandsecondpartsofright-handsidesofrules,fromshorttolong*/
forlength =2toN do
forstart =1toN −length +1 do
forlen1 =1toN −1do
len2 ←length−len1
foreachruleoftheform(X → Y Z [p])do
P[X,start,length]←MAX(P[X, start, length],
P[Y, start, len1] × P[Z, start +len1, len2] × p)
returnP
Figure 23.5 The CYK algorithm for parsing. Given a sequence of words, it finds the
most probable derivation for the whole sequence and for each subsequence. It returns the
wholetable, P, in whichan entry P[X, start, len] is the probabilityofthe mostprobable
X of lengthlen starting atposition start. If thereis no X of thatsize atthatlocation, the
probabilityis0.
equivalent to“Autumnabandons autumn.) WithE thesentence hasfourparses:
0
[S [S [NP Fallleaves]fall]and[S [NP springleaves]spring]
[S [S [NP Fallleaves]fall]and[S spring[VP leavesspring]]
[S [S Fall[VP leavesfall]]and[S [NP springleaves]spring]
[S [S Fall[VP leavesfall]]and[S spring[VP leavesspring]] .
Ifwehadctwo-ways-ambiguous conjoined subsentences, wewouldhave2c waysofchoos-
ing parses forthe subsentences.3 How does the CYK algorithm process these 2c parse trees
in O(c3) time? The answer is that it doesn’t examine all the parse trees; all it has to do is
compute the probability of the most probable tree. The subtrees are all represented in the P
table,andwithalittleworkwecouldenumeratethemall(inexponentialtime),butthebeauty
oftheCYKalgorithm isthatwedon’thavetoenumeratethemunlesswewantto.
Inpracticeweareusuallynotinterestedinallparses;justthebestoneorbestfew. Think
of the CYK algorithm as defining the complete state space defined by the “apply grammar
∗
rule” operator. It is possible to search just part of this space using A search. Each state
in this space is a list of items (words or categories), as shown in the bottom-up parse table
(Figure 23.4). The start state is a list of words, and a goal state is the single item S. The
3 TherealsowouldbeO(c!)ambiguityinthewaythecomponentsconjoin—forexample,(X and(Y andZ))
versus((XandY)andZ).Butthatisanotherstory,onetoldwellbyChurchandPatil(1982).
Section23.2. SyntacticAnalysis(Parsing) 895
[[S[NP-SBJ-2Hereyes]
[VPwere
[VPglazed
[NP*-2]
[SBAR-ADVasif
[S[NP-SBJshe]
[VPdidn’t
[VP[VPhear[NP*-1]]
or
[VP[ADVPeven]see[NP*-1]]
[NP-1him]]]]]]]]
.]
Figure23.6 Annotatedtree forthe sentence“Hereyeswere glazedasif she didn’thear
orevensee him.” fromthePennTreebank. Notethatinthisgrammarthereisadistinction
betweenanobjectnounphrase(NP)andasubjectnounphrase(NP-SBJ).Notealsoagram-
maticalphenomenonwe havenotcoveredyet: the movementof a phrasefrom onepartof
thetreetoanother.Thistreeanalyzesthephrase“hearorevenseehim”asconsistingoftwo
constituent VPs, [VP hear [NP *-1]] and [VP [ADVP even] see [NP *-1]], both of which
haveamissingobject,denoted*-1,whichreferstotheNP labeledelsewhereinthetreeas
[NP-1him].
costofastateistheinverse ofitsprobability asdefinedbytherulesapplied sofar, andthere
arevarious heuristics toestimatetheremaining distance tothegoal;thebestheuristics come
∗
frommachinelearningappliedtoacorpusofsentences. WiththeA algorithmwedon’thave
to search the entire state space, and we are guaranteed that the first parse found will be the
mostprobable.
23.2.1 Learning probabilities forPCFGs
A PCFG has many rules, with a probability for each rule. This suggests that learning the
grammarfromdatamightbebetterthanaknowledgeengineering approach. Learningiseas-
iestifwearegivenacorpusofcorrectlyparsedsentences,commonlycalledatreebank. The
TREEBANK
PennTreebank (Marcus etal., 1993) isthe best known; itconsists of3million words which
havebeenannotated withpartofspeech andparse-tree structure, usinghumanlaborassisted
bysomeautomated tools. Figure23.6showsanannotated tree fromthePennTreebank.
Givenacorpusoftrees,wecancreateaPCFGjustbycounting(andsmoothing). Inthe
exampleabove,therearetwonodesoftheform [S[NP...][VP ...]]. Wewouldcountthese,
and all the other subtrees with root S in the corpus. If there are 100,000 S nodes of which
60,000areofthisform,thenwecreatetherule:
S → NP VP [0.60].
What if a treebank is not available, but we have a corpus of raw unlabeled sentences? It is
still possible to learn a grammar from such a corpus, but it is more difficult. First of all,
weactually have twoproblems: learning the structure ofthe grammarrules and learning the
896 Chapter 23. NaturalLanguageforCommunication
probabilitiesassociatedwitheachrule. (WehavethesamedistinctioninlearningBayesnets.)
We’ll assume that we’re given the lexical and syntactic category names. (If not, we can just
assume categories X ,...X and use cross-validation to pick the best value of n.) We can
1 n
then assume that the grammar includes every possible (X → Y Z) or (X → word) rule,
although manyoftheseruleswillhaveprobability 0orclose to0.
Wecanthenuseanexpectation–maximization(EM)approach,justaswedidinlearning
HMMs. Theparameters wearetrying tolearn are therule probabilities; westartthem offat
random oruniform values. Thehidden variables aretheparse trees: wedon’t know whether
astringofwords w ...w isorisnotgenerated byarule(X → ...). TheEstepestimates
i j
the probability that each subsequence is generated by each rule. The M step then estimates
theprobability ofeachrule. Thewholecomputationcanbedoneinadynamic-programming
INSIDE–OUTSIDE fashion with an algorithm called the inside–outside algorithm in analogy to the forward–
ALGORITHM
backwardalgorithm forHMMs.
Theinside–outsidealgorithmseemsmagicalinthatitinducesagrammarfromunparsed
text. Butithasseveraldrawbacks. First,theparsesthatareassignedbytheinducedgrammars
areoften difficult tounderstand andunsatisfying tolinguists. Thismakes ithard tocombine
handcrafted knowledge with automated induction. Second, itis slow: O(n3m3), where nis
the number of words in a sentence and m is the number of grammar categories. Third, the
space of probability assignments is very large, and empirically it seems that getting stuck in
localmaximaisasevereproblem. Alternativessuchassimulatedannealing cangetcloserto
the global maximum, at a cost of even more computation. Lari and Young (1990) conclude
thatinside–outside is“computationally intractable forrealisticproblems.”
However,progress canbemadeifwearewillingtostepoutsidetheboundsoflearning
solelyfromunparsedtext. Oneapproachistolearnfrom prototypes: toseedtheprocesswith
adozenortworules,similartotherulesinE . Fromthere,morecomplexrulescanbelearned
1
moreeasily,andtheresultinggrammarparsesEnglishwithanoverallrecallandprecisionfor
sentences of about 80% (Haghighi and Klein, 2006). Another approach is to use treebanks,
butinadditiontolearningPCFGrulesdirectlyfromthebracketings,alsolearningdistinctions
thatarenotinthetreebank. Forexample,notthatthetreeinFigure23.6makesthedistinction
between NP and NP −SBJ. The latter is used for the pronoun “she,” the former for the
pronoun “her.” We will explore this issue in Section 23.6; for now let us just say that there
aremanywaysinwhich itwouldbe useful to splitacategory like NP—grammarinduction
systems that use treebanks but automatically split categories do better than those that stick
with the original category set (Petrov and Klein, 2007c). The error rates for automatically
learnedgrammarsarestillabout50%higherthanforhand-constructed grammar,butthegap
isdecreasing.
23.2.2 Comparing context-free andMarkov models
TheproblemwithPCFGsisthattheyarecontext-free. Thatmeansthatthedifferencebetween
P(“eat abanana”) and P(“eat abandanna”) depends only onP(Noun → “banana”) versus
P(Noun → “bandanna”) and not on the relation between “eat” and the respective objects.
AMarkov modelofordertwoormore, givenasufficiently large corpus, willknow that “eat
Section23.3. AugmentedGrammarsandSemanticInterpretation 897
a banana” is more probable. We can combine a PCFG and Markov model to get the best of
both. The simplest approach is to estimate the probability of a sentence with the geometric
meanoftheprobabilitiescomputedbybothmodels. Thenwewouldknowthat“eatabanana”
isprobable fromboththegrammaticalandlexicalpointofview. Butitstillwouldn’tpickup
the relation between “eat” and “banana” in “eat a slightly aging but still palatable banana”
because here the relation is more than two words away. Increasing the order of the Markov
model won’t get at the relation precisely; to do that we can use a lexicalized PCFG, as
described inthenextsection.
AnotherproblemwithPCFGsisthattheytendtohavetoostrongapreferenceforshorter
sentences. In a corpus such as the Wall Street Journal, the average length of a sentence
is about 25 words. But a PCFG will usually assign fairly high probability to many short
sentences, suchas“Heslept,”whereasintheJournalwe’remorelikelytoseesomethinglike
“Ithasbeenreportedbyareliablesourcethattheallegationthathesleptiscredible.” Itseems
thatthephrases inthe Journal really arenotcontext-free; instead thewriters haveanideaof
theexpectedsentencelengthandusethatlengthasasoftglobalconstraintontheirsentences.
ThisishardtoreflectinaPCFG.
23.3 AUGMENTED GRAMMARS AND SEMANTIC INTERPRETATION
In this section we see how to extend context-free grammars—to say that, for example, not
every NP isindependent ofcontext, butrather, certain NPsaremorelikely toappearinone
context, andothersinanothercontext.
23.3.1 LexicalizedPCFGs
Togetattherelationship betweentheverb“eat”andthenouns“banana” versus“bandanna,”
wecanusealexicalized PCFG,inwhichtheprobabilities foraruledepend ontherelation-
LEXICALIZEDPCFG
ship between words in the parse tree, not just on the adjacency of words in a sentence. Of
course, we can’t have the probability depend on every word in the tree, because we won’t
have enough training data to estimate all those probabilities. Itisuseful tointroduce the no-
tion of the head of a phrase—the most important word. Thus, “eat” is the head of the VP
HEAD
“eat a banana” and “banana” is the head of the NP “a banana.” Weuse the notation VP(v)
to denote a phrase with category VP whose head word is v. We say that the category VP
AUGMENTED is augmented with the head variable v. Here is an augmented grammar that describes the
GRAMMAR
verb–object relation:
VP(v) → Verb(v) NP(n) [P (v,n)]
1
VP(v) → Verb(v) [P (v)]
2
NP(n) → Article(a) Adjs(j) Noun(n) [P (n,a)]
3
Noun(banana) → banana [p ]
n
... ...
Herethe probability P (v,n)depends on thehead words v and n. Wewould set this proba-
1
bilitytoberelativelyhighwhenv is“eat”andnis“banana,”andlowwhennis“bandanna.”
898 Chapter 23. NaturalLanguageforCommunication
Note that since we are considering only heads, the distinction between “eat a banana” and
“eat a rancid banana” will not be caught by these probabilities. Another issue with this ap-
proachisthat,inavocabulary with,say,20,000nounsand5,000verbs,P needs100million
1
probability estimates. Onlyafewpercentofthesecancomefromacorpus;therestwillhave
to come from smoothing (see Section 22.1.2). For example, we can estimateP (v,n) for a
1
(v,n) pair that we have not seen often (or at all) by backing off to a model that depends
onlyonv. Theseobjectlessprobabilities arestillveryuseful;theycancapturethedistinction
betweenatransitiveverblike“eat”—whichwillhaveahighvalueforP andalowvaluefor
1
P —and an intransitive verb like “sleep,” which will have the reverse. It is quite feasible to
2
learntheseprobabilities fromatreebank.
23.3.2 Formaldefinition ofaugmented grammarrules
Augmented rules arecomplicated, sowewillgive them aformaldefinition byshowing how
anaugmented rulecanbetranslated intoalogical sentence. Thesentence willhavetheform
DEFINITECLAUSE ofadefiniteclause(seepage256),sotheresultiscalledadefiniteclausegrammar,orDCG.
GRAMMAR
We’ll use as an example a version of a rule from the lexicalized grammar for NP with one
newpieceofnotation:
NP(n) → Article(a) Adjs(j) Noun(n){Compatible(j,n)}.
Thenewaspecthereisthenotation{constraint}todenotealogicalconstraintonsomeofthe
variables;theruleonlyholdswhentheconstraintistrue. HerethepredicateCompatible(j,n)
ismeanttotestwhetheradjectivejandnounnarecompatible;itwouldbedefinedbyaseries
ofassertions such asCompatible(black,dog). Wecanconvert thisgrammarruleintoadef-
initeclause by(1)reversing theorderofright-andleft-hand sides, (2)makingaconjunction
ofalltheconstituentsandconstraints,(3)addingavariables tothelistofargumentsforeach
i
constituent torepresent thesequence ofwords spanned bytheconstituent, (4)adding aterm
for the concatenation of words, Append(s ,...), to the list of arguments for the root of the
1
tree. Thatgivesus
Article(a,s )∧Adjs(j,s )∧Noun(n,s )∧Compatible(j,n)
1 2 3
⇒ NP(n,Append(s ,s ,s )).
1 2 3
Thisdefiniteclausesaysthatifthepredicate Article istrueofaheadwordaandastring s ,
1
andAdjs issimilarly trueofahead word j andastring s ,andNoun istrueofaheadword
2
n and a string s , and if j and n are compatible, then the predicate NP is true of the head
3
wordnandtheresultofappending strings s ,s ,ands .
1 2 3
TheDCGtranslationleftouttheprobabilities, butwecould putthembackin: justaug-
menteachconstituent withonemorevariable representing theprobability oftheconstituent,
andaugment therootwithavariable thatistheproduct ofthe constituent probabilities times
theruleprobability.
The translation from grammar rule to definite clause allows us to talk about parsing
as logical inference. This makes it possible to reason about languages and strings in many
differentways. Forexample,itmeanswecandobottom-upparsingusingforwardchainingor
top-downparsingusingbackwardchaining. Infact,parsing naturallanguagewithDCGswas
Section23.3. AugmentedGrammarsandSemanticInterpretation 899
oneofthefirstapplications of(andmotivationsfor)theProloglogicprogramminglanguage.
LANGUAGE Itissometimespossible toruntheprocessbackwardanddo languagegeneration aswellas
GENERATION
parsing. Forexample, skipping ahead to Figure 23.10 (page 903), a logic program could be
giventhesemanticformLoves(John,Mary)andapplythedefinite-clause rulestodeduce
S(Loves(John,Mary),[John,loves,Mary]).
Thisworksfortoyexamples,butseriouslanguage-generationsystemsneedmorecontrolover
theprocessthanisaffordedbytheDCGrulesalone.
E : S → NP VP | ...
1 S
NP → Pronoun | Name | Noun | ...
S S
NP → Pronoun | Name | Noun | ...
O O
VP → VP NP | ...
O
PP → Prep NP
O
Pronoun → I| you|he| she|it | ...
S
Pronoun → me| you|him| her|it | ...
O
...
E : S(head) → NP(Sbj,pn,h) VP(pn,head) | ...
2
NP(c,pn,head) → Pronoun(c,pn,head)| Noun(c,pn,head) | ...
VP(pn,head) → VP(pn,head) NP(Obj,p,h) | ...
PP(head) → Prep(head) NP(Obj,pn,h)
Pronoun(Sbj,1S,I) → I
Pronoun(Sbj,1P,we) → we
Pronoun(Obj,1S,me) → me
Pronoun(Obj,3P,them) → them
...
Figure 23.7 Top: part of a grammarfor the language E , which handles subjective and
1
objective cases in noun phrases and thus does not overgeneratequite as badly as E . The
0
portionsthatareidenticaltoE havebeenomitted. Bottom: partofanaugmentedgrammar
0
forE , withthreeaugmentations: caseagreement,subject–verbagreement,andheadword.
2
Sbj,Obj,1S,1Pand3Pareconstants,andlowercasenamesarevariables.
23.3.3 Caseagreement andsubject–verb agreement
We saw in Section 23.1 that the simple grammar for E overgenerates, producing nonsen-
0
tencessuchas“Mesmellastench.” Toavoidthisproblem,ourgrammarwouldhavetoknow
that“me”isnotavalidNP whenitisthesubjectofasentence. Linguistssaythatthepronoun
“I” is in the subjective case, and “me” is in the objective case.4 We can account for this by
4 Thesubjectivecaseisalsosometimescalledthenominativecaseandtheobjectivecaseissometimescalled
theaccusativecase.Manylanguagesalsohaveadativecaseforwordsintheindirectobjectposition.
900 Chapter 23. NaturalLanguageforCommunication
splitting NP intotwocategories, NP andNP ,tostand fornounphrases inthesubjective
S O
and objective case, respectively. Wewould also need to split the category Pronoun into the
two categories Pronoun (which includes “I”) and Pronoun (which includes “me”). The
S O
toppartofFigure23.7showsthegrammarforcaseagreement;wecalltheresultinglanguage
CASEAGREEMENT
E . NoticethatalltheNP rulesmustbeduplicated, onceforNP andonceforNP .
1 S O
SUBJECT–VERB Unfortunately, E still overgenerates. English requires subject–verb agreement for
AGREEMENT 1
person and number of the subject and main verb of a sentence. For example, if “I” is the
subject, then “I smell” isgrammatical, but “I smells” isnot. If“it” isthe subject, weget the
reverse. In English, the agreement distinctions are minimal: most verbs have one form for
third-person singular subjects (he, she, or it), and a second form for all other combinations
ofperson and number. Thereisoneexception: the verb“tobe” hasthree forms, “I am/you
are / he is.” So one distinction (case) splits NP two ways, another distinction (person and
number)splitsNP threeways,andasweuncoverotherdistinctionswewouldendupwithan
exponential numberofsubscripted NP formsifwetooktheapproach ofE . Augmentations
1
areabetterapproach: theycanrepresent anexponential numberofformsasasinglerule.
In the bottom of Figure 23.7 wesee (part of) an augmented grammarfor the language
E , which handles case agreement, subject–verb agreement, and head words. We have just
2
one NP category, but NP(c,pn,head) has three augmentations: c is a parameter for case,
pn is a parameter for person and number, and head is a parameter for the head word of
the phrase. The other categories also are augmented with heads and other arguments. Let’s
consideroneruleindetail:
S(head) → NP(Sbj,pn,h) VP(pn,head).
Thisruleiseasiesttounderstand right-to-left: whenan NPandaVPareconjoined theyform
an S, but only if the NPhas the subjective (Sbj) case and the person and number (pn) of the
NP and VP are identical. If that holds, then we have an S whose head is the same as the
headoftheVP.NotetheheadoftheNP,denoted bythedummyvariable h,isnotpartofthe
augmentationoftheS.ThelexicalrulesforE fillinthevaluesoftheparametersandarealso
2
bestreadright-to-left. Forexample,therule
Pronoun(Sbj,1S,I) → I
saysthat“I”canbeinterpretedasaPronouninthesubjectivecase,first-personsingular,with
head “I.” For simplicity we have omitted the probabilities for these rules, but augmentation
does work with probabilities. Augmentation can also work with automated learning mecha-
nisms. Petrov and Klein (2007c) show how a learning algorithm can automatically split the
NP category intoNP andNP .
S O
23.3.4 Semantic interpretation
To show how to add semantics to a grammar, we start with an example that is simpler than
English: thesemanticsofarithmeticexpressions. Figure23.8showsagrammarforarithmetic
expressions, whereeachruleisaugmentedwithavariableindicatingthesemanticinterpreta-
tionofthephrase. Thesemanticsofadigitsuchas“3”isthedigititself. Thesemanticsofan
expression suchas“3+4”istheoperator“+”applied tothesemantics ofthephrase“3”and
Section23.3. AugmentedGrammarsandSemanticInterpretation 901
Exp(x) → Exp(x )Operator(op)Exp(x ) {x=Apply(op,x ,x )}
1 2 1 2
Exp(x) → (Exp(x) )
Exp(x) → Number(x)
Number(x) → Digit(x)
Number(x) → Number(x ) Digit(x ){x=10×x +x }
1 2 1 2
Digit(x) → x {0 ≤ x ≤ 9}
Operator(x) → x{x∈{+,−,÷,×}}
Figure23.8 Agrammarforarithmeticexpressions,augmentedwithsemantics.Eachvari-
ablexirepresentsthesemanticsofaconstituent.Notetheuseofthe{test}notationtodefine
logicalpredicatesthatmustbesatisfied,butthatarenotconstituents.
Exp(5)
Exp(2)
Exp(2)
Exp(3) Exp(4) Exp(2)
Number(3) Number(4) Number(2)
Digit(3) Operator(+) Digit(4) Operator(÷) Digit(2)
3 + ( 4 ÷ 2 )
Figure23.9 Parsetreewithsemanticinterpretationsforthestring“3+(4÷2)”.
COMPOSITIONAL the phrase “4.” The rules obey the principle of compositional semantics—the semantics of
SEMANTICS
aphraseisafunctionofthesemanticsofthesubphrases. Figure23.9showstheparsetreefor
3+(4÷2) according to this grammar. The root of the parse tree is Exp(5), an expression
whosesemanticinterpretation is5.
Nowlet’smoveontothesemantics ofEnglish, oratleast ofE . Westart bydetermin-
0
ingwhatsemanticrepresentations wewanttoassociatewithwhatphrases. Weusethesimple
examplesentence“JohnlovesMary.” TheNP “John”shouldhaveasitssemanticinterpreta-
tion the logical term John, and the sentence as a whole should have as its interpretation the
logical sentence Loves(John,Mary). That much seems clear. The complicated part is the
VP “loves Mary.” The semantic interpretation of this phrase is neither a logical term nor a
complete logical sentence. Intuitively, “loves Mary” is adescription that might ormight not
902 Chapter 23. NaturalLanguageforCommunication
apply to aparticular person. (In this case, it applies to John.) Thismeans that “loves Mary”
is a predicate that, when combined with a term that represents a person (the person doing
theloving), yields acomplete logical sentence. Using the λ-notation (see page 294), wecan
represent “lovesMary”asthepredicate
λx Loves(x,Mary).
Nowweneed arule thatsays “an NP withsemantics obj followed by aVP withsemantics
pred yieldsasentence whosesemanticsistheresultofapplying pred toobj:”
S(pred(obj)) → NP(obj) VP(pred).
Theruletellsusthatthesemanticinterpretation of“JohnlovesMary”is
(λx Loves(x,Mary))(John),
whichisequivalent toLoves(John,Mary).
The rest of the semantics follows in a straightforward way from the choices we have
madesofar. BecauseVPsarerepresentedaspredicates,itisagoodideatobeconsistentand
represent verbsaspredicates aswell. Theverb“loves”isrepresented as λy λx Loves(x,y),
thepredicatethat,whengiventheargumentMary,returnsthepredicateλx Loves(x,Mary).
WeendupwiththegrammarshowninFigure23.10andtheparsetreeshowninFigure23.11.
We could just as easily have added semantics to E ; we chose to work with E so that the
2 0
readercanfocusononetypeofaugmentation atatime.
Adding semantic augmentations to a grammar by hand is laborious and error prone.
Therefore, there have been several projects to learn semantic augmentations from examples.
CHILL (Zelle and Mooney, 1996) is an inductive logic programming (ILP) program that
learnsagrammarandaspecializedparserforthatgrammarfromexamples. Thetargetdomain
is natural language database queries. The training examples consist of pairs of word strings
andcorresponding semanticforms—forexample;
Whatisthecapitalofthestatewiththelargest population?
Answer(c,Capital(s,c)∧Largest(p,State(s)∧Population(s,p)))
CHILL’staskistolearnapredicate Parse(words,semantics)thatisconsistent withtheex-
amples and, hopefully, generalizes well to other examples. Applying ILP directly to learn
this predicate results inpoor performance: the induced parserhas only about 20% accuracy.
Fortunately, ILPlearners canimprovebyadding knowledge. Inthiscase, mostoftheParse
predicate was defined as a logic program, and CHILL’s task was reduced to inducing the
controlrulesthatguidetheparsertoselectoneparseoveranother. Withthisadditional back-
ground knowledge, CHILL can learn to achieve 70% to 85% accuracy on various database
querytasks.
23.3.5 Complications
ThegrammarofrealEnglishisendlessly complex. Wewillbrieflymentionsomeexamples.
Time and tense: Suppose we want to represent the difference between “John loves
TIMEANDTENSE
Mary”and“JohnlovedMary.” Englishusesverbtenses(past, present,andfuture)toindicate
Section23.3. AugmentedGrammarsandSemanticInterpretation 903
S(pred(obj)) → NP(obj) VP(pred)
VP(pred(obj)) → Verb(pred)NP(obj)
NP(obj) → Name(obj)
Name(John) → John
Name(Mary) → Mary
Verb(λy λx Loves(x,y)) → loves
Figure23.10 Agrammarthatcanderiveaparsetreeandsemanticinterpretationfor“John
lovesMary”(andthreeothersentences).Eachcategoryisaugmentedwithasingleargument
representingthesemantics.
S(Loves(John,Mary))
VP(λx Loves(x,Mary))
NP(John)
NP(Mary)
Name(John) Verb(λy λx Loves(x,y)) Name(Mary)
John loves Mary
Figure23.11 Aparsetreewithsemanticinterpretationsforthestring“JohnlovesMary”.
the relative time of an event. One good choice to represent the time of events is the event
calculus notationofSection12.3. Ineventcalculuswehave
Johnlovesmary:E ∈Loves(John,Mary)∧During(Now,Extent(E ))
1 1
Johnlovedmary:E ∈Loves(John,Mary)∧After(Now,Extent(E )).
2 2
Thissuggests thatourtwolexicalrulesforthewords“loves”and“loved”should bethese:
Verb(λy λx e∈Loves(x,y)∧During(Now,e)) → loves
Verb(λy λx e∈Loves(x,y)∧After(Now,e)) → loved.
Other than this change, everything else about the grammar remains the same, which is en-
couraging news; it suggests we are on the right track if wecan so easily add a complication
like the tense of verbs (although we have just scratched the surface of a complete grammar
fortimeandtense). Itisalsoencouraging thatthedistinction betweenprocesses anddiscrete
eventsthatwemadeinourdiscussion ofknowledge representation inSection12.3.1isactu-
ally reflected in language use. We can say “John slept a lot last night,” where Sleeping is a
process category, but it isodd to say “John found aunicorn a lot last night,” where Finding
is a discrete event category. A grammar would reflect that fact by having a low probability
foraddingtheadverbialphrase“alot”todiscrete events.
Quantification: Consider the sentence “Every agent feels abreeze.” Thesentence has
QUANTIFICATION
only one syntactic parse under E , but it is actually semantically ambiguous; the preferred
0
904 Chapter 23. NaturalLanguageforCommunication
meaning is “For every agent there exists a breeze that the agent feels,” but an acceptable
alternativemeaningis“Thereexistsabreezethateveryagentfeels.”5 Thetwointerpretations
canberepresented as
∀a a∈Agents ⇒
∃b b∈Breezes ∧∃e e∈Feel(a,b)∧During(Now,e);
∃b b∈Breezes ∀a a∈Agents ⇒
∃e e∈Feel(a,b)∧During(Now,e).
The standard approach to quantification is for the grammar to define not an actual logical
QUASI-LOGICAL semantic sentence, butrathera quasi-logical form thatisthenturned into alogical sentence
FORM
byalgorithmsoutside oftheparsing process. Thosealgorithms canhavepreference rulesfor
preferring one quantifier scope overanother—preferences that need not be reflected directly
inthegrammar.
Pragmatics: We have shown how an agent can perceive a string of words and use a
PRAGMATICS
grammar to derive a set of possible semantic interpretations. Now we address the problem
of completing the interpretation by adding context-dependent information about the current
situation. The most obvious need for pragmatic information is in resolving the meaning of
indexicals, which arephrases that referdirectly tothe current situation. Forexample, in the
INDEXICAL
sentence“IaminBostontoday,”both“I”and“today”areindexicals. Theword“I”wouldbe
represented bythefluent Speaker,anditwouldbeuptothehearertoresolvethemeaningof
the fluent—that is not considered part of the grammar but rather an issue of pragmatics; of
usingthecontextofthecurrentsituation tointerpret fluents.
Another part of pragmatics is interpreting the speaker’s intent. The speaker’s action is
considered a speech act, and it is up to the hearer to decipher what type of action it is—a
SPEECHACT
question, a statement, a promise, a warning, a command, and so on. A command such as
“go to22”implicitly refers tothe hearer. Sofar, ourgrammarfor S covers only declarative
sentences. We can easily extend it to cover commands. A command can be formed from
a VP, where the subject is implicitly the hearer. We need to distinguish commands from
statements, sowealtertherulesfor S toincludethetypeofspeechact:
S(Statement(Speaker,pred(obj))) → NP(obj)VP(pred)
S(Command(Speaker,pred(Hearer))) → VP(pred).
LONG-DISTANCE Long-distance dependencies: Questions introduce anew grammatical complexity. In
DEPENDENCIES
“Who did the agent tell you to give the gold to?” the final word “to” should be parsed as
[PP to ], where the “ ” denotes a gap or trace where an NP is missing; the missing NP
TRACE
is licensed by the first word of the sentence, “who.” A complex system of augmentations is
used to make sure that the missing NPs match up with the licensing words in just the right
way,andprohibitgapsinthewrongplaces. Forexample,youcan’thaveagapinonebranch
of an NP conjunction: “What did he play [NP Dungeons and ]?” is ungrammatical. But
youcanhave thesamegapinboth branches ofaVP conjunction: “Whatdidyou[VP [VP
smell ]and[VP shootanarrowat ]]?”
Ambiguity: Insomecases,hearersareconsciously awareofambiguityinanutterance.
AMBIGUITY
Herearesomeexamplestakenfromnewspaperheadlines:
5 Ifthisinterpretationseemsunlikely,consider“EveryProtestantbelievesinajustGod.”
Section23.3. AugmentedGrammarsandSemanticInterpretation 905
Squadhelpsdogbitevictim.
Policebegincampaigntorundownjaywalkers.
Helicopterpoweredbyhumanflies.
Once-sagging clothdiaperindustrysavedbyfulldumps.
Portabletoiletbombed;policehavenothingtogoon.
Teacherstrikesidlekids.
Includeyourchildren whenbakingcookies.
Hospitalsaresuedby7footdoctors.
Milkdrinkers areturningtopowder.
Safetyexpertssayschoolbuspassengers shouldbebelted.
Butmostofthetimethelanguage wehearseemsunambiguous. Thus,whenresearchers first
began to use computers to analyze language in the 1960s, they were quite surprised to learn
that almost every utterance is highly ambiguous, even though the alternative interpretations
mightnotbeapparenttoanativespeaker. Asystemwithalargegrammarandlexiconmight
find thousands of interpretations for a perfectly ordinary sentence. Lexical ambiguity, in
LEXICALAMBIGUITY
which a word has more than one meaning, is quite common; “back” can be an adverb (go
back), anadjective (backdoor), anoun(thebackoftheroom) oraverb(back upyourfiles).
“Jack”canbeaname,anoun(aplayingcard,asix-pointedmetalgamepiece,anauticalflag,
afish,asocket,oradeviceforraisingheavyobjects), oraverb(tojackupacar,tohuntwith
SYNTACTIC a light, or to hit a baseball hard). Syntactic ambiguity refers to a phrase that has multiple
AMBIGUITY
parses: “I smelled awumpus in 2,2” has twoparses: one where the prepositional phrase “in
2,2”modifiesthenounandonewhereitmodifiestheverb. Thesyntacticambiguityleadstoa
SEMANTIC semanticambiguity,becauseoneparsemeansthatthewumpusisin2,2andtheothermeans
AMBIGUITY
thatastenchisin2,2. Inthiscase,gettingthewronginterpretation couldbeadeadlymistake
fortheagent.
Finally, there can be ambiguity between literal and figurative meanings. Figures of
speech are important in poetry, but are surprisingly common in everyday speech as well. A
metonymy is a figure of speech in which one object is used to stand for another. When
METONYMY
we hear “Chrysler announced a new model,” we do not interpret it as saying that compa-
nies can talk; rather we understand that a spokesperson representing the company made the
announcement. Metonymyiscommonandisofteninterpretedunconsciouslybyhumanhear-
ers. Unfortunately, our grammar as it is written is not so facile. To handle the semantics of
metonymyproperly,weneedtointroduceawholenewlevelofambiguity. Wedothisbypro-
vidingtwoobjectsforthesemanticinterpretation ofeveryphraseinthesentence: oneforthe
object that the phrase literally refers to (Chrysler) and one for the metonymic reference (the
spokesperson). We then have to say that there is a relation between the two. In our current
grammar,“Chryslerannounced” getsinterpreted as
x = Chrysler ∧e∈ Announce(x)∧After(Now,Extent(e)).
Weneedtochange thatto
x = Chrysler ∧e∈ Announce(m)∧After(Now,Extent(e))
∧Metonymy(m,x).
906 Chapter 23. NaturalLanguageforCommunication
This says that there is one entity x that is equal to Chrysler, and another entity m that did
theannouncing, andthatthetwoareinametonymyrelation. Thenextstep istodefinewhat
kinds of metonymy relations can occur. The simplest case is when there is no metonymy at
all—theliteralobject xandthemetonymicobjectmareidentical:
∀m,x (m = x) ⇒ Metonymy(m,x).
Forthe Chrysler example, a reasonable generalization is that an organization can be used to
standforaspokesperson ofthatorganization:
∀m,x x∈Organizations ∧Spokesperson(m,x) ⇒ Metonymy(m,x).
Other metonymies include the author for the works (I read Shakespeare) or more generally
theproducer fortheproduct (Idrivea Honda)andthepartforthewhole(TheRedSoxneed
a strong arm). Some examples of metonymy, such as “The ham sandwich on Table 4 wants
anotherbeer,”aremorenovelandareinterpreted withrespecttoasituation.
Ametaphor isanother figure ofspeech, inwhich a phrase withone literal meaning is
METAPHOR
used to suggest a different meaning by way of an analogy. Thus, metaphor can be seen as a
kindofmetonymywheretherelationisoneofsimilarity.
Disambiguation is the process of recovering the most probable intended meaning of
DISAMBIGUATION
an utterance. In one sense wealready have a framework for solving this problem: each rule
has a probability associated with it, so the probability of an interpretation is the product of
the probabilities of the rules that led to the interpretation. Unfortunately, the probabilities
reflect how common the phrases are in the corpus from which the grammar was learned,
and thus reflect general knowledge, not specific knowledge of the current situation. To do
disambiguation properly, weneedtocombinefourmodels:
1. Theworldmodel: thelikelihoodthatapropositionoccursintheworld. Givenwhatwe
know about the world, it is more likely that a speaker who says “I’m dead” means “I
aminbigtrouble”ratherthan“Mylifeended,andyetIcanstilltalk.”
2. Thementalmodel: thelikelihood thatthespeakerformstheintention ofcommunicat-
ing a certain fact to the hearer. This approach combines models of what the speaker
believes, what the speaker believes the hearer believes, and so on. Forexample, when
a politician says, “I am not a crook,” the world model might assign a probability of
only 50% to the proposition that the politician is not a criminal, and 99.999% to the
proposition thatheisnotahooked shepherd’s staff. Nevertheless, weselecttheformer
interpretation because itisamorelikelythingtosay.
3. Thelanguagemodel: thelikelihoodthatacertainstringofwordswillbechosen,given
thatthespeakerhastheintention ofcommunicating acertainfact.
4. The acoustic model: for spoken communication, the likelihood that a particular se-
quenceofsounds willbegenerated, giventhatthespeaker haschosen agivenstringof
words. Section23.5coversspeechrecognition.
Section23.4. MachineTranslation 907
23.4 MACHINE TRANSLATION
Machinetranslationistheautomatictranslationoftextfromonenaturallanguage(thesource)
to another (the target). It was one of the first application areas envisioned for computers
(Weaver, 1949), but it is only in the past decade that the technology has seen widespread
usage. Hereisapassagefrompage1ofthisbook:
AI is one of the newestfields in science and engineering. Work started in earnest soon
afterWorldWarII,andthenameitselfwascoinedin1956. Alongwithmolecularbiol-
ogy, AI is regularlycited as the “field I wouldmost like to be in” by scientists in other
disciplines.
Andhereitistranslated fromEnglishtoDanishbyanonlinetool,GoogleTranslate:
AIerenafdenyesteomra˚derindenforvidenskabogteknik. Arbejdestartedeforalvor
lige efter Anden Verdenskrig, og navnet i sig selv var opfundet i 1956. Sammen med
molekylærbiologi, erAI jævnligtnævntsom “feltet Jeg ville de fleste gernevære i” af
forskereiandrediscipliner.
For those who don’t read Danish, here is the Danish translated back to English. The words
thatcameoutdifferentareinitalics:
AIisoneofthenewestfieldsofscienceandengineering.Workbeganinearnestjustafter
theSecondWorldWar,andthenameitselfwasinventedin1956.Togetherwithmolecular
biology,AIisfrequentlymentionedas “fieldIwouldmostliketobein”byresearchers
inotherdisciplines.
The differences are all reasonable paraphrases, such as frequently mentioned for regularly
cited. Theonly real erroristhe omission ofthe article the, denoted bythe symbol. Thisis
typical accuracy: of the two sentences, one has an error that would not be made by a native
speaker, yetthemeaningisclearlyconveyed.
Historically, there have been three main applications of machine translation. Rough
translation, as provided by free online services, gives the “gist” of a foreign sentence or
document, but contains errors. Pre-edited translation is used by companies to publish their
documentation and sales materials inmultiple languages. The original source text iswritten
in a constrained language that is easier to translate automatically, and the results are usually
edited by ahuman tocorrect anyerrors. Restricted-source translation worksfully automati-
cally,butonlyonhighlystereotypical language, suchasaweatherreport.
Translationisdifficultbecause,inthefullygeneralcase,itrequiresin-depthunderstand-
ing of the text. This is true even for very simple texts—even “texts” of one word. Consider
theword“Open”onthedoorofastore.6 Itcommunicates theideathatthestoreisaccepting
customers at the moment. Now consider the same word “Open” on a large banner outside a
newlyconstructed store. Itmeansthat the store isnowindaily operation, but readers ofthis
signwouldnotfeelmisledifthestoreclosed atnight withoutremoving thebanner. Thetwo
signs use the identical word to convey different meanings. In German the sign on the door
wouldbe“Offen”whilethebannerwouldread“NeuEro¨ffnet.”
6 ThisexampleisduetoMartinKay.
908 Chapter 23. NaturalLanguageforCommunication
The problem is that different languages categorize the world differently. Forexample,
the French word “doux” covers a wide range of meanings corresponding approximately to
the English words “soft,” “sweet,” and “gentle.” Similarly, the English word “hard” covers
virtually all uses of the German word “hart” (physically recalcitrant, cruel) and some uses
of the word “schwierig” (difficult). Therefore, representing the meaning of a sentence is
moredifficultfortranslationthanitisforsingle-language understanding. AnEnglishparsing
system could use predicates like Open(x), but for translation, the representation language
wouldhavetomakemoredistinctions,perhapswithOpen (x)representingthe“Offen”sense
1
andOpen (x)representing the“NeuEro¨ffnet” sense. Arepresentation language that makes
2
allthedistinctions necessary forasetoflanguages iscalledaninterlingua.
INTERLINGUA
A translator (human or machine) often needs to understand the actual situation de-
scribed in the source, not just the individual words. For example, to translate the English
word “him,” into Korean, a choice must be made between the humble and honorific form, a
choicethatdepends onthesocialrelationship betweenthespeakerandthereferent of“him.”
InJapanese, the honorifics are relative, so thechoice depends onthe social relationships be-
tweenthespeaker,thereferent,andthelistener. Translators(bothmachineandhuman)some-
timesfinditdifficult to makethis choice. Asanother example, totranslate “The baseball hit
the window. It broke.” into French, we must choose the feminine “elle” or the masculine
“il” for“it,” so wemust decide whether “it” refers tothe baseball orthe window. Toget the
translation right,onemustunderstand physicsaswellaslanguage.
Sometimes there is no choice that can yield a completely satisfactory translation. For
example, an Italian love poem that uses the masculine “ilsole” (sun) and feminine “laluna”
(moon) to symbolize two lovers will necessarily be altered when translated into German,
wherethegendersarereversed,andfurtheralteredwhentranslatedintoalanguagewherethe
gendersarethesame.7
23.4.1 Machinetranslationsystems
All translation systems must model the source and target languages, but systems vary in the
typeofmodelstheyuse. Somesystemsattempttoanalyzethesourcelanguagetextalltheway
into an interlingua knowledge representation and then generate sentences in the target lan-
guagefromthatrepresentation. Thisisdifficultbecause it involves threeunsolvedproblems:
creatingacompleteknowledgerepresentation ofeverything; parsingintothatrepresentation;
andgenerating sentences fromthatrepresentation.
Othersystemsarebasedonatransfermodel. Theykeepadatabaseoftranslationrules
TRANSFERMODEL
(orexamples), and wheneverthe rule (orexample) matches, they translate directly. Transfer
can occur at the lexical, syntactic, or semantic level. For example, a strictly syntactic rule
maps English [Adjective Noun] to French [Noun Adjective]. A mixed syntactic and lexical
rulemapsFrench[S “etpuis”S ]toEnglish[S “andthen”S ]. Figure23.12diagramsthe
1 2 1 2
varioustransferpoints.
7 WarrenWeaver(1949)reportsthatMaxZeldnerpointsoutthatthegreatHebrewpoetH.N.Bialikoncesaid
thattranslation“islikekissingthebridethroughaveil.”
Section23.4. MachineTranslation 909
Interlingua Semantics
Attraction(NamedJohn, NamedMary, High)
English Semantics French Semantics
Loves(John, Mary) Aime(Jean, Marie)
English Syntax French Syntax
S(NP(John), VP(loves, NP(Mary))) S(NP(Jean), VP(aime, NP(Marie)))
English Words French Words
John loves Mary Jean aime Marie
Figure 23.12 The Vauquois triangle: schematic diagram of the choices for a machine
translationsystem (Vauquois,1968). We startwith English textatthe top. An interlingua-
based system follows the solid lines, parsing English first into a syntactic form, then into
a semanticrepresentationandaninterlinguarepresentation,andthenthroughgenerationto
a semantic, syntactic, andlexicalformin French. A transfer-basedsystem usesthe dashed
linesasa shortcut. Differentsystemsmakethetransferatdifferentpoints; somemakeitat
multiplepoints.
23.4.2 Statisticalmachine translation
Now that we have seen how complex the translation task can be, it should come as no sur-
prisethatthemostsuccessfulmachinetranslationsystemsarebuiltbytrainingaprobabilistic
model using statistics gathered from a large corpus of text. This approach does not need
a complex ontology of interlingua concepts, nor does it need handcrafted grammars of the
sourceandtargetlanguages, norahand-labeled treebank. Allitneedsisdata—sampletrans-
lationsfromwhichatranslationmodelcanbelearned. Totranslateasentencein,say,English
∗
(e)intoFrench(f),wefindthestringofwords f thatmaximizes
f ∗ = argmaxP(f |e) = argmaxP(e|f)P(f).
f
Here the factor P(f)is the target language model forFrench; it says how probable a given
LANGUAGEMODEL
TRANSLATION sentence is in French. P(e|f) is the translation model; it says how probable an English
MODEL
sentence is as a translation for a given French sentence. Similarly, P(f |e) is a translation
modelfromEnglishtoFrench.
Shouldweworkdirectly on P(f|e),orapplyBayes’ruleandworkon P(e|f)P(f)?
In diagnostic applications like medicine, it is easier to model the domain in the causal di-
rection: P(symptoms|disease)ratherthan P(disease|symptoms). Butintranslation both
directions are equally easy. The earliest work in statistical machine translation did apply
Bayes’ rule—in part because the researchers had agood language model, P(f), andwanted
to make use of it, and in part because they came from a background in speech recognition,
which is a diagnostic problem. We follow their lead in this chapter, but we note that re-
cent work in statistical machine translation often optimizes P(f |e) directly, using a more
sophisticated modelthattakesintoaccount manyofthefeatures fromthelanguage model.
910 Chapter 23. NaturalLanguageforCommunication
The language model, P(f), could address any level(s) on the right-hand side of Fig-
ure 23.12, but the easiest and most common approach is to build an n-gram model from a
French corpus, as we have seen before. This captures only a partial, local idea of French
sentences; however,thatisoftensufficientforroughtranslation.8
Thetranslationmodelislearnedfromabilingualcorpus—acollectionofparalleltexts,
BILINGUALCORPUS
each an English/French pair. Now, if we had an infinitely large corpus, then translating a
sentence wouldjustbealookuptask: wewouldhaveseentheEnglishsentence beforeinthe
corpus, so we could just return the paired French sentence. But of course our resources are
finite, and most of the sentences we will be asked to translate will be novel. However, they
will be composed of phrases that we have seen before (even if some phrases are as short as
one word). For example, in this book, common phrases include “in this exercise we will,”
“sizeofthestatespace,”“asafunction ofthe”and“notesattheendofthechapter.” Ifasked
totranslatethenovelsentence“Inthisexercisewewillcomputethesizeofthestatespaceasa
function ofthenumberofactions.” intoFrench,weshouldbeabletobreakthesentenceinto
phrases, find the phrases in the English corpus (this book), find the corresponding French
phrases (from the French translation of the book), and then reassemble the French phrases
intoanorderthatmakessenseinFrench. Inotherwords,givenasourceEnglishsentence, e,
findingaFrenchtranslation f isamatterofthreesteps:
1. BreaktheEnglishsentenceintophrases e ,...,e .
1 n
2. For each phrase e , choose a corresponding French phrase f . We use the notation
i i
P(f |e )forthephrasal probability that f isatranslation ofe .
i i i i
3. Choose a permutation of the phrases f ,...,f . Wewill specify this permutation in a
1 n
way that seems a little complicated, but is designed to have a simple probability dis-
tribution: For each f , we choose a distortion d , which is the number of words that
DISTORTION i i
phrase f i hasmovedwithrespectto f i−1 ;positiveformovingtotheright, negativefor
movingtotheleft,andzeroiff i immediatelyfollowsf i−1 .
Figure 23.13 shows an example of the process. At the top, the sentence “There is a smelly
wumpus sleeping in 2 2” is broken into five phrases, e ,...,e . Each of them is translated
1 5
into a corresponding phrase f , and then these are permuted into the order f ,f ,f ,f ,f .
i 1 3 4 2 5
Wespecifythepermutation intermsofthedistortions d ofeachFrenchphrase, definedas
i
d i = START(f i )−END(f i−1 )−1,
where START(f
i
)istheordinal numberofthefirstwordofphrase f
i
intheFrenchsentence,
andEND(f i−1 )istheordinalnumberofthelastwordofphrase f i−1 . InFigure23.13wesee
thatf ,“a` 22,”immediatelyfollowsf ,“quidort,”andthusd =0. Phrasef ,however,has
5 4 5 2
movedonewords totheright of f ,sod =1. Asaspecial case wehave d =0, because f
1 2 1 1
startsatposition 1and END(f
0
)isdefinedtobe0(eventhough f
0
doesnotexist).
Now that we have defined the distortion, d , we can define the probability distribution
i
for distortion, P(d ). Note that for sentences bounded by length n we have |d | ≤ n , and
i i
8 For the finer points of translation, n-grams are clearly not enough. Marcel Proust’s 4000-page novel A la
re´cherchedutempsperdubeginsandendswiththesameword(longtemps),sosometranslatorshavedecidedto
dothesame,thusbasingthetranslationofthefinalwordononethatappearedroughly2millionwordsearlier.
Section23.4. MachineTranslation 911
so the full probability distribution P(d ) has only 2n + 1 elements, far fewer numbers to
i
learn than the number of permutations, n!. That is why we defined the permutation in this
circuitous way. Of course, this is a rather impoverished model of distortion. It doesn’t say
that adjectives are usually distorted to appear after the noun when we are translating from
EnglishtoFrench—that factisrepresented intheFrenchlanguage model,P(f). Thedistor-
tion probability is completely independent of the words in the phrases—it depends only on
the integer value d . Theprobability distribution provides a summary of the volatility of the
i
permutations; howlikelyadistortion ofP(d=2)is,comparedtoP(d=0),forexample.
We’re ready now to put it all together: we can define P(f,d|e), the probability that
thesequenceofphrases f withdistortions disatranslation ofthesequenceofphrases e. We
make the assumption that each phrase translation and each distortion is independent of the
others, andthuswecanfactortheexpression as
(cid:25)
P(f,d|e) = P(f |e )P(d )
i i i
i
e e e e e
1 2 3 4 5
There is a smelly wumpus sleeping in 2 2
f f f f f
1 3 2 4 5
Il y a un wumpus malodorant qui dort à 2 2
d = 0 d = -2 d = +1 d = +1 d = 0
1 3 2 4 5
Figure23.13 CandidateFrenchphrasesforeachphraseofanEnglishsentence,withdis-
tortion(d)valuesforeachFrenchphrase.
Thatgivesusawaytocomputetheprobability P(f,d|e)foracandidate translation f
anddistortion d. Buttofindthebestf anddwecan’t justenumerate sentences; withmaybe
100 French phrases for each English phrase in the corpus, there are 1005 different 5-phrase
translations, and5!reorderings foreachofthose. Wewillhavetosearchforagoodsolution.
A local beam search (see page 125) with a heuristic that estimates probability has proven
effectiveatfindinganearly-most-probable translation.
Allthat remains is to learn the phrasal and distortion probabilities. Wesketch the pro-
cedure;seethenotesattheendofthechapterfordetails.
1. Findparalleltexts: First,gatheraparallelbilingual corpus. Forexample,a Hansard9
HANSARD
is a record of parliamentary debate. Canada, Hong Kong, and other countries pro-
duce bilingual Hansards, the European Union publishes its official documents in 11
languages, and the United Nations publishes multilingual documents. Bilingual text is
also available online; some Web sites publish parallel content with parallel URLs, for
9 NamedafterWilliamHansard,whofirstpublishedtheBritishparliamentarydebatesin1811.
912 Chapter 23. NaturalLanguageforCommunication
example,/en/fortheEnglishpageand/fr/forthecorrespondingFrenchpage. The
leadingstatistical translation systemstrainonhundreds ofmillionsofwordsofparallel
textandbillions ofwordsofmonolingual text.
2. Segmentintosentences: Theunitoftranslation isasentence,sowewillhavetobreak
the corpus into sentences. Periods are strong indicators of the end of a sentence, but
consider “Dr. J. R. Smith of Rodeo Dr. paid $29.99 on 9.9.09.”; only the final period
ends a sentence. One way to decide if a period ends a sentence is to train a model
that takes as features the surrounding words and their parts of speech. This approach
achievesabout98%accuracy.
3. Alignsentences: Foreachsentence intheEnglishversion,determine whatsentence(s)
it corresponds to in the French version. Usually, the next sentence of English corre-
sponds tothe nextsentence ofFrench ina1:1 match, but sometimes there isvariation:
onesentenceinonelanguagewillbesplitintoa2:1match,ortheorderoftwosentences
willbeswapped,resultingina2:2match. Bylookingatthesentencelengthsalone(i.e.
shortsentences shouldalignwithshortsentences), itispossibletoalignthem(1:1,1:2,
or 2:2, etc.) with accuracy in the 90% to 99% range using a variation on the Viterbi
algorithm. Evenbetteralignmentcanbeachievedbyusinglandmarksthatarecommon
toboth languages, such asnumbers, dates, proper names, orwords that weknow from
abilingualdictionaryhaveanunambiguoustranslation. Forexample,ifthe3rdEnglish
and 4th French sentences contain the string “1989” and neighboring sentences do not,
thatisgoodevidencethatthesentences shouldbealignedtogether.
4. Alignphrases: Withinasentence,phrasescanbealignedbyaprocessthatissimilarto
that used for sentence alignment, but requiring iterative improvement. When we start,
wehavenowayofknowingthat“quidort”alignswith“sleeping,” butwecanarriveat
thatalignmentbyaprocessofaggregationofevidence. Overalltheexamplesentences
we have seen, we notice that “qui dort” and “sleeping” co-occur with high frequency,
and that in the pair of aligned sentences, no phrase other than “qui dort” co-occurs so
frequently in other sentences with “sleeping.” A complete phrase alignment over our
corpusgivesusthephrasalprobabilities (afterappropriate smoothing).
5. Extract distortions: Once we have an alignment of phrases we can define distortion
probabilities. Simplycount howoften distortion occurs in thecorpus foreachdistance
d= 0,±1,±2,...,andapplysmoothing.
6. ImproveestimateswithEM:Useexpectation–maximization toimprovetheestimates
of P(f |e) and P(d) values. We compute the best alignments with the current values
oftheseparametersintheEstep,thenupdatetheestimatesintheMstepanditeratethe
processuntilconvergence.
23.5 SPEECH RECOGNITION
SPEECH Speechrecognitionisthetaskofidentifyingasequenceofwordsutteredbyaspeaker, given
RECOGNITION
the acoustic signal. It has become one of the mainstream applications of AI—millions of
Section23.5. SpeechRecognition 913
people interact with speech recognition systems every day to navigate voice mail systems,
search the Web from mobile phones, and other applications. Speech is an attractive option
whenhands-free operation isnecessary, aswhenoperating machinery.
Speech recognition is difficult because the sounds made by a speaker are ambiguous
and, well, noisy. As a well-known example, the phrase “recognize speech” sounds almost
the same as “wreck a nice beach” when spoken quickly. Even this short example shows
several of the issues that make speech problematic. First, segmentation: written words in
SEGMENTATION
English have spaces between them, but in fast speech there are no pauses in “wreck a nice”
that would distinguish it as a multiword phrase as opposed to the single word “recognize.”
Second, coarticulation: when speaking quickly the “s” sound at the end of “nice” merges
COARTICULATION
with the “b” sound at the beginning of “beach,” yielding something that is close to a “sp.”
Another problem that does not show up in this example is homophones—words like “to,”
HOMOPHONES
“too,”and“two”thatsoundthesamebutdifferinmeaning.
Wecanviewspeech recognition asaproblem inmost-likely-sequence explanation. As
we saw in Section 15.2, this is the problem of computing the most likely sequence of state
variables, x , given a sequence of observations e . In this case the state variables are the
1:t 1:t
words,andtheobservationsaresounds. Moreprecisely,anobservationisavectoroffeatures
extractedfromtheaudiosignal. Asusual,themostlikelysequencecanbecomputedwiththe
helpofBayes’ruletobe:
argmaxP(word |sound )= argmaxP(sound |word )P(word ).
1:t 1:t 1:t 1:t 1:t
word1:t word1:t
Here P(sound |word ) is the acoustic model. It describes the sounds of words—that
ACOUSTICMODEL 1:t 1:t
“ceiling” begins with a soft “c” and sounds the same as “sealing.” P(word ) is known as
1:t
the language model. It specifies the prior probability of each utterance—for example, that
LANGUAGEMODEL
“ceiling fan”isabout500timesmorelikelyasawordsequence than“sealingfan.”
NOISYCHANNEL This approach was named the noisy channel model by Claude Shannon (1948). He
MODEL
described asituation inwhichanoriginal message (the wordsinourexample) istransmitted
over a noisy channel (such as a telephone line) such that a corrupted message (the sounds
in our example) are received at the other end. Shannon showed that no matter how noisy
the channel, it is possible to recover the original message with arbitrarily small error, if we
encode the original message in a redundant enough way. The noisy channel approach has
beenapplied tospeechrecognition, machinetranslation, spellingcorrection, andothertasks.
Once we define the acoustic and language models, we can solve for the most likely
sequence of words using the Viterbi algorithm (Section 15.2.3 on page 576). Most speech
recognition systems usealanguage modelthatmakestheMarkovassumption—that thecur-
rentstateWord dependsonlyonafixednumbernofpreviousstates—andrepresent Word
t t
asasinglerandom variable takingonafinitesetofvalues, whichmakesitaHiddenMarkov
Model(HMM).Thus,speechrecognitionbecomesasimpleapplicationoftheHMMmethod-
ology,asdescribedinSection15.3—simplethatis,oncewedefinetheacousticandlanguage
models. Wecoverthemnext.
914 Chapter 23. NaturalLanguageforCommunication
Vowels Consonants B–N Consonants P–Z
Phone Example Phone Example Phone Example
[iy] beat [b] bet [p] pet
[ih] bit [ch] Chet [r] rat
[eh] bet [d] debt [s] set
[æ] bat [f] fat [sh] shoe
[ah] but [g] get [t] ten
[ao] bought [hh] hat [th] thick
[ow] boat [hv] high [dh] that
[uh] book [jh] jet [dx] butter
[ey] bait [k] kick [v] vet
[er] Bert [l] let [w] wet
[ay] buy [el] bottle [wh] which
[oy] boy [m] met [y] yet
[axr] diner [em] bottom [z] zoo
[aw] down [n] net [zh] measure
[ax] about [en] button
[ix] roses [ng] sing
[aa] cot [eng] washing [-] silence
Figure 23.14 The ARPA phonetic alphabet, or ARPAbet, listing all the phonesused in
American English. There are several alternative notations, including an InternationalPho-
neticAlphabet(IPA),whichcontainsthephonesinallknownlanguages.
23.5.1 Acousticmodel
Sound waves are periodic changes in pressure that propagate through the air. When these
waves strike the diaphragm of a microphone, the back-and-forth movement generates an
electric current. An analog-to-digital converter measures the size of the current—which ap-
proximates theamplitude ofthesound wave—atdiscrete intervals called the samplingrate.
SAMPLINGRATE
Speechsounds, whicharemostlyintherangeof100Hz(100cyclespersecond)to1000Hz,
are typically sampled at arate of 8 kHz. (CDs and mp3 files are sampled at 44.1 kHz.) The
QUANTIZATION precisionofeachmeasurementisdeterminedbythequantizationfactor;speechrecognizers
FACTOR
typically keep 8 to 12 bits. That means that a low-end system, sampling at 8 kHzwith 8-bit
quantization, wouldrequirenearlyhalfamegabyteperminuteofspeech.
Since we only want to know what words were spoken, not exactly what they sounded
like, wedon’t need tokeep allthatinformation. Weonly need todistinguish between differ-
entspeechsounds. Linguistshaveidentifiedabout100speechsounds,orphones,thatcanbe
PHONE
composed to form all the words in all known human languages. Roughly speaking, a phone
is the sound that corresponds to a single vowel or consonant, but there are some complica-
tions: combinations ofletters,suchas“th”and“ng”producesinglephones, andsomeletters
producedifferentphonesindifferentcontexts(e.g.,the“a”inratandrate. Figure23.14lists
Section23.5. SpeechRecognition 915
allthephones thatareused inEnglish, withanexampleofeach. Aphonemeisthesmallest
PHONEME
unit of sound that has a distinct meaning to speakers of a particular language. Forexample,
the“t”in“stick”sounds similarenough tothe“t”in“tick”thatspeakers ofEnglishconsider
them thesamephoneme. Butthedifference issignificant intheThailanguage, sothere they
aretwophonemes. TorepresentspokenEnglishwewantarepresentationthatcandistinguish
betweendifferentphonemes,butonethatneednotdistinguishthenonphonemicvariationsin
sound: loudorsoft,fastorslow,maleorfemalevoice,etc.
First, we observe that although the sound frequencies in speech may be several kHz,
the changes in the content of the signal occur much less often, perhaps at no more than 100
Hz. Therefore, speechsystemssummarizetheproperties ofthesignalovertimeslicescalled
frames. Aframelength ofabout 10milliseconds (i.e.,80samples at 8kHz)isshort enough
FRAME
toensurethatfewshort-duration phenomena willbemissed. Overlapping framesareusedto
makesurethatwedon’tmissasignalbecause ithappens tofallonaframeboundary.
Eachframe issummarized byavectorof features. Picking outfeatures from aspeech
FEATURE
signal is like listening to an orchestra and saying “here the French horns are playing loudly
and the violins are playing softly.” We’ll give a brief overview of the features in a typical
system. First, a Fourier transform is used to determine the amount of acoustic energy at
about a dozen frequencies. Then we compute a measure called the mel frequency cepstral
MELFREQUENCY
CEPSTRAL coefficient (MFCC) or MFCC for each frequency. We also compute the total energy in
COEFFICIENT(MFCC)
the frame. That gives thirteen features; for each one we compute the difference between
this frame and the previous frame, and the difference between differences, for a total of 39
features. These arecontinuous-valued; theeasiest waytofitthem intothe HMMframework
istodiscretizethevalues. (ItisalsopossibletoextendtheHMMmodeltohandlecontinuous
mixtures of Gaussians.) Figure 23.15 shows the sequence of transformations from the raw
soundtoasequence offrameswithdiscrete features.
We have seen how to go from the raw acoustic signal to a series of observations, e .
t
Now we have to describe the (unobservable) states of the HMM and define the transition
model, P(X t |X t−1 ), and the sensor model, P(E t |X t ). The transition model can be broken
into two levels: word and phone. We’ll start from the bottom: the phone model describes
PHONEMODEL
Analog acoustic signal:
Sampled, quantized
digital signal:
10 15 38 22 63 24 10 12 73
Frames with features:
52 47 82 89 94 11
Figure23.15 Translatingthe acoustic signal into a sequence of frames. In this diagram
each frame is described by the discretized values of three acoustic features; a real system
wouldhavedozensoffeatures.
916 Chapter 23. NaturalLanguageforCommunication
Phone HMM for [m]:
0.3 0.9 0.4
0.7 0.1 0.6
Onset Mid End FINAL
Output probabilities for the phone HMM:
Onset: Mid: End:
C : 0.5 C : 0.2 C : 0.1
1 3 4
C : 0.2 C : 0.7 C : 0.5
2 4 6
C : 0.3 C : 0.1 C : 0.4
3 5 7
Figure 23.16 An HMM for the three-state phone [m]. Each state has several possible
outputs,eachwithitsownprobability.TheMFCCfeaturelabelsC throughC arearbitrary,
1 7
standingforsomecombinationoffeaturevalues.
(a) Word model with dialect variation:
0.5 [ey] 1.0
1.0 1.0 1.0
[t] [ow] [m] [t] [ow]
0.5 1.0
[aa]
(b) Word model with coarticulation and dialect var:iations
0.2 [ow] 1.0 0.5 [ey] 1.0
1.0
[t] [m] [t] [ow]
0.8 1.0 0.5 1.0
[ah] [aa]
Figure23.17 Twopronunciationmodelsoftheword“tomato.” Eachmodelis shownas
atransitiondiagramwithstatesascirclesandarrowsshowingallowedtransitionswiththeir
associatedprobabilities. (a)Amodelallowingfordialect differences. The0.5numbersare
estimatesbasedonthetwoauthors’preferredpronunciations.(b)Amodelwithacoarticula-
tioneffectonthefirstvowel,allowingeitherthe[ow]orthe[ah]phone.
Section23.5. SpeechRecognition 917
a phone as three states, the onset, middle, and end. For example, the [t] phone has a silent
beginning, asmallexplosive burst ofsound inthemiddle, and(usually) ahissing attheend.
Figure 23.16 shows an example for the phone [m]. Note that in normal speech, an average
phone has a duration of 50–100 milliseconds, or 5–10 frames. The self-loops in each state
allowsforvariation inthis duration. Bytaking manyself-loops (especially inthe midstate),
we can represent a long “mmmmmmmmmmm” sound. Bypassing the self-loops yields a
short“m”sound.
PRONUNCIATION In Figure 23.17 the phone models are strung together to form a pronunciation model
MODEL
fora word. According to Gershwin (1937), you say [t ow m ey t ow] and I say [t ow m aa t
ow]. Figure 23.17(a) shows a transition model that provides for this dialect variation. Each
ofthecirclesinthisdiagramrepresents aphonemodelliketheoneinFigure23.16.
Inaddition todialect variation, wordscan have coarticulation variation. Forexample,
the [t] phone is produced with the tongue at the top of the mouth, whereas the [ow] has the
tongue near the bottom. When speaking quickly, the tongue doesn’t have time to get into
position for the [ow], and we end up with [t ah] rather than [t ow]. Figure 23.17(b) gives
a model for “tomato” that takes this coarticulation effect into account. More sophisticated
phonemodelstakeintoaccountthecontextofthesurrounding phones.
There can be substantial variation in pronunciation for a word. The most common
pronunciation of “because” is [b iy k ah z], but that only accounts for about a quarter of
uses. Anotherquarter(approximately) substitutes [ix],[ih]or[ax]forthefirstvowel,andthe
remainder substitute [ax] or [aa] for the second vowel, [zh] or [s] for the final [z], or drop
“be”entirely, leaving“cuz.”
23.5.2 Languagemodel
For general-purpose speech recognition, the language model can be an n-gram model of
text learned from a corpus of written sentences. However, spoken language has different
characteristics than written language, so it is better to get a corpus of transcripts of spoken
language. For task-specific speech recognition, the corpus should be task-specific: to build
yourairlinereservationsystem,gettranscriptsofpriorcalls. Italsohelpstohavetask-specific
vocabulary, suchasalistofalltheairportsandcitiesserved,andalltheflightnumbers.
Partofthedesignofavoiceuserinterfaceistocoercetheuserintosayingthingsfroma
limitedsetofoptions,sothatthespeechrecognizerwillhaveatighterprobabilitydistribution
todeal with. Forexample, asking “What city do you wantto go to?” elicits aresponse with
ahighlyconstrained language model,whileasking“HowcanIhelpyou?” doesnot.
23.5.3 Building a speech recognizer
Thequalityofaspeechrecognition systemdependsonthequalityofallofitscomponents—
the language model, the word-pronunciation models, the phone models, and the signal-
processing algorithms used to extract spectral features from the acoustic signal. We have
discussed how the language model can be constructed from a corpus of written text, and we
leave the details of signal processing to other textbooks. We are left with the pronunciation
andphonemodels. Thestructureofthepronunciationmodels—suchasthetomatomodelsin
918 Chapter 23. NaturalLanguageforCommunication
Figure23.17—isusuallydevelopedbyhand. Largepronunciation dictionaries arenowavail-
able for English and other languages, although their accuracy varies greatly. The structure
of the three-state phone models is the same for all phones, as shown in Figure 23.16. That
leavestheprobabilities themselves.
Asusual, wewillacquire theprobabilities from acorpus, thistimeacorpus ofspeech.
The most common type of corpus to obtain is one that includes the speech signal for each
sentence paired with a transcript of the words. Building a model from this corpus is more
difficult than building an n-gram model of text, because we have to build a hidden Markov
model—thephonesequenceforeachwordandthephonestateforeachtimeframearehidden
variables. In the early days of speech recognition, the hidden variables were provided by
laborious hand-labeling of spectrograms. Recent systems use expectation–maximization to
automaticallysupplythemissingdata. Theideaissimple: givenanHMMandanobservation
sequence, wecan use thesmoothing algorithms from Sections 15.2 and15.3 tocompute the
probability ofeachstateateachtimestepand,byasimpleextension, theprobability ofeach
state–state pair at consecutive time steps. These probabilities can be viewed as uncertain
labels. From the uncertain labels, we can estimate new transition and sensor probabilities,
and the EM procedure repeats. The method is guaranteed to increase the fit between model
anddataoneachiteration,anditgenerallyconvergestoamuchbettersetofparametervalues
thanthoseprovidedbytheinitial, hand-labeled estimates.
The systems with the highest accuracy work by training a different model for each
speaker, thereby capturing differences indialect aswellasmale/female andothervariations.
This training can require several hours of interaction with the speaker, so the systems with
themostwidespread adoption donotcreatespeaker-specific models.
Theaccuracyofasystemdependsonanumberoffactors. First,thequalityofthesignal
matters: ahigh-qualitydirectionalmicrophoneaimedatastationarymouthinapaddedroom
will do much better than a cheap microphone transmitting a signal over phone lines from a
car in traffic with the radio playing. The vocabulary size matters: when recognizing digit
strings withavocabulary of11words(1-9plus“oh”and“zero”), theworderrorratewillbe
below 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary,
and20%onacorpuswitha64,000-wordvocabulary. Thetaskmatterstoo: whenthesystem
is trying to accomplish a specific task—book a flight or give directions to a restaurant—the
taskcanoftenbeaccomplished perfectly evenwithaworderrorrateof10%ormore.
23.6 SUMMARY
Natural language understanding is one of the most important subfields of AI. Unlike most
otherareasofAI,naturallanguageunderstandingrequires anempiricalinvestigationofactual
humanbehavior—which turnsouttobecomplexandinteresting.
• Formal language theory and phrase structure grammars (and in particular, context-
free grammar) are useful tools fordealing withsome aspects of natural language. The
probabilistic context-free grammar(PCFG)formalismiswidelyused.
Bibliographical andHistorical Notes 919
• Sentences in a context-free language can be parsed in O(n3) time by a chart parser
suchastheCYKalgorithm,whichrequiresgrammarrulestobein ChomskyNormal
Form.
• Atreebankcanbeusedtolearnagrammar. Itisalsopossibletolearnagrammarfrom
anunparsed corpusofsentences, butthisislesssuccessful.
• Alexicalized PCFGallows ustorepresent thatsome relationships between words are
morecommonthanothers.
• Itisconvenient toaugmentagrammartohandlesuchproblemsassubject–verb agree-
mentandpronouncase. Definiteclausegrammar(DCG)isaformalismthatallowsfor
augmentations. With DCG, parsing and semantic interpretation (and even generation)
canbedoneusinglogicalinference.
• Semanticinterpretation canalsobehandled byanaugmentedgrammar.
• Ambiguity is a very important problem in natural language understanding; most sen-
tences have manypossible interpretations, but usually only one is appropriate. Disam-
biguation relies on knowledge about the world, about the current situation, and about
languageuse.
• Machine translation systems have been implemented using a range of techniques,
from full syntactic and semantic analysis to statistical techniques based on phrase fre-
quencies. Currentlythestatistical modelsaremostpopularandmostsuccessful.
• Speech recognition systems are also primarily based on statistical principles. Speech
systemsarepopularanduseful, albeitimperfect.
• Together, machine translation and speech recognition are two of the big successes of
natural language technology. One reason that the models perform well is that large
corpora areavailable—both translation and speech aretasks that areperformed “inthe
wild” by people every day. In contrast, tasks like parsing sentences have been less
successful, in part because no large corpora of parsed sentences are available “in the
wild”andinpartbecauseparsing isnotusefulinandofitself.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Like semantic networks, context-free grammars (also known as phrase structure grammars)
are areinvention of atechnique firstused byancient Indian grammarians (especially Panini,
ca. 350 B.C.) studying Shastric Sanskrit (Ingerman, 1967). They were reinvented by Noam
Chomsky (1956) for the analysis of English syntax and independently by John Backus for
the analysis of Algol-58 syntax. PeterNaurextended Backus’s notation and is now credited
(Backus, 1996) with the “N” in BNF, which originally stood for “Backus Normal Form.”
ATTRIBUTE Knuth (1968) defined a kind of augmented grammar called attribute grammar that is use-
GRAMMAR
ful for programming languages. Definite clause grammars were introduced by Colmer-
auer(1975)anddeveloped andpopularized byPereiraandShieber(1987).
Probabilistic context-free grammars were investigated by Booth (1969) and Salo-
maa(1969). Otheralgorithms forPCFGsare presented in theexcellent short monograph by
920 Chapter 23. NaturalLanguageforCommunication
Charniak(1993)andtheexcellentlongtextbooksbyManningandSchu¨tze(1999)andJuraf-
sky and Martin (2008). Baker (1979) introduces the inside–outside algorithm for learning a
PCFG,andLariandYoung(1990)describeitsusesandlimitations. StolckeandOmohundro
(1994) showhowtolearngrammarruleswithBayesianmodelmerging; HaghighiandKlein
(2006)describe alearningsystem basedonprototypes.
Lexicalized PCFGs(Charniak, 1997; Hwa,1998) combine thebest aspects ofPCFGs
and n-gram models. Collins (1999) describes PCFG parsing that is lexicalized with head
features. Petrovand Klein(2007a) show howtogetthe advantages oflexicalization without
actuallexicalaugmentationsbylearningspecificsyntacticcategoriesfromatreebankthathas
generalcategories;forexample,thetreebankhasthecategoryNP,fromwhichmorespecific
categories suchasNP andNP canbelearned.
O S
There have been many attempts to write formal grammars of natural languages, both
in “pure” linguistics and in computational linguistics. There are several comprehensive but
informalgrammarsofEnglish(Quirk etal.,1985;McCawley,1988;HuddlestonandPullum,
2002). Since the mid-1980s, there has been a trend toward putting more information in the
lexicon and less in the grammar. Lexical-functional grammar, or LFG (Bresnan, 1982) was
the first major grammar formalism to be highly lexicalized. If we carry lexicalization to an
extreme, weendupwith categorial grammar (ClarkandCurran, 2004), inwhichthere can
be as few as two grammar rules, or with dependency grammar (Smith and Eisner, 2008;
Ku¨bleretal.,2009)inwhichtherearenosyntacticcategories, onlyrelationsbetweenwords.
Sleator and Temperley (1993) describe a dependency parser. Paskin (2001) shows that a
versionofdependency grammariseasiertolearnthanPCFGs.
The first computerized parsing algorithms were demonstrated by Yngve (1955). Ef-
ficient algorithms were developed in the late 1960s, with a few twists since then (Kasami,
1965;Younger, 1967;Earley,1970;Graham etal.,1980). MaxwellandKaplan(1993)show
how chart parsing with augmentations can be made efficient in the average case. Church
and Patil (1982) address the resolution of syntactic ambiguity. Klein and Manning (2003)
∗ ∗
describe A parsing, and Pauls and Klein (2009) extend that to K-best A parsing, in which
theresultisnotasingleparsebutthe Kbest.
Leading parsers today include those by Petrov and Klein (2007b), which achieved
90.6% accuracy on the Wall Street Journal corpus, Charniak and Johnson (2005), which
achieved 92.0%, and Kooet al. (2008), which achieved 93.2% on the Penn treebank. These
numbersarenotdirectlycomparable,andthereissomecriticismofthefieldthatitisfocusing
toonarrowlyonafewselectcorpora, andperhapsoverfitting onthem.
Formal semantic interpretation of natural languages originates within philosophy and
formallogic, particularly AlfredTarski’s(1935) workonthesemantics offormallanguages.
Bar-Hillel (1954) was the firstto consider the problems ofpragmatics and propose that they
could be handled by formal logic. For example, he introduced C. S. Peirce’s (1902) term
indexical into linguistics. Richard Montague’s essay “English as a formal language” (1970)
is a kind of manifesto for the logical analysis of language, but the books by Dowty et al.
(1991)andPortnerandPartee(2002)aremorereadable.
The first NLP system to solve an actual task was probably the BASEBALL question
answeringsystem(Green etal.,1961),whichhandledquestionsaboutadatabaseofbaseball
Bibliographical andHistorical Notes 921
statistics. CloseafterthatwasWoods’s (1973) LUNAR,whichanswered questions about the
rocks brought back from the moon by the Apollo program. Roger Schank and his students
built a series of programs (Schank and Abelson, 1977; Schank and Riesbeck, 1981) that
all had the task of understanding language. Modern approaches to semantic interpretation
usually assume that the mapping from syntax to semantics will be learned from examples
(ZelleandMooney,1996;ZettlemoyerandCollins,2005).
Hobbsetal. (1993) describes aquantitative nonprobabilistic framework forinterpreta-
tion. More recent work follows an explicitly probabilistic framework (Charniak and Gold-
man, 1992; Wu, 1993; Franz, 1996). Inlinguistics, optimality theory (Kager, 1999) isbased
on the idea of building soft constraints into the grammar, giving a natural ranking to inter-
pretations (similartoaprobability distribution), ratherthan having thegrammargenerate all
possibilities with equal rank. Norvig (1988) discusses the problems of considering multiple
simultaneous interpretations, rather than settling for a single maximum-likelihood interpre-
tation. Literary critics (Empson, 1953; Hobbs, 1990) have been ambiguous about whether
ambiguityissomethingtoberesolved orcherished.
Nunberg(1979)outlinesaformalmodelofmetonymy. LakoffandJohnson(1980)give
anengaginganalysisandcatalogofcommonmetaphorsinEnglish. Martin(1990)andGibbs
(2006)offercomputational modelsofmetaphorinterpretation.
The first important result on grammar induction was a negative one: Gold (1967)
showed that it is not possible to reliably learn a correct context-free grammar, given a set of
strings fromthatgrammar. Prominentlinguists, suchasChomsky(1957) andPinker(2003),
UNIVERSAL have used Gold’s result to argue that there must be an innate universal grammar that all
GRAMMAR
children have from birth. The so-called Poverty of the Stimulus argument says that children
aren’t given enough input tolearn aCFG,so theymust already “know” the grammarand be
merelytuningsomeofitsparameters. Whilethisargumentcontinuestoholdswaythroughout
muchofChomskyanlinguistics,ithasbeendismissedbysomeotherlinguists(Pullum,1996;
Elman et al., 1997) and most computer scientists. Asearly as 1969, Horning showed that it
ispossibletolearn,inthesenseofPAClearning, aprobabilistic context-freegrammar. Since
then, there have been many convincing empirical demonstrations of learning from positive
examplesalone,suchastheILPworkofMooney(1999)andMuggletonandDeRaedt(1994),
thesequencelearningofNevill-ManningandWitten(1997),andtheremarkablePh.D.theses
of Schu¨tze (1995) and de Marcken (1996). There is an annual International Conference on
Grammatical Inference (ICGI). It is possible to learn other grammar formalisms, such as
regularlanguages(Denis,2001)andfinitestateautomata(ParekhandHonavar,2001). Abney
(2007)isatextbookintroduction tosemi-supervised learning forlanguage models.
Wordnet(Fellbaum,2001)isapubliclyavailabledictionaryofabout100,000wordsand
phrases, categorized into parts of speech and linked by semantic relations such as synonym,
antonym, and part-of. The Penn Treebank (Marcus et al., 1993) provides parse trees for a
3-million-word corpus of English. Charniak (1996) and Klein and Manning (2001) discuss
parsing with treebank grammars. The British National Corpus (Leech et al., 2001) contains
100 million words, and the World Wide Web contains several trillion words; (Brants et al.,
2007)describe n-grammodelsovera2-trillion-word Webcorpus.
922 Chapter 23. NaturalLanguageforCommunication
Inthe 1930s PetrTroyanskii applied fora patent fora“translating machine,” but there
werenocomputersavailabletoimplementhisideas. InMarch1947,theRockefellerFounda-
tion’sWarrenWeaverwrotetoNorbertWiener,suggesting thatmachinetranslation mightbe
possible. Drawingonworkincryptography andinformation theory, Weaverwrote, “When I
lookatanarticleinRussian,Isay: ‘ThisisreallywritteninEnglish,butithasbeencodedin
strange symbols. I will now proceed to decode.”’ Forthe next decade, the community tried
to decode in this way. IBM exhibited a rudimentary system in 1954. Bar-Hillel (1960) de-
scribes the enthusiasm of this period. However, the U.S. government subsequently reported
(ALPAC,1966) that “there isno immediate orpredictable prospect ofuseful machine trans-
lation.” However, limited work continued, and starting in the 1980s, computer power had
increased tothepointwheretheALPACfindingswerenolonger correct.
Thebasic statistical approach wedescribe inthe chapter is based on early workby the
IBM group (Brown et al., 1988, 1993) and the recent work by the ISI and Google research
groups (Och and Ney, 2004; Zollmann et al., 2008). A textbook introduction on statistical
machinetranslationisgivenbyKoehn(2009),andashorttutorialbyKevinKnight(1999)has
beeninfluential. EarlyworkonsentencesegmentationwasdonebyPalmerandHearst(1994).
OchandNey(2003)andMoore(2005)coverbilingual sentence alignment.
The prehistory of speech recognition began in the 1920s with Radio Rex, a voice-
activated toy dog. Rex jumped out of his doghouse in response to the word “Rex!” (or
actually almostanysufficientlyloudword). Somewhatmoreserious workbeganafterWorld
WarII. At AT&TBell Labs, a system was built for recognizing isolated digits (Davis et al.,
1952)bymeansofsimplepatternmatchingofacousticfeatures. Startingin1971,theDefense
Advanced Research Projects Agency (DARPA)of the United States Department of Defense
funded four competing five-year projects to develop high-performance speech recognition
systems. Thewinner,andtheonlysystemtomeetthegoalof90%accuracywitha1000-word
vocabulary, was the HARPY system at CMU (Lowerre and Reddy, 1980). The final version
ofHARPYwasderivedfromasystemcalled DRAGONbuiltbyCMUgraduatestudentJames
Baker (1975); DRAGON was the first to use HMMs for speech. Almost simultaneously, Je-
linek (1976) at IBM had developed another HMM-based system. Recent years have been
characterized by steady incremental progress, larger data sets and models, and more rigor-
ouscompetitions onmorerealistic speech tasks. In1997, BillGatespredicted, “ThePCfive
yearsfrom now—youwon’trecognize it,because speech willcomeintotheinterface.” That
didn’t quitehappen, butin2008 hepredicted “Infiveyears, Microsoft expects moreInternet
searches to be done through speech than through typing on a keyboard.” History will tell if
heisrightthistimearound.
Several good textbooks on speech recognition are available (Rabiner and Juang, 1993;
Jelinek, 1997; GoldandMorgan, 2000;Huangetal.,2001). Thepresentation inthischapter
drewonthesurveybyKay,Gawron,andNorvig(1994)andonthe textbookbyJurafsky and
Martin(2008). Speechrecognition researchispublishedin ComputerSpeechandLanguage,
Speech Communications, and the IEEE Transactions on Acoustics, Speech, and Signal Pro-
cessing and at the DARPAWorkshops on Speech and Natural Language Processing and the
Eurospeech, ICSLP,andASRUconferences.
Exercises 923
Ken Church (2004) shows that natural language research has cycled between concen-
trating on the data (empiricism) and concentrating on theories (rationalism). The linguist
JohnFirth(1957)proclaimed“Youshallknowawordbythecompanyitkeeps,”andlinguis-
tics of the 1940s and early 1950s was based largely on word frequencies, although without
the computational power we have available today. Then Noam (Chomsky, 1956) showed
thelimitations offinite-state models, andsparked aninterest intheoretical studies ofsyntax,
disregarding frequency counts. This approach dominated fortwenty years, until empiricism
made a comeback based on the success of work in statistical speech recognition (Jelinek,
1976). Today,mostworkacceptsthestatisticalframework, butthereisgreatinterestinbuild-
ingstatistical modelsthat consider higher-level models, such assyntactic treesand semantic
relations, notjustsequences ofwords.
Workonapplications oflanguageprocessingispresentedatthebiennialAppliedNatu-
ralLanguageProcessingconference(ANLP),theconference onEmpiricalMethodsinNatu-
ralLanguageProcessing(EMNLP),andthejournal NaturalLanguageEngineering. Abroad
rangeofNLPworkappearsinthejournalComputationalLinguisticsanditsconference,ACL,
andintheComputational Linguistics (COLING)conference.
EXERCISES
23.1 Read the following text once for understanding, and remember as much of it as you
can. Therewillbeatestlater.
Theprocedureisactuallyquitesimple. Firstyouarrangethingsintodifferentgroups. Of
course,onepilemaybesufficientdependingonhowmuchthereistodo.Ifyouhavetogo
somewhereelseduetolackoffacilitiesthatisthenextstep,otherwiseyouareprettywell
set. Itisimportantnottooverdothings. Thatis,itisbettertodotoofewthingsatonce
thantoomany.Intheshortrunthismaynotseemimportantbutcomplicationscaneasily
arise.Amistakeisexpensiveaswell. Atfirstthewholeprocedurewillseemcomplicated.
Soon,however,itwillbecomejustanotherfacetoflife. Itisdifficulttoforeseeanyend
tothenecessityforthistaskintheimmediatefuture,butthenonecannevertell. Afterthe
procedureiscompletedonearrangesthematerialintodifferentgroupsagain. Thenthey
canbeputintotheirappropriateplaces. Eventuallytheywillbeusedoncemoreandthe
wholecyclewillhavetoberepeated.However,thisispartoflife.
23.2 AnHMMgrammarisessentially astandard HMMwhosestatevariable isN (nonter-
minal,withvaluessuchasDet,Adjective,Nounandsoon)andwhoseevidencevariableis
W (word,withvaluessuchasis,duck,andsoon). TheHMMmodelincludesapriorP(N ),
0
atransitionmodelP(N |N ),andasensormodelP(W |N ). ShowthateveryHMMgram-
t+1 t t t
mar can be written as a PCFG. [Hint: start by thinking about how the HMM prior can be
represented by PCFGrules forthe sentence symbol. Youmay findit helpful to illustrate for
theparticularHMMwithvaluesA,B forN andvaluesx,y forW.]
924 Chapter 23. NaturalLanguageforCommunication
23.3 ConsiderthefollowingPCFGforsimpleverbphrases:
0.1 :VP → Verb
0.2 :VP → CopulaAdjective
0.5 :VP → VerbtheNoun
0.2 :VP → VP Adverb
0.5 :Verb → is
0.5 :Verb → shoots
0.8 :Copula→ is
0.2 :Copula→ seems
0.5 :Adjective → unwell
0.5 :Adjective → well
0.5 :Adverb → well
0.5 :Adverb → badly
0.6 :Noun → duck
0.4 :Noun → well
a. Which of the following have a nonzero probability as a VP? (i) shoots the duck well
wellwell (ii)seemsthewellwell (iii)shootstheunwellwellbadly
b. Whatistheprobability ofgenerating “iswellwell”?
c. Whattypesofambiguity areexhibited bythephrasein(b)?
d. Given any PCFG, is it possible to calculate the probability that the PCFG generates a
stringofexactly10words?
23.4 Outline the major differences between Java (or any other computer language with
which you are familiar) and English, commenting on the “understanding” problem in each
case. Think about such things as grammar, syntax, semantics, pragmatics, compositional-
ity, context-dependence, lexical ambiguity, syntactic ambiguity, reference finding (including
pronouns),backgroundknowledge,andwhatitmeansto“understand”inthefirstplace.
23.5 Thisexerciseconcerns grammarsforverysimplelanguages.
a. Writeacontext-free grammarforthelanguage anbn.
b. Writeacontext-free grammarforthepalindrome language: thesetofallstrings whose
secondhalfisthereverseofthefirsthalf.
c. Write a context-sensitive grammar for the duplicate language: the set of all strings
whosesecondhalfisthesameasthefirsthalf.
23.6 Consider the sentence “Someone walked slowly to the supermarket” and a lexicon
consisting ofthefollowingwords:
Pronoun → someone Verb → walked
Adv → slowly Prep → to
Article → the Noun → supermarket
Whichofthefollowingthreegrammars,combinedwiththelexicon,generates thegivensen-
tence? Showthecorresponding parsetree(s).
Exercises 925
(A): (B): (C):
S → NP VP S → NP VP S → NP VP
NP → Pronoun NP → Pronoun NP → Pronoun
NP → Article Noun NP → Noun NP → Article NP
VP → VP PP NP → Article NP VP → Verb Adv
VP → VP Adv Adv VP → Verb Vmod Adv → Adv Adv
VP → Verb Vmod → Adv Vmod Adv → PP
PP → Prep NP Vmod → Adv PP → Prep NP
NP → Noun Adv → PP NP → Noun
PP → Prep NP
For each of the preceding three grammars, write down three sentences of English and three
sentences of non-English generated by the grammar. Each sentence should be significantly
different, should be at least six words long, and should include some new lexical entries
(which you should define). Suggest ways to improve each grammar to avoid generating the
non-English sentences.
23.7 Collect some examples of time expressions, such as “two o’clock,” “midnight,” and
“12:46.” Alsothink upsomeexamplesthatareungrammatical, such as“thirteen o’clock” or
“halfpasttwofifteen.” Writeagrammarforthetimelanguage.
23.8 In this exercise you will transform E into Chomsky Normal Form (CNF). There are
0
five steps: (a) Add a new start symbol, (b) Eliminate (cid:2) rules, (c) Eliminate multiple words
on right-hand sides, (d) Eliminate rules of the form (X → Y), (e) Convert long right-hand
sidesintobinaryrules.
a. Thestartsymbol,S,canoccuronlyontheleft-handsideinCNF.Addanewruleofthe
formS (cid:2) → S,usinganewsymbolS (cid:2) .
b. Theemptystring, (cid:2)cannot appearontheright-hand sideinCNF.E doesnothaveany
0
ruleswith(cid:2),sothisisnotanissue.
c. A word can appear on the right-hand side in a rule only of the form (X → word).
Replace each rule of the form (X → ...word...) with (X → ...W (cid:2) ...) and (W (cid:2)
→ word),usinganewsymbolW (cid:2) .
d. A rule (X → Y) is not allowed in CNF; it must be (X → Y Z) or (X → word).
Replace each rule of the form (X → Y) with a set of rules of the form (X → ...),
oneforeachrule(Y → ...),where(...) indicates oneormoresymbols.
e. Replace each ruleofthe form (X → Y Z ...) withtworules, (X → Y Z (cid:2) )and(Z (cid:2)
→ Z ...),whereZ (cid:2) isanewsymbol.
Showeachstepoftheprocess andthefinalsetofrules.
23.9 Using DCG notation, write a grammar for a language that is just like E , except that
1
itenforces agreement between thesubject andverb ofasentence andthus does notgenerate
ungrammatical sentences suchas“Ismellsthewumpus.”
926 Chapter 23. NaturalLanguageforCommunication
23.10 ConsiderthefollowingPCFG:
S →NP VP [1.0]
NP →Noun [0.6]|Pronoun [0.4]
VP →Verb NP [0.8]|Modal Verb [0.2]
Noun →can[0.1]|fish[0.3]|...
Pronoun →I[0.4]|...
Verb →can[0.01]|fish[0.1]|...
Modal →can[0.3]|...
The sentence “I can fish” has two parse trees with this grammar. Show the two trees, their
priorprobabilities, andtheirconditional probabilities, giventhesentence.
23.11 Anaugmented context-free grammarcan represent languages thataregularcontext-
free grammar cannot. Show an augmented context-free grammar for the language anbncn.
The allowable values for augmentation variables are 1 and SUCCESSOR(n), where n is a
value. Theruleforasentenceinthislanguage is
S(n) → A(n) B(n)C(n).
Showtherule(s)foreachof A,B,andC.
23.12 Augment the E grammar so that it handles article–noun agreement. That is, make
1
surethat“agents” and“anagent”are NPs,but“agent”and“anagents” arenot.
23.13 Considerthefollowingsentence (from TheNewYorkTimes,July28,2008):
Banks struggling to recover from multibillion-dollar loans on real estate are cur-
tailingloanstoAmericanbusinesses,deprivingevenhealthycompaniesofmoney
forexpansion andhiring.
a. Whichofthewordsinthissentence arelexicallyambiguous?
b. Findtwocasesofsyntactic ambiguity inthissentence (therearemorethantwo.)
c. Giveaninstance ofmetaphorinthissentence.
d. Canyoufindsemanticambiguity?
23.14 Without lookingbackatExercise23.1,answerthefollowing questions:
a. Whatarethefourstepsthatarementioned?
b. Whatstepisleftout?
c. Whatis“thematerial”thatismentioned inthetext?
d. Whatkindofmistakewouldbeexpensive?
e. Isitbettertodotoofewthingsortoomany? Why?
23.15 Select five sentences and submit them to an online translation service. Translate
themfromEnglishtoanotherlanguage andbacktoEnglish. Ratetheresulting sentences for
grammaticality and preservation of meaning. Repeat the process; does the second round of
Exercises 927
iteration give worse results or the same results? Does the choice of intermediate language
make a difference to the quality of the results? If you know a foreign language, look at the
translation of one paragraph into that language. Count and describe the errors made, and
conjecture whytheseerrorsweremade.
23.16 The D values for the sentence in Figure 23.13 sum to 0. Will that be true of every
i
translation pair? Proveitorgiveacounterexample.
23.17 (Adapted from Knight (1999).) Ourtranslation model assumes that, afterthe phrase
translationmodelselectsphrasesandthedistortionmodelpermutesthem,thelanguagemodel
can unscramble the permutation. This exercise investigates how sensible that assumption is.
Trytounscramble theseproposed listsofphrases intothecorrectorder:
a. have,programming, a,seen,never,I,language, better
b. loves,john,mary
c. is the, communication, exchange of, intentional, information brought, by, about, the
production, perception of, and signs, from, drawn, a, of, system, signs, conventional,
shared
d. created,that,weholdthese, tobe,allmen,truths, are,equal,self-evident
Which ones could you do? What type of knowledge did you draw upon? Train a bigram
model from atraining corpus, and use it tofind thehighest-probability permutation of some
sentences fromatestcorpus. Reportontheaccuracyofthismodel.
23.18 Calculate the most probable path through the HMM in Figure 23.16 for the output
sequence [C ,C ,C ,C ,C ,C ,C ]. Alsogiveitsprobability.
1 2 3 4 4 6 7
23.19 We forgot to mention that the text in Exercise 23.1 is entitled “Washing Clothes.”
Reread the text and answer the questions in Exercise 23.14. Did you do better this time?
BransfordandJohnson(1973)usedthistextinacontrolledexperimentandfoundthatthetitle
helpedsignificantly. Whatdoesthistellyouabouthowlanguage andmemoryworks?
24
PERCEPTION
Inwhichweconnect thecomputertotheraw,unwashedworld.
Perceptionprovidesagentswithinformationabouttheworldtheyinhabitbyinterpreting the
PERCEPTION
response of sensors. A sensor measures some aspect of the environment in a form that can
SENSOR
beusedasinputbyanagentprogram. Thesensorcouldbeassimpleasaswitch,whichgives
onebittellingwhetheritisonoroff,orascomplexastheeye. Avarietyofsensorymodalities
are available to artificial agents. Those they share with humans include vision, hearing, and
touch. Modalities that are not available to the unaided human include radio, infrared, GPS,
andwirelesssignals. Somerobotsdoactivesensing,meaningtheysendoutasignal,suchas
radarorultrasound, andsensethereflectionofthissignaloffoftheenvironment. Ratherthan
tryingtocoverallofthese, thischapterwillcoveronemodalityindepth: vision.
We saw in our description of POMDPs (Section 17.4, page 658) that a model-based
decision-theoretic agent in a partially observable environment has a sensor model—a prob-
ability distribution P(E|S) over the evidence that its sensors provide, given a state of the
world. Bayes’rulecanthenbeusedtoupdatetheestimationofthestate.
For vision, the sensor model can be broken into two components: An object model
OBJECTMODEL
describes the objects that inhabit the visual world—people, buildings, trees, cars, etc. The
objectmodelcouldincludeaprecise3Dgeometricmodeltakenfromacomputer-aideddesign
(CAD)system,oritcouldbevagueconstraints,suchasthefactthathumaneyesareusually5
to7cmapart. Arenderingmodeldescribesthephysical,geometric,andstatisticalprocesses
RENDERINGMODEL
that produce thestimulus from theworld. Rendering models are quite accurate, but they are
ambiguous. For example, a white object under low light may appear as the same color as a
black object underintense light. Asmall nearby object maylook thesame asa large distant
object. Without additional evidence, we cannot tell if the image that fills the frame is a toy
Godzillaorarealmonster.
Ambiguity can be managed with prior knowledge—we know Godzilla is not real, so
the image must be a toy—or by selectively choosing to ignore the ambiguity. For example,
the vision system for an autonomous car may not be able to interpret objects that are far in
the distance, but the agent can choose to ignore the problem, because it is unlikely to crash
intoanobjectthatismilesaway.
928
Section24.1. ImageFormation 929
Adecision-theoretic agent isnot theonlyarchitecture that canmakeuseofvision sen-
sors. Forexample, fruit flies (Drosophila) are in part reflex agents: they have cervical giant
fibersthatformadirectpathwayfromtheirvisualsystemtothewingmusclesthatinitiatean
escape response—an immediate reaction, without deliberation. Flies and many other flying
animals make used of a closed-loop control architecture to land on an object. The visual
system extracts an estimate of the distance to the object, and the control system adjusts the
wingmusclesaccordingly,allowingveryfastchangesofdirection,withnoneedforadetailed
modeloftheobject.
Compared to the data from other sensors (such as the single bit that tells the vacuum
robot that it has bumped into a wall), visual observations are extraordinarily rich, both in
the detail they can reveal and in the sheer amount of data they produce. A video camera
for robotic applications might produce a million 24-bit pixels at 60 Hz; a rate of 10 GB per
minute. The problem for a vision-capable agent then is: Which aspects of the rich visual
stimulusshouldbeconsideredtohelptheagentmakegoodactionchoices,andwhichaspects
should be ignored? Vision—and all perception—serves to further the agent’s goals, not as
anendtoitself.
FEATURE We can characterize three broad approaches to the problem. The feature extraction
EXTRACTION
approach, as exhibited by Drosophila, emphasizes simple computations applied directly to
the sensor observations. Inthe recognition approach an agent drawsdistinctions among the
RECOGNITION
objectsitencountersbasedonvisualandotherinformation. Recognitioncouldmeanlabeling
eachimagewithayesornoastowhetheritcontainsfoodthatweshouldforage,orcontains
Grandma’s face. Finally, in the reconstruction approach an agent builds ageometric model
RECONSTRUCTION
oftheworldfromanimageorasetofimages.
The last thirty years of research have produced powerful tools and methods for ad-
dressing these approaches. Understanding these methods requires an understanding of the
processes bywhich imagesareformed. Therefore, wenowcoverthephysical andstatistical
phenomena thatoccurintheproduction ofanimage.
24.1 IMAGE FORMATION
Imaging distorts the appearance of objects. For example, a picture taken looking down a
long straight set of railway tracks will suggest that the rails converge and meet. As another
example, if you hold your hand in front of your eye, you can block out the moon, which is
not smaller than your hand. Asyou moveyour hand back and forth ortilt it, your hand will
seem toshrink and grow in the image, but itis not doing so inreality (Figure 24.1). Models
oftheseeffectsareessential forbothrecognition andreconstruction.
24.1.1 Imageswithoutlenses: The pinholecamera
Image sensors gather light scattered from objects in a scene and create a two-dimensional
SCENE
image. In the eye, the image is formed on the retina, which consists of two types of cells:
IMAGE
about 100 million rods, which are sensitive to light at a wide range of wavelengths, and 5
930 Chapter 24. Perception
Figure24.1 Imagingdistortsgeometry. Parallel lines appearto meet in the distance, as
intheimageoftherailwaytracksontheleft. Inthecenter,a smallhandblocksoutmostof
a largemoon. Onthe rightisa foreshorteningeffect: thehandistilted awayfromthe eye,
makingitappearshorterthaninthecenterfigure.
million cones. Cones, which are essential for color vision, are of three main types, each of
which is sensitive to a different set of wavelengths. In cameras, the image is formed on an
image plane, which can be a piece of film coated with silver halides or a rectangular grid
of a few million photosensitive pixels, each a complementary metal-oxide semiconductor
PIXEL
(CMOS) or charge-coupled device (CCD). Each photon arriving at the sensor produces an
effect, whose strength depends on the wavelength of the photon. The output of the sensor
is the sum of all effects due to photons observed in some time window, meaning that image
sensorsreportaweightedaverageoftheintensity oflightarrivingatthesensor.
To see a focused image, we must ensure that all the photons from approximately the
samespotinthescenearriveatapproximatelythesamepointintheimageplane. Thesimplest
way to form a focused image is to view stationary objects with a pinhole camera, which
PINHOLECAMERA
consists ofapinhole opening, O,atthefrontofabox, and animageplane atthebackofthe
box (Figure 24.2). Photons from the scene must pass through the pinhole, so if it is small
enough then nearby photons in the scene will be nearby in the image plane, and the image
willbeinfocus.
Thegeometryofsceneandimageiseasiesttounderstand withthepinholecamera. We
useathree-dimensional coordinatesystemwiththeoriginatthepinhole,andconsiderapoint
(cid:2)
P in the scene, with coordinates (X,Y,Z). P gets projected to the point P in the image
planewithcoordinates (x,y,z). Iff isthedistancefromthepinholetotheimageplane,then
bysimilartriangles, wecanderivethefollowingequations:
−x X −y Y −fX −fY
= , = ⇒ x = , y = .
f Z f Z Z Z
PERSPECTIVE These equations define animage-formation process known as perspective projection. Note
PROJECTION
that the Z inthe denominator means that the farther awayan object is, the smallerits image
Section24.1. ImageFormation 931
Image
plane Y P
X
Z
P′
Pinhole
f
Figure24.2 Eachlight-sensitiveelementintheimageplaneatthebackofapinholecam-
erareceiveslightfromathesmallrangeofdirectionsthatpassesthroughthepinhole. Ifthe
pinholeissmallenough,theresultisafocusedimageatthebackofthepinhole.Theprocess
ofprojectionmeansthatlarge,distantobjectslookthesameassmaller,nearbyobjects.Note
thattheimageisprojectedupsidedown.
will be. Also, note that the minus signs mean that the image is inverted, both left–right and
up–down, comparedwiththescene.
Under perspective projection, distant objects look small. This is what allows you to
coverthemoonwithyourhand(Figure24.1). Animportantresultofthiseffectisthatparallel
linesconverge toapointonthehorizon. (Thinkofrailwaytracks, Figure24.1.) Alineinthe
sceneinthedirection (U,V,W)andpassingthroughthepoint(X ,Y ,Z )canbedescribed
0 0 0
as the set of points (X +λU,Y +λV,Z +λW), with λvarying between −∞ and +∞.
0 0 0
Differentchoices of(X ,Y ,Z )yielddifferent linesparallel tooneanother. Theprojection
0 0 0
ofapointP fromthislineontotheimageplaneisgivenby
(cid:13) λ (cid:14)
X +λU Y +λV
0 0
f ,f .
Z +λW Z +λW
0 0
As λ → ∞ or λ → −∞, this becomes p∞ = (fU/W,fV/W) if W (cid:7)= 0. This means that
two parallel lines leaving different points in space will converge in the image—for large λ,
theimagepointsarenearlythesame,whateverthevalueof(X ,Y ,Z )(again,thinkrailway
0 0 0
VANISHINGPOINT
tracks, Figure 24.1). We call p∞ the vanishing point associated with the family of straight
lineswithdirection(U,V,W). Lineswiththesamedirectionsharethesamevanishingpoint.
24.1.2 Lens systems
The drawback of the pinhole camera is that we need a small pinhole to keep the image in
focus. Butthesmallerthepinhole,thefewerphotonsgetthrough,meaningtheimagewillbe
dark. Wecan gather more photons by keeping the pinhole open longer, but then wewill get
motion blur—objects inthe scene thatmovewillappearblurred because theysend photons
MOTIONBLUR
to multiple locations on the image plane. If we can’t keep the pinhole open longer, we can
trytomakeitbigger. Morelightwillenter,butlightfromasmallpatchofobjectinthescene
willnowbespreadoverapatchontheimageplane, causingablurred image.
932 Chapter 24. Perception
Image plane
Light Source
Iris
Cornea Fovea
Visual Axis
Optic Nerve
Lens
Optical Axis
Lens
System Retina
Figure24.3 Lensescollectthelightleavingascenepointinarangeofdirections,andsteer
italltoarriveatasinglepointontheimageplane. Focusingworksforpointslyingcloseto
a focal plane in space; other points will not be focused properly. In cameras, elements of
thelenssystemmovetochangethefocalplane,whereasintheeye,theshapeofthelensis
changedbyspecializedmuscles.
Vertebrate eyes and modern cameras use a lens system to gather sufficient light while
LENS
keeping the image in focus. A large opening is covered with a lens that focuses light from
nearby object locations downtonearby locations intheimageplane. However, lenssystems
have a limited depth of field: they can focus light only from points that lie within a range
DEPTHOFFIELD
of depths (centered around a focal plane). Objects outside this range willbe out of focus in
FOCALPLANE
the image. Tomovethe focal plane, thelens inthe eyecan change shape (Figure 24.3); in a
camera,thelensesmovebackandforth.
24.1.3 Scaledorthographic projection
Perspective effects aren’t always pronounced. For example, spots on a distant leopard may
looksmallbecausetheleopardisfaraway,buttwospotsthatarenexttoeachotherwillhave
aboutthesamesize. Thisisbecausethedifferenceindistancetothespotsissmallcompared
tothedistance tothem,andsowecansimplify theprojection model. Theappropriate model
SCALED is scaled orthographic projection. The idea is as follows: If the depth Z of points on the
ORTHOGRAPHIC
PROJECTION object varies within some range Z ± ΔZ, with ΔZ * Z , then the perspective scaling
0 0
factor f/Z canbeapproximated byaconstant s = f/Z . Theequations forprojection from
0
the scene coordinates (X,Y,Z) to the image plane become x = sX and y = sY. Scaled
orthographicprojectionisanapproximationthatisvalidonlyforthosepartsofthescenewith
notmuchinternaldepthvariation. Forexample,scaledorthographic projectioncanbeagood
modelforthefeaturesonthefrontofadistant building.
24.1.4 Lightand shading
The brightness of a pixel in the image is a function of the brightness of the surface patch in
thescenethatprojectstothepixel. Wewillassumealinearmodel(currentcamerashavenon-
linearities attheextremesoflightanddark,butarelinear inthemiddle). Imagebrightness is
Section24.1. ImageFormation 933
Diffuse reflection, bright
Specularities
Diffuse reflection, dark Cast shadow
Figure24.4 Avarietyofilluminationeffects. Therearespecularities onthemetalspoon
andonthemilk. Thebrightdiffusesurfaceisbrightbecauseitfacesthelightdirection. The
darkdiffusesurfaceisdarkbecauseitistangentialtotheilluminationdirection.Theshadows
appearatsurfacepointsthatcannotseethelightsource.PhotobyMikeLinksvayer(mlinksva
onflickr).
astrong,ifambiguous,cuetotheshapeofanobject,andfromtheretoitsidentity. Peopleare
usually able to distinguish the three main causes of varying brightness and reverse-engineer
the object’s properties. Thefirstcause is overall intensity ofthelight. Eventhough awhite
OVERALLINTENSITY
object inshadow maybelessbright thanablackobject indirectsunlight, theeyecandistin-
guishrelativebrightnesswell,andperceivethewhiteobjectaswhite. Second,differentpoints
in the scene may reflect more or less of the light. Usually, the result is that people perceive
REFLECT
these points aslighterordarker, andsoseetexture ormarkings ontheobject. Third, surface
patches facing thelightarebrighter thansurface patches tilted awayfrom thelight, aneffect
known as shading. Typically, people can tell that this shading comes from the geometry of
SHADING
theobject, butsometimesgetshading andmarkingsmixedup. Forexample, astreakofdark
makeupunderacheekbonewilloftenlooklikeashadingeffect,makingthefacelookthinner.
DIFFUSE Most surfaces reflect light by a process of diffuse reflection. Diffuse reflection scat-
REFLECTION
terslightevenlyacross thedirections leaving asurface, sothebrightness ofadiffusesurface
doesn’t depend ontheviewingdirection. Mostcloth, paints, roughwoodensurfaces, vegeta-
tion, and rough stone are diffuse. Mirrors are not diffuse, because what you see depends on
the direction in which you look at the mirror. The behavior of a perfect mirror is known as
SPECULAR specularreflection. Somesurfaces—such asbrushed metal, plastic, orawetfloor—display
REFLECTION
smallpatcheswherespecularreflection hasoccurred, calledspecularities. Theseareeasyto
SPECULARITIES
identify,becausetheyaresmallandbright(Figure24.4). Foralmostallpurposes,itisenough
tomodelallsurfaces asbeingdiffusewithspecularities.
934 Chapter 24. Perception
θ
θ
A B
Figure24.5 Twosurfacepatchesareilluminatedbyadistantpointsource,whoseraysare
shown as gray arrowheads. Patch A is tilted away from the source (θ is close to 900) and
collectslessenergy,becauseitcutsfewerlightraysperunitsurfacearea. PatchB,facingthe
source(θiscloseto00),collectsmoreenergy.
Themainsourceofillumination outsideisthesun,whoseraysalltravelparalleltoone
DISTANTPOINT another. Wemodel this behavior asa distantpointlight source. Thisisthemostimportant
LIGHTSOURCE
model of lighting, and is quite effective for indoor scenes as well as outdoor scenes. The
amountoflightcollectedbyasurfacepatchinthismodeldependsontheangleθbetweenthe
illumination direction andthenormaltothesurface.
A diffuse surface patch illuminated by a distant point light source will reflect some
fraction of the light it collects; this fraction is called the diffuse albedo. White paper and
DIFFUSEALBEDO
snowhaveahighalbedo,about0.90,whereasflatblackvelvetandcharcoalhavealowalbedo
of about 0.05 (which means that 95% of the incoming light is absorbed within the fibers of
LAMBERT’SCOSINE thevelvetortheporesofthecharcoal). Lambert’scosinelawstatesthatthebrightness ofa
LAW
diffusepatchisgivenby
I = ρI cosθ ,
0
whereρisthediffusealbedo,I istheintensityofthelightsourceandθistheanglebetween
0
the light source direction and the surface normal (see Figure 24.5). Lampert’s law predicts
bright image pixels come from surface patches that face the light directly and dark pixels
come from patches that see the light only tangentially, so that the shading on a surface pro-
vides some shape information. We explore this cue in Section 24.4.5. If the surface is not
reached bythelight source, thenitisin shadow. Shadowsareveryseldom auniform black,
SHADOW
because the shadowed surface receives some light from other sources. Outdoors, the most
important such source is the sky, which is quite bright. Indoors, light reflected from other
surfaces illuminates shadowed patches. These interreflections can have a significant effect
INTERREFLECTIONS
on the brightness of other surfaces, too. These effects are sometimes modeled by adding a
AMBIENT constant ambientilluminationtermtothepredicted intensity.
ILLUMINATION
Section24.2. EarlyImage-Processing Operations 935
24.1.5 Color
Fruit is a bribe that a tree offers to animals to carry its seeds around. Trees have evolved to
have fruit that turns red oryellow when ripe, and animals have evolved todetect these color
changes. Light arriving at the eye has different amounts of energy at different wavelengths;
thiscanberepresented byaspectralenergydensityfunction. Humaneyesrespondtolightin
the 380–750nm wavelength region, with three different types of color receptor cells, which
have peakreceptiveness at420mm (blue), 540nm (green), and 570nm (red). Thehumaneye
cancaptureonlyasmallfractionofthefullspectralenergy densityfunction—butitisenough
totellwhenthefruitisripe.
PRINCIPLEOF Theprincipleoftrichromacystatesthatforanyspectralenergydensity,nomatterhow
TRICHROMACY
complicated,itispossibletoconstructanotherspectralenergydensityconsistingofamixture
ofjustthreecolors—usually red,green,andblue—suchthat ahumancan’ttellthedifference
between the two. That means that our TVs and computer displays can get by with just the
three red/green/blue (or R/G/B) color elements. It makes our computer vision algorithms
easier, too. Each surface can be modeled with three different albedos for R/G/B. Similarly,
each light source can be modeled with three R/G/B intensities. We then apply Lambert’s
cosine law to each to get three R/G/B pixel values. This model predicts, correctly, that the
same surface will produce different colored image patches under different-colored lights. In
fact,humanobserversarequitegoodatignoringtheeffects ofdifferentcoloredlightsandare
abletoestimatethecolorofthesurfaceunderwhitelight,aneffectknownascolorconstancy.
COLORCONSTANCY
Quiteaccurate colorconstancy algorithms arenowavailable; simpleversions showupinthe
“auto white balance” function of your camera. Note that if we wanted to build a camera for
mantis shrimp, we would need 12 different pixel colors, corresponding to the 12 types of
colorreceptorsofthecrustacean.
24.2 EARLY IMAGE-PROCESSING OPERATIONS
Wehave seen how light reflects off objects in the scene to form an image consisting of, say,
fivemillion 3-byte pixels. With all sensors there will be noise in the image, and in any case
thereisalotofdatatodealwith. Sohowdowegetstartedonanalyzing thisdata?
Inthis section wewillstudy threeuseful image-processing operations: edge detection,
texture analysis, and computation of optical flow. These are called “early” or “low-level”
operations because they are the first in a pipeline of operations. Early vision operations are
characterized by their local nature (they can be carried out in one part of the image without
regard for anything more than a few pixels away) and by their lack of knowledge: we can
perform these operations without consideration of the objects that might be present in the
scene. This makes the low-level operations good candidates for implementation in parallel
hardware—either in a graphics processor unit (GPU) or an eye. We will then look at one
mid-leveloperation: segmenting theimageintoregions.
936 Chapter 24. Perception
A
B
1
2 2
1
1
3
4
Figure 24.6 Different kinds of edges: (1) depth discontinuities; (2) surface orientation
discontinuities;(3)reflectancediscontinuities;(4)illuminationdiscontinuities(shadows).
24.2.1 Edgedetection
Edges are straight lines or curves in the image plane across which there is a “significant”
EDGE
change in image brightness. The goal of edge detection is to abstract away from the messy,
multimegabyte imageandtowardamorecompact, abstract representation, asinFigure24.6.
The motivation is that edge contours in the image correspond to important scene contours.
In the figure we have three examples of depth discontinuity, labeled 1; two surface-normal
discontinuities, labeled 2; a reflectance discontinuity, labeled 3; and an illumination discon-
tinuity (shadow), labeled 4. Edgedetection isconcerned only withthe image, and thus does
notdistinguish betweenthesedifferenttypesofscenediscontinuities; laterprocessing will.
Figure 24.7(a) shows an image of a scene containing a stapler resting on a desk, and
(b) shows the output of an edge-detection algorithm on this image. As you can see, there
is a difference between the output and an ideal line drawing. There are gaps where no edge
appears, andthereare“noise”edgesthatdonotcorrespond toanythingofsignificance inthe
scene. Laterstagesofprocessing willhavetocorrectfortheseerrors.
Howdowedetect edgesinanimage? Considertheprofileofimagebrightness along a
one-dimensional cross-section perpendicular to an edge—for example, the one between the
leftedgeofthedeskandthewall. ItlookssomethinglikewhatisshowninFigure24.8(top).
Edgescorrespondtolocationsinimageswherethebrightnessundergoesasharpchange,
so anaive idea would be todifferentiate the image and look forplaces where the magnitude
(cid:2)
ofthederivative I (x)islarge. Thatalmostworks. InFigure24.8(middle),weseethatthere
isindeedapeakatx=50,buttherearealsosubsidiarypeaksatotherlocations(e.g.,x=75).
These arise because of the presence of noise in the image. If wesmooth the image first, the
spurious peaksarediminished, asweseeinthebottomofthefigure.
Section24.2. EarlyImage-Processing Operations 937
(a) (b)
Figure24.7 (a)Photographofastapler. (b)Edgescomputedfrom(a).
2
1
0
−1
0 10 20 30 40 50 60 70 80 90 100
1
0
−1
0 10 20 30 40 50 60 70 80 90 100
1
0
−1
0 10 20 30 40 50 60 70 80 90 100
Figure24.8 Top:IntensityprofileI(x)alongaone-dimensionalsectionacrossanedgeat
x=50. Middle: Thederivativeofintensity,I(cid:5)(x). Largevaluesofthisfunctioncorrespond
to edges, but the function is noisy. Bottom: The derivative of a smoothed version of the
intensity,(I∗Gσ)(cid:5),whichcanbecomputedinonestepastheconvolutionI∗G(cid:5)
σ
. Thenoisy
candidateedgeatx=75hasdisappeared.
The measurement of brightness at a pixel in a CCD camera is based on a physical
process involving the absorption of photons and the release of electrons; inevitably there
will be statistical fluctuations of the measurement—noise. The noise can be modeled with
938 Chapter 24. Perception
a Gaussian probability distribution, with each pixel independent of the others. One way to
smooth animage istoassign toeach pixel theaverage ofits neighbors. Thistends tocancel
outextremevalues. Buthowmanyneighborsshouldweconsider—onepixelaway,ortwo,or
more? Onegoodanswerisaweighted average thatweights thenearest pixelsthemost,then
gradually decreases the weight for more distant pixels. The Gaussian filter does just that.
GAUSSIANFILTER
(UsersofPhotoshoprecognize thisastheGaussianbluroperation.) RecallthattheGaussian
function withstandard deviation σ andmean0is
N (x) = √1 e −x2/2σ2 inonedimension, or
σ
2πσ
N (x,y) = 1 e −(x2+y2)/2σ2 intwodimensions.
σ 2πσ2
The application of the Gaussian filter replaces the intensity I(x ,y ) with the sum, over all
0 0
(x,y) pixels, of I(x,y)N (d), where d is the distance from (x ,y ) to (x,y). This kind of
σ 0 0
weighted sum isso common that there is aspecial name and notation forit. Wesay that the
function histheconvolution oftwofunctions f andg (denoted f ∗g)ifwehave
CONVOLUTION
(cid:12)+∞
h(x) = (f ∗g)(x) = f(u)g(x−u) inonedimension, or
u=−∞
(cid:12)+∞ (cid:12)+∞
h(x,y) = (f ∗g)(x,y) = f(u,v)g(x−u,y−v) intwo.
u=−∞v=−∞
Sothesmoothingfunctionisachievedbyconvolving theimagewiththeGaussian,I∗N . A
σ
σof1pixelisenoughtosmoothoverasmallamountofnoise,whereas2pixelswillsmootha
largeramount,butatthelossofsomedetail. BecausetheGaussian’s influencefadesquickly
atadistance, wecanreplace the±∞inthesumswith±3σ.
Wecanoptimizethecomputation bycombiningsmoothingandedgefindingintoasin-
gleoperation. Itisatheoremthatforanyfunctions f andg,thederivativeoftheconvolution,
(f ∗g) (cid:2) , is equal to the convolution with the derivative, f ∗(g (cid:2) ). So rather than smoothing
theimage and then differentiating, wecan just convolve the imagewith thederivative ofthe
(cid:2)
smoothing function, N . We then mark as edges those peaks in the response that are above
σ
somethreshold.
Thereisanatural generalization ofthisalgorithm fromone-dimensional crosssections
togeneraltwo-dimensional images. Intwodimensionsedgesmaybeatanyangleθ. Consid-
eringtheimagebrightness asascalarfunction ofthevariables x,y,itsgradient isavector
(cid:31)
(cid:13) (cid:14)
∂I
I
∇I = ∂x = x .
∂I I
y
∂y
Edgescorrespond tolocationsinimageswherethebrightnessundergoes asharpchange, and
so the magnitude of the gradient, +∇I+, should be large at an edge point. Of independent
interestisthedirectionofthegradient
(cid:13) (cid:14)
∇I cosθ
= .
+∇I+ sinθ
Thisgivesusaθ = θ(x,y)ateverypixel,whichdefinestheedgeorientation atthatpixel.
ORIENTATION
Section24.2. EarlyImage-Processing Operations 939
Asinonedimension, toformthegradientwedon’tcompute∇I,butrather∇(I∗N ),
σ
the gradient after smoothing the image by convolving it with a Gaussian. And again, the
shortcut is that this is equivalent to convolving the image with the partial derivatives of a
Gaussian. Oncewehavecomputed thegradient, wecanobtain edges byfinding edge points
and linking them together. To tell whether a point is an edge point, we must look at other
points a small distance forward and back along the direction of the gradient. If the gradient
magnitude at one of these points is larger, then we could get a better edge point by shifting
the edge curve very slightly. Furthermore, if the gradient magnitude is too small, the point
cannot be an edge point. So at an edge point, the gradient magnitude is a local maximum
alongthedirectionofthegradient, andthegradient magnitudeisaboveasuitablethreshold.
Oncewehavemarkededgepixelsbythisalgorithm,thenextstageistolinkthosepixels
thatbelongtothesameedgecurves. Thiscanbedonebyassumingthatanytwoneighboring
edgepixelswithconsistent orientations mustbelongtothesameedgecurve.
24.2.2 Texture
In everyday language, texture is the visual feel of a surface—what you see evokes what
TEXTURE
the surface might feel like if you touched it (“texture” has the same root as “textile”). In
computational vision, texture refers to a spatially repeating pattern on a surface that can be
sensedvisually. Examplesincludethepatternofwindowsonabuilding,stitchesonasweater,
spots on a leopard, blades of grass on a lawn, pebbles on a beach, and people in a stadium.
Sometimes the arrangement is quite periodic, as in the stitches on a sweater; in other cases,
suchaspebblesonabeach,theregularity isonlystatistical.
Whereasbrightnessisapropertyofindividualpixels,theconceptoftexturemakessense
only for a multipixel patch. Given such a patch, we could compute the orientation at each
pixel,andthencharacterize thepatchbyahistogram oforientations. Thetextureofbricksin
awallwould havetwopeaks inthe histogram (one vertical and onehorizontal), whereas the
textureofspotsonaleopard’s skinwouldhaveamoreuniform distribution oforientations.
Figure24.9showsthatorientationsarelargelyinvarianttochangesinillumination. This
makes texture an important clue for object recognition, because other clues, such as edges,
canyielddifferent resultsindifferent lighting conditions.
Inimagesoftexturedobjects,edgedetectiondoesnotworkaswellasitdoesforsmooth
objects. This is because the most important edges can be lost among the texture elements.
Quiteliterally,wemaymissthetigerforthestripes. Thesolutionistolookfordifferencesin
texture properties, just thewaywelook fordifferences in brightness. Apatch on atigerand
apatchonthegrassybackgroundwillhaveverydifferentorientationhistograms,allowingus
tofindtheboundary curvebetweenthem.
24.2.3 Opticalflow
Next, let us consider what happens when we have a video sequence, instead of just a single
static image. When an object in the video is moving, orwhen the camera is moving relative
to an object, the resulting apparent motion in the image is called optical flow. Optical flow
OPTICALFLOW
describes the direction and speed of motion of features in the image—the optical flow of a
940 Chapter 24. Perception
(a) (b)
Figure24.9 Twoimagesofthesametextureofcrumpledricepaper,withdifferentillumi-
nationlevels. Thegradientvectorfield(ateveryeighthpixel)isplottedontopofeachone.
Noticethat, asthe lightgetsdarker,allthegradientvectorsgetshorter. Thevectorsdonot
rotate,sothegradientorientationsdonotchange.
video of a race car would be measured in pixels per second, not miles per hour. Theoptical
flow encodes useful information about scene structure. For example, in a video of scenery
taken from a moving train, distant objects have slower apparent motion than close objects;
thus, the rate of apparent motion can tell us something about distance. Optical flow also
enablesustorecognizeactions. InFigure24.10(a)and(b),weshowtwoframesfromavideo
of a tennis player. In (c) we display the optical flow vectors computed from these images,
showingthattheracketandfrontlegaremovingfastest.
The optical flow vector field can be represented at any point (x,y) by its components
v (x,y)inthexdirectionandv (x,y)intheydirection. Tomeasureopticalflowweneedto
x y
findcorresponding pointsbetween onetimeframeandthenext. Asimple-minded technique
is based on the fact that image patches around corresponding points have similar intensity
patterns. Consider a block of pixels centered at pixel p, (x ,y ), at time t . This block
0 0 0
of pixels is to be compared with pixel blocks centered at various candidate pixels at (x +
0
D ,y + D ) at time t + D . One possible measure of similarity is the sum of squared
x 0 y 0 t
SUMOFSQUARED differences(SSD):
DIFFERENCES (cid:12)
SSD(D ,D ) = (I(x,y,t)−I(x+D ,y+D ,t+D ))2 .
x y x y t
(x,y)
Here, (x,y) ranges over pixels in the block centered at (x ,y ). We find the (D ,D ) that
0 0 x y
minimizes the SSD.The optical flow at (x ,y ) is then (v ,v ) = (D /D ,D /D ). Note
0 0 x y x t y t
thatforthistowork,thereneedstobesometextureorvariationinthescene. Ifoneislooking
at a uniform white wall, then the SSD is going to be nearly the same for the different can-
Section24.2. EarlyImage-Processing Operations 941
Figure24.10 Twoframesofavideosequence. Ontherightistheopticalflowfieldcor-
responding to the displacement from one frame to the other. Note how the movement of
thetennisracketandthe frontleg iscapturedbythe directionsof thearrows. (Courtesyof
ThomasBrox.)
didate matches, and the algorithm is reduced to making a blind guess. The best-performing
algorithms for measuring optical flow rely on a variety of additional constraints when the
sceneisonlypartially textured.
24.2.4 Segmentation ofimages
Segmentationistheprocessofbreakinganimageintoregionsofsimilarpixels. Eachimage
SEGMENTATION
pixel can be associated with certain visual properties, such as brightness, color, and texture.
REGIONS
Within an object, or a single part of an object, these attributes vary relatively little, whereas
across an inter-object boundary there is typically a large change in one or more of these at-
tributes. Therearetwoapproachestosegmentation,onefocusingondetectingtheboundaries
oftheseregions, andtheotherondetecting theregionsthemselves(Figure24.11).
Aboundary curvepassing throughapixel (x,y)willhaveanorientation θ,sooneway
toformalizetheproblem ofdetecting boundarycurvesisasa machinelearningclassification
problem. Based on features from a local neighborhood, wewant to compute the probability
P (x,y,θ)thatindeedthereisaboundarycurveatthatpixelalongthatorientation. Consider
b
a circular disk centered at (x,y), subdivided into two half disks by adiameter oriented at θ.
If there isa boundary at (x,y,θ) the two half disks might be expected to differsignificantly
intheirbrightness,color,andtexture. Martin,Fowlkes,andMalik(2004)usedfeaturesbased
on differences in histograms of brightness, color, and texture values measured in these two
halfdisks, andthen trained aclassifier. Forthistheyused a datasetofnatural images where
humanshadmarkedthe“groundtruth”boundaries, andthegoaloftheclassifierwastomark
exactlythoseboundaries markedbyhumansandnoothers.
Boundariesdetectedbythistechniqueturnouttobesignificantlybetterthanthosefound
usingthesimpleedge-detectiontechniquedescribedpreviously. Butstilltherearetwolimita-
tions. (1)Theboundarypixelsformedbythresholding P (x,y,θ)arenotguaranteed toform
b
closed curves, sothisapproach doesn’t deliverregions, and (2)thedecision making exploits
onlylocalcontextanddoesnotuseglobalconsistency constraints.
942 Chapter 24. Perception
(a) (b) (c) (d)
Figure24.11 (a)Originalimage. (b)Boundarycontours,wherethehigher thePb value,
the darker the contour. (c) Segmentation into regions, correspondingto a fine partition of
theimage. Regionsarerenderedintheirmeancolors. (d)Segmentationintoregions,corre-
spondingtoa coarserpartitionoftheimage,resultinginfewerregions. (CourtesyofPablo
Arbelaez,MichaelMaire,CharlesFowlkes,andJitendraMalik)
Thealternativeapproachisbasedontryingto“cluster”the pixelsintoregionsbasedon
their brightness, color, and texture. Shi and Malik (2000) set this up as a graph partitioning
problem. The nodes of the graph correspond to pixels, and edges to connections between
pixels. TheweightW ontheedgeconnectingapairofpixelsiandjisbasedonhowsimilar
ij
thetwopixelsareinbrightness, color, texture, etc. Partitionsthatminimizeanormalized cut
criterion are then found. Roughly speaking, the criterion for partitioning the graph is to
minimize the sum of weights of connections across the groups of pixels and maximize the
sumofweightsofconnections withinthegroups.
Segmentation based purely on low-level, local attributes such as brightness and color
cannot be expected to deliver the final correct boundaries of all the objects in the scene. To
reliably find object boundaries we need high-level knowledge of the likely kinds of objects
inthescene. Representing thisknowledge isatopicofactiveresearch. Apopularstrategy is
to produce an over-segmentation of animage, containing hundreds of homogeneous regions
known as superpixels. From there, knowledge-based algorithms can take over; they will
SUPERPIXELS
finditeasier todeal withhundreds ofsuperpixels ratherthan millions ofrawpixels. Howto
exploithigh-level knowledgeofobjects isthesubjectofthenextsection.
24.3 OBJECT RECOGNITION BY APPEARANCE
Appearanceisshorthand forwhatanobject tendstolooklike. Someobject categories—for
APPEARANCE
example, baseballs—vary rather little in appearance; all of the objects in the category look
about the same under most circumstances. In this case, we can compute a set of features
describing eachclassofimageslikelytocontain theobject, thentestitwithaclassifier.
Section24.3. ObjectRecognition byAppearance 943
Otherobjectcategories—for example,houses orballetdancers—vary greatly. Ahouse
canhavedifferentsize,color,andshapeandcanlookdifferentfromdifferentangles. Adancer
looksdifferentineachpose,orwhenthestagelightschange colors. Ausefulabstractionisto
saythatsomeobjectsaremadeupoflocalpatternswhichtendtomovearoundwithrespectto
oneanother. Wecanthenfindtheobjectbylookingatlocalhistogramsofdetectorresponses,
whichexposewhethersomepartispresent butsuppress thedetailsofwhereitis.
Testing each class of images with a learned classifier is an important general recipe.
It works extremely well for faces looking directly at the camera, because at low resolution
and underreasonable lighting, allsuch faces look quite similar. Theface isround, and quite
brightcomparedtotheeyesockets;thesearedark,becausetheyaresunken,andthemouthis
adarkslash,asaretheeyebrows. Majorchangesofilluminationcancausesomevariationsin
this pattern, but the range of variation is quite manageable. That makes it possible to detect
face positions in an image that contains faces. Once a computational challenge, this feature
isnowcommonplace ineveninexpensive digitalcameras.
Forthe moment, we will consider only faces where the nose is oriented vertically; we
will deal with rotated faces below. We sweep a round window of fixed size over the image,
compute features for it, and present the features to a classifier. This strategy is sometimes
calledtheslidingwindow. Featuresneedtoberobusttoshadowsandtochangesinbrightness
SLIDINGWINDOW
causedbyilluminationchanges. Onestrategyistobuildfeaturesoutofgradientorientations.
Another is to estimate and correct the illumination in each image window. To find faces of
different sizes, repeat the sweep over larger or smaller versions of the image. Finally, we
postprocess theresponses acrossscalesandlocations toproduce thefinalsetofdetections.
Postprocessing is important, because it is unlikely that we have chosen a window size
that is exactly the right size for a face (even if we use multiple sizes). Thus, we will likely
have several overlapping windows that each report a match for a face. However, if we use
aclassifierthat can report strength ofresponse (forexample, logistic regression orasupport
vector machine) we can combine these partial overlapping matches at nearby locations to
yieldasinglehigh-quality match. Thatgivesusafacedetectorthatcansearchoverlocations
and scales. To search rotations as well, we use two steps. We train a regression procedure
to estimate the best orientation of any face present in a window. Now, for each window, we
estimatetheorientation, reorientthewindow,thentestwhetheraverticalfaceispresentwith
ourclassifier. Allthisyieldsasystemwhosearchitecture issketched inFigure24.12.
Training data is quite easily obtained. There are several data sets of marked-up face
images, and rotated face windows are easy to build (just rotate a window from a training
data set). One trick that is widely used is to take each example window, then produce new
examples by changing the orientation of the window, the center of the window, or the scale
very slightly. This is an easy way of getting a bigger data set that reflects real images fairly
well; the trick usually improves performance significantly. Face detectors built along these
linesnowperform verywellforfrontalfaces(sideviewsare harder).
944 Chapter 24. Perception
Non-maximal
suppresion
Image Responses Detections
Estimate
orientation
Correct Rotate
Features Classifier
illumination window
Figure 24.12 Face finding systems vary, but most follow the architecture illustrated in
two parts here. On the top, we go from images to responses, then apply non-maximum
suppressionto findthestrongestlocalresponse. Theresponsesareobtainedbytheprocess
illustratedonthebottom. Wesweepawindowoffixedsizeoverlargerandsmallerversions
of the image, so as to find smaller or larger faces, respectively. The illumination in the
window is corrected, and then a regression engine (quite often, a neural net) predicts the
orientationofthe face. Thewindowiscorrectedto thisorientationandthen presentedto a
classifier. Classifieroutputsarethenpostprocessedtoensurethatonlyonefaceisplacedat
eachlocationintheimage.
24.3.1 Complexappearance andpattern elements
Many objects produce much more complex patterns than faces do. This is because several
effectscanmovefeaturesaroundinanimageoftheobject. Effectsinclude (Figure24.13)
• Foreshortening,whichcausesapatternviewedataslanttobesignificantly distorted.
• Aspect, which causes objects to look different when seen from different directions.
Evenassimpleanobjectasadoughnut hasseveralaspects;seenfromtheside,itlooks
likeaflattenedoval,butfromaboveitisanannulus.
• Occlusion, where some parts are hidden from some viewing directions. Objects can
occlude one another, or parts of an object can occlude other parts, an effect known as
self-occlusion.
• Deformation, where internal degrees of freedom of the object change its appearance.
Forexample,peoplecanmovetheirarmsandlegsaround,generatingaverywiderange
ofdifferentbodyconfigurations.
However, our recipe of searching across location and scale can still work. This is because
some structure will be present in the images produced by the object. Forexample, apicture
of a car is likely to show some of headlights, doors, wheels, windows, and hubcaps, though
theymaybeinsomewhatdifferentarrangementsindifferentpictures. Thissuggestsmodeling
objectswithpatternelements—collectionsofparts. Thesepatternelementsmaymovearound
Section24.3. ObjectRecognition byAppearance 945
Foreshortening Aspect
Occlusion Deformation
Figure 24.13 Sources of appearancevariation. First, elements can foreshorten, like the
circular patch on the top left. This patch is viewed at a slant, and so is elliptical in the
image.Second,objectsviewedfromdifferentdirectionscanchangeshapequitedramatically,
aphenomenonknownasaspect. On thetoprightarethreedifferentaspectsofadoughnut.
Occlusion causes the handle of the mug on the bottom left to disappear when the mug is
rotated. In this case, because the body and handle belong to the same mug, we have self-
occlusion.Finally,onthebottomright,someobjectscandeformdramatically.
withrespect toone another, but ifmostofthe pattern elements arepresent inabout theright
place,thentheobjectispresent. Anobjectrecognizeristhenacollectionoffeaturesthatcan
tellwhetherthepatternelementsarepresent, andwhethertheyareinabouttherightplace.
The most obvious approach is to represent the image window with a histogram of the
pattern elements that appear there. This approach does not work particularly well, because
too many patterns get confused with one another. For example, if the pattern elements are
color pixels, the French, UK, and Netherlands flags will get confused because they have
approximately the same color histograms, though the colors are arranged in very different
ways. Quite simple modifications of histograms yield very useful features. The trick is to
preserve some spatial detail in the representation; for example, headlights tend to be at the
front of a car and wheels tend to be at the bottom. Histogram-based features have been
successful inawidevarietyofrecognition applications; wewillsurveypedestrian detection.
24.3.2 Pedestriandetection withHOGfeatures
TheWorldBankestimatesthateachyearcaraccidentskillabout1.2millionpeople,ofwhom
abouttwothirdsarepedestrians. Thismeansthatdetecting pedestrians isanimportantappli-
cation problem, because cars that can automatically detect and avoid pedestrians might save
many lives. Pedestrians wear many different kinds of clothing and appear in many different
configurations, but, at relatively low resolution, pedestrians can have a fairly characteristic
appearance. The most usual cases are lateral or frontal views of a walk. In these cases,
946 Chapter 24. Perception
Image Orientation Positive Negative
histograms components components
Figure 24.14 Local orientation histograms are a powerful feature for recognizing even
quitecomplexobjects. Ontheleft,animageofapedestrian. Onthecenterleft,localorien-
tation histogramsforpatches. We then applya classifier such as a supportvectormachine
tofindtheweightsforeachhistogramthatbestseparatethepositiveexamplesofpedestrians
fromnon-pedestrians. Weseethatthepositivelyweightedcomponentslookliketheoutline
ofa person. Thenegativecomponentsarelessclear;theyrepresentallthepatternsthatare
notpedestrians.FigurefromDalalandTriggs(2005)(cid:2)c IEEE.
we see either a “lollipop” shape — the torso is wider than the legs, which are together in
the stance phase of the walk — or a “scissor” shape — where the legs are swinging in the
walk. Weexpect to see someevidence ofarms and legs, and the curve around theshoulders
and head also tends to visible and quite distinctive. This means that, with a careful feature
construction, wecanbuildausefulmoving-window pedestrian detector.
There isn’t always a strong contrast between the pedestrian and the background, so it
isbettertouseorientations thanedgestorepresent theimagewindow. Pedestrians canmove
their arms and legs around, so we should use a histogram to suppress some spatial detail in
thefeature. Webreakupthewindowintocells,whichcouldoverlap,andbuildanorientation
histogram in each cell. Doing so will produce a feature that can tell whether the head-and-
shoulders curve is at the top of the window orat the bottom, but will not change if the head
movesslightly.
One further trick is required to make a good feature. Because orientation features are
not affected by illumination brightness, we cannot treat high-contrast edges specially. This
meansthatthedistinctive curvesontheboundary ofapedestrian aretreatedinthesameway
as fine texture detail in clothing or in the background, and so the signal may be submerged
innoise. Wecanrecovercontrastinformationbycountinggradientorientations withweights
that reflect how significant a gradient is compared to other gradients in the same cell. We
will write || ∇Ix || for the gradient magnitude at point x in the image, write C for the cell
whosehistogram wewishtocompute, and write wx,C fortheweight thatwewilluseforthe
Section24.4. Reconstructing the3DWorld 947
Figure 24.15 Another example of object recognition, this one using the SIFT feature
(ScaleInvariantFeatureTransform),anearlierversionof theHOGfeature. Ontheleft,im-
agesofashoeandatelephonethatserveasobjectmodels.Inthecenter,atestimage.Onthe
right,theshoeandthetelephonehavebeendetectedby: findingpointsintheimagewhose
SIFTfeaturedescriptionsmatchamodel;computinganestimateofposeofthemodel;and
verifyingthatestimate. Astrongmatchisusuallyverifiedwithrarefalsepositives. Images
fromLowe(1999)(cid:2)c IEEE.
orientation atxforthiscell. Anaturalchoiceofweightis
wx,C = (cid:2)
u
|
∈
|∇
C
I
||
x
∇
||
Iu ||
.
This compares the gradient magnitude to others in the cell, so gradients that are large com-
pared to their neighbors get a large weight. The resulting feature is usually called a HOG
feature(forHistogramOfGradientorientations).
HOGFEATURE
This feature construction is the main way in which pedestrian detection differs from
facedetection. Otherwise,buildingapedestrian detector isverylikebuilding afacedetector.
The detector sweeps a window across the image, computes features for that window, then
presents it to a classifier. Non-maximum suppression needs to be applied to the output. In
mostapplications, thescale andorientation oftypical pedestrians isknown. Forexample, in
driving applications in which a camera is fixedto the car, we expect to view mainly vertical
pedestrians, and we are interested only in nearby pedestrians. Several pedestrian data sets
havebeenpublished, andthesecanbeusedfortrainingtheclassifier.
Pedestrians are not the only type of object we can detect. In Figure 24.15 we see that
similartechniques canbeusedtofindavarietyofobjectsindifferent contexts.
24.4 RECONSTRUCTING THE 3D WORLD
In this section we show how to go from the two-dimensional image to a three-dimensional
representation of the scene. The fundamental question is this: Given that all points in the
scenethatfallalongaraytothepinholeareprojected tothe samepointintheimage,howdo
werecoverthree-dimensional information? Twoideascometoourrescue:
948 Chapter 24. Perception
• Ifwehavetwo(ormore)imagesfrom different camerapositions, thenwecantriangu-
latetofindtheposition ofapointinthescene.
• We can exploit background knowledge about the physical scene that gave rise to the
image. Givenanobject modelP(Scene)andarendering model P(Image|Scene),we
cancomputeaposteriordistribution P(Scene|Image).
Thereisasyetnosingle unifiedtheoryforscene reconstruction. Wesurveyeightcommonly
usedvisualcues: motion,binocularstereopsis, multipleviews,texture,shading,contour,
andfamiliarobjects.
24.4.1 Motionparallax
Ifthecameramovesrelativetothethree-dimensional scene, theresulting apparent motionin
theimage, optical flow,canbeasource ofinformation forboth themovementofthecamera
and depth in the scene. To understand this, we state (without proof) an equation that relates
theopticalflowtotheviewer’stranslational velocity Tandthedepthinthescene.
Thecomponents oftheopticalflowfieldare
−T +xT −T +yT
x z y z
v (x,y) = , v (x,y) = ,
x y
Z(x,y) Z(x,y)
where Z(x,y) is the z-coordinate of the point in the scene corresponding to the point in the
imageat(x,y).
Note that both components of the optical flow, v (x,y) and v (x,y), are zero at the
x y
FOCUSOF point x = T /T ,y = T /T . This point is called the focus of expansion of the flow
EXPANSION x z y z
field. Suppose we change the origin in the x–y plane to lie at the focus of expansion; then
(cid:2) (cid:2)
the expressions for optical flow take on a particularly simple form. Let (x,y ) be the new
coordinates definedbyx (cid:2) = x−T /T ,y (cid:2) =y−T /T . Then
x z y z
(cid:2) (cid:2)
xT y T
(cid:2) (cid:2) z (cid:2) (cid:2) z
v (x,y ) = , v (x,y )= .
x Z(x(cid:2),y(cid:2)) y Z(x(cid:2),y(cid:2))
Notethatthere isascale-factor ambiguity here. Ifthecamerawasmoving twiceasfast, and
everyobjectinthescenewastwiceasbigandattwicethedistance tothecamera,theoptical
flowfieldwouldbeexactlythesame. Butwecanstillextractquiteusefulinformation.
1. Suppose you are a fly trying to land on a wall and you want to know the time-to-
contact at the current velocity. This time is given by Z/T . Note that although the
z
instantaneous optical flow field cannot provide either the distance Z or the velocity
component T , it can provide the ratio of the two and can therefore be used to control
z
thelanding approach. Thereisconsiderable experimental evidence thatmanydifferent
animalspeciesexploitthiscue.
2. Consider two points at depths Z , Z , respectively. We may not know the absolute
1 2
value of either of these, but by considering the inverse of the ratio of the optical flow
magnitudes atthese points, wecandetermine thedepthratio Z /Z . Thisisthecueof
1 2
motion parallax, one we use when we look out of the side window of a moving car or
trainandinferthattheslowermovingpartsofthelandscape arefartheraway.
Section24.4. Reconstructing the3DWorld 949
Perceived object
Left image Right image
Right Left
Disparity
(a) (b)
Figure 24.16 Translating a camera parallel to the image plane causes image features to
move in the camera plane. The disparity in positions that results is a cue to depth. If we
superimposeleftandrightimage,asin(b),weseethedisparity.
24.4.2 Binocularstereopsis
Most vertebrates have two eyes. This is useful for redundancy in case of a lost eye, but it
helps in other ways too. Most prey have eyes on the side of the head to enable a widerfield
BINOCULAR of vision. Predators have the eyes in the front, enabling them to use binocular stereopsis.
STEREOPSIS
Theidea issimilartomotion parallax, except thatinstead ofusing images overtime, weuse
two (or more) images separated in space. Because a given feature in the scene will be in a
different place relative to the z-axis of each image plane, if we superpose the two images,
there willbeadisparity inthe location oftheimage feature inthe twoimages. Youcan see
DISPARITY
this in Figure 24.16, where the nearest point of the pyramid is shifted to the left in the right
imageandtotherightintheleftimage.
Note that to measure disparity we need to solve the correspondence problem, that is,
determine for a point in the left image, the point in the right image that results from the
projection of the same scene point. This is analogous to what one has to do in measuring
optical flow, and the most simple-minded approaches are somewhat similar and based on
comparingblocksofpixelsaroundcorrespondingpointsusingthesumofsquareddifferences.
Inpractice,weusemuchmoresophisticatedalgorithms,whichexploitadditionalconstraints.
Assuming that we can measure disparity, how does this yield information about depth
in the scene? We will need to work out the geometrical relationship between disparity and
depth. First, wewill consider the case when both the eyes (or cameras) are looking forward
withtheiropticalaxesparallel. Therelationship oftherightcameratotheleftcameraisthen
just a displacement along the x-axis by an amount b, the baseline. We can use the optical
flow equations from the previous section, if we think of this as resulting from a translation
950 Chapter 24. Perception
Left
eye δθ/2
P
L
θ
b P P
0
P
R
Right
eye Z δZ
Figure24.17 Therelationbetweendisparityanddepthinstereopsis. Thecentersofpro-
jectionofthetwoeyesarebapart,andtheopticalaxesintersectatthefixationpointP . The
0
point P in the scene projects to points PL and PR in the two eyes. In angular terms, the
disparitybetweentheseisδθ. Seetext.
vector T acting for time δt, with T = b/δt and T = T = 0. The horizontal and vertical
x y z
disparityaregivenbytheopticalflowcomponents,multipliedbythetimestepδt,H = v δt,
x
V = v δt. Carryingoutthesubstitutions, wegettheresultthat H = b/Z,V = 0. Inwords,
y
the horizontal disparity is equal to the ratio of the baseline to the depth, and the vertical
disparity iszero. Giventhatweknow b,wecanmeasure H andrecoverthedepth Z.
Under normal viewing conditions, humans fixate; that is, there is some point in the
FIXATE
sceneatwhichtheopticalaxesofthetwoeyesintersect. Figure24.17showstwoeyesfixated
at a point P , which is at a distance Z from the midpoint of the eyes. For convenience,
0
we will compute the angular disparity, measured in radians. The disparity at the point of
fixation P is zero. For some other point P in the scene that is δZ farther away, we can
0
compute the angular displacements of the left and right images of P, which wewill call P
L
and P , respectively. If each of these is displaced by an angle δθ/2 relative to P , then the
R 0
displacement betweenP andP ,whichisthedisparityofP,isjustδθ. FromFigure24.17,
L R
tanθ = b/2 andtan(θ−δθ/2) = b/2 ,butforsmallangles, tanθ ≈ θ,so
Z Z+δZ
b/2 b/2 bδZ
δθ/2 = − ≈
Z Z +δZ 2Z2
and,sincetheactualdisparity isδθ,wehave
bδZ
disparity = .
Z2
Inhumans,b(thebaselinedistancebetweentheeyes)isabout6cm. SupposethatZ isabout
BASELINE
100 cm. If the smallest detectable δθ (corresponding to the pixel size) is about 5 seconds
of arc, this gives a δZ of 0.4 mm. For Z = 30 cm, we get the impressively small value
δZ = 0.036 mm. Thatis, atadistance of30cm,humans candiscriminate depths thatdiffer
byaslittleas0.036mm,enabling ustothreadneedlesandthelike.
Section24.4. Reconstructing the3DWorld 951
Figure24.18 (a)Fourframesfroma videosequencein whichthe camerais movedand
rotatedrelativetotheobject.(b)Thefirstframeofthesequence,annotatedwithsmallboxes
highlightingthefeaturesfoundbythefeaturedetector.(CourtesyofCarloTomasi.)
24.4.3 Multipleviews
Shapefromopticalfloworbinoculardisparityaretwoinstancesofamoregeneralframework,
thatofexploiting multipleviewsforrecovering depth. Incomputervision, thereisnoreason
forustoberestrictedtodifferentialmotionortoonlyusetwocamerasconvergingatafixation
point. Therefore, techniques have been developed that exploit the information available in
multiple views, even from hundreds or thousands of cameras. Algorithmically, there are
threesubproblems thatneedtobesolved:
• The correspondence problem, i.e., identifying features in the different images that are
projections ofthesamefeatureinthethree-dimensional world.
• The relative orientation problem, i.e., determining the transformation (rotation and
translation) betweenthecoordinate systemsfixedtothedifferent cameras.
• Thedepthestimationproblem,i.e.,determiningthedepthsofvariouspointsintheworld
forwhichimageplaneprojections wereavailable inatleast twoviews
The development of robust matching procedures for the correspondence problem, accompa-
niedbynumericallystablealgorithmsforsolvingforrelativeorientations andscenedepth,is
oneofthesuccessstoriesofcomputervision. ResultsfromonesuchapproachduetoTomasi
andKanade(1992)areshowninFigures24.18and24.19.
24.4.4 Texture
Earlierwesaw how texture wasused forsegmenting objects. Itcan also beused toestimate
distances. InFigure 24.20weseethatahomogeneous texture inthescene results invarying
textureelements, ortexels, intheimage. Allthepaving tilesin(a)areidentical inthescene.
TEXEL
Theyappeardifferent intheimagefortworeasons:
952 Chapter 24. Perception
(a) (b)
Figure24.19 (a)Three-dimensionalreconstructionofthelocationsoftheimagefeatures
inFigure24.18,shownfromabove.(b)Therealhouse,takenfromthesameposition.
1. Differencesinthedistancesofthetexelsfromthecamera. Distantobjectsappearsmaller
byascaling factorof1/Z.
2. Differences in the foreshortening of the texels. If all the texels are in the ground plane
then distance ones are viewed at an angle that is farther off the perpendicular, and so
are more foreshortened. The magnitude of the foreshortening effect is proportional to
cosσ, where σ is the slant, the angle between the Z-axis and n, the surface normal to
thetexel.
Researchers have developed various algorithms that try to exploit the variation in the
appearance of the projected texels as a basis for determining surface normals. However, the
accuracy and applicability of these algorithms is not anywhere as general as those based on
usingmultipleviews.
24.4.5 Shading
Shading—variation inthe intensity oflight received from different portions ofasurface ina
scene—is determined by the geometry of the scene and by the reflectance properties of the
surfaces. In computer graphics, the objective is to compute the image brightness I(x,y),
given the scene geometry and reflectance properties of the objects in the scene. Computer
visionaimstoinverttheprocess—that is,torecoverthegeometryandreflectance properties,
given the image brightness I(x,y). This has proved to be difficult to do in anything but the
simplestcases.
From the physical model of section 24.1.4, we know that if a surface normal points
toward the light source, the surface is brighter, and if it points away, the surface is darker.
We cannot conclude that a dark patch has its normal pointing away from the light; instead,
it could have low albedo. Generally, albedo changes quite quickly in images, and shading
Section24.4. Reconstructing the3DWorld 953
(a) (b)
Figure24.20 (a)Atexturedscene.Assumingthattherealtextureisuniformallowsrecov-
eryofthesurfaceorientation. Thecomputedsurfaceorientationisindicatedbyoverlayinga
blackcircleandpointer,transformedasifthecirclewerepaintedonthesurfaceatthatpoint.
(b)Recoveryofshapefromtextureforacurvedsurface(whitecircleandpointerthistime).
ImagescourtesyofJitendraMalikandRuthRosenholtz(1994).
changes rather slowly, and humans seem to be quite good at using this observation to tell
whether low illumination, surface orientation, or albedo caused a surface patch to be dark.
To simplify the problem, let us assume that the albedo is known at every surface point. It
is still difficult to recover the normal, because the image brightness is one measurement but
thenormalhastwounknownparameters, sowecannot simplysolveforthenormal. Thekey
to this situation seems to be that nearby normals will be similar, because most surfaces are
smooth—they donothavesharpchanges.
Therealdifficultycomesindealingwithinterreflections. Ifweconsideratypicalindoor
scene, such as the objects inside an office, surfaces are illuminated not only by the light
sources, but also by the light reflected from other surfaces in the scene that effectively serve
assecondary light sources. These mutual illumination effects are quite significant and make
itquitedifficulttopredicttherelationshipbetweenthenormalandtheimagebrightness. Two
surface patches with the same normal might have quite different brightnesses, because one
receives light reflected from a large white wall and the other faces only a dark bookcase.
Despite these difficulties, the problem is important. Humans seem to be able to ignore the
effects of interreflections and get a useful perception of shape from shading, but we know
frustratingly littleaboutalgorithms todothis.
24.4.6 Contour
When we look at a line drawing, such as Figure 24.21, we get a vivid perception of three-
dimensional shapeandlayout. How? Itisacombination ofrecognition offamiliarobjectsin
thesceneandtheapplication ofgeneric constraints suchasthefollowing:
• Occluding contours, such as the outlines of the hills. Oneside ofthe contour is nearer
totheviewer, theotherside isfarther away. Features suchaslocal convexity andsym-
954 Chapter 24. Perception
Figure24.21 Anevocativelinedrawing.(CourtesyofIshaMalik.)
metryprovidecuestosolvingthefigure-groundproblem—assigning whichsideofthe
FIGURE-GROUND
contour is figure (nearer), and which is ground (farther). At an occluding contour, the
lineofsightistangential tothesurfaceinthescene.
• T-junctions. When one object occludes another, the contour of the farther object is
interrupted,assumingthatthenearerobjectisopaque. AT-junctionresultsintheimage.
• Position on the ground plane. Humans, like many other terrestrial animals, are very
ofteninascenethatcontainsagroundplane,withvariousobjectsatdifferentlocations
GROUNDPLANE
onthis plane. Because ofgravity, typical objects don’t floatinairbut aresupported by
thisgroundplane,andwecanexploittheveryspecialgeometryofthisviewingscenario.
Let us work out the projection of objects of different heights and at different loca-
tions on the ground plane. Suppose that the eye, or camera, is at a height h above
c
theground plane. Consider an object of height δY resting on the ground plane, whose
bottom is at (X,−h ,Z) and top is at (X,δY − h ,Z). The bottom projects to the
c c
imagepoint(fX/Z,−fh /Z)andthetopto(fX/Z,f(δY −h )/Z). Thebottomsof
c c
nearerobjects(smallZ)projecttopointslowerintheimageplane;fartherobjects have
bottomsclosertothehorizon.
24.4.7 Objects and thegeometric structure ofscenes
A typical adult human head is about 9 inches long. This means that for someone who is 43
feetaway,theanglesubtendedbytheheadatthecamerais1degree. Ifweseeapersonwhose
head appears to subtend just half a degree, Bayesian inference suggests we are looking at a
normal person who is 86 feet away, rather than someone with a half-size head. This line of
reasoning suppliesuswithamethodtochecktheresultsofapedestrian detector, aswellasa
methodtoestimatethedistancetoanobject. Forexample,allpedestrians areaboutthesame
height,andtheytendtostandonagroundplane. Ifweknowwherethehorizonisinanimage,
wecanrankpedestrians bydistancetothecamera. Thisworks becauseweknowwheretheir
Section24.4. Reconstructing the3DWorld 955
Image plane
Horizon
Ground plane
C
C B
B
A
A
Figure 24.22 In an image of people standing on a ground plane, the people whose feet
areclosertothehorizonintheimagemustbefartheraway(topdrawing). Thismeansthey
mustlooksmallerintheimage(leftlowerdrawing).Thismeansthatthesizeandlocationof
realpedestriansinanimagedependupononeanotherandonthelocationofthehorizon.To
exploitthis, we need to identify the groundplane, which is done using shape-from-texture
methods.Fromthisinformation,andfromsomelikelypedestrians,wecanrecoverahorizon
asshowninthecenterimage.Ontheright,acceptablepedestrianboxesgiventhisgeometric
context. Noticethatpedestrianswhoarehigherinthescenemustbesmaller. Iftheyarenot,
thentheyarefalsepositives.ImagesfromHoiemetal.(2008)(cid:2)c IEEE.
feet are, and pedestrians whose feet are closer to the horizon in the image are farther away
fromthecamera(Figure24.22). Pedestrianswhoarefarther awayfromthecameramustalso
besmallerintheimage. Thismeanswecanruleoutsomedetectorresponses—ifadetector
finds a pedestrian who is large in the image and whose feet are close to the horizon, it has
found an enormous pedestrian; these don’t exist, so the detector is wrong. In fact, many or
mostimagewindowsarenotacceptable pedestrian windows,andneednotevenbepresented
tothedetector.
There are several strategies for finding the horizon, including searching for a roughly
horizontal line with a lot of blue above it, and using surface orientation estimates obtained
from texture deformation. A more elegant strategy exploits the reverse of our geometric
constraints. Areasonablyreliablepedestriandetectoris capableofproducingestimatesofthe
horizon, if there are several pedestrians in the scene at different distances from the camera.
Thisisbecause therelativescaling ofthepedestrians isacuetowherethehorizon is. Sowe
canextractahorizonestimatefromthedetector,thenusethisestimatetoprunethepedestrian
detector’s mistakes.
956 Chapter 24. Perception
Iftheobjectisfamiliar,wecanestimatemorethanjustthedistancetoit,becausewhatit
lookslikeintheimagedependsverystronglyonitspose,i.e.,itspositionandorientationwith
respecttotheviewer. Thishasmanyapplications. Forinstance,inanindustrialmanipulation
task, the robot arm cannot pick up an object until the pose is known. In the case of rigid
objects, whether three-dimensional ortwo-dimensional, this problem hasasimple and well-
definedsolution basedonthealignmentmethod,whichwenowdevelop.
ALIGNMENTMETHOD
The object is represented by M features or distinguished points m ,m ,...,m in
1 2 M
three-dimensional space—perhaps the vertices of a polyhedral object. These are measured
in some coordinate system that is natural for the object. The points are then subjected to
an unknown three-dimensional rotation R, followed by translation by an unknown amount t
and then projection to give rise to image feature points p ,p ,...,p on the image plane.
1 2 N
In general, N (cid:7)= M, because some model points may be occluded, and the feature detector
couldmisssomefeatures(orinventfalseonesduetonoise). Wecanexpressthisas
p = Π(Rm +t) = Q(m )
i i i
for a three-dimensional model point m and the corresponding image point p . Here, R
i i
is a rotation matrix, t is a translation, and Π denotes perspective projection or one of its
approximations, suchasscaled orthographic projection. Thenetresultisatransformation Q
that will bring the model point m into alignment with the image point p . Although we do
i i
notknowQinitially,wedoknow(forrigidobjects)thatQmustbethesameforallthemodel
points.
WecansolveforQ,giventhethree-dimensional coordinates ofthreemodelpoints and
theirtwo-dimensional projections. Theintuition is asfollows: wecanwrite downequations
relating the coordinates of p to those of m . In these equations, the unknown quantities
i i
correspond totheparametersoftherotationmatrix Randthetranslation vectort. Ifwehave
enough equations, we ought to be able to solve for Q. We will not give a proof here; we
merelystatethefollowingresult:
Given three noncollinear points m , m , and m in the model, and their scaled
1 2 3
orthographic projections p , p , and p on the image plane, there exist exactly
1 2 3
twotransformationsfromthethree-dimensionalmodelcoordinateframetoatwo-
dimensional imagecoordinate frame.
Thesetransformationsarerelatedbyareflectionaroundthe imageplaneandcanbecomputed
by asimple closed-form solution. If wecould identify the corresponding model features for
threefeatures intheimage,wecouldcompute Q,theposeoftheobject.
Letusspecifypositionandorientationinmathematicalterms. ThepositionofapointP
inthesceneischaracterizedbythreenumbers,the (X,Y,Z)coordinatesofP inacoordinate
frame with its origin at the pinhole and the Z-axis along the optical axis (Figure 24.2 on
page 931). What we have available is the perspective projection (x,y) of the point in the
image. This specifies the ray from the pinhole along which P lies; what we do not know is
thedistance. Theterm“orientation” couldbeusedintwosenses:
1. The orientation of the object as a whole. This can be specified in terms of a three-
dimensional rotationrelating itscoordinate frametothat ofthecamera.
Section24.5. ObjectRecognition fromStructuralInformation 957
2. Theorientation of thesurface oftheobject atP. Thiscan be specified by anormal
vector,n—whichisavectorspecifyingthedirectionthatisperpendiculartothesurface.
Often weexpress the surface orientation using the variables slant and tilt. Slant is the
SLANT
anglebetweentheZ-axisandn. TiltistheanglebetweentheX-axisandtheprojection
TILT
ofnontheimageplane.
When the camera moves relative to an object, both the object’s distance and its orientation
change. What is preserved is the shape of the object. If the object is a cube, that fact is
SHAPE
notchanged whentheobjectmoves. Geometershavebeenattempting toformalizeshapefor
centuries,thebasicconceptbeingthatshapeiswhatremainsunchangedundersomegroupof
transformations—for example,combinations ofrotations andtranslations. Thedifficultylies
infindingarepresentationofglobalshapethatisgeneralenoughtodealwiththewidevariety
ofobjectsintherealworld—notjustsimpleformslikecylinders,cones,andspheres—andyet
canberecovered easily fromthevisual input. Theproblem of characterizing the localshape
of a surface is much better understood. Essentially, this can be done in terms of curvature:
howdoesthesurface normalchangeasonemovesindifferentdirections onthesurface? For
a plane, there is no change at all. For a cylinder, if one moves parallel to the axis, there is
no change, but in the perpendicular direction, the surface normal rotates at a rate inversely
proportional to the radius of the cylinder, and so on. All this is studied in the subject called
differential geometry.
Theshape ofanobject isrelevantforsomemanipulation tasks(e.g.,deciding whereto
graspanobject), butitsmostsignificant roleisinobjectrecognition, wheregeometric shape
alongwithcolorandtextureprovidethemostsignificantcuestoenableustoidentifyobjects,
classifywhatisintheimageasanexampleofsomeclassonehasseenbefore,andsoon.
24.5 OBJECT RECOGNITION FROM STRUCTURAL INFORMATION
Puttingaboxaroundpedestriansinanimagemaywellbeenoughtoavoiddrivingintothem.
Wehaveseenthatwecanfindaboxbypooling theevidence provided byorientations, using
histogram methodstosuppress potentially confusing spatial detail. Ifwewanttoknowmore
aboutwhatsomeoneisdoing,wewillneedtoknowwheretheirarms,legs,body,andheadlie
inthepicture. Individual bodyparts arequitedifficulttodetect ontheirownusingamoving
windowmethod,becausetheircolorandtexturecanvarywidelyandbecausetheyareusually
small in images. Often, forearms and shins are as small as two to three pixels wide. Body
parts do not usually appear on their own, and representing what is connected to what could
bequitepowerful,becausepartsthatareeasytofindmighttelluswheretolookforpartsthat
aresmallandhardtodetect.
Inferring thelayoutofhumanbodies inpictures isanimportant taskinvision, because
the layout of the body often reveals what people are doing. A model called a deformable
DEFORMABLE templatecantelluswhichconfigurations areacceptable: theelbowcanbendbuttheheadis
TEMPLATE
neverjoinedtothefoot. Thesimplestdeformabletemplatemodelofapersonconnectslower
armstoupperarms,upperarmstothetorso,andsoon. Therearerichermodels: forexample,
958 Chapter 24. Perception
we could represent the fact that left and right upper arms tend to have the same color and
texture,asdoleftandrightlegs. Theserichermodelsremaindifficulttoworkwith,however.
24.5.1 The geometry ofbodies: Finding arms andlegs
For the moment, we assume that we know what the person’s body parts look like (e.g., we
know the color and texture of the person’s clothing). We can model the geometry of the
body asatreeofeleven segments (upperandlowerleftandright armsandlegsrespectively,
a torso, a face, and hair on top of the face) each of which is rectangular. We assume that
theposition andorientation (pose)oftheleftlowerarmisindependent ofallothersegments
POSE
given the pose of the left upper arm; that the pose of the left upper arm is independent of
all segments given the pose of the torso; and extend these assumptions in the obvious way
to include the right arm and the legs, the face, and the hair. Such models are often called
“cardboardpeople”models. Themodelformsatree,whichisusuallyrootedatthetorso. We
willsearchtheimageforthebestmatchtothiscardboardpersonusinginferencemethodsfor
atree-structured Bayesnet(seeChapter14).
There are two criteria for evaluating a configuration. First, an image rectangle should
looklikeitssegment. Forthemoment,wewillremainvagueaboutpreciselywhatthatmeans,
butweassumewehaveafunctionφ thatscoreshowwellanimagerectanglematchesabody
i
segment. For each pair of related segments, we have another function ψ that scores how
well relations between a pair of image rectangles match those to be expected from the body
segments. The dependencies between segments form a tree, so each segment has only one
parent, and we could write ψ i,pa(i) . All the functions will be larger if the match is better,
so we can think of them as being like a log probability. The cost of a particular match that
allocates imagerectangle m tobodysegmentiisthen
(cid:12) i (cid:12)
φ i (m i )+ ψ i,pa(i) (m i ,mpa(i) ).
i∈segments i∈segments
Dynamicprogramming canfindthebestmatch,because therelational modelisatree.
Itisinconvenienttosearchacontinuousspace,andwewilldiscretizethespaceofimage
rectangles. We do so by discretizing the location and orientation of rectangles of fixed size
(the sizes may be different for different segments). Because ankles and knees are different,
◦
we need to distinguish between a rectangle and the same rectangle rotated by 180 . One
couldvisualize theresultasasetofverylargestacks ofsmallrectangles ofimage, cutoutat
different locations and orientations. There is one stack per segment. We must now find the
best allocation of rectangles to segments. This will be slow, because there are many image
rectangles and,forthemodelwehavegiven,choosing therighttorsowillbeO(M6)ifthere
are M image rectangles. However, various speedups are available foran appropriate choice
of ψ, and the method is practical (Figure 24.23). Themodel is usually known asa pictorial
PICTORIAL structuremodel.
STRUCTUREMODEL
Recallourassumptionthatweknowwhatweneedtoknowaboutwhatthepersonlooks
like. If weare matching a person in a single image, the most useful feature forscoring seg-
ment matches turns out tobe color. Texture features don’t work wellin mostcases, because
foldsonlooseclothingproducestrongshadingpatternsthatoverlaytheimagetexture. These
Section24.5. ObjectRecognition fromStructuralInformation 959
Figure24.23 Apictorialstructuremodelevaluatesamatchbetweenasetofimagerect-
angles and a cardboard person (shown on the left) by scoring the similarity in appearance
betweenbodysegmentsandimagesegmentsandthespatialrelationsbetweentheimageseg-
ments.Generally,amatchisbetteriftheimagesegmentshaveabouttherightappearanceand
areinabouttherightplacewithrespecttooneanother. Theappearancemodelusesaverage
colors forhair, head, torso, and upperand lower arms and legs. The relevantrelations are
shownasarrows.Ontheright,thebestmatchforaparticularimage,obtainedusingdynamic
programming. The match is a fair estimate of the configuration of the body. Figure from
FelzenszwalbandHuttenlocher(2000)(cid:2)c IEEE.
patternsarestrongenoughtodisruptthetruetextureofthe cloth. Incurrentwork,ψtypically
reflects the need for the ends of the segments to be reasonably close together, but there are
usually no constraints on the angles. Generally, we don’t know what a person looks like,
and must build a model of segment appearances. We call the description of what a person
APPEARANCE looksliketheappearancemodel. Ifwemustreporttheconfiguration ofapersoninasingle
MODEL
image, wecan start with apoorly tuned appearance model, estimate configuration with this,
then re-estimate appearance, and so on. In video, wehave many frames of the same person,
andthiswillrevealtheirappearance.
24.5.2 Coherent appearance: Tracking people invideo
Tracking people in video is an important practical problem. If we could reliably report the
location of arms, legs, torso, and head in video sequences, we could build much improved
game interfaces and surveillance systems. Filtering methods have not had much success
withthisproblem, because people canproduce large accelerations andmovequite fast. This
means that for 30 Hz video, the configuration of the body in frame i doesn’t constrain the
configurationofthebodyinframei+1allthatstrongly. Currently,themosteffectivemethods
exploit thefactthatappearance changes veryslowlyfromframetoframe. Ifwecaninferan
appearance model of an individual from the video, then we can use this information in a
pictorial structure model to detect that person in each frame of the video. We can then link
theselocations acrosstimetomakeatrack.
960 Chapter 24. Perception
torso
arm
Lateral walking Appearance Body part Detected figure
detector model maps
motion blur
& interlacing
Figure 24.24 We can track moving people with a pictorial structure model by first ob-
taininganappearancemodel,thenapplyingit. Toobtaintheappearancemodel,wescanthe
image to find a lateral walking pose. The detector does not need to be very accurate, but
shouldproducefewfalsepositives. Fromthedetectorresponse,we canreadoffpixelsthat
lieoneachbodysegment,andothersthatdonotlieonthatsegment.Thismakesitpossibleto
buildadiscriminativemodeloftheappearanceofeachbodypart,andthesearetiedtogether
intoapictorialstructuremodelofthepersonbeingtracked. Finally,wecanreliablytrackby
detectingthismodelineachframe.Astheframesinthelowerpartoftheimagesuggest,this
procedurecantrackcomplicated,fast-changingbodyconfigurations,despitedegradationof
thevideosignalduetomotionblur. FigurefromRamananetal.(2007)(cid:2)c IEEE.
There are several ways to infer a good appearance model. We regard the video as a
large stack of pictures of the person we wish to track. We can exploit this stack by looking
for appearance models that explain many of the pictures. This would work by detecting
bodysegmentsineachframe,usingthefactthatsegmentshaveroughly paralleledges. Such
detectors are not particularly reliable, but the segments we want to find are special. They
will appear at least once in most of the frames of video; such segments can be found by
clustering the detector responses. It is best to start with the torso, because it is big and
because torso detectors tend to be reliable. Once we have a torso appearance model, upper
leg segments should appear near the torso, and so on. This reasoning yields an appearance
model, but it can be unreliable if people appear against a near-fixed background where the
segment detector generates lots of false positives. An alternative is to estimate appearance
formanyoftheframesofvideobyrepeatedlyreestimatingconfigurationandappearance; we
then see if one appearance model explains manyframes. Another alternative, which isquite
Section24.6. UsingVision 961
Figure 24.25 Some complex human actions produce consistent patterns of appearance
andmotion.Forexample,drinkinginvolvesmovementsofthehandinfrontoftheface. The
firstthreeimagesarecorrectdetectionsofdrinking;thefourthisafalse-positive(thecookis
lookingintothecoffeepot,butnotdrinkingfromit). FigurefromLaptevandPerez(2007)
(cid:2)c IEEE.
reliableinpractice,istoapplyadetectorforafixedbodyconfigurationtoalloftheframes. A
goodchoiceofconfiguration isonethatiseasytodetectreliably, andwherethereisastrong
chance theperson willappearinthat configuration even inashort sequence (lateral walking
isagoodchoice). Wetunethedetector tohavealowfalsepositive rate,soweknowwhenit
responds that we have found a real person; and because wehave localized their torso, arms,
legs,andhead,weknowwhatthesesegmentslooklike.
24.6 USING VISION
If vision systems could analyze video and understood what people are doing, we would be
able to: design buildings and public places better by collecting and using data about what
peopledoinpublic;buildmoreaccurate,moresecure,andlessintrusivesurveillancesystems;
buildcomputersportscommentators;andbuildhuman-computerinterfacesthatwatchpeople
and react to their behavior. Applications for reactive interfaces range from computer games
thatmakeaplayergetupandmovearoundtosystemsthatsaveenergybymanagingheatand
lightinabuilding tomatchwheretheoccupants areandwhattheyaredoing.
Some problems are well understood. If people are relatively small in the video frame,
andthebackgroundisstable,itiseasytodetectthepeoplebysubtractingabackgroundimage
from the current frame. If the absolute value of the difference is large, this background
BACKGROUND subtraction declares the pixel to be a foreground pixel; by linking foreground blobs over
SUBTRACTION
time,weobtainatrack.
Structuredbehaviors likeballet,gymnastics, ortaichihavespecificvocabularies ofac-
tions. Whenperformed againstasimplebackground, videos oftheseactionsareeasytodeal
with. Background subtraction identifies the major moving regions, and we can build HOG
features(keepingtrackofflowratherthanorientation)topresenttoaclassifier. Wecandetect
consistent patterns of action with a variant of our pedestrian detector, where the orientation
featuresarecollected intohistogram bucketsovertimeaswellasspace(Figure24.25).
Moregeneral problems remain open. Thebig research question isto link observations
of the body and the objects nearby to the goals and intentions of the moving people. One
source ofdifficulty isthat welack asimple vocabulary ofhuman behavior. Behavior isalot
962 Chapter 24. Perception
like color, in that people tend to think they know a lot of behavior names but can’t produce
longlistsofsuchwordsondemand. Thereisquitealotofevidencethatbehaviorscombine—
you can, for example, drink a milkshake while visiting an ATM—but we don’t yet know
what the pieces are, how the composition works, or how many composites there might be.
Asecond source ofdifficulty isthatwedon’t know whatfeatures expose whatishappening.
Forexample,knowingsomeoneisclosetoanATMmaybeenoughtotellthatthey’revisiting
theATM.Athirddifficultyisthattheusualreasoningabouttherelationshipbetweentraining
and test data is untrustworthy. For example, we cannot argue that a pedestrian detector is
safesimply because itperforms wellonalarge dataset, because thatdatasetmaywellomit
important, butrare,phenomena (forexample, people mounting bicycles). Wewouldn’t want
ourautomateddrivertorunoverapedestrian whohappened to dosomething unusual.
24.6.1 Wordsandpictures
Many Web sites offer collections of images for viewing. How can we find the images we
want? Let’ssuppose theuserenters atextquery, suchas“bicycle race.” Someoftheimages
willhavekeywordsorcaptionsattached, orwillcomefromWebpagesthatcontain textnear
the image. Forthese, image retrieval can be like text retrieval: ignore the images and match
theimage’stextagainstthequery(seeSection22.3onpage867).
However, keywords are usually incomplete. Forexample, a picture of a cat playing in
thestreetmightbetaggedwithwordslike“cat”and“street,”butitiseasytoforgettomention
the“garbagecan”orthe“fishbones.” Thusaninterestingtaskistoannotateanimage(which
mayalready haveafewkeywords)withadditional appropriate keywords.
In the most straightforward version of this task, we have a set of correctly tagged ex-
ample images, and we wish to tag some test images. This problem is sometimes known as
auto-annotation. Themostaccurate solutions areobtained using nearest-neighbors methods.
One finds the training images that are closest tothe test image in a feature space metric that
istrained usingexamples, thenreportstheirtags.
Another version of the problem involves predicting which tags to attach to which re-
gionsinatestimage. Herewedonotknowwhichregionsproducedwhichtagsforthetrain-
ingdata. Wecanuseaversionofexpectationmaximizationtoguessaninitialcorrespondence
between textandregions, andfrom thatestimate abetterdecomposition into regions, andso
on.
24.6.2 Reconstruction from many views
Binocular stereopsis works because for each point we have four measurements constraining
three unknown degrees of freedom. The four measurements are the (x,y) positions of the
pointineachview,andtheunknowndegreesoffreedomarethe(x,y,z)coordinatevaluesof
thepointinthescene. Thisrathercrudeargumentsuggests, correctly,thattherearegeometric
constraints thatpreventmostpairsofpointsfrombeingacceptablematches. Manyimagesof
asetofpointsshouldrevealtheirpositions unambiguously.
We don’t always need a second picture to get a second view of a set of points. If we
believe the original set of points comes from a familiar rigid 3D object, then wemight have
Section24.6. UsingVision 963
anobject modelavailable asasource ofinformation. Ifthis objectmodelconsists ofasetof
3Dpointsorofasetofpictures oftheobject, andifwecanestablish pointcorrespondences,
wecandeterminetheparametersofthecamerathatproducedthepointsintheoriginalimage.
This is very powerful information. We could use it to evaluate our original hypothesis that
the points come from an object model. We do this by using some points to determine the
parameters of the camera, then projecting model points in this camera and checking to see
whetherthereareimagepointsnearby.
Wehavesketchedhereatechnologythatisnowveryhighlydeveloped. Thetechnology
can be generalized to deal with views that are not orthographic; to deal with points that are
observed in only some views; to deal with unknown camera properties like focal length; to
exploitvarioussophisticated searchesforappropriatecorrespondences; andtodoreconstruc-
tion from verylarge numbers ofpoints andofviews. Ifthelocations ofpoints intheimages
areknownwithsomeaccuracyandtheviewingdirectionsarereasonable, veryhighaccuracy
cameraandpointinformation canbeobtained. Someapplications are
• Model-building: For example, one might build a modeling system that takes a video
sequence depicting an object and produces a very detailed three-dimensional mesh of
texturedpolygonsforuseincomputergraphicsandvirtualrealityapplications. Models
like this can now be built from apparently quite unpromising sets of pictures. For ex-
ample, Figure 24.26 shows a model of the Statue of Liberty built from pictures found
ontheInternet.
• Matching moves: To place computer graphics characters into real video, we need to
know how the camera moved for the real video, so that we can render the character
correctly.
• Path reconstruction: Mobile robots need to know where they have been. If they are
moving in a world of rigid objects, then performing a reconstruction and keeping the
camerainformation isonewaytoobtainapath.
24.6.3 Usingvisionforcontrolling movement
Oneoftheprincipal usesofvisionistoprovideinformation bothformanipulating objects—
pickingthemup,graspingthem,twirlingthem,andsoon—andfornavigatingwhileavoiding
obstacles. The ability to use vision for these purposes is present in the most primitive of
animal visual systems. In many cases, the visual system is minimal, in the sense that it
extracts from the available light field just the information the animal needs to inform its
behavior. Quite probably, modern vision systems evolved from early, primitive organisms
that used a photosensitive spot at one end to orient themselves toward (or away from) the
light. We saw in Section 24.4 that flies use a very simple optical flow detection system to
land on walls. A classic study, What the Frog’s Eye Tells the Frog’s Brain (Lettvin et al.,
1959),observesofafrogthat,“Hewillstarvetodeathsurroundedbyfoodifitisnotmoving.
Hischoiceoffoodisdetermined onlybysizeandmovement.”
Let us consider a vision system for an automated vehicle driving on a freeway. The
tasksfacedbythedriverincludethefollowing:
964 Chapter 24. Perception
a b c
(a) (b) (c)
Figure24.26 Thestateoftheartinmultiple-viewreconstructionisnowhighlyadvanced.
This figureoutlines a system builtby MichaelGoesele and colleaguesfrom the University
ofWashington, TU Darmstadt, and MicrosoftResearch. Froma collectionof picturesof a
monumenttakenbyalargecommunityofusersandpostedontheInternet(a),theirsystem
candeterminetheviewingdirectionsforthosepictures,shownbythesmallblackpyramids
in(b)andacomprehensive3Dreconstructionshownin(c).
1. Lateral control—ensure that the vehicle remains securely within its lane or changes
lanessmoothlywhenrequired.
2. Longitudinal control—ensure thatthereisasafedistancetothevehicleinfront.
3. Obstacleavoidance—monitorvehiclesinneighboringlanesandbepreparedforevasive
maneuversifoneofthemdecidestochangelanes.
The problem for the driver is to generate appropriate steering, acceleration, and braking ac-
tionstobestaccomplish thesetasks.
Forlateralcontrol,oneneedstomaintainarepresentation ofthepositionandorientation
ofthecarrelativetothelane. Wecanuseedge-detectionalgorithmstofindedgescorrespond-
ingtothelane-marker segments. Wecanthen fitsmooth curves tothese edgeelements. The
parameters of these curves carry information about the lateral position of the car, the direc-
tion it is pointing relative to the lane, and the curvature of the lane. This information, along
with information about the dynamics of the car, is all that is needed by the steering-control
system. Ifwehave good detailed mapsofthe road, then the vision system serves toconfirm
ourposition (andtowatchforobstacles thatarenotonthemap).
Forlongitudinal control, oneneeds toknowdistances tothe vehicles infront. Thiscan
be accomplished with binocular stereopsis or optical flow. Using these techniques, vision-
controlled carscannowdrivereliably athighwayspeeds.
Themoregeneralcaseofmobilerobotsnavigating invarious indoorandoutdoorenvi-
ronments has been studied, too. One particular problem, localizing the robot in its environ-
ment, now has pretty good solutions. A group at Sarnoff has developed a system based on
two cameras looking forward that track feature points in 3D and use that to reconstruct the
Section24.7. Summary 965
position of the robot relative tothe environment. In fact, they have twostereoscopic camera
systems, one looking front and one looking back—this gives greater robustness in case the
robothastogothroughafeaturelesspatchduetodarkshadows,blankwalls,andthelike. Itis
unlikelythattherearenofeatureseitherinthefrontorintheback. Nowofcourse,thatcould
happen,soabackupisprovidedbyusinganinertialmotionunit(IMU)somewhatakintothe
mechanisms for sensing acceleration that we humans have in our inner ears. By integrating
the sensed acceleration twice, one can keep track of the change in position. Combining the
datafromvisionandtheIMUisaproblemofprobabilisticevidencefusionandcanbetackled
usingtechniques, suchasKalmanfiltering, wehavestudiedelsewhereinthebook.
In the use of visual odometry (estimation of change in position), as in other problems
of odometry, there is the problem of “drift,” positional errors accumulating over time. The
solution for this is to use landmarks to provide absolute position fixes: as soon as the robot
passes a location in its internal map, it can adjust its estimate of its position appropriately.
Accuracies ontheorderofcentimeters havebeendemonstrated withthethesetechniques.
Thedriving examplemakesonepoint veryclear: foraspecific task, onedoes notneed
to recover all the information that, in principle, can be recovered from an image. One does
notneedtorecovertheexactshapeofeveryvehicle,solveforshape-from-textureonthegrass
surfaceadjacenttothefreeway,andsoon. Instead,avisionsystemshouldcomputejustwhat
isneeded toaccomplish thetask.
24.7 SUMMARY
Although perception appears to be an effortless activity for humans, it requires a significant
amountofsophisticated computation. Thegoalofvisionistoextractinformation neededfor
taskssuchasmanipulation, navigation, andobjectrecognition.
• The process of image formation is well understood in its geometric and physical as-
pects. Givenadescriptionofathree-dimensionalscene,wecaneasilyproduceapicture
ofitfromsomearbitrary cameraposition (thegraphicsproblem). Inverting theprocess
bygoingfromanimagetoadescription ofthesceneismoredifficult.
• To extract the visual information necessary for the tasks of manipulation, navigation,
and recognition, intermediate representations have to be constructed. Early vision
image-processing algorithms extract primitive features from the image, such as edges
andregions.
• There are various cues in the image that enable one to obtain three-dimensional in-
formation about the scene: motion, stereopsis, texture, shading, and contour analysis.
Eachof these cues relies on background assumptions about physical scenes to provide
nearlyunambiguous interpretations.
• Objectrecognitioninitsfullgeneralityisaveryhardproblem. Wediscussedbrightness-
based and feature-based approaches. We also presented a simple algorithm for pose
estimation. Otherpossibilities exist.
966 Chapter 24. Perception
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Theeye developed in the Cambrian explosion (530 million years ago), apparently in acom-
mon ancestor. Since then, endless variations have developed in different creatures, but the
same gene, Pax-6, regulates the development of the eye in animals as diverse as humans,
mice,andDrosophila.
Systematic attempts to understand human vision can be traced back to ancient times.
Euclid (ca. 300 B.C.) wrote about natural perspective—the mapping that associates, with
eachpointP inthethree-dimensionalworld,thedirectionoftheray OP joiningthecenterof
projection O tothepointP. Hewaswellawareofthenotion ofmotionparallax. Theuseof
perspective in art was developed in ancient Roman culture, as evidenced by art found in the
ruinsofPompeii(A.D.79),butwasthenlargelylostfor1300years. Themathematicalunder-
standingofperspectiveprojection, thistimeinthecontextofprojectionontoplanarsurfaces,
haditsnextsignificantadvanceinthe15th-century inRenaissanceItaly. Brunelleschi (1413)
isusually credited withcreating thefirstpaintings based on geometrically correct projection
ofa three-dimensional scene. In 1435, Alberti codified therules and inspired generations of
artistswhoseartisticachievementsamazeustothisday. Particularlynotableintheirdevelop-
mentofthescienceofperspective,asitwascalledinthosedays,wereLeonardodaVinciand
Albrecht Du¨rer. Leonardo’s late15th century descriptions oftheinterplay oflightandshade
(chiaroscuro), umbraandpenumbraregionsofshadows,andaerialperspectivearestillworth
reading in translation (Kemp, 1989). Stork (2004) analyzes thecreation ofvarious pieces of
Renaissance artusingcomputervisiontechniques.
Although perspective was known to the ancient Greeks, they were curiously confused
bytheroleoftheeyesinvision. Aristotlethoughtoftheeyesasdevicesemittingrays,rather
inthemannerofmodernlaserrangefinders. Thismistakenviewwaslaidtorestbythework
ofArabscientists, suchasAbuAliAlhazen, inthe10thcentury. Alhazenalsodeveloped the
cameraobscura,aroom(cameraisLatinfor“room”or“chamber”)withapinholethatcasts
an image on the opposite wall. Of course the image was inverted, which caused no end of
confusion. If the eye was to be thought of as such an imaging device, how do we see right-
side up? This enigma exercised the greatest minds of the era (including Leonardo). Kepler
firstproposedthatthelensoftheeyefocusesanimageontheretina,andDescartessurgically
removedanoxeyeanddemonstrated thatKeplerwasright. Therewasstillpuzzlement asto
whywedonotseeeverythingupsidedown;todaywerealizeitisjustaquestionofaccessing
theretinaldatastructure intherightway.
Inthe first half of the 20th century, the most significant research results in vision were
obtained bytheGestalt school ofpsychology, ledbyMaxWertheimer. Theypointed outthe
importance of perceptual organization: for a human observer, the image is not a collection
of pointillist photoreceptor outputs (pixels in computer vision terminology); rather it is or-
ganized into coherent groups. One could trace the motivation in computer vision of finding
regions and curves back to this insight. The Gestaltists also drew attention to the “figure–
ground” phenomenon—a contour separating two image regions that, in the world, are at
different depths, appears tobelong onlytothenearerregion, the“figure,”andnotthefarther
Bibliographical andHistorical Notes 967
region,the“ground.” Thecomputervisionproblem ofclassifying imagecurvesaccording to
theirsignificance inthescenecanbethoughtofasageneralization ofthisinsight.
The period after World War II was marked by renewed activity. Most significant was
theworkofJ.J.Gibson(1950,1979),whopointedouttheimportanceofopticalflow,aswell
astexturegradientsintheestimationofenvironmentalvariablessuchassurfaceslantandtilt.
Hereemphasizedtheimportanceofthestimulusandhowrichitwas. Gibsonemphasizedthe
roleoftheactiveobserverwhoseself-directedmovementfacilitatesthepickupofinformation
abouttheexternalenvironment.
Computer vision was founded in the 1960s. Roberts’s (1963) thesis at MIT was one
of the earliest publications in the field, introducing key ideas such as edge detection and
model-based matching. There is an urban legend that Marvin Minsky assigned the problem
of“solving”computervisiontoagraduatestudentasasummerproject. AccordingtoMinsky
the legend is untrue—it was actually an undergraduate student. But it was an exceptional
undergraduate, GeraldJaySussman(whoisnowaprofessor at MIT)andthetaskwasnotto
“solve”vision, buttoinvestigate someaspectsofit.
Inthe1960sand1970s,progresswasslow,hamperedconsiderably bythelackofcom-
putationalandstorageresources. Low-levelvisualprocessing receivedalotofattention. The
widely used Canny edge-detection technique was introduced in Canny (1986). Techniques
forfindingtextureboundariesbasedonmultiscale,multiorientationfilteringofimagesdateto
work such as Malik and Perona (1990). Combining multiple clues—brightness, texture and
color—for finding boundary curves in a learning framework wasshown by Martin, Fowlkes
andMalik(2004)toconsiderably improveperformance.
The closely related problem of finding regions of coherent brightness, color, and tex-
ture, naturally lends itself to formulations in which finding the best partition becomes an
optimization problem. Three leading examples are the Markov Random Fields approach of
Geman and Geman (1984), the variational formulation of Mumford and Shah (1989), and
normalized cutsbyShiandMalik(2000).
Through much of the 1960s, 1970s and 1980s, there were two distinct paradigms in
which visual recognition was pursued, dictated by different perspectives on what was per-
ceivedtobetheprimaryproblem. Computervisionresearchonobjectrecognitionlargelyfo-
cusedonissuesarisingfromtheprojectionofthree-dimensionalobjectsontotwo-dimensional
images. Theideaofalignment,alsofirstintroducedbyRoberts,resurfacedinthe1980sinthe
work of Lowe (1987) and Huttenlocher and Ullman (1990). Also popular was an approach
GENERALIZED based on describing shapes in terms of volumetric primitives, with generalized cylinders,
CYLINDER
introduced byTom Binford(1971),proving particularly popular.
Incontrast,thepatternrecognitioncommunityviewedthe3D-to-2Daspectsoftheprob-
lem asnot significant. Theirmotivating examples wereindomains such as optical character
recognition andhandwritten zipcoderecognition wherethe primaryconcern isthatoflearn-
ing the typical variations characteristic of a class of objects and separating them from other
classes. SeeLeCunetal.(1995)foracomparison ofapproaches.
In the late 1990s, these two paradigms started to converge, as both sides adopted the
probabilistic modeling and learning techniques that were becoming popular throughout AI.
Twolinesofworkcontributed significantly. Onewasresearch onfacedetection, suchasthat
968 Chapter 24. Perception
of Rowley, Baluja and Kanade (1996), and of Viola and Jones (2002b) which demonstrated
thepowerofpattern recognition techniques onclearly important and useful tasks. Theother
wasthedevelopmentofpointdescriptors, whichenableonetoconstructfeaturevectorsfrom
parts of objects. This was pioneered by Schmid and Mohr (1996). Lowe’s (2004) SIFT
descriptoriswidelyused. TheHOGdescriptorisduetoDalal andTriggs(2005).
Ullman (1979) and Longuet-Higgins (1981) are influential early works in reconstruc-
tion from multiple images. Concerns about the stability of structure from motion were sig-
nificantlyallayedbytheworkofTomasiandKanade(1992)whoshowedthatwiththeuseof
multiple frames shape could berecovered quite accurately. Inthe 1990s, withgreat increase
incomputerspeedandstorage,motionanalysis foundmanynewapplications. Buildinggeo-
metrical models of real-world scenes for rendering by computer graphics techniques proved
particularlypopular, ledbyreconstruction algorithmssuchastheonedevelopedbyDebevec,
Taylor, and Malik (1996). The books by Hartley and Zisserman (2000) and Faugeras et al.
(2001)provideacomprehensive treatmentofthegeometryof multipleviews.
Forsingle images, inferring shape from shading was first studied by Horn (1970), and
Hornand Brooks (1989) present anextensive survey ofthemainpapers from aperiod when
this was a much-studied problem. Gibson (1950) was the first to propose texture gradients
asacuetoshape,thoughacomprehensive analysisforcurved surfacesfirstappears inGard-
ing (1992) and Malik and Rosenholtz (1997). The mathematics of occluding contours, and
more generally understanding the visual events in the projection of smooth curved objects,
owesmuchtotheworkofKoenderink andvanDoorn, whichfindsanextensive treatment in
Koenderink’s(1990)SolidShape. Inrecentyears,attentionhasturnedtotreatingtheproblem
ofshapeandsurfacerecoveryfromasingleimageasaprobabilisticinferenceproblem,where
geometrical cues are not modeled explicitly, but used implicitly in a learning framework. A
goodrepresentative istheworkofHoiem,Efros,andHebert(2008).
Forthereaderinterested inhumanvision, Palmer(1999)provides thebestcomprehen-
sive treatment; Bruce et al. (2003) is a shorter textbook. The books by Hubel (1988) and
Rock (1984) are friendly introductions centered on neurophysiology and perception respec-
tively. DavidMarr’sbookVision(Marr,1982)playedahistoricalroleinconnectingcomputer
vision topsychophysics andneurobiology. Whilemanyofhis specific models haven’t stood
the test of time, the theoretical perspective from which each task is analyzed at an informa-
tional, computational, andimplementation levelisstillilluminating.
For computer vision, the most comprehensive textbook is Forsyth and Ponce (2002).
TruccoandVerri(1998)isashorteraccount. Horn(1986)and Faugeras(1993)aretwoolder
andstillusefultextbooks.
Themain journals forcomputer vision areIEEE Transactions onPattern Analysis and
Machine Intelligence and International Journal of Computer Vision. Computer vision con-
ferences include ICCV (International Conference on Computer Vision), CVPR (Computer
Vision and Pattern Recognition), and ECCV (European Conference on Computer Vision).
ResearchwithamachinelearningcomponentisalsopublishedintheNIPS(NeuralInforma-
tionProcessingSystems)conference,andworkontheinterfacewithcomputergraphicsoften
appearsattheACMSIGGRAPH(SpecialInterestGroupinGraphics) conference.
Exercises 969
EXERCISES
24.1 In the shadow of a tree with a dense, leafy canopy, one sees a number of light spots.
Surprisingly, they all appear to be circular. Why? After all, the gaps between the leaves
through whichthesunshinesarenotlikelytobecircular.
24.2 Consider a picture of a white sphere floating in front of a black backdrop. The im-
age curve separating white pixels from black pixels is sometimes called the “outline” of the
sphere. Showthat the outline ofasphere, viewed inaperspective camera, canbe anellipse.
Whydospheresnotlooklikeellipses toyou?
24.3 Consideraninfinitelylongcylinderofradius r oriented withitsaxisalongthey-axis.
The cylinder has a Lambertian surface and is viewed by a camera along the positive z-axis.
What will you expect to see in the image if the cylinder is illuminated by a point source
at infinity located on the positive x-axis? Draw the contours of constant brightness in the
projected image. Arethecontours ofequalbrightness uniformly spaced?
24.4 Edges in an image can correspond to a variety of events in a scene. Consider Fig-
ure24.4(page933),andassumethatitisapictureofarealthree-dimensional scene. Identify
ten different brightness edges in the image, and for each, state whether it corresponds to a
discontinuity in(a)depth,(b)surfaceorientation, (c)reflectance, or(d)illumination.
24.5 Astereoscopicsystemisbeingcontemplatedforterrainmapping. Itwillconsistoftwo
CCDcameras, each having 512×512pixels ona10cm×10cmsquare sensor. Thelenses
to be used have a focal length of 16 cm, with the focus fixed at infinity. For corresponding
points (u ,v ) in the left image and (u ,v ) in the right image, v = v because the x-axes
1 1 2 2 1 2
in the two image planes are parallel to the epipolar lines—the lines from the object to the
camera. Theoptical axesofthetwocamerasare parallel. The baseline between thecameras
is1meter.
a. Ifthenearestdistancetobemeasuredis16meters,whatisthelargestdisparitythatwill
occur(inpixels)?
b. Whatisthedistance resolution at16meters,duetothepixelspacing?
c. Whatdistance corresponds toadisparity ofonepixel?
24.6 Whichofthefollowingaretrue,andwhicharefalse?
a. Finding corresponding points in stereo images is the easiest phase of the stereo depth-
findingprocess.
b. Shape-from-texture canbedonebyprojecting agridoflight-stripes ontothescene.
c. Lineswithequallengths inthescenealwaysprojecttoequallengths intheimage.
d. Straightlinesintheimagenecessarily correspond tostraightlinesinthescene.
970 Chapter 24. Perception
D
A
X
B
C
Y
E
Figure 24.27 Top view of a two-camera vision system observing a bottle with a wall
behindit.
24.7 (Courtesy ofPietroPerona.) Figure24.27showstwocameras atXandYobserving a
scene. Draw the image seen at each camera, assuming that all named points are in the same
horizontal plane. Whatcan beconcluded fromthese twoimages about therelative distances
ofpointsA,B,C,D,andEfromthecamerabaseline, andonwhatbasis?
25
ROBOTICS
Inwhichagentsareendowedwithphysicaleffectors withwhichtodomischief.
25.1 INTRODUCTION
Robotsarephysical agentsthatperformtasksbymanipulating thephysicalworld. Todoso,
ROBOT
they are equipped with effectors such as legs, wheels, joints, and grippers. Effectors have
EFFECTOR
a single purpose: to assert physical forces on the environment.1 Robots are also equipped
with sensors, which allow them to perceive their environment. Present day robotics em-
SENSOR
ploys adiverse setofsensors, including camerasandlasers tomeasure theenvironment, and
gyroscopes andaccelerometers tomeasuretherobot’sownmotion.
Mostoftoday’srobotsfallintooneofthreeprimarycategories. Manipulators,orrobot
MANIPULATOR
arms (Figure 25.1(a)), are physically anchored to their workplace, for example in a factory
assembly line or on the International Space Station. Manipulator motion usually involves
a chain of controllable joints, enabling such robots to place their effectors in any position
within the workplace. Manipulators are by far the most common type of industrial robots,
with approximately one million units installed worldwide. Some mobile manipulators are
used in hospitals to assist surgeons. Few car manufacturers could survive without robotic
manipulators, andsomemanipulators haveevenbeenusedtogenerate original artwork.
Thesecondcategoryisthemobilerobot. Mobilerobotsmoveabouttheirenvironment
MOBILEROBOT
using wheels, legs, or similar mechanisms. They have been put to use delivering food in
hospitals, moving containers at loading docks, and similar tasks. Unmanned ground vehi-
cles,orUGVs,driveautonomously onstreets, highways, andoff-road. Theplanetaryrover
UGV
showninFigure25.2(b)exploredMarsforaperiodof3months in1997. Subsequent NASA
PLANETARYROVER
robots include thetwinMarsExploration Rovers(oneisdepicted onthecoverofthisbook),
which landed in 2003 and were still operating six years later. Other types of mobile robots
includeunmannedairvehicles(UAVs),commonlyusedforsurveillance,crop-spraying, and
UAV
1 InChapter2wetalkedaboutactuators,noteffectors. Herewedistinguishtheeffector(thephysicaldevice)
fromtheactuator(thecontrollinethatcommunicatesacommandtotheeffector).
971
972 Chapter 25. Robotics
(a) (b)
Figure 25.1 (a) An industrial robotic manipulator for stacking bags on a pallet. Image
courtesyofNachiRoboticSystems. (b)Honda’sP3andAsimohumanoidrobots.
(a) (b)
Figure 25.2 (a) Predator, an unmanned aerial vehicle (UAV) used by the U.S. Military.
ImagecourtesyofGeneralAtomicsAeronauticalSystems. (b)NASA’sSojourner,amobile
robotthatexploredthesurfaceofMarsinJuly1997.
military operations. Figure 25.2(a) shows a UAV commonly used by the U.S. military. Au-
tonomous underwater vehicles (AUVs) are used in deep sea exploration. Mobile robots
AUV
deliverpackagesintheworkplaceandvacuumthefloorsathome.
The third type of robot combines mobility with manipulation, and is often called a
MOBILE mobilemanipulator. Humanoidrobotsmimicthehuman torso. Figure25.1(b) showstwo
MANIPULATOR
early humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulators
HUMANOIDROBOT
Section25.2. RobotHardware 973
canapplytheireffectorsfurtherafieldthananchoredmanipulators can,buttheirtaskismade
harderbecausetheydon’thavetherigidity thattheanchorprovides.
The field of robotics also includes prosthetic devices (artificial limbs, ears, and eyes
forhumans), intelligent environments (such as an entire house that is equipped withsensors
andeffectors), andmultibodysystems,whereinroboticactionisachievedthroughswarmsof
smallcooperating robots.
Real robots must cope with environments that are partially observable, stochastic, dy-
namic, and continuous. Many robot environments are sequential and multiagent as well.
Partial observability and stochasticity are the result of dealing with a large, complex world.
Robot cameras cannot see around corners, and motion commands are subject to uncertainty
due to gears slipping, friction, etc. Also, the real world stubbornly refuses to operate faster
thanrealtime. Inasimulatedenvironment,itispossibletousesimplealgorithms(suchasthe
Q-learning algorithm described inChapter21) tolearn inafew CPUhours from millions of
trials. Inarealenvironment, itmighttakeyearstorunthesetrials. Furthermore, realcrashes
reallyhurt,unlikesimulatedones. Practicalroboticsystemsneedtoembodypriorknowledge
abouttherobot,itsphysicalenvironment, andthetasksthattherobotwillperformsothatthe
robotcanlearnquicklyandperform safely.
Robotics brings together many of the concepts we have seen earlier in the book, in-
cluding probabilistic state estimation, perception, planning, unsupervised learning, and re-
inforcement learning. For some of these concepts robotics serves as a challenging example
application. Forotherconceptsthischapterbreaksnewgroundinintroducing thecontinuous
versionoftechniques thatwepreviously sawonlyinthediscrete case.
25.2 ROBOT HARDWARE
Sofarinthisbook,wehavetakentheagentarchitecture—sensors, effectors,andprocessors—
asgiven,andwehaveconcentratedontheagentprogram. Thesuccessofrealrobotsdepends
atleastasmuchonthedesignofsensorsandeffectors thatareappropriate forthetask.
25.2.1 Sensors
Sensors are the perceptual interface between robot and environment. Passive sensors, such
PASSIVESENSOR
ascameras, aretrueobservers oftheenvironment: theycapture signals thataregenerated by
other sources in the environment. Active sensors, such as sonar, send energy into the envi-
ACTIVESENSOR
ronment. Theyrelyonthefactthatthisenergy isreflectedbacktothesensor. Activesensors
tendtoprovidemoreinformationthanpassivesensors,butattheexpenseofincreasedpower
consumption and with a danger of interference when multiple active sensors are used at the
sametime. Whetheractive orpassive, sensors canbe divided into three types, depending on
whethertheysensetheenvironment,therobot’slocation,ortherobot’sinternalconfiguration.
Range finders are sensors that measure the distance to nearby objects. In the early
RANGEFINDER
days of robotics, robots were commonly equipped with sonar sensors. Sonar sensors emit
SONARSENSORS
directional sound waves, which are reflected by objects, with some of the sound making it
974 Chapter 25. Robotics
(a) (b)
Figure25.3 (a) Time offlightcamera; image courtesyof Mesa ImagingGmbH. (b)3D
rangeimageobtainedwiththiscamera.Therangeimagemakesitpossibletodetectobstacles
andobjectsinarobot’svicinity.
back into the sensor. The time and intensity of the returning signal indicates the distance
to nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.
Stereovision(seeSection24.4.2)reliesonmultiplecamerastoimagethe environment from
STEREOVISION
slightly different viewpoints, analyzing theresulting parallax intheseimagestocomputethe
range of surrounding objects. For mobile ground robots, sonar and stereo vision are now
rarelyused,becausetheyarenotreliablyaccurate.
Mostgroundrobotsarenowequippedwithopticalrangefinders. Justlikesonarsensors,
opticalrangesensorsemitactivesignals(light)andmeasurethetimeuntilareflectionofthis
TIMEOFFLIGHT signalarrivesbackatthesensor. Figure25.3(a)showsa timeofflightcamera. Thiscamera
CAMERA
acquires range images like the one shown in Figure 25.3(b) at up to 60 frames per second.
Other range sensors use laser beams and special 1-pixel cameras that can be directed using
complex arrangements of mirrors or rotating elements. These sensors are called scanning
lidars (short for light detection and ranging). Scanning lidars tend to provide longer ranges
SCANNINGLIDARS
thantimeofflightcameras,andtendtoperformbetterinbrightdaylight.
Other common range sensors include radar, which is often the sensor of choice for
UAVs. Radar sensors can measure distances of multiple kilometers. On the other extreme
end of range sensing are tactile sensors such as whiskers, bump panels, and touch-sensitive
TACTILESENSORS
skin. These sensors measure range based on physical contact, and can be deployed only for
sensingobjects veryclosetotherobot.
A second important class of sensors is location sensors. Most location sensors use
LOCATIONSENSORS
rangesensingasaprimarycomponenttodeterminelocation. Outdoors,theGlobalPosition-
GLOBAL
ing System (GPS)is the most common solution to the localization problem. GPS measures
POSITIONING
SYSTEM
the distance to satellites that emit pulsed signals. At present, there are 31 satellites in orbit,
transmitting signalsonmultiple frequencies. GPSreceiverscanrecoverthedistance tothese
satellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPS
Section25.2. RobotHardware 975
receivers candetermine theirabsolute location onEarthto withinafewmeters. Differential
GPSinvolves asecond ground receiver with known location, providing millimeteraccuracy
DIFFERENTIALGPS
under ideal conditions. Unfortunately, GPS does not work indoors or underwater. Indoors,
localization is often achieved by attaching beacons in the environment at known locations.
Many indoor environments are full of wireless base stations, which can help robots localize
through the analysis of the wireless signal. Underwater, active sonar beacons can provide a
senseoflocation, usingsoundtoinformAUVsoftheirrelativedistances tothosebeacons.
PROPRIOCEPTIVE Thethirdimportantclassisproprioceptive sensors,whichinformtherobotofitsown
SENSOR
motion. To measure the exact configuration of a robotic joint, motors are often equipped
withshaftdecodersthatcounttherevolution ofmotorsinsmallincrements. Onrobotarms,
SHAFTDECODER
shaft decoders can provide accurate information overany period of time. Onmobile robots,
shaftdecoders thatreportwheelrevolutions canbeusedfor odometry—themeasurementof
ODOMETRY
distance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only
over short distances. External forces, such as the current for AUVs and the wind for UAVs,
increase positional uncertainty. Inertial sensors, such as gyroscopes, rely on the resistance
INERTIALSENSOR
ofmasstothechangeofvelocity. Theycanhelpreduceuncertainty.
Otherimportant aspects of robot state are measured by force sensors and torque sen-
FORCESENSOR
sors. Theseareindispensablewhenrobotshandlefragileobjectsorobjectswhoseexactshape
TORQUESENSOR
and location is unknown. Imagine aone-ton robotic manipulator screwing in alight bulb. It
would be all too easy to apply too much force and break the bulb. Force sensors allow the
robottosensehowharditisgripping thebulb, andtorquesensors allowittosensehowhard
it is turning. Good sensors can measure forces in all three translational and three rotational
directions. Theydothisatafrequency ofseveralhundred timesasecond, sothatarobotcan
quicklydetectunexpected forcesandcorrect itsactionsbeforeitbreaksalightbulb.
25.2.2 Effectors
Effectors are the means by which robots move and change the shape of their bodies. To
understand thedesign ofeffectors, itwillhelptotalkaboutmotionandshapeintheabstract,
DEGREEOF using the concept of a degree of freedom (DOF)We count one degree of freedom for each
FREEDOM
independentdirectioninwhicharobot,oroneofitseffectors,canmove. Forexample,arigid
mobile robot such as an AUV has six degrees of freedom, three for its (x,y,z) location in
space and three for its angular orientation, known as yaw, roll, and pitch. These six degrees
definethekinematicstate2 orposeoftherobot. Thedynamicstateofarobotincludesthese
KINEMATICSTATE
sixplusanadditionalsixdimensionsfortherateofchangeofeachkinematicdimension,that
POSE
is,theirvelocities.
DYNAMICSTATE
Fornonrigidbodies,thereareadditional degreesoffreedomwithintherobotitself. For
example, the elbow of a human arm possesses two degree of freedom. It can flex the upper
arm towards oraway, and can rotate right orleft. Thewrist has three degrees offreedom. It
can move up and down, side to side, and can also rotate. Robot joints also have one, two,
or three degrees of freedom each. Six degrees of freedom are required to place an object,
such as a hand, at a particular point in a particular orientation. The arm in Figure 25.4(a)
2 “Kinematic”isfromtheGreekwordformotion,asis“cinema.”
976 Chapter 25. Robotics
has exactly six degrees of freedom, created by five revolute joints that generate rotational
REVOLUTEJOINT
motionandoneprismaticjointthatgeneratesslidingmotion. Youcanverifythatthehuman
PRISMATICJOINT
armasawholehasmorethansixdegreesoffreedom byasimpleexperiment: putyourhand
onthetableandnoticethatyoustillhavethefreedom torotateyourelbowwithoutchanging
theconfigurationofyourhand. Manipulators thathaveextra degreesoffreedomareeasierto
control than robots with only the minimum number of DOFs. Many industrial manipulators
therefore havesevenDOFs,notsix.
P
R R
R R
θ
R
(x, y)
(a) (b)
Figure25.4 (a)TheStanfordManipulator,anearlyrobotarmwithfiverevolutejoints(R)
andoneprismaticjoint(P),foratotalofsix degreesoffreedom. (b)Motionofa nonholo-
nomicfour-wheeledvehiclewithfront-wheelsteering.
Formobilerobots,theDOFsarenotnecessarilythesameasthenumberofactuatedele-
ments. Consider,forexample,youraveragecar: itcanmoveforwardorbackward,anditcan
turn, giving it two DOFs. In contrast, a car’s kinematic configuration is three-dimensional:
onanopenflatsurface, onecaneasily maneuveracartoany (x,y)point, inanyorientation.
(SeeFigure 25.4(b).) Thus, the carhasthree effective degrees of freedom but twocontrol-
EFFECTIVEDOF
lable degrees of freedom. We say a robot is nonholonomic if it has more effective DOFs
CONTROLLABLEDOF
than controllable DOFsand holonomic if the two numbers are the same. Holonomic robots
NONHOLONOMIC
areeasiertocontrol—itwouldbemucheasiertoparkacarthatcouldmovesidewaysaswell
asforwardandbackward—but holonomicrobotsarealsomechanically morecomplex. Most
robotarmsareholonomic, andmostmobilerobotsarenonholonomic.
Mobile robots have a range of mechanisms for locomotion, including wheels, tracks,
and legs. Differential drive robots possess two independently actuated wheels (or tracks),
DIFFERENTIALDRIVE
one on each side, as on a military tank. If both wheels move at the same velocity, the robot
movesonastraight line. Iftheymoveinopposite directions, therobot turnsonthespot. An
alternativeisthesynchrodrive,inwhicheachwheelcanmoveandturnarounditsownaxis.
SYNCHRODRIVE
To avoid chaos, the wheels are tightly coordinated. When moving straight, for example, all
wheelspointinthesamedirectionandmoveatthesamespeed. Bothdifferentialandsynchro
drives are nonholonomic. Some more expensive robots use holonomic drives, which have
threeormorewheelsthatcanbeoriented andmovedindependently.
Some mobile robots possess arms. Figure 25.5(a) displays a two-armed robot. This
robot’s arms use springs to compensate for gravity, and they provide minimal resistance to
Section25.2. RobotHardware 977
(a) (b)
Figure 25.5 (a) Mobile manipulatorpluggingits charge cable into a wall outlet. Image
courtesyofWillowGarage,(cid:2)c 2009.(b)OneofMarcRaibert’sleggedrobotsinmotion.
external forces. Such a design minimizes the physical danger to people who might stumble
intosucharobot. Thisisakeyconsideration indeploying robotsindomesticenvironments.
Legs, unlike wheels, can handle rough terrain. However, legs are notoriously slow on
flatsurfaces, andtheyaremechanically difficulttobuild. Roboticsresearchers havetriedde-
signsrangingfromoneleguptodozensoflegs. Leggedrobots havebeenmadetowalk,run,
andevenhop—asweseewiththeleggedrobotinFigure25.5(b). Thisrobotisdynamically
DYNAMICALLY stable, meaning that it can remain upright while hopping around. A robot that can remain
STABLE
upright without moving its legs is called statically stable. A robot is statically stable if its
STATICALLYSTABLE
centerofgravityisabovethepolygonspannedbyitslegs. Thequadruped(four-legged)robot
shown in Figure 25.6(a) may appear statically stable. However, it walks by lifting multiple
legs atthesame time, which renders itdynamically stable. Therobot can walkon snow and
ice, and it will not fall over even if you kick it (as demonstrated in videos available online).
Two-leggedrobotssuchasthoseinFigure25.6(b)aredynamically stable.
Other methods of movement are possible: air vehicles use propellers or turbines; un-
derwater vehicles use propellers or thrusters, similar to those used on submarines. Robotic
blimpsrelyonthermaleffectstokeepthemselvesaloft.
Sensorsandeffectorsalonedonotmakearobot. Acomplete robotalsoneedsasource
of power to drive its effectors. The electric motor is the most popular mechanism for both
ELECTRICMOTOR
PNEUMATIC manipulator actuation and locomotion, but pneumatic actuation using compressed gas and
ACTUATION
HYDRAULIC hydraulicactuation usingpressurized fluidsalsohavetheirapplication niches.
ACTUATION
978 Chapter 25. Robotics
(a) (b)
Figure25.6 (a)Four-leggeddynamically-stablerobot“BigDog.” ImagecourtesyBoston
Dynamics,(cid:2)c 2009.(b)2009RoboCupStandardPlatformLeaguecompetition,showingthe
winningteam,B-Human,fromtheDFKIcenterattheUniversityofBremen.Throughoutthe
match, B-Human outscored their opponents64:1. Their success was built on probabilistic
stateestimationusingparticlefiltersandKalmanfilters;onmachine-learningmodelsforgait
optimization;andondynamickickingmoves.ImagecourtesyDFKI,(cid:2)c 2009.
25.3 ROBOTIC PERCEPTION
Perceptionistheprocessbywhichrobotsmapsensormeasurementsintointernalrepresenta-
tions of the environment. Perception is difficult because sensors are noisy, and the environ-
ment is partially observable, unpredictable, and often dynamic. In other words, robots have
all the problems of state estimation (or filtering) that we discussed in Section 15.2. As a
rule of thumb, good internal representations for robots have three properties: they contain
enoughinformationfortherobottomakegooddecisions, theyarestructuredsothattheycan
be updated efficiently, and they are natural in the sense that internal variables correspond to
naturalstatevariablesinthephysical world.
InChapter15, wesawthat Kalmanfilters, HMMs,anddynamic Bayesnets canrepre-
sentthetransitionandsensormodelsofapartiallyobservableenvironment, andwedescribed
bothexactandapproximate algorithms forupdating the beliefstate—the posteriorprobabil-
ity distribution over the environment state variables. Several dynamic Bayes net models for
this process were shown in Chapter 15. For robotics problems, we include the robot’s own
past actions as observed variables in the model. Figure 25.7 shows the notation used in this
chapter: X isthestateoftheenvironment(includingtherobot)attimet,Z istheobservation
t t
receivedattimet,andA istheactiontakenaftertheobservation isreceived.
t
Section25.3. RoboticPerception 979
A t−2 A t−1 A t
X t−1 X t X t+1
Z t−1 Z t Z t+1
Figure 25.7 Robot perception can be viewed as temporal inference from sequences of
actionsandmeasurements,asillustratedbythisdynamicBayesnetwork.
Wewould liketocompute thenew belief state, P(X | z ,a ), fromthecurrent
t+1 1:t+1 1:t
belief state P(X t | z 1:t ,a 1:t−1 ) and the new observation z t+1 . We did this in Section 15.2,
but here there are two differences: we condition explicitly on the actions as well as the ob-
servations, and wedeal with continuous rather than discrete variables. Thus, we modify the
recursivefilteringequation (15.5onpage572)touseintegration ratherthansummation:
P(X |z ,a )
t+1 1:t+1 1:t (cid:26)
= αP(z t+1 | X t+1 ) P(X t+1 |x t ,a t ) P(x t |z 1:t ,a 1:t−1 ) dx t . (25.1)
This equation states that the posterior over the state variables X at time t +1 is calculated
recursively from the corresponding estimate one time step earlier. This calculation involves
the previous action a and the current sensor measurement z . For example, if our goal
t t+1
is to develop a soccer-playing robot, X might be the location of the soccer ball relative
t+1
to the robot. The posterior P(X t |z 1:t ,a 1:t−1 ) isa probability distribution overall states that
captureswhatweknowfrompastsensormeasurementsandcontrols. Equation(25.1)tellsus
how to recursively estimate this location, by incrementally folding in sensor measurements
(e.g.,cameraimages)androbotmotioncommands. TheprobabilityP(X |x ,a )iscalled
t+1 t t
thetransitionmodelormotionmodel,andP(z | X )isthesensormodel.
MOTIONMODEL t+1 t+1
25.3.1 Localizationand mapping
Localization is the problem of finding out where things are—including the robot itself.
LOCALIZATION
Knowledge about where things are is at the core of any successful physical interaction with
the environment. For example, robot manipulators must know the location of objects they
seektomanipulate; navigating robotsmustknowwheretheyaretofindtheirwayaround.
To keep things simple, let us consider a mobile robot that moves slowly in a flat 2D
world. Letusalso assume therobot isgiven anexact mapofthe environment. (Anexample
of such a map appears in Figure 25.10.) The pose of such a mobile robot is defined by its
two Cartesian coordinates with values x and y and its heading with value θ, as illustrated in
Figure25.8(a). Ifwearrange those threevalues inavector, thenanyparticular stateisgiven
(cid:12)
byX =(x ,y ,θ ) . Sofarsogood.
t t t t
980 Chapter 25. Robotics
ω Δt
t
x y
i, i
θ
t+1
h(x) v Δt
t t
Z Z Z Z
x 1 2 3 4
t+1
θ
t
x
t
(a) (b)
Figure25.8 (a)Asimplifiedkinematicmodelofamobilerobot. Therobotisshownasa
circlewithaninteriorlinemarkingtheforwarddirection. Thestatextconsistsofthe(xt,yt)
position (shown implicitly) and the orientation θt. The new state xt+1 is obtained by an
updateinpositionofvtΔt andinorientationofωtΔt. Alsoshownisalandmarkat(xi,yi)
observedattimet. (b)Therange-scansensormodel.Twopossiblerobotposesareshownfor
agivenrangescan(z ,z ,z ,z ). Itismuchmorelikelythattheposeontheleftgenerated
1 2 3 4
therangescanthantheposeontheright.
In the kinematic approximation, each action consists of the “instantaneous” specifica-
tionoftwovelocities—a translational velocity v andarotationalvelocity ω . Forsmalltime
t t
intervals Δt,acrudedeterministic modelofthemotionofsuchrobotsisgivenby
⎛ ⎞
v Δtcosθ
t t
Xˆ = f(X ,v ,ω )= X + ⎝ v Δtsinθ ⎠ .
t+1 t (cid:27)t(cid:28)(cid:29)(cid:30)t t t t
ω Δt
at t
The notation
Xˆ
refers to a deterministic state prediction. Of course, physical robots are
somewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean
f(X ,v ,ω )andcovariance Σ . (SeeAppendixAforamathematicaldefinition.)
t t t x
P(X |X ,v ,ω ) = N(Xˆ ,Σ ).
t+1 t t t t+1 x
Thisprobability distribution istherobot’s motionmodel. Itmodels theeffects ofthemotion
a onthelocation oftherobot.
t
Next, we need a sensor model. We will consider two kinds of sensor model. The
first assumes that the sensors detect stable, recognizable features of the environment called
landmarks. Foreachlandmark,therangeandbearingarereported. Supposetherobot’sstate
LANDMARK
(cid:12) (cid:12)
isx =(x ,y ,θ ) anditsensesalandmarkwhoselocationisknowntobe(x ,y ) . Without
t t t t i i
noise,therangeandbearingcanbecalculatedbysimplegeometry. (SeeFigure25.8(a).) The
exactprediction oftheobservedrangeandbearingwouldbe
(cid:13) (cid:10) (cid:14)
(x −x )2+(y −y )2
ˆz t =h(x t ) = a t rctan i yi −yt − t θ i .
xi −xt t
Section25.3. RoboticPerception 981
Again, noise distorts ourmeasurements. Tokeep things simple, onemight assume Gaussian
noisewithcovariance Σ ,givingusthesensormodel
z
P(z |x ) = N(ˆz ,Σ ).
t t t z
A somewhat different sensor model is used for an array of range sensors, each of which
has a fixed bearing relative to the robot. Such sensors produce a vector of range values
(cid:12)
z = (z ,...,z ) . Given apose x , let zˆ be the exact range along the jth beam direction
t 1 M t j
fromx tothenearestobstacle. Asbefore,thiswillbecorruptedbyGaussiannoise. Typically,
t
we assume that the errors for the different beam directions are independent and identically
distributed, sowehave
(cid:25)M
P(z |x ) = α e −(zj −zˆj)/2σ2 .
t t
j=1
Figure 25.8(b) shows an example of a four-beam range scan and two possible robot poses,
oneofwhichisreasonablylikelytohaveproducedtheobservedscanandoneofwhichisnot.
Comparing the range-scan model to the landmark model, we see that the range-scan model
has the advantage that there is no need to identify a landmark before the range scan can be
interpreted; indeed, in Figure 25.8(b), the robot faces a featureless wall. On the other hand,
ifthere arevisible, identifiable landmarks, theymayprovideinstant localization.
Chapter 15 described the Kalman filter, which represents the belief state as a single
multivariate Gaussian, andtheparticle filter,whichrepresents thebeliefstatebyacollection
of particles that correspond to states. Most modern localization algorithms use one of two
representations oftherobot’s belief P(X t |z 1:t ,a 1:t−1 ).
MONTECARLO Localization using particle filtering is called Monte Carlo localization, or MCL. The
LOCALIZATION
MCLalfgorithm isaninstance oftheparticle-filtering algorithm ofFigure15.17(page598).
All we need to do is supply the appropriate motion model and sensor model. Figure 25.9
showsoneversionusingtherange-scanmodel. Theoperationofthealgorithmisillustratedin
Figure25.10astherobotfindsoutwhereitisinsideanofficebuilding. Inthefirstimage,the
particles areuniformly distributed based ontheprior, indicating globaluncertainty aboutthe
robot’s position. In the second image, the first set of measurements arrives and the particles
form clusters in the areas of high posterior belief. In the third, enough measurements are
available topushalltheparticles toasinglelocation.
The Kalman filter is the other major way to localize. A Kalman filter represents the
posteriorP(X t |z 1:t ,a 1:t−1 )byaGaussian. ThemeanofthisGaussianwillbedenotedμ t and
itscovariance Σ . ThemainproblemwithGaussianbeliefsisthattheyareonly closedunder
t
linearmotionmodelsf andlinearmeasurementmodels h. Fornonlinear f orh,theresultof
updating a filteris in general not Gaussian. Thus, localization algorithms using the Kalman
filter linearize the motion and sensor models. Linearization is a local approximation of a
LINEARIZATION
nonlinear function by a linear function. Figure 25.11 illustrates the concept of linearization
fora(one-dimensional) robotmotionmodel. Ontheleft,itdepictsanonlinearmotionmodel
f(x ,a ) (the control a is omitted in this graph since it plays no role in the linearization).
t t t
Ontheright,thisfunctionisapproximatedbyalinearfunctionf˜(x ,a ). Thislinearfunction
t t
is tangent to f at the point μ , the mean of ourstate estimate at time t. Such alinearization
t
982 Chapter 25. Robotics
functionMONTE-CARLO-LOCALIZATION(a,z,N,P(X(cid:5)|X, v, ω),P(z|z∗),m)returns
asetofsamplesforthenexttimestep
inputs:a,robotvelocitiesv andω
z,rangescanz
1
,...,zM
P(X(cid:5)|X, v, ω),motionmodel
P(z|z∗),rangesensornoisemodel
m,2Dmapoftheenvironment
persistent: S,avectorofsamplesofsizeN
localvariables: W,avectorofweightsofsizeN
S(cid:5),atemporaryvectorofparticlesofsizeN
W(cid:5),avectorofweightsofsize N
ifS isemptythen /*initializationphase*/
fori=1toN do
S[i]←samplefromP(X )
0
fori=1toN do /*updatecycle*/
S(cid:5)[i]←samplefromP(X(cid:5)|X =S[i],v,ω)
W(cid:5)[i]←1
forj =1toM do
z∗←RAYCAST(j,X =S(cid:5)[i],m)
W(cid:5)[i]←W(cid:5)[i] · P(zj |z∗)
S←WEIGHTED-SAMPLE-WITH-REPLACEMENT(N,S(cid:5),W(cid:5))
returnS
Figure25.9 AMonteCarlolocalizationalgorithmusingarange-scansensormodelwith
independentnoise.
is called (firstdegree) Taylor expansion. AKalman filterthat linearizes f and h via Taylor
TAYLOREXPANSION
expansion is called an extended Kalman filter (or EKF). Figure 25.12 shows a sequence
of estimates of a robot running an extended Kalman filter localization algorithm. As the
robotmoves,theuncertainty initslocationestimateincreases, asshownbytheerrorellipses.
Its error decreases as it senses the range and bearing to a landmark with known location
and increases again as the robot loses sight of the landmark. EKF algorithms work well if
landmarks are easily identified. Otherwise, the posterior distribution may be multimodal, as
inFigure25.10(b). Theproblem ofneeding toknow theidentity oflandmarks isaninstance
ofthedataassociation problemdiscussed inFigure15.6.
Insomesituations, nomapoftheenvironment isavailable. Thentherobotwillhaveto
acquire amap. Thisis abit ofachicken-and-egg problem: the navigating robot willhave to
determine its location relative to a map it doesn’t quite know, at the same time building this
mapwhileitdoesn’tquiteknowitsactuallocation. Thisproblemisimportantformanyrobot
applications, and it has been studied extensively under the name simultaneous localization
SIMULTANEOUS
andmapping,abbreviated asSLAM.
LOCALIZATIONAND
MAPPING
SLAM problems are solved using many different probabilistic techniques, including
theextended Kalmanfilterdiscussed above. UsingtheEKFisstraightforward: justaugment
Section25.3. RoboticPerception 983
Robot position
(a)
Robot position
(b)
Robot position
(c)
Figure25.10 MonteCarlolocalization,aparticlefilteringalgorithmformobilerobotlo-
calization. (a)Initial,globaluncertainty. (b)Approximatelybimodaluncertaintyafternavi-
gatinginthe(symmetric)corridor.(c)Unimodaluncertaintyafterenteringaroomandfinding
ittobedistinctive.
984 Chapter 25. Robotics
~
f(X, a) = f(μ, a) + F(X − μ)
X X t t t t t t t
t+1 f(X, a) t+1 f(X, a)
t t t t
~
Σ f(μ, a) Σ f(μ, a)
t+1 t t t+1 t t Σ
t+1
μ X μ X
t t t t
Σ Σ
t t
(a) (b)
Figure25.11 One-dimensionalillustrationofalinearizedmotionmodel:(a)Thefunction
f,andtheprojectionofameanμ
t
andacovarianceinterval(basedonΣ t)intotimet+1.
(b)Thelinearizedversionisthetangentoff atμ . Theprojectionofthemeanμ iscorrect.
t t
However,theprojectedcovarianceΣ˜ t+1 differsfromΣ t+1 .
robot
landmark
Figure25.12 ExampleoflocalizationusingtheextendedKalmanfilter. Therobotmoves
ona straightline. As it progresses,its uncertaintyincreasesgradually,asillustrated bythe
errorellipses. Whenitobservesalandmarkwithknownposition,theuncertaintyisreduced.
the state vector to include the locations of the landmarks in the environment. Luckily, the
EKFupdatescalesquadratically, soforsmallmaps(e.g.,afewhundredlandmarks)thecom-
putation is quite feasible. Richer maps are often obtained using graph relaxation methods,
similartotheBayesian network inference techniques discussed inChapter14. Expectation–
maximization isalsousedforSLAM.
25.3.2 Othertypes ofperception
Not all of robot perception is about localization or mapping. Robots also perceive the tem-
perature, odors, acoustic signals, andsoon. Manyofthese quantities canbeestimated using
variants of dynamic Bayes networks. Allthat is required for such estimators are conditional
probability distributions that characterize the evolution ofstate variables overtime, andsen-
sormodelsthatdescribe therelationofmeasurementstostatevariables.
It is also possible to program a robot as a reactive agent, without explicitly reasoning
aboutprobability distributions overstates. Wecoverthat approach inSection25.6.3.
The trend in robotics is clearly towards representations with well-defined semantics.
Section25.3. RoboticPerception 985
(a) (b) (c)
Figure 25.13 Sequence of “drivable surface” classifier results using adaptive vision. In
(a)onlytheroadisclassifiedasdrivable(stripedarea). TheV-shapeddarklineshowswhere
thevehicleisheading. In(b)thevehicleiscommandedtodriveofftheroad,ontoa grassy
surface, and the classifier is beginningto classify some of the grass as drivable. In (c) the
vehiclehasupdateditsmodelofdrivablesurfacetocorrespondtograssaswellasroad.
Probabilistictechniquesoutperformotherapproachesinmanyhardperceptualproblemssuch
aslocalizationandmapping. However,statisticaltechniquesaresometimestoocumbersome,
and simplersolutions maybejust as effective inpractice. Tohelp decide whichapproach to
take,experience workingwithrealphysical robotsisyourbestteacher.
25.3.3 Machinelearning inrobot perception
Machine learning plays an important role in robot perception. This is particularly the case
when the best internal representation is not known. One common approach is to map high-
dimensionalsensorstreamsintolower-dimensionalspacesusingunsupervisedmachinelearn-
LOW-DIMENSIONAL ing methods (see Chapter 18). Such an approach is called low-dimensional embedding.
EMBEDDING
Machine learning makes it possible to learn sensor and motion models from data, while si-
multaneously discovering asuitableinternal representations.
Another machine learning technique enables robots to continuously adapt to broad
changes in sensor measurements. Picture yourself walking from a sun-lit space into a dark
neon-litroom. Clearlythingsaredarkerinside. Butthechangeoflightsourcealsoaffectsall
the colors: Neon light has a stronger component of green light than sunlight. Yet somehow
weseem not to notice the change. If wewalk together with people into a neon-lit room, we
don’t think that suddenly theirfaces turned green. Ourperception quickly adapts tothe new
lighting conditions, andourbrainignores thedifferences.
Adaptive perception techniques enable robots to adjust to such changes. One example
is shown in Figure 25.13, taken from the autonomous driving domain. Here an unmanned
ground vehicle adapts its classifier of the concept “drivable surface.” How does this work?
The robot uses a laser to provide classification for a small area right in front of the robot.
When this area is found to be flat in the laser range scan, it is used as a positive training
examplefortheconcept “drivable surface.” Amixture-of-Gaussians technique similartothe
EM algorithm discussed in Chapter 20 is then trained to recognize the specific color and
texture coefficients of the small sample patch. The images in Figure 25.13 are the result of
applying thisclassifiertothefullimage.
986 Chapter 25. Robotics
Methods that make robots collect theirowntraining data (with labels!) are called self-
SELF-SUPERVISED supervised. Inthisinstance,therobotusesmachinelearningtoleverageashort-rangesensor
LEARNING
that works wellforterrain classification into asensor that cansee much farther. Thatallows
therobot todrive faster, slowingdownonly whenthesensor modelsaysthere isachange in
theterrainthatneedstobeexaminedmorecarefully bytheshort-range sensors.
25.4 PLANNING TO MOVE
Allof arobot’s deliberations ultimately comedown to deciding how tomoveeffectors. The
POINT-TO-POINT point-to-pointmotionproblemistodelivertherobotoritsendeffectortoadesignatedtarget
MOTION
location. A greater challenge is the compliant motion problem, in which a robot moves
COMPLIANTMOTION
whilebeinginphysicalcontact withanobstacle. Anexampleofcompliantmotionisarobot
manipulatorthatscrewsinalightbulb,orarobotthatpushesaboxacrossatabletop.
We begin by finding a suitable representation in which motion-planning problems can
bedescribed andsolved. Itturnsoutthatthe configuration space—thespaceofrobotstates
defined by location, orientation, and joint angles—is abetterplace towork than the original
3Dspace. Thepathplanningproblem istofindapathfromone configuration toanotherin
PATHPLANNING
configurationspace. Wehavealreadyencounteredvariousversionsofthepath-planningprob-
lem throughout this book; the complication added by robotics is that path planning involves
continuousspaces. Therearetwomainapproaches: celldecompositionandskeletonization.
Each reduces the continuous path-planning problem to a discrete graph-search problem. In
thissection,weassumethatmotionisdeterministicandthatlocalizationoftherobotisexact.
Subsequent sections willrelaxtheseassumptions.
25.4.1 Configurationspace
We will start with a simple representation for a simple robot motion problem. Consider the
robot arm shown in Figure 25.14(a). It has two joints that move independently. Moving
the joints alters the (x,y) coordinates of the elbow and the gripper. (The arm cannot move
in the z direction.) This suggests that the robot’s configuration can be described by a four-
dimensionalcoordinate: (x ,y )forthelocationoftheelbowrelativetotheenvironmentand
e e
(x ,y ) for the location of the gripper. Clearly, these four coordinates characterize the full
g g
WORKSPACE state of the robot. They constitute what is known as workspace representation, since the
REPRESENTATION
coordinates oftherobotarespecifiedinthesamecoordinate system astheobjects itseeksto
manipulate (or to avoid). Workspace representations are well-suited for collision checking,
especially iftherobotandallobjectsarerepresented bysimplepolygonal models.
The problem with the workspace representation is that not all workspace coordinates
areactually attainable, eveninthe absence ofobstacles. Thisisbecause ofthe linkagecon-
LINKAGE straints on the space of attainable workspace coordinates. Forexample, the elbow position
CONSTRAINTS
(x ,y ) and the gripper position (x ,y ) are always a fixed distance apart, because they are
e e g g
joined byarigid forearm. Arobot motion plannerdefined over workspace coordinates faces
the challenge of generating paths that adhere to these constraints. This is particularly tricky
Section25.4. PlanningtoMove 987
table
eelb
table
sshou
ee vertical
obstacle
left wall
ss
(a) (b)
Figure25.14 (a)Workspacerepresentationofarobotarmwith2DOFs. Theworkspace
is a box with a flat obstacle hangingfromthe ceiling. (b)Configurationspace of the same
robot. Onlywhiteregionsinthespaceareconfigurationsthatarefreeofcollisions. Thedot
inthisdiagramcorrespondstotheconfigurationoftherobotshownontheleft.
because thestatespaceiscontinuous andtheconstraints arenonlinear. Itturnsouttobeeas-
CONFIGURATION iertoplanwithaconfigurationspacerepresentation. Insteadofrepresenting thestateofthe
SPACE
robot by the Cartesian coordinates of its elements, we represent the state by a configuration
of the robot’s joints. Our example robot possesses two joints. Hence, we can represent its
state with the two angles ϕ and ϕ for the shoulder joint and elbow joint, respectively. In
s e
theabsenceofanyobstacles,arobotcouldfreelytakeonanyvalueinconfigurationspace. In
particular, whenplanning apath one could simply connect the present configuration and the
target configuration by a straight line. In following this path, the robot would then move its
jointsataconstant velocity, untilatargetlocationisreached.
Unfortunately,configurationspaceshavetheirownproblems. Thetaskofarobotisusu-
ally expressed in workspace coordinates, not in configuration space coordinates. This raises
the question of how tomapbetween workspace coordinates and configuration space. Trans-
forming configuration space coordinates into workspace coordinates is simple: it involves
a series of straightforward coordinate transformations. These transformations are linear for
prismaticjointsandtrigonometricforrevolutejoints. Thischainofcoordinatetransformation
isknownaskinematics.
KINEMATICS
Theinverseproblem ofcalculating theconfiguration ofarobotwhoseeffectorlocation
INVERSE isspecifiedinworkspacecoordinatesisknownasinversekinematics. Calculatingtheinverse
KINEMATICS
kinematicsishard,especiallyforrobotswithmanyDOFs. Inparticular,thesolutionisseldom
unique. Figure 25.14(a) shows one of twopossible configurations that put the gripper in the
samelocation. (Theotherconfiguration wouldhastheelbowbelowtheshoulder.)
988 Chapter 25. Robotics
conf-2
conf-1
conf-3
conf-3
conf-1
conf-2
ee
ss
(a) (b)
Figure25.15 Threerobotconfigurations,showninworkspaceandconfigurationspace.
In general, this two-link robot arm has between zero and two inverse kinematic solu-
tions for any set of workspace coordinates. Most industrial robots have sufficient degrees
of freedom to find infinitely many solutions to motion problems. To see how this is possi-
ble, simply imagine that we added a third revolute joint to our example robot, one whose
rotational axis is parallel to the ones of the existing joints. In such a case, we can keep the
location(butnottheorientation!) ofthegripperfixedandstillfreelyrotateitsinternaljoints,
formostconfigurationsoftherobot. Withafewmorejoints(howmany?) wecanachievethe
same effect while keeping the orientation of the gripper constant as well. We have already
seen an example of this in the “experiment” of placing your hand on the desk and moving
your elbow. The kinematic constraint of your hand position is insufficient to determine the
configuration of your elbow. In other words, the inverse kinematics of your shoulder–arm
assemblypossesses aninfinitenumberofsolutions.
The second problem with configuration space representations arises from the obsta-
cles that may exist in the robot’s workspace. Ourexample in Figure 25.14(a) shows several
suchobstacles, including afree-hanging obstacle thatprotrudes intothecenteroftherobot’s
workspace. In workspace, such obstacles take on simple geometric forms—especially in
most robotics textbooks, which tend to focus on polygonal obstacles. But how do they look
inconfiguration space?
Figure25.14(b)showstheconfigurationspaceforourexamplerobot,underthespecific
obstacleconfigurationshowninFigure25.14(a). Theconfigurationspacecanbedecomposed
into twosubspaces: thespace ofallconfigurations that arobot mayattain, commonly called
free space, and the space of unattainable configurations, called occupied space. The white
FREESPACE
area inFigure 25.14(b) corresponds to the free space. All other regions correspond to occu-
OCCUPIEDSPACE
Section25.4. PlanningtoMove 989
piedspace. Thedifferent shadings oftheoccupied spacecorresponds tothedifferent objects
in the robot’s workspace; the black region surrounding the entire free space corresponds to
configurations inwhich the robot collides with itself. Itis easy tosee that extreme values of
the shoulder or elbow angles cause such a violation. The two oval-shaped regions on both
sidesoftherobotcorrespondtothetableonwhichtherobotismounted. Thethirdovalregion
corresponds tothe leftwall. Finally, themostinteresting object inconfiguration space isthe
verticalobstaclethathangsfromtheceilingandimpedestherobot’smotions. Thisobjecthas
afunnyshapeinconfiguration space: itishighlynonlinearandatplacesevenconcave. With
a little bit of imagination the reader will recognize the shape of the gripper at the upper left
end. We encourage the reader to pause for a moment and study this diagram. The shape of
thisobstacle isnot atallobvious! Thedotinside Figure25.14(b) markstheconfiguration of
the robot, as shown inFigure 25.14(a). Figure 25.15 depicts three additional configurations,
both in workspace and in configuration space. In configuration conf-1, the gripper encloses
theverticalobstacle.
Eveniftherobot’sworkspaceisrepresentedbyflatpolygons,theshapeofthefreespace
can be very complicated. In practice, therefore, one usually probes a configuration space
instead ofconstructing it explicitly. Aplanner may generate aconfiguration and then test to
see ifit isin free space by applying the robot kinematics and then checking forcollisions in
workspace coordinates.
25.4.2 Celldecompositionmethods
CELL The first approach to path planning uses cell decomposition—that is, it decomposes the
DECOMPOSITION
free space into a finite number of contiguous regions, called cells. These regions have the
important property that the path-planning problem within a single region can be solved by
simple means (e.g., movingalong astraight line). Thepath-planning problem then becomes
adiscretegraph-searchproblem,verymuchlikethesearchproblemsintroducedinChapter3.
The simplest cell decomposition consists of a regularly spaced grid. Figure 25.16(a)
shows a square grid decomposition of the space and a solution path that is optimal for this
gridsize. Grayscale shadingindicates the valueofeachfree-space gridcell—i.e.,thecostof
theshortestpathfromthatcelltothegoal. (Thesevaluescanbecomputedbyadeterministic
formoftheVALUE-ITERATION algorithmgiveninFigure17.4onpage653.) Figure25.16(b)
∗
showsthecorresponding workspacetrajectoryforthearm. Ofcourse,wecanalsousetheA
algorithm tofindashortest path.
Such a decomposition has the advantage that it is extremely simple to implement, but
it also suffers from three limitations. First, itis workable only forlow-dimensional configu-
rationspaces,becausethenumberofgridcellsincreasesexponentially withd,thenumberof
dimensions. Sounds familiar? This is the curse!dimensionality@of dimensionality. Second,
thereistheproblemofwhattodowithcellsthatare“mixed”—thatis,neitherentirelywithin
free space norentirely within occupied space. Asolution path that includes such a cell may
not be areal solution, because there may be no wayto cross the cell in the desired direction
inastraight line. Thiswouldmakethepathplanner unsound. Ontheotherhand, ifweinsist
thatonlycompletely freecellsmaybeused, theplannerwillbeincomplete, becauseitmight
990 Chapter 25. Robotics
goal
start
goal
start
(a) (b)
Figure25.16 (a)Valuefunctionandpathfoundforadiscretegridcellapproximationof
theconfigurationspace. (b)Thesamepathvisualizedinworkspacecoordinates.Noticehow
therobotbendsitselbowtoavoidacollisionwiththeverticalobstacle.
bethe casethat theonly paths tothegoal gothrough mixedcells—especially ifthecell size
is comparable to that of the passageways and clearances in the space. And third, any path
through adiscretized statespacewillnotbesmooth. Itisgenerally difficulttoguarantee that
a smooth solution exists near the discrete path. So a robot may not be able to execute the
solution foundthroughthisdecomposition.
Cell decomposition methods can be improved in a number of ways, to alleviate some
oftheseproblems. Thefirstapproachallows furthersubdivision ofthemixedcells—perhaps
using cells of half the original size. This can be continued recursively until a path is found
that lies entirely within free cells. (Of course, the method only works if there is a way to
decideifagivencellisamixedcell,whichiseasyonlyiftheconfigurationspaceboundaries
haverelativelysimplemathematicaldescriptions.) Thismethodiscompleteprovidedthereis
aboundonthesmallestpassagewaythroughwhichasolutionmustpass. Althoughitfocuses
most of the computational effort on the tricky areas within the configuration space, it still
fails to scale well to high-dimensional problems because each recursive splitting of a cell
creates2d smallercells. Asecondwaytoobtainacompletealgorithmistoinsistonanexact
EXACTCELL cell decomposition ofthe free space. Thismethod must allow cells tobe irregularly shaped
DECOMPOSITION
where they meet the boundaries of free space, but the shapes must still be “simple” in the
sense that it should be easy to compute a traversal of any free cell. This technique requires
somequiteadvanced geometricideas,soweshallnotpursueitfurtherhere.
Examining the solution path shown in Figure 25.16(a), we can see an additional diffi-
cultythatwillhavetoberesolved. Thepathcontainsarbitrarilysharpcorners;arobotmoving
at any finite speed could not execute such a path. This problem is solved by storing certain
continuous values foreach grid cell. Consider an algorithm which stores, foreach grid cell,
Section25.4. PlanningtoMove 991
the exact, continuous state that was attained with the cell was first expanded in the search.
Assumefurther, thatwhenpropagating information tonearbygridcells,weusethiscontinu-
ousstateasabasis,andapplythecontinuousrobotmotionmodelforjumpingtonearbycells.
In doing so, we can now guarantee that the resulting trajectory is smooth and can indeed be
executedbytherobot. Onealgorithm thatimplementsthisishybridA*.
HYBRIDA*
25.4.3 Modified costfunctions
NoticethatinFigure25.16, thepathgoesveryclosetotheobstacle. Anyonewhohasdriven
acarknowsthataparkingspacewithonemillimeterofclearanceoneithersideisnotreallya
parking spaceatall;forthesamereason, wewouldprefersolution pathsthatarerobustwith
respecttosmallmotionerrors.
This problem can be solved by introducing a potential field. A potential field is a
POTENTIALFIELD
functiondefinedoverstatespace,whosevaluegrowswiththedistancetotheclosestobstacle.
Figure25.17(a)showssuchapotential field—thedarkeraconfiguration state,thecloseritis
toanobstacle.
Thepotentialfieldcanbeusedasanadditionalcosttermintheshortest-pathcalculation.
Thisinducesaninterestingtradeoff. Ontheonehand,therobotseekstominimizepathlength
tothegoal. Ontheotherhand,ittriestostayawayfromobstaclesbyvirtueofminimizingthe
potentialfunction. Withtheappropriateweightbalancing thetwoobjectives, aresultingpath
maylook liketheone showninFigure 25.17(b). Thisfigurealsodisplays thevalue function
derived from the combined cost function, again calculated by value iteration. Clearly, the
resulting pathislonger, butitisalsosafer.
There exist many other ways to modify the cost function. For example, it may be
desirable to smooth the control parameters over time. For example, when driving a car, a
smooth pathisbetterthan ajerky one. Ingeneral, suchhigher-order constraints arenoteasy
toaccommodate intheplanning process, unless wemakethemostrecent steering command
apartofthestate. However,itisofteneasytosmooththeresulting trajectory afterplanning,
using conjugate gradient methods. Such post-planning smoothing is essential in many real-
worldapplications.
25.4.4 Skeletonization methods
Thesecondmajorfamilyofpath-planningalgorithmsisbasedontheideaofskeletonization.
SKELETONIZATION
Thesealgorithmsreducetherobot’sfreespacetoaone-dimensionalrepresentation, forwhich
theplanning problemiseasier. Thislower-dimensional representation iscalledaskeletonof
theconfiguration space.
Figure 25.18 shows an example skeletonization: it is a Voronoi graph of the free
VORONOIGRAPH
space—the set of all points that are equidistant to two or more obstacles. To do path plan-
ning with a Voronoi graph, the robot first changes its present configuration to a point on the
Voronoi graph. It is easy to show that this can always be achieved by a straight-line motion
inconfiguration space. Second,therobotfollowstheVoronoigraphuntilitreachesthepoint
nearest to the target configuration. Finally, the robot leaves the Voronoi graph and moves to
thetarget. Again,thisfinalstepinvolves straight-line motioninconfiguration space.
992 Chapter 25. Robotics
start goal
(a) (b)
Figure 25.17 (a) A repelling potential field pushes the robot away from obstacles. (b)
Pathfoundbysimultaneouslyminimizingpathlengthandthepotential.
(a) (b)
Figure25.18 (a)TheVoronoigraphisthesetofpointsequidistanttotwoormoreobsta-
clesinconfigurationspace. (b)Aprobabilisticroadmap,composedof400randomlychosen
pointsinfreespace.
In this way, the original path-planning problem is reduced to finding a path on the
Voronoi graph, which is generally one-dimensional (except in certain nongeneric cases) and
hasfinitelymanypointswherethreeormoreone-dimensional curvesintersect. Thus,finding
Section25.5. PlanningUncertainMovements 993
the shortest path along the Voronoi graph is a discrete graph-search problem of the kind
discussed in Chapters 3 and 4. Following the Voronoi graph may not give us the shortest
path, but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph
techniquesarethattheyaredifficulttoapplytohigher-dimensional configurationspaces,and
that they tend to induce unnecessarily large detours when the configuration space is wide
open. Furthermore,computingtheVoronoigraphcanbedifficult,especially inconfiguration
space,wheretheshapesofobstacles canbecomplex.
PROBABILISTIC An alternative to the Voronoi graphs is the probabilistic roadmap, a skeletonization
ROADMAP
approach that offers morepossible routes, andthus deals betterwithwide-open spaces. Fig-
ure25.18(b)showsanexampleofaprobabilisticroadmap. Thegraphiscreatedbyrandomly
generating a large number of configurations, and discarding those that do not fall into free
space. Two nodes are joined by an arc if it is “easy” to reach one node from the other–for
example, by a straight line in free space. The result of all this is a randomized graph in the
robot’s free space. If we add the robot’s start and goal configurations to this graph, path
planning amounts to adiscrete graph search. Theoretically, this approach is incomplete, be-
cause a bad choice of random points may leave us without any paths from start to goal. It
is possible to bound the probability of failure in terms of the number of points generated
and certain geometric properties of the configuration space. It is also possible to direct the
generation of sample points towards the areas where a partial search suggests that a good
path may be found, working bidirectionally from both the start and the goal positions. With
theseimprovements,probabilisticroadmapplanningtendstoscalebettertohigh-dimensional
configuration spacesthanmostalternative path-planning techniques.
25.5 PLANNING UNCERTAIN MOVEMENTS
Noneoftherobotmotion-planning algorithmsdiscussedthusfaraddressesakeycharacteris-
ticofroboticsproblems: uncertainty. Inrobotics,uncertaintyarisesfrompartialobservability
oftheenvironment andfromthestochastic (orunmodeled) effectsoftherobot’s actions. Er-
rorscanalsoarisefrom theuseofapproximation algorithms suchasparticle filtering, which
doesnotprovide therobotwithanexactbelief stateevenifthestochastic nature oftheenvi-
ronmentismodeledperfectly.
Most of today’s robots use deterministic algorithms for decision making, such as the
path-planning algorithms of the previous section. To do so, it is common practice to extract
the most likely state from the probability distribution produced by the state estimation al-
MOSTLIKELYSTATE
gorithm. The advantage of this approach is purely computational. Planning paths through
configuration space is already a challenging problem; it would be worse if we had to work
with a full probability distribution overstates. Ignoring uncertainty in this way works when
theuncertainty issmall. Infact,whentheenvironment modelchangesovertimeastheresult
ofincorporating sensormeasurements, manyrobotsplanpathsonlineduring planexecution.
Thisistheonlinereplanningtechnique ofSection11.3.3.
ONLINEREPLANNING
994 Chapter 25. Robotics
Unfortunately, ignoring the uncertainty does not always work. In some problems the
robot’s uncertainty is simply too massive: How can we use a deterministic path planner to
control amobile robot that has no clue where itis? Ingeneral, if the robot’s true state is not
the one identified by the maximum likelihood rule, the resulting control will be suboptimal.
Depending on the magnitude of the error this can lead to all sorts of unwanted effects, such
ascollisions withobstacles.
Thefieldofroboticshasadoptedarangeoftechniquesforaccommodatinguncertainty.
Somearederived from thealgorithms given inChapter17fordecision making underuncer-
tainty. Iftherobotfacesuncertaintyonlyinitsstatetransition,butitsstateisfullyobservable,
theproblemisbestmodeledasaMarkovdecisionprocess(MDP).ThesolutionofanMDPis
anoptimalpolicy,whichtellstherobotwhattodoineverypossible state. Inthisway,itcan
handleallsortsofmotionerrors,whereasasingle-path solution fromadeterministic planner
NAVIGATION would be muchless robust. Inrobotics, policies arecalled navigation functions. Thevalue
FUNCTION
function shown in Figure 25.16(a) can be converted into such a navigation function simply
byfollowingthegradient.
JustasinChapter17,partialobservability makestheproblemmuchharder. Theresult-
ing robot control problem is a partially observable MDP,or POMDP.In such situations, the
robot maintains aninternal beliefstate, liketheonesdiscussed inSection25.3. Thesolution
to a POMDP is a policy defined over the robot’s belief state. Put differently, the input to
thepolicyisanentire probability distribution. Thisenables therobottobase itsdecision not
only on what it knows, but also on what it does not know. For example, if it is uncertain
INFORMATION aboutacriticalstatevariable,itcanrationallyinvokeaninformationgatheringaction. This
GATHERINGACTION
is impossible inthe MDPframework, since MDPsassume full observability. Unfortunately,
techniquesthatsolvePOMDPsexactlyareinapplicabletorobotics—therearenoknowntech-
niquesforhigh-dimensionalcontinuousspaces. DiscretizationproducesPOMDPsthatarefar
too large to handle. Oneremedy isto makethe minimization of uncertainty acontrol objec-
COASTAL tive. For example, the coastal navigation heuristic requires the robot to stay near known
NAVIGATION
landmarks to decrease its uncertainty. Another approach applies variants of the probabilis-
tic roadmap planning method to the belief space representation. Such methods tend to scale
bettertolargediscretePOMDPs.
25.5.1 Robustmethods
Uncertaintycanalsobehandledusingso-calledrobustcontrolmethods(seepage836)rather
ROBUSTCONTROL
than probabilistic methods. A robust method is one that assumes a bounded amount of un-
certainty in each aspect of a problem, but does not assign probabilities to values within the
allowed interval. A robust solution is one that works no matter what actual values occur,
providedtheyarewithintheassumedinterval. Anextremeformofrobustmethodisthecon-
formant planningapproach given in Chapter11—it produces plans that work with nostate
information atall.
FINE-MOTION Here, we look at a robust method that is used for fine-motion planning (or FMP) in
PLANNING
robotic assembly tasks. Fine-motion planning involves moving a robot arm in very close
proximity to a static environment object. The main difficulty with fine-motion planning is
Section25.5. PlanningUncertainMovements 995
initial Cv
configuration
motion v
envelope
Figure25.19 Atwo-dimensionalenvironment,velocityuncertaintycone,andenvelopeof
possiblerobotmotions. Theintendedvelocityis v, butwithuncertaintytheactualvelocity
couldbeanywhereinCv,resultinginafinalconfigurationsomewhereinthemotionenvelope,
whichmeanswewouldn’tknowifwehittheholeornot.
initial
Cv
configuration
v
motion
envelope
Figure25.20 Thefirstmotioncommandandtheresultingenvelopeofpossiblerobotmo-
tions. No matter what the error, we know the final configuration will be to the left of the
hole.
thattherequiredmotionsandtherelevantfeaturesoftheenvironmentareverysmall. Atsuch
smallscales,therobotisunabletomeasureorcontrolitspositionaccuratelyandmayalsobe
uncertain of the shape of the environment itself; we will assume that these uncertainties are
all bounded. The solutions to FMP problems will typically be conditional plans or policies
thatmakeuseofsensorfeedbackduringexecutionandareguaranteedtoworkinallsituations
consistent withtheassumeduncertainty bounds.
A fine-motion plan consists of a series of guarded motions. Each guarded motion
GUARDEDMOTION
consistsof(1)amotioncommandand(2)aterminationcondition,whichisapredicateonthe
robot’s sensor values, and returns true to indicate the end of the guarded move. The motion
commands are typically compliant motions that allow the effector to slide if the motion
COMPLIANTMOTION
commandwouldcausecollisionwithanobstacle. Asanexample,Figure25.19showsatwo-
dimensional configuration space with a narrow vertical hole. It could be the configuration
space forinsertion ofarectangular pegintoaholeoracarkeyintotheignition. Themotion
commandsareconstant velocities. Thetermination conditions arecontact withasurface. To
modeluncertainty incontrol, weassumethatinsteadofmovinginthecommandeddirection,
the robot’s actual motion lies in the cone C about it. The figure shows what would happen
v
996 Chapter 25. Robotics
Cv
v
motion
envelope
Figure25.21 Thesecondmotioncommandandtheenvelopeofpossiblemotions. Even
witherror,wewilleventuallygetintothehole.
if we commanded a velocity straight down from the initial configuration. Because of the
uncertainty in velocity, the robot could move anywhere in the conical envelope, possibly
going into the hole, but more likely landing to one side of it. Because the robot would not
thenknowwhichsideoftheholeitwason,itwouldnotknowwhichwaytomove.
A more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20, the
robotdeliberatelymovestoonesideofthehole. Themotioncommandisshowninthefigure,
and the termination test is contact with any surface. In Figure 25.21, a motion command is
given that causes the robot to slide along the surface and into the hole. Because all possible
velocities inthemotionenvelope aretotheright, therobot willslidetotherightwheneverit
is in contact with a horizontal surface. It will slide down the right-hand vertical edge of the
holewhenittouches it,becauseallpossiblevelocities aredownrelativetoaverticalsurface.
It will keep moving until it reaches the bottom of the hole, because that is its termination
condition. In spite of the control uncertainty, all possible trajectories of the robot terminate
incontactwiththebottomofthehole—thatis,unlesssurfaceirregularitiescausetherobotto
stickinoneplace.
As one might imagine, the problem of constructing fine-motion plans is not trivial; in
fact, it is a good deal harder than planning with exact motions. One can either choose a
fixed number of discrete values foreach motion oruse the environment geometry to choose
directions that givequalitatively different behavior. Afine-motion planner takes asinput the
configuration-spacedescription,theangleofthevelocityuncertaintycone,andaspecification
ofwhatsensing ispossible fortermination (surface contact inthiscase). Itshould produce a
multistepconditional planorpolicythatisguaranteed tosucceed, ifsuchaplanexists.
Ourexample assumes that the planner has an exact model ofthe environment, but itis
possible to allow forbounded errorinthis modelas follows. Iftheerror canbedescribed in
termsofparameters,thoseparameterscanbeaddedasdegreesoffreedomtotheconfiguration
space. In the last example, if the depth and width of the hole were uncertain, we could add
them as two degrees of freedom to the configuration space. It is impossible to move the
robot in these directions in the configuration space or to sense its position directly. But
boththoserestrictionscanbeincorporated whendescribingthisproblemasanFMPproblem
by appropriately specifying control and sensor uncertainties. This gives a complex, four-
dimensional planning problem, but exactly the same planning techniques can be applied.
Section25.6. Moving 997
Noticethatunlikethedecision-theoretic methodsinChapter17,thiskindofrobustapproach
results in plans designed for the worst-case outcome, rather than maximizing the expected
qualityoftheplan. Worst-caseplansareoptimalinthedecision-theoretic senseonlyiffailure
duringexecution ismuchworsethananyoftheothercostsinvolved inexecution.
25.6 MOVING
So far, we have talked about how to plan motions, but not about how to move. Ourplans—
particularlythoseproducedbydeterministicpathplanners—assumethattherobotcansimply
followanypaththatthealgorithm produces. Intherealworld,ofcourse, thisisnotthecase.
Robots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In
mostcases,therobotgetstoexertforcesratherthanspecifypositions. Thissectiondiscusses
methodsforcalculating theseforces.
25.6.1 Dynamicsand control
Section 25.2introduced thenotion of dynamicstate, whichextends thekinematic state ofa
robotbyitsvelocity. Forexample,inaddition totheangleofarobotjoint, thedynamicstate
also captures the rate of change of the angle, and possibly even its momentary acceleration.
The transition model for a dynamic state representation includes the effect of forces on this
DIFFERENTIAL rate of change. Such models are typically expressed via differential equations, which are
EQUATION
equations that relate a quantity (e.g., a kinematic state) to the change of the quantity over
time(e.g.,velocity). Inprinciple, wecould havechosen toplanrobot motionusing dynamic
models, instead of ourkinematic models. Such a methodology would lead to superior robot
performance, if wecould generate the plans. However, the dynamic state has higher dimen-
sion than the kinematic space, and the curse of dimensionality would render many motion
planningalgorithmsinapplicable forallbutthemostsimplerobots. Forthisreason,practical
robotsystemoftenrelyonsimplerkinematicpathplanners.
A common technique to compensate for the limitations of kinematic plans is to use a
separate mechanism, acontroller, forkeeping therobotontrack. Controllers aretechniques
CONTROLLER
for generating robot controls in real time using feedback from the environment, so as to
achieve a control objective. If the objective is to keep the robot on a preplanned path, it is
REFERENCE oftenreferredtoasareferencecontrollerandthepathiscalledareferencepath. Controllers
CONTROLLER
that optimize aglobal cost function are known as optimal controllers. Optimal policies for
REFERENCEPATH
OPTIMAL continuous MDPsare,ineffect,optimalcontrollers.
CONTROLLERS
On the surface, the problem of keeping a robot on a prespecified path appears to be
relatively straightforward. In practice, however, even this seemingly simple problem has its
pitfalls. Figure 25.22(a) illustrates what can go wrong; it shows the path of a robot that
attempts to follow a kinematic path. Whenever a deviation occurs—whether due to noise or
toconstraints ontheforcestherobotcanapply—therobotprovidesanopposingforcewhose
magnitude is proportional to this deviation. Intuitively, this might appear plausible, since
deviations should be compensated by a counterforce to keep the robot on track. However,
998 Chapter 25. Robotics
(a) (b) (c)
Figure 25.22 Robot arm control using (a) proportionalcontrol with gain factor 1.0, (b)
proportionalcontrolwith gain factor 0.1, and (c) PD (proportionalderivative)controlwith
gainfactors0.3fortheproportionalcomponentand0.8forthedifferentialcomponent.Inall
casestherobotarmtriestofollowthepathshowningray.
as Figure 25.22(a) illustrates, our controller causes the robot to vibrate rather violently. The
vibration is the result of a natural inertia of the robot arm: once driven back to its reference
positiontherobotthenovershoots,whichinducesasymmetricerrorwithoppositesign. Such
overshooting may continue along an entire trajectory, and the resulting robot motion is far
fromdesirable.
Before we can define a better controller, let us formally describe what went wrong.
Controllers that provide force in negative proportion to the observed error are known as P
controllers. The letter ‘P’ stands for proportional, indicating that the actual control is pro-
PCONTROLLER
portional totheerroroftherobotmanipulator. Moreformally, lety(t)bethereference path,
parameterized bytimeindex t. Thecontrol a generated byaPcontrollerhastheform:
t
a = K (y(t)−x ).
t P t
Herex isthestateoftherobotattimetandK isaconstantknownasthegainparameterof
GAINPARAMETER t P
thecontrolleranditsvalueiscalledthegainfactor); K regulateshowstronglythecontroller
p
corrects fordeviations between the actual state x and the desired one y(t). Inour example,
t
K = 1. At first glance, one might think that choosing a smaller value for K would
P P
remedy the problem. Unfortunately, this is not the case. Figure 25.22(b) shows a trajectory
for K = .1, still exhibiting oscillatory behavior. Lower values of the gain parameter may
P
simply slow down the oscillation, but do not solve the problem. In fact, in the absence of
friction, the P controller is essentially a spring law; so it will oscillate indefinitely around a
fixedtargetlocation.
Traditionally, problems of this type fall into the realm of control theory, a field of
increasingimportancetoresearchersinAI.Decadesofresearchinthisfieldhaveledtoalarge
numberofcontrollers thataresuperiortothesimplecontrollawgivenabove. Inparticular, a
referencecontrollerissaidtobestableifsmallperturbationsleadtoaboundederrorbetween
STABLE
the robot and the reference signal. It is said to be strictly stable if it is able to return to and
STRICTLYSTABLE
Section25.6. Moving 999
thenstayonitsreference pathuponsuchperturbations. Our Pcontrollerappears tobestable
butnotstrictlystable, sinceitfailstostayanywherenear itsreference trajectory.
The simplest controller that achieves strict stability in our domain is a PD controller.
PDCONTROLLER
Theletter‘P’standsagainforproportional, and‘D’standsforderivative. PDcontrollers are
described bythefollowingequation:
∂(y(t)−x )
a = K (y(t)−x )+K t . (25.2)
t P t D
∂t
As this equation suggests, PD controllers extend P controllers by a differential component,
which adds to the value of a a term that is proportional to the first derivative of the error
t
y(t)−x overtime. Whatistheeffectofsuchaterm? Ingeneral, aderivativetermdampens
t
thesystemthatisbeingcontrolled. Toseethis,considerasituationwheretheerror(y(t)−x )
t
ischanging rapidlyovertime,asisthecaseforourPcontroller above. Thederivativeofthis
error will then counteract the proportional term, which will reduce the overall response to
theperturbation. However,ifthesameerrorpersists anddoesnotchange, thederivativewill
vanishandtheproportional termdominates thechoiceofcontrol.
Figure25.22(c) showstheresult ofapplying thisPDcontroller toourrobot arm, using
asgainparameters K = .3andK = .8. Clearly,theresulting pathismuchsmoother, and
P D
doesnotexhibitanyobviousoscillations.
PD controllers do have failure modes, however. In particular, PD controllers may fail
to regulate an error down to zero, even in the absence of external perturbations. Often such
a situation is the result of a systematic external force that is not part of the model. An au-
tonomouscardrivingonabanked surface, forexample, mayfinditselfsystematically pulled
to one side. Wearand tear in robot arms cause similar systematic errors. In such situations,
anover-proportionalfeedbackisrequiredtodrivetheerrorclosertozero. Thesolutiontothis
problemliesinaddingathirdtermtothecontrollaw,basedontheintegratederrorovertime:
(cid:26)
∂(y(t)−x )
a = K (y(t)−x )+K (y(t)−x )dt+K t . (25.3)
t P t I t D
∂t
+
HereK isyetanothergainparameter. Theterm (y(t)−x )dtcalculatestheintegralofthe
I t
error over time. The effect of this term is that long-lasting deviations between the reference
signal and the actual state are corrected. If, for example, x is smaller than y(t) for a long
t
periodoftime,thisintegralwillgrowuntiltheresulting control a forcesthiserrortoshrink.
t
Integral terms, then, ensure thatacontroller doesnotexhibitsystematic error, attheexpense
ofincreased danger ofoscillatory behavior. Acontroller withallthree terms iscalled a PID
controller (forproportional integral derivative). PIDcontrollers arewidelyused inindustry,
PIDCONTROLLER
foravarietyofcontrolproblems.
25.6.2 Potential-fieldcontrol
We introduced potential fields as an additional cost function in robot motion planning, but
theycanalsobeusedforgenerating robotmotiondirectly, dispensing withthepathplanning
phase altogether. To achieve this, we have to define an attractive force that pulls the robot
towardsitsgoalconfiguration andarepellent potential fieldthat pushes therobot awayfrom
obstacles. Such a potential field is shown in Figure 25.23. Its single global minimum is
1000 Chapter 25. Robotics
goal start goal
start
(a) (b)
Figure 25.23 Potential field control. The robot ascends a potential field composed of
repelling forces asserted from the obstacles and an attracting force that correspondsto the
goalconfiguration.(a)Successfulpath. (b)Localoptimum.
the goal configuration, and the value is the sum of the distance to this goal configuration
and the proximity to obstacles. No planning was involved in generating the potential field
shown in the figure. Because of this, potential fields are well suited to real-time control.
Figure 25.23(a) shows a trajectory of a robot that performs hill climbing in the potential
field. In many applications, the potential field can be calculated efficiently for any given
configuration. Moreover, optimizing the potential amounts to calculating the gradient of the
potential for the present robot configuration. These calculations can be extremely efficient,
especially when compared to path-planning algorithms, all of which are exponential in the
dimensionality oftheconfiguration space(theDOFs)intheworstcase.
The fact that the potential field approach manages to find a path to the goal in such
an efficient manner, even over long distances in configuration space, raises the question as
to whether there is a need for planning in robotics at all. Are potential field techniques
sufficient, or were we just lucky in our example? The answer is that we were indeed lucky.
Potentialfieldshavemanylocalminimathatcantraptherobot. InFigure25.23(b),therobot
approaches theobstacle bysimplyrotating itsshoulderjoint, untilitgetsstuckonthewrong
side of the obstacle. The potential field is not rich enough to make the robot bend its elbow
sothatthearmfitsundertheobstacle. Inotherwords,potentialfieldcontrolisgreatforlocal
robotmotionbutsometimeswestillneedglobalplanning. Anotherimportantdrawbackwith
potentialfieldsisthattheforcestheygeneratedependonlyontheobstacleandrobotpositions,
notontherobot’svelocity. Thus,potentialfieldcontrolisreallyakinematicmethodandmay
failiftherobotismovingquickly.
Section25.6. Moving 1001
retract, lift higher
yes
move
forward no
S 3 stuck? S 4
lift up set down
push backward
S S
2 1
(a) (b)
Figure 25.24 (a) Genghis, a hexapod robot. (b) An augmented finite state machine
(AFSM) for the control of a single leg. Notice that this AFSM reacts to sensor feedback:
ifalegisstuckduringtheforwardswingingphase,itwillbeliftedincreasinglyhigher.
25.6.3 Reactivecontrol
Sofarwehave considered control decisions that require some modelof theenvironment for
constructing either areference path orapotential field. There are some difficulties with this
approach. First, models that are sufficiently accurate are often difficult to obtain, especially
in complex or remote environments, such as the surface of Mars, or for robots that have
few sensors. Second, even in cases where we can devise a model with sufficient accuracy,
computational difficulties and localization error might render these techniques impractical.
Insomecases,areflexagentarchitecture using reactive controlismoreappropriate.
REACTIVECONTROL
Forexample,picturealeggedrobotthatattemptstoliftalegoveranobstacle. Wecould
givethisrobotarulethatsaysliftthelegasmallheight handmoveitforward,andiftheleg
encounters anobstacle, moveitbackandstart againatahigher height. Youcould saythat h
ismodeling anaspect oftheworld, butwecan alsothink of hasanauxiliary variable ofthe
robotcontroller, devoidofdirectphysicalmeaning.
One such example is the six-legged (hexapod) robot, shown in Figure 25.24(a), de-
signed forwalking through rough terrain. Therobot’s sensors areinadequate toobtain mod-
elsoftheterrainforpathplanning. Moreover,evenifweaddedsufficientlyaccuratesensors,
the twelve degrees of freedom (two for each leg) would render the resulting path planning
problem computationally intractable.
It is possible, nonetheless, to specify a controller directly without an explicit environ-
mental model. (We have already seen this with the PD controller, which was able to keep a
complexrobotarmontarget withoutanexplicitmodeloftherobotdynamics;itdid,however,
require a reference path generated from a kinematic model.) Forthe hexapod robot we first
choose agait, orpattern ofmovementofthelimbs. Onestatically stable gaitistofirstmove
GAIT
the right front, right rear, and left center legs forward (keeping the other three fixed), and
then move the other three. This gait works well on flat terrain. On rugged terrain, obstacles
may prevent a leg from swinging forward. This problem can be overcome by a remarkably
simple control rule: when a leg’s forward motion is blocked, simply retract it, lift it higher,
1002 Chapter 25. Robotics
Figure25.25 Multiple exposuresofan RC helicopterexecutinga flip based ona policy
learnedwithreinforcementlearning.ImagescourtesyofAndrewNg,StanfordUniversity.
andtry again. Theresulting controller isshowninFigure25.24(b) asafinitestate machine;
itconstitutes areflex agent with state, where the internal state isrepresented by the index of
thecurrentmachinestate(s through s ).
1 4
Variantsofthissimplefeedback-driven controllerhavebeenfoundtogenerate remark-
ably robust walking patterns, capable ofmaneuvering therobot overrugged terrain. Clearly,
such a controller is model-free, and it does not deliberate or use search for generating con-
trols. Environmental feedbackplaysacrucialroleinthecontroller’sexecution. Thesoftware
alonedoesnotspecifywhatwillactuallyhappenwhentherobotisplacedinanenvironment.
Behavior that emerges through the interplay of a (simple) controller and a (complex) envi-
EMERGENT ronment is often referred to as emergent behavior. Strictly speaking, all robots discussed
BEHAVIOR
in this chapter exhibit emergent behavior, due to the fact that no model is perfect. Histori-
cally, however, the term has been reserved for control techniques that do not utilize explicit
environmental models. Emergentbehaviorisalsocharacteristic ofbiological organisms.
25.6.4 Reinforcement learning control
Oneparticularlyexcitingformofcontrolisbasedonthepolicysearchformofreinforcement
learning (see Section 21.5). This work has been enormously influential in recent years, at
is has solved challenging robotics problems for which previously no solution existed. An
example is acrobatic autonomous helicopter flight. Figure 25.25 shows an autonomous flip
of a small RC (radio-controlled) helicopter. This maneuver is challenging due to the highly
nonlinear nature of the aerodynamics involved. Only the most experienced of human pilots
areable toperform it. Yetapolicy search method(asdescribed inChapter21), using only a
fewminutesofcomputation, learnedapolicythatcansafely executeaflipeverytime.
Policy search needs an accurate model of the domain before it can find a policy. The
input to this model is the state of the helicopter at time t, the controls at time t, and the
resultingstateattimet+Δt. Thestateofahelicoptercanbedescribedbythe3Dcoordinates
of the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.
The controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,
and rudder. Allthat remains is the resulting state—how are wegoing to define a model that
accurately says how the helicopter responds to each control? The answer is simple: Let an
expert human pilot fly the helicopter, and record the controls that the expert transmits over
the radio and the state variables of the helicopter. About four minutes of human-controlled
flightsufficestobuildapredictive modelthatissufficiently accuratetosimulatethevehicle.
Section25.7. RoboticSoftwareArchitectures 1003
What is remarkable about this example is the ease with which this learning approach
solvesachallengingroboticsproblem. Thisisoneofthemanysuccessesofmachinelearning
inscientificfieldspreviously dominated bycarefulmathematicalanalysis andmodeling.
25.7 ROBOTIC SOFTWARE ARCHITECTURES
SOFTWARE Amethodologyforstructuring algorithmsiscalleda softwarearchitecture. Anarchitecture
ARCHITECTURE
includes languages and tools forwriting programs, as wellas an overall philosophy forhow
programscanbebroughttogether.
Modern-day software architectures for robotics must decide how to combine reactive
control and model-based deliberative planning. In many ways, reactive and deliberate tech-
niques have orthogonal strengths and weaknesses. Reactive control is sensor-driven and ap-
propriate for making low-level decisions in real time. However, it rarely yields a plausible
solutionatthegloballevel,becauseglobalcontroldecisionsdependoninformationthatcan-
not be sensed at the time of decision making. For such problems, deliberate planning is a
moreappropriate choice.
Consequently, most robot architectures use reactive techniques at the lower levels of
control anddeliberative techniques atthe higherlevels. Weencountered such acombination
in our discussion of PD controllers, where we combined a (reactive) PD controller with a
(deliberate) path planner. Architectures that combine reactive and deliberate techniques are
HYBRID calledhybridarchitectures.
ARCHITECTURE
25.7.1 Subsumption architecture
SUBSUMPTION Thesubsumptionarchitecture (Brooks, 1986) isaframework forassembling reactive con-
ARCHITECTURE
trollers out of finite state machines. Nodes in these machines may contain tests for certain
sensorvariables, inwhichcasetheexecutiontraceofafinitestatemachineisconditioned on
the outcome of such a test. Arcs can be tagged with messages that will be generated when
traversingthem,andthataresenttotherobot’smotorsorto otherfinitestatemachines. Addi-
tionally, finitestatemachines possess internal timers(clocks) thatcontrol thetimeittakesto
AUGMENTEDFINITE traverse anarc. Theresulting machinesarerefereed toas augmentedfinitestatemachines,
STATEMACHINE
orAFSMs,wheretheaugmentation referstotheuseofclocks.
An example of a simple AFSM is the four-state machine shown in Figure 25.24(b),
which generates cyclic leg motion for a hexapod walker. This AFSM implements a cyclic
controller, whose execution mostly does not rely on environmental feedback. The forward
swing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has
failed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and
attempts to execute the forward swing once again. Thus, the controller is able to react to
contingencies arisingfromtheinterplay oftherobotanditsenvironment.
The subsumption architecture offers additional primitives for synchronizing AFSMs,
and for combining output values of multiple, possibly conflicting AFSMs. In this way, it
enablestheprogrammertocomposeincreasinglycomplexcontrollersinabottom-upfashion.
1004 Chapter 25. Robotics
In our example, wemight begin with AFSMsforindividual legs, followed by an AFSMfor
coordinating multiple legs. On top of this, wemight implement higher-level behaviors such
ascollision avoidance, whichmightinvolvebacking upandturning.
The idea of composing robot controllers from AFSMs is quite intriguing. Imagine
how difficult it would be to generate the same behavior with any of the configuration-space
path-planning algorithms described in the previous section. First, we would need an accu-
rate model of the terrain. The configuration space of a robot with six legs, each of which
is driven by two independent motors, totals eighteen dimensions (twelve dimensions for the
configuration of the legs, and six for the location and orientation of the robot relative to its
environment). Evenifourcomputerswerefastenoughtofindpathsinsuchhigh-dimensional
spaces, we would have to worry about nasty effects such as the robot sliding down a slope.
Because of such stochastic effects, a single path through configuration space would almost
certainly be too brittle, and even a PID controller might not be able to cope with such con-
tingencies. Inother words, generating motion behavior deliberately issimply too complex a
problem forpresent-day robotmotionplanning algorithms.
Unfortunately, the subsumption architecture has its own problems. First, the AFSMs
are driven by raw sensor input, an arrangement that works if the sensor data is reliable and
containsallnecessary informationfordecisionmaking,butfailsifsensordatahastobeinte-
gratedinnontrivialwaysovertime. Subsumption-stylecontrollershavethereforemostlybeen
appliedtosimpletasks,suchasfollowingawallormovingtowardsvisiblelightsources. Sec-
ond,thelackofdeliberationmakesitdifficulttochangethetaskoftherobot. Asubsumption-
style robot usually does just one task, and it has no notion of how to modify its controls to
accommodate different goals (just like the dung beetle on page 39). Finally, subsumption-
stylecontrollers tendtobedifficulttounderstand. Inpractice, theintricateinterplaybetween
dozens of interacting AFSMs (and the environment) is beyond what most human program-
mers can comprehend. For all these reasons, the subsumption architecture is rarely used in
robotics, despite its great historical importance. However, it has had an influence on other
architectures, andonindividual components ofsomearchitectures.
25.7.2 Three-layer architecture
Hybrid architectures combine reaction with deliberation. The most popular hybrid architec-
THREE-LAYER ture is the three-layer architecture, which consists of a reactive layer, an executive layer,
ARCHITECTURE
andadeliberative layer.
Thereactivelayerprovideslow-levelcontroltotherobot. Itischaracterized byatight
REACTIVELAYER
sensor–action loop. Itsdecision cycleisoftenontheorder ofmilliseconds.
Theexecutivelayer(orsequencing layer)servesasthegluebetweenthereactivelayer
EXECUTIVELAYER
andthedeliberative layer. Itaccepts directives bythedeliberative layer, andsequences them
for the reactive layer. For example, the executive layer might handle a set of via-points
generated by a deliberative path planner, and make decisions as to which reactive behavior
to invoke. Decision cycles at the executive layer are usually in the order of a second. The
executive layer is also responsible for integrating sensor information into an internal state
representation. Forexample,itmayhosttherobot’slocalizationandonlinemappingroutines.
Section25.7. RoboticSoftwareArchitectures 1005
SENSOR INTERFACE PERCEPTION PLANNING&CONTROL USER INTERFACE
RDDF database corridor Top level control Touch screen UI
pause/disable command Wireless E-Stop
Laser 1 interface
RDDF corridor (smoothed and original) driving mode
Laser 2 interface
Laser 3 interface Road finder road center Path planner
Laser 4 interface laser map
Laser 5 interface Laser mapper map trajectory VEHICLE
Camera interface Vision mapper vision map INTERFACE
Radar interface Radar mapper obstacle list Steering control
vehicle state (pose, velocity) Touareg interface
vehicle
GPS position UKF Pose estimation state Throttle/brake control
Power server interface
GPS compass vehicle state (pose, velocity)
IMU interface Surface assessment velocity limit
Wheel velocity
Brake/steering
heart beats Linux processes start/stop emergency stop
health status
Process controller Health monitor
power on/off
data
GLOBAL
Data logger File system
SERVICES
Communication requests Communication channels clocks
Inter-process communication (IPC) server Time server
Figure 25.26 Software architecture of a robot car. This software implements a data
pipeline,inwhichallmodulesprocessdatasimultaneously.
The deliberative layer generates global solutions to complex tasks using planning.
DELIBERATIVELAYER
Because of the computational complexity involved in generating such solutions, its decision
cycleisoftenintheorderofminutes. Thedeliberative layer(orplanning layer) usesmodels
for decision making. Those models might be either learned from data or supplied and may
utilizestateinformation gathered attheexecutivelayer.
Variantsofthethree-layerarchitecturecanbefoundinmostmodern-dayrobotsoftware
systems. Thedecomposition intothreelayersisnotverystrict. Somerobotsoftwaresystems
possessadditionallayers,suchasuserinterfacelayersthatcontroltheinteractionwithpeople,
oramultiagent level forcoordinating arobot’s actions withthat of otherrobots operating in
thesameenvironment.
25.7.3 Pipelinearchitecture
PIPELINE Anotherarchitectureforrobotsisknownasthe pipelinearchitecture. Justlikethesubsump-
ARCHITECTURE
tionarchitecture,thepipelinearchitectureexecutesmultipleprocessinparallel. However,the
specificmodulesinthisarchitecture resemblethoseinthethree-layer architecture.
Figure 25.26 shows an example pipeline architecture, which is used to control an au-
SENSORINTERFACE tonomous car. Dataenters thispipeline atthe sensor interface layer. Theperception layer
LAYER
PERCEPTIONLAYER
1006 Chapter 25. Robotics
(a) (b)
Figure25.27 (a)TheHelpmaterobottransportsfoodandothermedicalitemsindozens
of hospitals worldwide. (b) Kiva robotsare part of a material-handlingsystem formoving
shelvesinfulfillmentcenters. ImagecourtesyofKivaSystems.
then updates the robot’s internal models of the environment based on this data. Next, these
PLANNINGAND models are handed to the planning and control layer, which adjusts the robot’s internal
CONTROLLAYER
plansturnsthemintoactualcontrols fortherobot. Thosearethencommunicated backtothe
VEHICLEINTERFACE vehiclethroughthevehicleinterfacelayer.
LAYER
The key to the pipeline architecture is that this all happens in parallel. While the per-
ception layer processes the most recent sensor data, the control layer bases its choices on
slightly older data. In this way, the pipeline architecture is similar to the human brain. We
don’tswitchoffourmotioncontrollerswhenwedigestnewsensordata. Instead,weperceive,
plan, andactallatthesametime. Processes inthepipeline architecture runasynchronously,
andallcomputation isdata-driven. Theresulting systemisrobust, anditisfast.
ThearchitectureinFigure25.26alsocontainsother,cross-cuttingmodules,responsible
forestablishing communication betweenthedifferentelementsofthepipeline.
25.8 APPLICATION DOMAINS
Herearesomeoftheprimeapplication domainsforrobotictechnology.
IndustryandAgriculture. Traditionally, robotshavebeenfieldedinareasthatrequire
difficult human labor, yet are structured enough to be amenable to robotic automation. The
best example is the assembly line, where manipulators routinely perform tasks such as as-
sembly, part placement, material handling, welding, and painting. In many of these tasks,
robots have become more cost-effective than human workers. Outdoors, many of the heavy
machines that we use to harvest, mine, or excavate earth have been turned into robots. For
Section25.8. Application Domains 1007
(a) (b)
Figure25.28 (a)RoboticcarBOSS,whichwontheDARPAUrbanChallenge. Courtesy
ofCarnegieMellonUniversity.(b)Surgicalrobotsintheoperatingroom.Imagecourtesyof
daVinciSurgicalSystems.
example,aprojectatCarnegieMellonUniversityhasdemonstratedthatrobotscanstrippaint
offlargeshipsabout50timesfasterthanpeoplecan,andwithamuchreducedenvironmental
impact. Prototypes ofautonomous miningrobotshavebeenfoundtobefasterandmorepre-
cisethanpeopleintransportingoreinundergroundmines. Robotshavebeenusedtogenerate
high-precision maps of abandoned mines and sewer systems. While many of these systems
are still in their prototype stages, it is only a matter of time until robots will take overmuch
ofthesemimechanical workthatispresently performedbypeople.
Transportation. Robotictransportationhasmanyfacets: fromautonomoushelicopters
thatdeliverpayloads tohard-to-reach locations, toautomatic wheelchairs thattransport peo-
plewhoareunabletocontrolwheelchairsbythemselves,toautonomousstraddlecarriersthat
outperform skilledhumandriverswhentransporting containers fromshipstotrucksonload-
ingdocks. Aprimeexampleofindoortransportation robots, orgofers, istheHelpmaterobot
shown in Figure 25.27(a). This robot has been deployed in dozens of hospitals to transport
food and other items. In factory settings, autonomous vehicles are now routinely deployed
to transport goods in warehouses and between production lines. The Kiva system, shown in
Figure25.27(b),helpsworkersatfulfillmentcenterspackagegoodsintoshippingcontainers.
Manyoftheserobotsrequireenvironmentalmodificationsfortheiroperation. Themost
common modifications are localization aids such as inductive loops in the floor, active bea-
cons, or barcode tags. An open challenge in robotics is the design of robots that can use
naturalcues,insteadofartificialdevices,tonavigate,particularlyinenvironmentssuchasthe
deepoceanwhereGPSisunavailable.
Robotic cars. Most of use cars every day. Many of us make cell phone calls while
driving. Some of us even text. The sad result: more than a million people die every year in
trafficaccidents. Robotic carslike BOSS and STANLEY offerhope: Notonlywilltheymake
drivingmuchsafer,buttheywillalsofreeusfromtheneedtopayattentiontotheroadduring
ourdailycommute.
Progress in robotic cars was stimulated by the DARPA Grand Challenge, a race over
100milesofunrehearseddesertterrain,whichrepresented amuchmorechallengingtaskthan
1008 Chapter 25. Robotics
(a) (b)
Figure25.29 (a) A robotmappingan abandonedcoalmine. (b) A 3D mapof the mine
acquiredbytherobot.
hadeverbeenaccomplishedbefore. Stanford’sSTANLEYvehiclecompletedthecourseinless
thansevenhours in2005, winninga$2million prizeandaplaceintheNationalMuseum of
American History. Figure 25.28(a) depicts BOSS, which in 2007 won the DARPA Urban
Challenge,acomplicatedroadraceoncitystreetswhererobotsfacedotherrobotsandhadto
obeytrafficrules.
Healthcare. Robotsareincreasinglyusedtoassistsurgeonswithinstrumentplacement
whenoperatingonorgansasintricateasbrains,eyes,andhearts. Figure25.28(b)showssuch
asystem. Robotshavebecomeindispensable toolsinarangeofsurgical procedures, suchas
hip replacements, thanks to their high precision. In pilot studies, robotic devices have been
found to reduce the danger of lesions when performing colonoscopy. Outside the operating
room, researchers have begun to develop robotic aides for elderly and handicapped people,
suchasintelligentroboticwalkersandintelligenttoysthatprovidereminderstotakemedica-
tion andprovide comfort. Researchers arealso working onrobotic devices forrehabilitation
thataidpeopleinperforming certainexercises.
Hazardous environments. Robots have assisted people in cleaning up nuclear waste,
most notably in Chernobyl and Three Mile Island. Robots were present after the collapse
of the World Trade Center, where they entered structures deemed too dangerous for human
searchandrescuecrews.
Somecountries haveused robots totransport ammunition and todefuse bombs—a no-
toriously dangerous task. A number of research projects are presently developing prototype
robots for clearing minefields, on land and at sea. Most existing robots for these tasks are
teleoperated—a human operates them by remote control. Providing such robots with auton-
omyisanimportant nextstep.
Exploration. Robots have gone where no one has gone before, including the surface
of Mars (see Figure 25.2(b) and the cover). Robotic arms assist astronauts in deploying
and retrieving satellites and in building the International Space Station. Robots also help
exploreunderthesea. Theyareroutinelyusedtoacquiremapsofsunkenships. Figure25.29
showsarobotmappinganabandonedcoalmine,alongwitha3Dmodelofthemineacquired
Section25.8. Application Domains 1009
(a) (b)
Figure25.30 (a)Roomba,theworld’sbest-sellingmobilerobot,vacuumsfloors. Image
courtesyofiRobot,(cid:2)c 2009. (b)Robotichandmodeledafterhumanhand. Imagecourtesy
ofUniversityofWashingtonandCarnegieMellonUniversity.
using range sensors. In 1996, a team of researches released a legged robot into the crater
of an active volcano to acquire data for climate research. Unmanned air vehicles known as
dronesareusedinmilitaryoperations. Robotsarebecomingveryeffectivetoolsforgathering
DRONE
information indomainsthataredifficult(ordangerous) for peopletoaccess.
Personal Services. Service is an up-and-coming application domain of robotics. Ser-
vice robots assist individuals in performing daily tasks. Commercially available domestic
service robots include autonomous vacuum cleaners, lawn mowers, and golf caddies. The
world’s most popular mobile robot is a personal service robot: the robotic vacuum cleaner
Roomba, shown in Figure 25.30(a). More than three million Roombas have been sold.
ROOMBA
Roombacannavigateautonomously andperform itstaskswithouthumanhelp.
Other service robots operate in public places, such as robotic information kiosks that
have been deployed in shopping malls and trade fairs, or in museums as tour guides. Ser-
vice tasks require human interaction, and theability tocoperobustly withunpredictable and
dynamicenvironments.
Entertainment. Robots have begun to conquer the entertainment and toy industry.
In Figure 25.6(b) we see robotic soccer, a competitive game very much like human soc-
ROBOTICSOCCER
cer, but played with autonomous mobile robots. Robot soccer provides great opportunities
for research in AI, since it raises a range of problems relevant to many other, more serious
robot applications. Annual robotic soccer competitions have attracted large numbers of AI
researchers andaddedalotofexcitementtothefieldofrobotics.
Human augmentation. A final application domain of robotic technology is that of
human augmentation. Researchers have developed legged walking machines that can carry
people around, very much like a wheelchair. Several research efforts presently focus on the
developmentofdevicesthatmakeiteasierforpeopletowalkormovetheirarmsbyproviding
additionalforcesthroughextraskeletalattachments. Ifsuchdevicesareattachedpermanently,
1010 Chapter 25. Robotics
they can be thought of as artificial robotic limbs. Figure 25.30(b) shows a robotic hand that
mayserveasaprosthetic deviceinthefuture.
Robotic teleoperation, or telepresence, is another form of human augmentation. Tele-
operation involves carrying out tasks over long distances with the aid of robotic devices.
A popular configuration for robotic teleoperation is the master–slave configuration, where
a robot manipulator emulates the motion of a remote human operator, measured through a
haptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth
thatwouldbedangerous forhumansbutcanstillbeguided bythehumanoperator. Allthese
systemsaugmentpeople’sabilitytointeractwiththeirenvironments. Someprojectsgoasfar
asreplicating humans,atleastataverysuperficiallevel. Humanoidrobotsarenowavailable
commercially throughseveralcompaniesinJapan.
25.9 SUMMARY
Robotics concerns itself with intelligent agents that manipulate the physical world. In this
chapter, wehavelearned thefollowingbasicsofrobothardwareandsoftware.
• Robots are equipped with sensors for perceiving their environment and effectors with
which they can assert physical forces on their environment. Most robots are either
manipulators anchored atfixedlocations ormobilerobotsthatcanmove.
• Robotic perception concerns itself with estimating decision-relevant quantities from
sensor data. To do so, we need an internal representation and a method for updating
this internal representation overtime. Commonexamples of hard perceptual problems
includelocalization, mapping,andobjectrecognition.
• ProbabilisticfilteringalgorithmssuchasKalmanfiltersandparticle filtersareuseful
forrobotperception. Thesetechniquesmaintainthebelief state,aposteriordistribution
overstatevariables.
• Theplanningofrobotmotionisusuallydoneinconfigurationspace,whereeachpoint
specifiesthelocation andorientation oftherobotanditsjointangles.
• Configuration space search algorithms include cell decomposition techniques, which
decomposethespaceofallconfigurations intofinitelymanycells,andskeletonization
techniques, whichprojectconfiguration spacesontolower-dimensional manifolds. The
motionplanning problem isthensolvedusingsearchinthese simplerstructures.
• Apath found byasearch algorithm can be executed byusing the path asthe reference
trajectory for a PID controller. Controllers are necessary in robotics to accommodate
smallperturbations; pathplanning aloneisusually insufficient.
• Potential field techniques navigate robots by potential functions, defined overthe dis-
tance to obstacles and the goal location. Potential field techniques may get stuck in
localminima,buttheycangeneratemotiondirectlywithouttheneedforpathplanning.
• Sometimes itis easier to specify a robot controller directly, rather than deriving apath
from an explicit model of the environment. Such controllers can often be written as
simplefinitestatemachines.
Bibliographical andHistorical Notes 1011
• There exist different architectures for software design. The subsumption architec-
tureenablesprogrammerstocomposerobotcontrollersfrominterconnectedfinitestate
machines. Three-layer architectures are common frameworks for developing robot
software that integrate deliberation, sequencing of subgoals, and control. The related
pipelinearchitecture processes datainparallel through asequence ofmodules, corre-
sponding toperception, modeling, planning, control, androbotinterfaces.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The word robot waspopularized by Czech playwright Karel Capek in his 1921 play R.U.R.
(Rossum’s Universal Robots). The robots, which were grown chemically rather than con-
structed mechanically, end up resenting their masters and decide to take over. It appears
(Glanc, 1978) it was Capek’s brother, Josef, who first combined the Czech words “robota”
(obligatory work)and“robotnik” (serf)toyield“robot” in his1917shortstory Opilec.
Theterm robotics was firstused by Asimov(1950). Robotics (under other names) has
amuchlongerhistory,however. InancientGreekmythology, amechanicalmannamedTalos
was supposedly designed and built by Hephaistos, the Greek god of metallurgy. Wonderful
automata were built in the 18th century—Jacques Vaucanson’s mechanical duck from 1738
being one early example—but the complex behaviors they exhibited were entirely fixed in
advance. Possiblytheearliestexampleofaprogrammablerobot-likedevicewastheJacquard
loom(1805),described onpage14.
ThefirstcommercialrobotwasarobotarmcalledUnimate,shortforuniversalautoma-
UNIMATE
tion, developed by Joseph Engelberger and George Devol. In 1961, the first Unimate robot
was sold to General Motors, where it was used for manufacturing TV picture tubes. 1961
wasalsotheyearwhenDevolobtained thefirstU.S.patent ona robot. Elevenyears later, in
1972, Nissan Corp. wasamong thefirst toautomate anentire assembly line withrobots, de-
velopedbyKawasakiwithrobotssupplied byEngelberger andDevol’scompanyUnimation.
This development initiated a major revolution that took place mostly in Japan and the U.S.,
andthatisstillongoing. Unimationfollowedupin1978withthedevelopmentofthePUMA
PUMA
robot, short forProgrammable Universal Machine forAssembly. ThePUMArobot, initially
developedforGeneralMotors,wasthedefactostandardforroboticmanipulationforthetwo
decadesthatfollowed. Atpresent,thenumberofoperatingrobotsisestimatedatonemillion
worldwide,morethanhalfofwhichareinstalled inJapan.
Theliteratureonroboticsresearchcanbedividedroughlyintotwoparts: mobilerobots
and stationary manipulators. Grey Walter’s “turtle,” built in 1948, could be considered the
firstautonomousmobilerobot,althoughitscontrolsystemwasnotprogrammable. The“Hop-
kins Beast,” built in the early 1960s at Johns Hopkins University, was much more sophisti-
cated; it had pattern-recognition hardware and could recognize the cover plate of a standard
ACpoweroutlet. Itwascapableofsearchingforoutlets,pluggingitselfin,andthenrecharg-
ing its batteries! Still, the Beast had a limited repertoire of skills. The first general-purpose
mobilerobotwas“Shakey,”developedatwhatwasthentheStanfordResearchInstitute(now
1012 Chapter 25. Robotics
SRI) in the late 1960s (Fikes and Nilsson, 1971; Nilsson, 1984). Shakey was the first robot
to integrate perception, planning, and execution, and much subsequent research in AI was
influenced by this remarkable achievement. Shakey appears on the cover of this book with
project leader Charlie Rosen (1917–2002). Other influential projects include the Stanford
Cart and the CMU Rover(Moravec, 1983). Cox and Wilfong (1990) describes classic work
onautonomous vehicles.
The field of robotic mapping has evolved from two distinct origins. The first thread
began with work by Smith and Cheeseman (1986), who applied Kalman filters to the si-
multaneous localization and mapping problem. This algorithm was first implemented by
Moutarlier and Chatila (1989), and later extended by Leonard and Durrant-Whyte (1992);
see Dissanayake et al. (2001) for an overview of early Kalman filtervariations. The second
thread began with the development of the occupancy grid representation for probabilistic
OCCUPANCYGRID
mapping, which specifies the probability that each (x,y) location is occupied by an obsta-
cle (Moravec and Elfes, 1985). Kuipers and Levitt (1988) were among the first to propose
topological rather than metric mapping, motivated by models ofhuman spatial cognition. A
seminal paperbyLuand Milios(1997) recognized thesparseness ofthesimultaneous local-
ization and mapping problem, which gave rise tothedevelopment ofnonlinear optimization
techniques by Konolige (2004) and Montemerlo and Thrun (2004), as well as hierarchical
methodsbyBosseetal.(2004). ShatkayandKaelbling(1997)andThrunetal.(1998)intro-
duced theEMalgorithm into thefieldof robotic mapping fordata association. Anoverview
ofprobabilistic mappingmethodscanbefoundin(Thrun etal.,2005).
Early mobile robot localization techniques are surveyed by Borenstein et al. (1996).
Although Kalman filtering was well known as a localization method in control theory for
decades, the general probabilistic formulation of the localization problem did not appear
in the AI literature until much later, through the work of Tom Dean and colleagues (Dean
etal., 1990, 1990) andofSimmonsand Koenig (1995). Thelatterwork introduced theterm
MARKOV Markovlocalization. Thefirstreal-worldapplicationofthistechniquewasbyBurgardetal.
LOCALIZATION
(1999), through a series of robots that were deployed in museums. Monte Carlo localiza-
tion based on particle filters was developed by Fox et al. (1999) and is now widely used.
RAO-
TheRao-Blackwellized particlefiltercombinesparticlefilteringforrobotlocalization with
BLACKWELLIZED
PARTICLEFILTER
exactfilteringformapbuilding (MurphyandRussell,2001;Montemerlo etal.,2002).
HAND–EYE The study of manipulator robots, originally called hand–eye machines, has evolved
MACHINES
along quite different lines. The first major effort at creating a hand–eye machine was Hein-
richErnst’sMH-1,describedinhisMITPh.D.thesis(Ernst,1961). TheMachineIntelligence
project at Edinburgh also demonstrated an impressive early system for vision-based assem-
bly called FREDDY (Michie, 1972). After these pioneering efforts, a great deal of work fo-
cused on geometric algorithms fordeterministic and fully observable motion planning prob-
lems. The PSPACE-hardness of robot motion planning was shown in a seminal paper by
Reif(1979). Theconfiguration spacerepresentation isduetoLozano-Perez (1983). Aseries
of papers by Schwartz and Sharir on what they called piano movers problems (Schwartz
PIANOMOVERS
etal.,1987)washighlyinfluential.
RecursivecelldecompositionforconfigurationspaceplanningwasoriginatedbyBrooks
and Lozano-Perez (1985) and improved significantly by Zhu and Latombe (1991). The ear-
Bibliographical andHistorical Notes 1013
liest skeletonization algorithms were based on Voronoi diagrams (Rowat, 1979) and visi-
bility graphs (Wesley and Lozano-Perez, 1979). Guibas et al. (1992) developed efficient
VISIBILITYGRAPH
techniques for calculating Voronoi diagrams incrementally, and Choset (1996) generalized
Voronoi diagrams tobroader motion-planning problems. John Canny (1988) established the
firstsingly exponential algorithm formotion planning. The seminal text by Latombe (1991)
coversavarietyofapproachestomotion-planning, asdothetextsbyChosetetal.(2004)and
LaValle(2006). Kavrakietal.(1996) developed probabilistic roadmaps, whicharecurrently
one of the most effective methods. Fine-motion planning with limited sensing was investi-
gated by Lozano-Perez et al. (1984) and Canny and Reif (1987). Landmark-based naviga-
tion (Lazanas and Latombe, 1992) uses many of the same ideas in the mobile robot arena.
Keyworkapplying POMDPmethods(Section17.4)tomotionplanning underuncertainty in
robotics isduetoPineau etal.(2003)andRoyetal.(2005).
Thecontrolofrobotsasdynamicalsystems—whetherformanipulationornavigation—
has generated a huge literature that is barely touched on by this chapter. Important works
include a trilogy on impedance control by Hogan (1985) and a general study of robot dy-
namics by Featherstone (1987). Deanand Wellman (1991) were among the firstto try to tie
togethercontroltheoryandAIplanningsystems. Threeclassictextbooksonthemathematics
ofrobotmanipulation areduetoPaul(1981), Craig(1989), andYoshikawa(1990). Thearea
ofgraspingisalsoimportant inrobotics—the problem ofdetermining astablegraspisquite
GRASPING
difficult(MasonandSalisbury, 1985). Competentgrasping requires touchsensing, orhaptic
feedback,todetermine contactforcesanddetectslip(FearingandHollerbach, 1985).
HAPTICFEEDBACK
Potential-field control, which attempts to solve the motion planning and control prob-
lemssimultaneously, wasintroduced into therobotics literature byKhatib(1986). Inmobile
robotics, thisideawasviewedasapractical solution tothe collision avoidance problem, and
VECTORFIELD was later extended into an algorithm called vector field histograms by Borenstein (1991).
HISTOGRAMS
Navigation functions, the robotics version of a control policy for deterministic MDPs, were
introduced byKoditschek(1987). Reinforcementlearninginroboticstookoffwiththesemi-
nalworkbyBagnellandSchneider(2001)andNgetal.(2004),whodevelopedtheparadigm
inthecontextofautonomous helicoptercontrol.
The topic of software architectures for robots engenders much religious debate. The
good old-fashioned AI candidate—the three-layer architecture—dates back to the design of
ShakeyandisreviewedbyGat(1998). ThesubsumptionarchitectureisduetoBrooks(1986),
although similar ideas were developed independently by Braitenberg (1984), whose book,
Vehicles, describes a series of simple robots based on the behavioral approach. The suc-
cess of Brooks’s six-legged walking robot was followed by many other projects. Connell,
in his Ph.D. thesis (1989), developed a mobile robot capable of retrieving objects that was
entirely reactive. Extensions of the behavior-based paradigm to multirobot systems can be
found in (Mataric, 1997) and (Parker, 1996). GRL (Horswill, 2000) and COLBERT (Kono-
lige,1997)abstracttheideasofconcurrentbehavior-based roboticsintogeneralrobotcontrol
languages. Arkin(1998)surveyssomeofthemostpopularapproaches inthisfield.
Researchonmobileroboticshasbeenstimulatedoverthelastdecadebyseveralimpor-
tantcompetitions. Theearliestcompetition, AAAI’sannualmobilerobotcompetition, began
in 1992. The first competition winner was CARMEL (Congdon et al., 1992). Progress has
1014 Chapter 25. Robotics
beensteadyandimpressive: inmorerecentcompetitions robotsenteredtheconference com-
plex, found theirwayto the registration desk, registered forthe conference, and even gave a
short talk. The Robocup competition, launched in 1995 by Kitano and colleagues (1997a),
ROBOCUP
aims to “develop a team of fully autonomous humanoid robots that can win against the hu-
man world champion team in soccer” by 2050. Play occurs in leagues for simulated robots,
wheeled robots of different sizes, and humanoid robots. In 2009 teams from 43 countries
participated and theeventwasbroadcast tomillions ofviewers. VisserandBurkhard (2007)
tracktheimprovements thathavebeenmadeinperception, teamcoordination, andlow-level
skillsoverthepastdecade.
DARPAGRAND The DARPA Grand Challenge, organized by DARPA in 2004 and 2005, required
CHALLENGE
autonomous robots to travel more than 100 miles through unrehearsed desert terrain in less
than 10 hours (Buehler et al., 2006). In the original event in 2004, no robot traveled more
than 8miles, leading manyto believe the prize would neverbe claimed. In 2005, Stanford’s
robot STANLEY wonthe competition in just under 7 hours of travel (Thrun, 2006). DARPA
thenorganizedtheUrbanChallenge,acompetitioninwhichrobotshadtonavigate60miles
URBANCHALLENGE
in an urban environment with other traffic. Carnegie Mellon University’s robot BOSS took
firstplaceandclaimedthe$2millionprize(UrmsonandWhittaker, 2008). Earlypioneersin
thedevelopmentofroboticcarsincludedDickmannsandZapp(1987)andPomerleau(1993).
Twoearly textbooks, by Dudek and Jenkin (2000) and Murphy (2000), cover robotics
generally. A more recent overview is due to Bekey (2008). An excellent book on robot
manipulation addresses advanced topics such as compliant motion (Mason, 2001). Robot
motion planning is covered in Choset et al. (2004) and LaValle (2006). Thrun et al. (2005)
provide an introduction into probabilistic robotics. The premiere conference for robotics is
Robotics: Science and Systems Conference, followed bythe IEEEInternational Conference
onRoboticsandAutomation. LeadingroboticsjournalsincludeIEEERoboticsandAutoma-
tion,theInternational JournalofRoboticsResearch,andRoboticsandAutonomousSystems.
EXERCISES
25.1 MonteCarlolocalization isbiasedforanyfinitesamplesize—i.e., theexpected value
of the location computed by the algorithm differs from the true expected value—because of
thewayparticlefilteringworks. Inthisquestion, youareaskedtoquantify thisbias.
Tosimplify,consideraworldwithfourpossiblerobotlocations: X = {x ,x ,x ,x }.
1 2 3 4
Initially, we draw N ≥ 1 samples uniformly from among those locations. As usual, it is
perfectly acceptable if morethan one sample is generated forany of the locations X. LetZ
beaBooleansensorvariable characterized bythefollowing conditional probabilities:
P(z | x ) = 0.8 P(¬z | x ) = 0.2
1 1
P(z | x ) = 0.4 P(¬z | x ) = 0.6
2 2
P(z | x ) = 0.1 P(¬z | x ) = 0.9
3 3
P(z | x ) = 0.1 P(¬z | x ) = 0.9.
4 4
Exercises 1015
B
A A
B
Starting configuration <−0.5, 7> Ending configuration <−0.5, −7>
Figure25.31 ARobotmanipulatorintwoofitspossibleconfigurations.
MCLusestheseprobabilitiestogenerateparticleweights,whicharesubsequentlynormalized
and used in the resampling process. Forsimplicity, let us assume wegenerate only one new
sample in the resampling process, regardless of N. This sample might correspond to any of
thefourlocationsinX. Thus,thesamplingprocessdefinesaprobabilitydistributionoverX.
a. Whatistheresulting probability distribution over X forthisnewsample? Answerthis
questionseparately forN = 1,...,10,andforN = ∞.
b. Thedifference between twoprobability distributions P and Q can be measured by the
KLdivergence, whichisdefinedas
(cid:12)
P(x )
i
KL(P,Q) = P(x )log .
i
Q(x )
i
i
WhataretheKLdivergences betweenthedistributions in(a)andthetrueposterior?
c. What modification of the problem formulation (not the algorithm!) would guarantee
that the specific estimator above is unbiased even for finite values of N? Provide at
leasttwosuchmodifications (eachofwhichshouldbesufficient).
25.2 Implement Monte Carlolocalization forasimulated robot withrange sensors. Agrid
map and range data are available from the code repository at aima.cs.berkeley.edu.
Youshoulddemonstrate successful globallocalization oftherobot.
25.3 Considerarobotwithtwosimplemanipulators,asshowninfigure25.31. Manipulator
A is a square block of side 2 which can slide back and on a rod that runs along the x-axis
from x=−10 to x=10. Manipulator B is a square block of side 2 which can slide back and
on a rod that runs along the y-axis from y=−10 to y=10. The rods lie outside the plane of
1016 Chapter 25. Robotics
manipulation, sothe rodsdo notinterfere withthe movement oftheblocks. Aconfiguration
isthenapair(cid:16)x,y(cid:17)wherexisthex-coordinateofthecenterofmanipulatorAandwhere yis
they-coordinate ofthecenterofmanipulator B.Drawtheconfiguration space forthisrobot,
indicating thepermittedandexcluded zones.
25.4 Suppose that you are working with the robot in Exercise 25.3 and you are given the
problem of finding a path from the starting configuration of figure 25.31 to the ending con-
figuration. Considerapotentialfunction
1
D(A,Goal)2+D(B,Goal)2+
D(A,B)2
whereD(A,B)isthedistance betweentheclosest pointsofAandB.
a. Showthathillclimbinginthispotential fieldwillgetstuckinalocalminimum.
b. Describe a potential field where hill climbing will solve this particular problem. You
neednotworkouttheexact numerical coefficients needed, justthegeneral formofthe
solution. (Hint: Add a term that “rewards” the hill climber for moving A out of B’s
way,eveninacaselikethiswherethisdoesnotreducethedistance fromAtoBinthe
abovesense.)
25.5 Consider the robot arm shown in Figure 25.14. Assume that the robot’s base element
is60cmlongandthatitsupperarmandforearmareeach40cmlong. Asarguedonpage987,
theinversekinematicsofarobotisoftennotunique. Stateanexplicitclosed-formsolutionof
theinversekinematicsforthisarm. Underwhatexactconditionsisthesolutionunique?
25.6 Implement an algorithm for calculating the Voronoi diagram of an arbitrary 2D en-
vironment, described by an n×n Boolean array. Illustrate your algorithm by plotting the
Voronoidiagram for10interesting maps. Whatisthecomplexity ofyouralgorithm?
25.7 This exercise explores the relationship between workspace and configuration space
usingtheexamplesshowninFigure25.32.
a. Consider the robot configurations shown in Figure 25.32(a) through (c), ignoring the
obstacleshownineachofthediagrams. Drawthecorresponding armconfigurations in
configurationspace. (Hint: Eacharmconfigurationmapstoasinglepointinconfigura-
tionspace,asillustrated inFigure25.14(b).)
b. Draw the configuration space for each of the workspace diagrams in Figure 25.32(a)–
(c). (Hint: The configuration spaces share with the one shown in Figure 25.32(a) the
regionthatcorrespondstoself-collision, butdifferencesarisefromthelackofenclosing
obstacles andthedifferent locations oftheobstacles intheseindividual figures.)
c. ForeachoftheblackdotsinFigure25.32(e)–(f),drawthecorrespondingconfigurations
oftherobotarminworkspace. Pleaseignoretheshadedregionsinthisexercise.
d. The configuration spaces shown in Figure 25.32(e)–(f) have all been generated by a
single workspace obstacle (dark shading), plus the constraints arising from the self-
collision constraint (light shading). Draw, for each diagram, the workspace obstacle
thatcorresponds tothedarklyshadedarea.
Exercises 1017
(a) (b) (c)
(d) (e) (f)
Figure25.32 DiagramsforExercise25.7.
e. Figure 25.32(d) illustrates that a single planar obstacle can decompose the workspace
into two disconnected regions. What is the maximum number of disconnected re-
gionsthatcanbecreatedbyinsertingaplanarobstacleinto anobstacle-free, connected
workspace, for a 2DOF robot? Give an example, and argue why no larger number of
disconnected regionscanbecreated. Howaboutanon-planar obstacle?
25.8 Consider a mobile robot moving on a horizontal surface. Suppose that the robot can
executetwokindsofmotions:
• Rollingforwardaspecifieddistance.
• Rotatinginplacethrough aspecifiedangle.
The state of such a robot can be characterized in terms of three parameters (cid:16)x,y,φ, the x-
coordinate and y-coordinate of the robot (more precisely, of its center of rotation) and the
robot’sorientationexpressedastheanglefromthepositivexdirection. Theaction“Roll(D)”
has the effect of changing state (cid:16)x,y,φ to (cid:16)x +Dcos(φ),y +Dsin(φ),φ(cid:17), and the action
Rotate(θ)hastheeffectofchanging state(cid:16)x,y,φ(cid:17)to(cid:16)x,y,φ+θ(cid:17).
a. Supposethattherobotisinitiallyat(cid:16)0,0,0(cid:17)andthenexecutestheactionsRotate(60 ◦ ),
◦
Roll(1),Rotate(25 ),Roll(2). Whatisthefinalstateoftherobot?
1018 Chapter 25. Robotics
robot
sensor
range
goal
Figure25.33 Simplifiedrobotinamaze. SeeExercise25.9.
b. Now suppose that the robot has imperfect control of its own rotation, and that, if it
attemptstorotatebyθ,itmayactuallyrotatebyanyanglebetweenθ−10 ◦ andθ+10 ◦ .
In that case, if the robot attempts to carry out the sequence of actions in (A), there is
a range of possible ending states. What are the minimal and maximal values of the
x-coordinate, they-coordinate andtheorientation inthefinalstate?
c. Let us modify the model in (B) to a probabilistic model in which, when the robot
attempts to rotate by θ, its actual angle of rotation follows a Gaussian distribution
◦
with mean θ and standard deviation 10 . Suppose that the robot executes the actions
◦
Rotate(90 ), Roll(1). Giveasimpleargument that (a)theexpected value oftheloca-
◦
tionattheendisnotequaltotheresultofrotatingexactly 90 andthenrollingforward
1 unit, and (b) that the distribution of locations at the end does not follow a Gaussian.
(Donotattempttocalculate thetruemeanorthetruedistribution.)
Thepoint of this exercise is that rotational uncertainty quickly gives rise to alot of
positional uncertainty and that dealing with rotational uncertainty is painful, whether
uncertainty istreated in terms of hard intervals orprobabilistically, due to the fact that
therelation betweenorientation andposition isbothnon-linearandnon-monotonic.
25.9 Consider the simplified robot shown in Figure 25.33. Suppose the robot’s Cartesian
coordinates are known at all times, as are those of its goal location. However, the locations
of the obstacles are unknown. The robot can sense obstacles in its immediate proximity, as
illustrated in this figure. For simplicity, let us assume the robot’s motion is noise-free, and
thestatespaceisdiscrete. Figure25.33isonlyoneexample;inthisexerciseyouarerequired
toaddress allpossible gridworldswithavalidpathfromthe starttothegoallocation.
a. Design a deliberate controller that guarantees that the robot always reaches its goal
location ifatallpossible. Thedeliberate controller canmemorizemeasurements inthe
formofamapthatisbeing acquired astherobot moves. Betweenindividual moves,it
mayspendarbitrarytimedeliberating.
Exercises 1019
b. Nowdesign a reactive controller forthe same task. Thiscontroller may not memorize
pastsensormeasurements. (Itmaynotbuildamap!) Instead,ithastomakealldecisions
based on the current measurement, which includes knowledge of its own location and
that of the goal. The time to make a decision must be independent of the environment
sizeorthenumberofpasttimesteps. Whatisthemaximumnumberofstepsthatitmay
takeforyourrobottoarriveatthegoal?
c. Howwillyourcontrollersfrom(a)and(b)performifanyofthefollowingsixconditions
apply: continuous state space, noise in perception, noise in motion, noise in both per-
ception andmotion, unknown location ofthegoal (thegoal canbedetected only when
withinsensorrange),ormovingobstacles. Foreachcondition andeachcontroller, give
anexampleofasituation wheretherobotfails(orexplainwhyitcannotfail).
25.10 InFigure25.24(b) onpage 1001, weencountered anaugmented finitestatemachine
for the control of a single leg of a hexapod robot. In this exercise, the aim is to design an
AFSM that, when combined with six copies of the individual leg controllers, results in effi-
cient, stable locomotion. Forthispurpose, you havetoaugment theindividual legcontroller
topassmessagestoyournewAFSMandtowaituntilothermessagesarrive. Arguewhyyour
controller is efficient, in that it does not unnecessarily waste energy (e.g., by sliding legs),
andinthatitpropels therobot atreasonably high speeds. Provethatyourcontroller satisfies
thedynamicstabilitycondition givenonpage977.
25.11 (This exercise was first devised by Michael Genesereth and Nils Nilsson. It works
for first graders through graduate students.) Humans are so adept at basic household tasks
that they often forget how complex these tasks are. In this exercise you will discover the
complexity and recapitulate the last 30 years of developments in robotics. Consider the task
ofbuilding anarchoutofthreeblocks. Simulatearobotwith fourhumansasfollows:
Brain. The Brain direct the hands in the execution of a plan to achieve the goal. The
Brain receives input from the Eyes, but cannot see the scene directly. The brain is the only
onewhoknowswhatthegoalis.
Eyes. TheEyesreportabriefdescription ofthescenetotheBrain: “Thereisaredbox
standingontopofagreenbox,whichisonitsside”Eyescanalsoanswerquestionsfromthe
Brain such as, “Is there agap between the Left Hand and the red box?” If you have a video
camera,pointitatthesceneandallowtheeyestolookattheviewfinderofthevideocamera,
butnotdirectlyatthescene.
Lefthandandrighthand. OnepersonplayseachHand. ThetwoHandsstandnextto
each other, each wearing an oven mitt on one hand, Hands execute only simple commands
from the Brain—for example, “Left Hand, move two inches forward.” They cannot execute
commandsotherthanmotions;forexample,theycannotbecommandedto“Pickupthebox.”
The Hands must be blindfolded. The only sensory capability they have is the ability to tell
when their path is blocked by an immovable obstacle such as a table or the other Hand. In
suchcases, theycanbeeptoinformtheBrainofthedifficulty.
26
PHILOSOPHICAL
FOUNDATIONS
In which we consider what it means to think and whether artifacts could and
shouldeverdoso.
Philosophers have been around far longer than computers and have been trying to resolve
some questions that relate to AI: How do minds work? Is it possible for machines to act
intelligently in the way that people do, and if they did, would they have real, conscious
minds? Whataretheethicalimplications ofintelligent machines?
First,someterminology: theassertionthatmachinescouldactasiftheywereintelligent
iscalled theweakAIhypothesis byphilosophers, andtheassertion thatmachines thatdoso
WEAKAI
areactuallythinking (notjustsimulating thinking) iscalledthestrongAIhypothesis.
STRONGAI
MostAIresearchers take the weakAIhypothesis forgranted, and don’t careabout the
strong AI hypothesis—as long as their program works, they don’t care whether you call it a
simulation of intelligence or real intelligence. All AI researchers should be concerned with
theethicalimplications oftheirwork.
26.1 WEAK AI: CAN MACHINES ACT INTELLIGENTLY?
The proposal for the 1956 summer workshop that defined the field of Artificial Intelligence
(McCarthyetal.,1955)madetheassertionthat“Everyaspectoflearningoranyotherfeature
ofintelligencecanbesopreciselydescribedthatamachinecanbemadetosimulateit.” Thus,
AI wasfounded on theassumption that weakAI ispossible. Others have asserted that weak
AI is impossible: “Artificial intelligence pursued within the cult of computationalism stands
notevenaghostofachanceofproducing durableresults” (Sayre,1993).
Clearly, whetherAI isimpossible depends on how itisdefined. InSection 1.1, wede-
finedAIasthequestforthebestagentprogramonagivenarchitecture. Withthisformulation,
AI isby definition possible: forany digital architecture with k bits ofprogram storage there
areexactly2k agentprograms,andallwehavetodotofindthebestoneisenumerateandtest
them all. This might not be feasible for large k, but philosophers deal with the theoretical,
notthepractical.
1020
Section26.1. WeakAI:CanMachinesActIntelligently? 1021
Our definition of AI works well for the engineering problem of finding a good agent,
given an architecture. Therefore, we’re tempted to end this section right now, answering the
title question in the affirmative. But philosophers are interested in the problem of compar-
ing two architectures—human and machine. Furthermore, they have traditionally posed the
CANMACHINES question notintermsofmaximizingexpected utilitybutratheras,“Canmachinesthink?”
THINK?
Thecomputerscientist EdsgerDijkstra (1984) said that “Thequestion ofwhether Ma-
CANSUBMARINES chines CanThink ...isabout asrelevant asthequestion ofwhether Submarines CanSwim.”
SWIM?
The American Heritage Dictionary’s first definition of swim is “To move through water by
means of the limbs, fins, or tail,” and most people agree that submarines, being limbless,
cannot swim. Thedictionary alsodefines flyas“Tomovethrough theairbymeansofwings
orwinglikeparts,”andmostpeopleagreethatairplanes,havingwinglikeparts,canfly. How-
ever,neitherthequestionsnortheanswershaveanyrelevance tothedesignorcapabilities of
airplanes andsubmarines; rathertheyareabout theusage of wordsinEnglish. (Thefactthat
ships do swim in Russian only amplifies this point.). The practical possibility of “thinking
machines”hasbeenwithusforonly50yearsorso,notlongenoughforspeakersofEnglishto
settleonameaningfortheword“think”—does itrequire“abrain”orjust“brain-like parts.”
AlanTuring,inhisfamouspaper“ComputingMachineryandIntelligence”(1950),sug-
gested that instead of asking whether machines can think, we should ask whether machines
canpassabehavioral intelligencetest,whichhascometobecalledtheTuringTest. Thetest
TURINGTEST
is for a program to have a conversation (via online typed messages) with an interrogator for
five minutes. The interrogator then has to guess if the conversation is with a program or a
person; the program passes the test if it fools the interrogator 30% of the time. Turing con-
jectured that,bytheyear2000, acomputerwithastorage of 109 unitscouldbeprogrammed
wellenoughtopassthetest. Hewaswrong—programshaveyettofoolasophisticatedjudge.
On the other hand, many people have been fooled when they didn’t know they might
be chatting with a computer. The ELIZA program and Internet chatbots such as MGONZ
(Humphrys, 2008) and NATACHATA have fooled their correspondents repeatedly, and the
chatbotCYBERLOVER hasattractedtheattentionoflawenforcementbecauseofitspenchant
fortrickingfellowchattersintodivulging enoughpersonalinformation thattheiridentitycan
be stolen. The Loebner Prize competition, held annually since 1991, is the longest-running
TuringTest-likecontest. Thecompetitions haveledtobettermodelsofhumantypingerrors.
Turing himself examined a wide variety of possible objections to the possibility of in-
telligent machines, including virtually all of those that have been raised in the half-century
sincehispaperappeared. Wewilllookatsomeofthem.
26.1.1 The argument from disability
The“argument from disability” makesthe claim that“amachine can neverdo X.”Asexam-
plesofX,Turingliststhefollowing:
Bekind,resourceful,beautiful,friendly,haveinitiative,haveasenseofhumor,tellright
from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone
fallin lovewith it, learnfromexperience,use wordsproperly,bethe subjectofitsown
thought,haveasmuchdiversityofbehaviorasman,dosomethingreallynew.
1022 Chapter 26. Philosophical Foundations
In retrospect, some of these are rather easy—we’re all familiar with computers that “make
mistakes.” We are also familiar with a century-old technology that has had a proven ability
to “make someone fall in love with it”—the teddy bear. Computer chess expert David Levy
predicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).
Asforarobotfallinginlove,thatisacommonthemeinfiction,1 buttherehasbeenonlylim-
itedspeculation aboutwhetheritisinfactlikely(Kimetal.,2007). Programsdoplaychess,
checkers and other games; inspect parts on assembly lines, steer cars and helicopters; diag-
nose diseases; and do hundreds of other tasks as well as or better than humans. Computers
have made small but significant discoveries in astronomy, mathematics, chemistry, mineral-
ogy, biology, computer science, and other fields. Each of these required performance at the
levelofahumanexpert.
Given what we now know about computers, it is not surprising that they do well at
combinatorial problems such as playing chess. Butalgorithms also perform athuman levels
on tasks that seemingly involve human judgment, oras Turing put it, “learning from experi-
ence” and the ability to “tell right from wrong.” As far back as 1955, Paul Meehl (see also
Groveand Meehl, 1996) studied the decision-making processes of trained experts atsubjec-
tive tasks such as predicting the success of a student in a training program orthe recidivism
of a criminal. In 19 out of the 20 studies he looked at, Meehl found that simple statistical
learning algorithms(suchaslinearregression ornaiveBayes)predictbetterthantheexperts.
The Educational Testing Service has used an automated program to grade millions of essay
questions on the GMAT exam since 1999. The program agrees with human graders 97% of
thetime,aboutthesamelevelthattwohumangraders agree(Burstein etal.,2001).
Itisclearthatcomputerscandomanythingsaswellasorbetterthanhumans,including
thingsthatpeoplebelieverequiregreathumaninsightandunderstanding. Thisdoesnotmean,
ofcourse,thatcomputersuseinsightandunderstanding inperformingthesetasks—thoseare
not part of behavior, and we address such questions elsewhere—but the point is that one’s
firstguessaboutthementalprocesses requiredtoproduceagivenbehaviorisoftenwrong. It
is also true, of course, that there are many tasks at which computers do not yet excel (to put
itmildly),including Turing’staskofcarrying onanopen-ended conversation.
26.1.2 The mathematical objection
It is well known, through the work of Turing (1936) and Go¨del (1931), that certain math-
ematical questions are in principle unanswerable by particular formal systems. Go¨del’s in-
completeness theorem (see Section 9.5) isthe most famous example of this. Briefly, forany
formal axiomatic system F powerful enough to do arithmetic, it is possible to construct a
so-called Go¨delsentence G(F)withthefollowingproperties:
• G(F)isasentence ofF,butcannotbeprovedwithinF.
• IfF isconsistent, thenG(F)istrue.
1 Forexample,theoperaCoppe´lia(1870),thenovelDoAndroidsDreamofElectricSheep?(1968),themovies
AI(2001)andWall-E(2008),andinsong,NoelCoward’s1955versionofLet’sDoIt:Let’sFallinLovepredicted
“probablywe’lllivetoseemachinesdoit.”Hedidn’t.
Section26.1. WeakAI:CanMachinesActIntelligently? 1023
Philosophers suchasJ.R.Lucas(1961) haveclaimedthatthistheorem showsthatmachines
arementallyinferiortohumans,becausemachinesareformalsystemsthatarelimitedbythe
incompletenesstheorem—theycannotestablishthetruthoftheirownGo¨delsentence—while
humans have no such limitation. This claim has caused decades of controversy, spawning a
vast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994)
thatrepeattheclaimwithsomefreshtwists(suchasthehypothesisthathumansaredifferent
becausetheirbrainsoperatebyquantumgravity). Wewillexamineonlythreeoftheproblems
withtheclaim.
First,Go¨del’sincompletenesstheoremappliesonlytoformalsystemsthatarepowerful
enough to do arithmetic. This includes Turing machines, and Lucas’s claim is in part based
ontheassertionthatcomputersareTuringmachines. Thisisagoodapproximation, butisnot
quitetrue. Turingmachines areinfinite, whereascomputers arefinite,andanycomputercan
therefore bedescribed asa(verylarge) system inpropositional logic, whichisnotsubject to
Go¨del’s incompleteness theorem. Second, anagent should notbetooashamed thatitcannot
establish thetruthofsomesentence whileotheragentscan. Considerthesentence
J.R.Lucascannotconsistentlyassertthatthissentenceistrue.
If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas
cannotconsistently assertit,andhenceitmustbetrue. Wehavethusdemonstrated thatthere
isasentencethatLucascannotconsistentlyassertwhileotherpeople(andmachines)can. But
thatdoesnotmakeusthinklessofLucas. Totakeanotherexample,nohumancouldcompute
the sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in
seconds. Still, wedonotseethisasafundamental limitation inthehuman’s ability tothink.
Humanswerebehavingintelligentlyforthousandsofyearsbeforetheyinventedmathematics,
soitisunlikelythatformalmathematicalreasoningplaysmorethanaperipheralroleinwhat
itmeanstobeintelligent.
Third, and most important, even if we grant that computers have limitations on what
they can prove, there is no evidence that humans are immune from those limitations. It is
all too easy to show rigorously that a formal system cannot do X, and then claim that hu-
manscandoX usingtheirowninformalmethod,withoutgivinganyevidenceforthisclaim.
Indeed, itisimpossible toprovethathumansarenotsubjecttoGo¨del’sincompleteness theo-
rem,becauseanyrigorousproofwouldrequireaformalizationoftheclaimedunformalizable
human talent, and hence refute itself. So weare left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This appeal is expressed
witharguments suchas“wemustassumeourownconsistency, ifthought istobepossible at
all” (Lucas, 1976). But if anything, humans are known to be inconsistent. This is certainly
true for everyday reasoning, but it is also true for careful mathematical thought. A famous
example is the four-color map problem. Alfred Kempe published a proof in 1879 that was
widely accepted and contributed to his election as a Fellow of the Royal Society. In 1890,
however,PercyHeawoodpointed outaflawandthetheorem remainedunproveduntil1977.
1024 Chapter 26. Philosophical Foundations
26.1.3 The argument from informality
OneofthemostinfluentialandpersistentcriticismsofAIasanenterprise wasraisedbyTur-
ingasthe“argument from informality ofbehavior.” Essentially, this istheclaim thathuman
behavior is fartoo complex to be captured by any simple set of rules and that because com-
puters candonomorethanfollowasetofrules, theycannot generate behavior asintelligent
as that of humans. The inability to capture everything in a set of logical rules is called the
QUALIFICATION qualificationprobleminAI.
PROBLEM
The principal proponent of this view has been the philosopher Hubert Dreyfus, who
hasproduced aseriesofinfluentialcritiques ofartificialintelligence: WhatComputersCan’t
Do (1972), the sequel What Computers Still Can’t Do (1992), and, with his brother Stuart,
MindOverMachine(1986).
The position they criticize came to be called “Good Old-Fashioned AI,” or GOFAI, a
term coined by philosopher John Haugeland (1985). GOFAI is supposed to claim that all
intelligent behaviorcanbecapturedbyasystemthatreasonslogicallyfromasetoffactsand
rules describing thedomain. Ittherefore corresponds tothe simplest logical agent described
inChapter7. Dreyfusiscorrectinsayingthatlogicalagentsarevulnerabletothequalification
problem. AswesawinChapter13,probabilistic reasoning systemsaremoreappropriate for
open-ended domains. TheDreyfus critique therefore isnot addressed against computers per
se, but rather against one particular way of programming them. It is reasonable to suppose,
however,thatabookcalled WhatFirst-OrderLogicalRule-BasedSystemsWithoutLearning
Can’tDomighthavehadlessimpact.
UnderDreyfus’sview,humanexpertisedoesincludeknowledgeofsomerules,butonly
asa“holistic context” or“background” withinwhichhumans operate. Hegivestheexample
ofappropriate social behavior in giving and receiving gifts: “Normally one simply responds
intheappropriate circumstances bygiving anappropriate gift.” Oneapparently has“adirect
sense ofhow things are done andwhattoexpect.” Thesameclaim ismadeinthe context of
chess playing: “Amere chess mastermight need tofigure out whatto do, but a grandmaster
justseestheboardasdemandingacertainmove...therightresponsejustpopsintohisorher
head.” Itiscertainlytruethatmuchofthethoughtprocessesofapresent-giverorgrandmaster
is done at a level that is not open to introspection by the conscious mind. But that does not
mean that the thought processes do not exist. The important question that Dreyfus does not
answer is how the right move gets into the grandmaster’s head. One is reminded of Daniel
Dennett’s(1984)comment,
Itisratherasifphilosophersweretoproclaimthemselvesexpertexplainersofthemeth-
ods of stage magicians, and then, when we ask how the magician does the sawing-the-
lady-in-halftrick,theyexplainthatitisreallyquiteobvious: themagiciandoesn’treally
sawherinhalf;hesimplymakesitappearthathedoes. “Buthowdoeshedothat?” we
ask. “Notourdepartment,”saythephilosophers.
Dreyfus and Dreyfus (1986) propose a five-stage process of acquiring expertise, beginning
with rule-based processing (of the sort proposed in GOFAI) and ending with the ability to
select correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in
effectmovefrombeingAIcriticstoAItheorists—theyproposeaneuralnetworkarchitecture
Section26.1. WeakAI:CanMachinesActIntelligently? 1025
organized into a vast “case library,” but point out several problems. Fortunately, all of their
problems havebeenaddressed, somewithpartialsuccess and somewithtotalsuccess. Their
problemsincludethefollowing:
1. Good generalization from examples cannot be achieved without background knowl-
edge. They claim no one has any idea how to incorporate background knowledge into
the neural network learning process. In fact, we saw in Chapters 19 and 20 that there
are techniques for using prior knowledge in learning algorithms. Those techniques,
however,relyontheavailability ofknowledgeinexplicitform,somethingthatDreyfus
andDreyfus strenuously deny. Inourview, thisisagood reason foraserious redesign
of current models of neural processing so that they can take advantage of previously
learnedknowledgeinthewaythatotherlearning algorithms do.
2. Neural network learning is a form of supervised learning (see Chapter 18), requiring
the prior identification of relevant inputs and correct outputs. Therefore, they claim,
it cannot operate autonomously without the help of a human trainer. In fact, learning
without a teacher can be accomplished by unsupervised learning (Chapter 20) and
reinforcementlearning(Chapter21).
3. Learning algorithms do not perform well with many features, and if we pick a subset
offeatures,“thereisnoknownwayofaddingnewfeaturesshouldthecurrentsetprove
inadequate to account for the learned facts.” In fact, new methods such as support
vector machines handle large feature sets very well. With the introduction of large
Web-based data sets, manyapplications inareas such aslanguage processing (Shaand
Pereira, 2003) and computervision (ViolaandJones, 2002a) routinely handle millions
of features. We saw in Chapter 19 that there are also principled ways to generate new
features, although muchmoreworkisneeded.
4. The brain is able to direct its sensors to seek relevant information and to process it
to extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,
“Currently, nodetails ofthismechanism areunderstood orevenhypothesized inaway
that could guide AI research.” In fact, the field of active vision, underpinned by the
theory of information value (Chapter 16), is concerned with exactly the problem of
directing sensors, and already some robots have incorporated the theoretical results
obtained. STANLEY’s132-mile trip through thedesert (page 28) wasmadepossible in
largepartbyanactivesensingsystem ofthiskind.
Insum, manyofthe issues Dreyfus has focused on—background commonsense knowledge,
the qualification problem, uncertainty, learning, compiled forms of decision making—are
indeed important issues, and have by now been incorporated into standard intelligent agent
design. Inourview,thisisevidence ofAI’sprogress, notof itsimpossibility.
One of Dreyfus’ strongest arguments is for situated agents rather than disembodied
logicalinferenceengines. Anagentwhoseunderstandingof“dog”comesonlyfromalimited
set of logical sentences such as “Dog(x) ⇒ Mammal(x)” is at a disadvantage compared
to an agent that has watched dogs run, has played fetch with them, and has been licked by
one. As philosopher Andy Clark (1998) says, “Biological brains are first and foremost the
control systems for biological bodies. Biological bodies move and act in rich real-world
1026 Chapter 26. Philosophical Foundations
surroundings.” Tounderstandhowhuman(orotheranimal)agentswork,wehavetoconsider
EMBODIED thewholeagent,notjusttheagentprogram. Indeed,theembodiedcognitionapproachclaims
COGNITION
that it makes no sense to consider the brain separately: cognition takes place within a body,
which is embedded in an environment. We need to study the system as a whole; the brain
augmentsitsreasoning byreferringtotheenvironment, asthereaderdoesinperceiving (and
creating) marks on paper to transfer knowledge. Under the embodied cognition program,
robotics, vision, andothersensorsbecomecentral, notperipheral.
26.2 STRONG AI: CAN MACHINES REALLY THINK?
Many philosophers have claimed that a machine that passes the Turing Test would still not
be actually thinking, but would be only a simulation of thinking. Again, the objection was
foreseen byTuring. HecitesaspeechbyProfessorGeoffreyJefferson(1949):
Notuntilamachinecouldwriteasonnetorcomposeaconcertobecauseofthoughtsand
emotionsfelt,andnotbythechancefallofsymbols,couldweagreethatmachineequals
brain—thatis,notonlywriteitbutknowthatithadwrittenit.
Turing calls this theargument from consciousness—the machine has to beaware ofits own
mentalstatesandactions. Whileconsciousness isanimportant subject, Jefferson’s keypoint
actually relates to phenomenology, or the study of direct experience: the machine has to
actually feel emotions. Others focus on intentionality—that is, the question of whether the
machine’s purported beliefs, desires, and other representations are actually “about” some-
thingintherealworld.
Turing’s response to the objection is interesting. He could have presented reasons that
machines can in fact be conscious (or have phenomenology, or have intentions). Instead, he
maintains that the question is just as ill-defined as asking, “Can machines think?” Besides,
why should we insist on a higher standard for machines than we do for humans? After all,
in ordinary life we never have any direct evidence about the internal mental states of other
humans. Nevertheless,Turingsays,“Insteadofarguingcontinuallyoverthispoint,itisusual
tohavethepoliteconvention thateveryonethinks.”
Turing argues that Jefferson would be willing to extend the polite convention to ma-
chinesifonlyhehadexperiencewithonesthatactintelligently. Hecitesthefollowingdialog,
whichhasbecomesuchapartofAI’soraltraditionthatwesimplyhavetoinclude it:
HUMAN: Inthefirstlineofyoursonnetwhichreads“shallIcomparetheetoasummer’s
day,”wouldnota“springday”doaswellorbetter?
MACHINE: Itwouldn’tscan.
HUMAN: Howabout“awinter’sday.”Thatwouldscanallright.
MACHINE: Yes,butnobodywantstobecomparedtoawinter’sday.
HUMAN: WouldyousayMr.PickwickremindedyouofChristmas?
MACHINE: Inaway.
HUMAN: YetChristmasisawinter’sday,andIdonotthinkMr.Pickwickwouldmind
thecomparison.
Section26.2. StrongAI:CanMachines ReallyThink? 1027
MACHINE: Idon’tthinkyou’reserious. Byawinter’sdayonemeansatypicalwinter’s
day,ratherthanaspecialonelikeChristmas.
One can easily imagine some future time in which such conversations with machines are
commonplace, and it becomes customary to make no linguistic distinction between “real”
and“artificial”thinking. Asimilartransition occurred in theyearsafter1848,whenartificial
urea wassynthesized forthe first timeby Frederick Wo¨hler. Priorto this event, organic and
inorganic chemistry were essentially disjoint enterprises and many thought that no process
could existthatwouldconvert inorganic chemicals intoorganic material. Oncethesynthesis
was accomplished, chemists agreed that artificial urea was urea, because it had all the right
physical properties. Those who had posited an intrinsic property possessed by organic ma-
terial that inorganic material could never have were faced with the impossibility of devising
anytestthatcouldrevealthesupposed deficiencyofartificialurea.
For thinking, we have not yet reached our 1848 and there are those who believe that
artificialthinking,nomatterhowimpressive,willneverbereal. Forexample,thephilosopher
JohnSearle(1980)argues asfollows:
Noonesupposesthatacomputersimulationofastormwillleaveusallwet...Whyon
earthwouldanyoneinhisrightmindsupposeacomputersimulationofmentalprocesses
actuallyhadmentalprocesses?(pp.37–38)
While it is easy to agree that computer simulations of storms do not make us wet, it is not
clear how to carry this analogy over to computer simulations of mental processes. After
all, a Hollywood simulation of a storm using sprinklers and wind machines does make the
actors wet, andavideo gamesimulation ofastorm does makethe simulated characters wet.
Most people are comfortable saying that a computer simulation of addition is addition, and
ofchessischess. Infact,wetypically speakofanimplementation ofaddition orchess,nota
simulation. Arementalprocesses morelikestorms,ormorelikeaddition?
Turing’s answer—the polite convention—suggests that the issue will eventually go
away by itself once machines reach a certain level of sophistication. This would have the
effect ofdissolving the difference between weakand strong AI. Against this, one may insist
that there is a factual issue at stake: humans do have real minds, and machines might or
might not. To address this factual issue, we need to understand how it is that humans have
real minds, not just bodies that generate neurophysiological processes. Philosophical efforts
MIND–BODY to solve this mind–bodyproblem are directly relevant to the question ofwhether machines
PROBLEM
couldhaverealminds.
Themind–bodyproblemwasconsideredbytheancientGreekphilosophers andbyvar-
ious schools of Hindu thought, but was first analyzed in depth by the 17th-century French
philosopher andmathematician Rene´ Descartes. HisMeditations onFirstPhilosophy(1641)
considered themind’s activity ofthinking (aprocess withnospatial extent ormaterial prop-
erties) andthephysical processes ofthebody, concluding that thetwomustexist inseparate
realms—what we would now call a dualist theory. The mind–body problem faced by du-
DUALISM
alists is the question of how the mind can control the body if the two are really separate.
Descartesspeculated thatthetwomightinteractthroughthepinealgland,whichsimplybegs
thequestion ofhowthemindcontrolsthepinealgland.
1028 Chapter 26. Philosophical Foundations
Themonisttheoryofmind,oftencalled physicalism,avoids thisproblem byasserting
MONISM
themindisnotseparate fromthebody—that mentalstates arephysical states. Mostmodern
PHYSICALISM
philosophers ofmindarephysicalistsofoneformoranother, andphysicalismallows,atleast
in principle, for the possibility of strong AI. The problem for physicalists is to explain how
physicalstates—in particular, themolecularconfigurations andelectrochemical processes of
thebrain—cansimultaneouslybementalstates,suchasbeinginpain,enjoyingahamburger,
MENTALSTATES
knowingthatoneisridingahorse,orbelieving thatViennaisthecapitalofAustria.
26.2.1 Mental states andthe brainina vat
Physicalistphilosophershaveattemptedtoexplicatewhatitmeanstosaythataperson—and,
byextension, acomputer—is inaparticularmentalstate. Theyhavefocused inparticularon
intentionalstates. Thesearestates,suchasbelieving, knowing,desiring, fearing,andsoon,
INTENTIONALSTATE
thatrefertosomeaspectoftheexternalworld. Forexample, theknowledgethatoneiseating
ahamburgerisabelief aboutthehamburgerandwhatishappening toit.
If physicalism is correct, it must be the case that the proper description of a person’s
mental state is determined by that person’s brain state. Thus, if I am currently focused on
eatingahamburgerinamindfulway,myinstantaneousbrainstateisaninstanceoftheclassof
mentalstates“knowingthatoneiseatingahamburger.” Ofcourse,thespecificconfigurations
of all the atoms ofmy brain are not essential: there are manyconfigurations of mybrain, or
ofotherpeople’sbrain,thatwouldbelongtothesameclassofmentalstates. Thekeypointis
that the same brain state could not correspond toa fundamentally distinct mental state, such
astheknowledge thatoneiseatingabanana.
The simplicity of this view is challenged by some simple thought experiments. Imag-
ine, if you will, that your brain was removed from your body at birth and placed in a mar-
velouslyengineered vat. Thevatsustainsyourbrain,allowingittogrowanddevelop. Atthe
sametime,electronic signalsarefedtoyourbrainfromacomputersimulation ofanentirely
fictitious world, and motor signals from your brain are intercepted and used to modify the
simulation as appropriate.2 In fact, the simulated life you live replicates exactly the life you
would have lived, had your brain not been placed in the vat, including simulated eating of
simulatedhamburgers. Thus,youcouldhaveabrainstateidenticaltothatofsomeonewhois
really eating areal hamburger, but it would be literally false to say that you have the mental
state “knowing that one is eating a hamburger.” You aren’t eating a hamburger, you have
neverevenexperienced ahamburger, andyoucouldnot,therefore, havesuchamentalstate.
Thisexampleseemstocontradicttheviewthatbrainstatesdeterminementalstates. One
waytoresolvethedilemmaistosaythatthecontentofmentalstatescanbeinterpreted from
two different points of view. The “wide content” view interprets it from the point of view
WIDECONTENT
of an omniscient outside observer with access to the whole situation, who can distinguish
differencesintheworld. Underthisview,thecontentofmentalstatesinvolvesboththebrain
state and the environment history. Narrow content, on the other hand, considers only the
NARROWCONTENT
brainstate. Thenarrowcontentofthebrainstatesofarealhamburger-eater andabrain-in-a-
vat“hamburger”-“eater” isthesameinbothcases.
2 Thissituationmaybefamiliartothosewhohaveseenthe1999filmTheMatrix.
Section26.2. StrongAI:CanMachines ReallyThink? 1029
Widecontent isentirelyappropriate ifone’sgoalsaretoascribe mentalstatestoothers
who share one’s world, to predict their likely behavior and its effects, and so on. This is the
settinginwhichourordinarylanguage aboutmentalcontent hasevolved. Ontheotherhand,
if one is concerned with the question of whether AI systems are really thinking and really
do have mental states, then narrow content is appropriate; it simply doesn’t make sense to
say that whether or not an AI system is really thinking depends on conditions outside that
system. Narrow content is also relevant if we are thinking about designing AI systems or
understandingtheiroperation,becauseitisthenarrowcontentofabrainstatethatdetermines
whatwillbethe(narrow content ofthe)nextbrainstate. Thisleads naturally totheideathat
what matters about a brain state—what makes it have one kind of mental content and not
another—is itsfunctional rolewithinthementaloperation oftheentityinvolved.
26.2.2 Functionalism andthe brainreplacement experiment
The theory of functionalism says that a mental state is any intermediate causal condition
FUNCTIONALISM
between input and output. Under functionalist theory, any two systems with isomorphic
causal processes would have the same mental states. Therefore, a computer program could
have thesamemental states as aperson. Ofcourse, wehavenot yetsaid what“isomorphic”
really means, but the assumption is that there is some level of abstraction below which the
specificimplementation doesnotmatter.
The claims of functionalism are illustrated most clearly by the brain replacement ex-
periment. This thought experiment was introduced by the philosopher Clark Glymour and
wastouchedonbyJohnSearle(1980),butismostcommonlyassociatedwithroboticistHans
Moravec(1988). Itgoeslikethis: Supposeneurophysiology hasdevelopedtothepointwhere
theinput–outputbehaviorandconnectivityofalltheneuronsinthehumanbrainareperfectly
understood. Supposefurtherthatwecanbuildmicroscopicelectronicdevicesthatmimicthis
behavior and can be smoothly interfaced to neural tissue. Lastly, suppose that some mirac-
ulous surgical technique can replace individual neurons with the corresponding electronic
devices without interrupting the operation of the brain as a whole. The experiment consists
ofgradually replacing alltheneurons insomeone’s headwithelectronic devices.
We are concerned with both the external behavior and the internal experience of the
subject, during and after the operation. By the definition of the experiment, the subject’s
external behavior must remain unchanged compared with what would be observed if the
operation were not carried out.3 Now although the presence or absence of consciousness
cannot easily be ascertained by a third party, the subject of the experiment ought at least to
be able to record any changes in his or her own conscious experience. Apparently, there is
a direct clash of intuitions as to what would happen. Moravec, a robotics researcher and
functionalist, isconvinced hisconsciousness wouldremainunaffected. Searle,aphilosopher
andbiological naturalist, isequallyconvinced hisconsciousness wouldvanish:
You find, to your total amazement, that you are indeed losing control of your external
behavior. You find, forexample, thatwhen doctorstest yourvision, youhearthem say
“Weareholdinguparedobjectinfrontofyou;pleasetelluswhatyousee.” Youwant
3 Onecanimagineusinganidentical“control”subjectwhoisgivenaplacebooperation,forcomparison.
1030 Chapter 26. Philosophical Foundations
tocryout“Ican’tseeanything. I’mgoingtotallyblind.” Butyouhearyourvoicesaying
in a way thatis completelyout of yourcontrol, “I see a red object in frontof me.” ...
your conscious experience slowly shrinks to nothing, while your externally observable
behaviorremainsthesame.(Searle,1992)
One can do more than argue from intuition. First, note that, for the external behavior to re-
mainthesamewhilethesubject gradually becomesunconscious, itmustbethecasethatthe
subject’s volition is removed instantaneously and totally; otherwise the shrinking of aware-
nesswouldbereflectedinexternalbehavior—“Help, I’mshrinking!” orwordstothateffect.
This instantaneous removal of volition as a result of gradual neuron-at-a-time replacement
seemsanunlikely claimtohavetomake.
Second, consider what happens if we do ask the subject questions concerning his or
herconscious experience during the period when no real neurons remain. Bythe conditions
of the experiment, we will get responses such as “I feel fine. I must say I’m a bit surprised
becauseIbelievedSearle’sargument.” Orwemightpokethesubjectwithapointedstickand
observetheresponse, “Ouch,thathurt.” Now,inthenormalcourseofaffairs,theskepticcan
dismiss suchoutputs fromAIprograms asmerecontrivances. Certainly, itiseasy enough to
usearule suchas“Ifsensor12reads ‘High’thenoutput ‘Ouch.’” Butthepoint hereisthat,
because we have replicated the functional properties of a normal human brain, we assume
thattheelectronic braincontainsnosuchcontrivances. Thenwemusthaveanexplanation of
themanifestations ofconsciousness produced bytheelectronic brainthatappealsonlytothe
functional properties of the neurons. Andthis explanation must also apply to the real brain,
whichhasthesamefunctional properties. Therearethreepossible conclusions:
1. Thecausalmechanismsofconsciousnessthatgeneratethesekindsofoutputsinnormal
brainsarestilloperating intheelectronic version, which istherefore conscious.
2. Theconsciousmentaleventsinthenormalbrainhavenocausalconnectiontobehavior,
andaremissingfromtheelectronic brain, whichistherefore notconscious.
3. Theexperimentisimpossible, andtherefore speculation aboutitismeaningless.
Althoughwecannotruleoutthesecondpossibility, itreducesconsciousnesstowhatphiloso-
pherscallanepiphenomenalrole—something thathappens, butcastsnoshadow,asitwere,
EPIPHENOMENON
on the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it
cannotbethecasethatthesubjectsays“Ouch”becauseithurts—thatis,becauseofthecon-
sciousexperience ofpain. Instead, thebrainmustcontain a second, unconscious mechanism
thatisresponsible forthe“Ouch.”
Patricia Churchland (1986) points out that the functionalist arguments that operate at
the level of the neuron can also operate at the level of any larger functional unit—a clump
of neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if
youacceptthenotionthatthebrainreplacementexperiment showsthatthereplacementbrain
is conscious, then you should also believe that consciousness is maintained when the entire
brainisreplacedbyacircuitthatupdatesitsstateandmaps frominputstooutputsviaahuge
lookup table. This is disconcerting to many people (including Turing himself), who have
theintuition thatlookup tablesarenotconscious—or atleast, thattheconscious experiences
generated during table lookup are not the same as those generated during the operation of a
Section26.2. StrongAI:CanMachines ReallyThink? 1031
system thatmightbedescribed (eveninasimple-minded, computational sense)asaccessing
andgenerating beliefs, introspections, goals,andsoon.
26.2.3 Biologicalnaturalism and theChinese Room
A strong challenge to functionalism has been mounted by John Searle’s (1980) biological
BIOLOGICAL naturalism,accordingtowhichmentalstatesarehigh-levelemergentfeaturesthatarecaused
NATURALISM
by low-level physical processes in the neurons, and it is the (unspecified) properties of the
neurons that matter. Thus, mental states cannot be duplicated just on the basis of some pro-
gram having the same functional structure with the same input–output behavior; we would
requirethattheprogramberunningonanarchitecturewiththesamecausalpowerasneurons.
Tosupporthisview,Searledescribes ahypothetical system thatisclearlyrunning aprogram
andpassestheTuringTest,butthatequallyclearly(according toSearle)doesnotunderstand
anything of its inputs and outputs. His conclusion is that running the appropriate program
(i.e.,havingtherightoutputs) isnotasufficient condition forbeingamind.
The system consists of a human, who understands only English, equipped with a rule
book, writteninEnglish, and various stacks ofpaper, someblank, somewithindecipherable
inscriptions. (The human therefore plays the role of the CPU, the rule book is the program,
and the stacks of paper are the storage device.) The system is inside a room with a small
opening totheoutside. Through theopening appear slips ofpaperwithindecipherable sym-
bols. Thehuman findsmatching symbols intherule book, andfollows theinstructions. The
instructionsmayincludewritingsymbolsonnewslipsofpaper,findingsymbolsinthestacks,
rearrangingthestacks,andsoon. Eventually,theinstructionswillcauseoneormoresymbols
tobetranscribed ontoapieceofpaperthatispassedbacktotheoutsideworld.
Sofar, so good. But from the outside, wesee a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as “intelligent” as those
in the conversation imagined by Turing.4 Searle then argues: the person in the room does
not understand Chinese (given). The rule book and the stacks of paper, being just pieces of
paper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,
accordingtoSearle,runningtherightprogramdoesnotnecessarilygenerateunderstanding.
Like Turing, Searle considered and attempted to rebuff a number of replies to his ar-
gument. Several commentators, including John McCarthy and Robert Wilensky, proposed
what Searle calls the systems reply. The objection is that asking if the human in the room
understands Chinese is analogous to asking if the CPU can take cube roots. In both cases,
the answer is no, and in both cases, according to the systems reply, the entire system does
havethecapacityinquestion. Certainly,ifoneaskstheChineseRoomwhetheritunderstands
Chinese, theanswerwouldbeaffirmative(influentChinese). ByTuring’spoliteconvention,
thisshouldbeenough. Searle’sresponse istoreiteratethe pointthattheunderstanding isnot
inthehumanand cannot beinthepaper, sothere cannot beanyunderstanding. Heseemsto
be relying on the argument that a property of the whole must reside in one of the parts. Yet
4 Thefact that thestacks of paper might contain trillionsof pages and thegeneration of answerswould take
millionsofyearshasnobearingonthelogicalstructureoftheargument. Oneaimofphilosophicaltrainingisto
developafinelyhonedsenseofwhichobjectionsaregermaneandwhicharenot.
1032 Chapter 26. Philosophical Foundations
wateriswet, even though neither HnorO is. Therealclaim madebySearle rests upon the
2
followingfouraxioms(Searle,1990):
1. Computerprogramsareformal(syntactic).
2. Humanmindshavementalcontents (semantics).
3. Syntaxbyitselfisneitherconstitutive ofnorsufficient forsemantics.
4. Brainscauseminds.
From the first three axioms Searle concludes that programs are not sufficient for minds. In
otherwords,anagentrunningaprogram mightbeamind,butitisnotnecessarilyamindjust
by virtue of running the program. From the fourth axiom he concludes “Any other system
capable of causing minds would have to have causal powers (at least) equivalent to those
of brains.” From there he infers that any artificial brain would have to duplicate the causal
powers of brains, not just run a particular program, and that human brains do not produce
mentalphenomena solelybyvirtueofrunningaprogram.
The axioms are controversial. For example, axioms 1 and 2 rely on an unspecified
distinction between syntax and semantics that seems to be closely related to the distinction
betweennarrowandwidecontent. Ontheonehand,wecanviewcomputersasmanipulating
syntactic symbols; on the other, we can view them as manipulating electric current, which
happens to be what brains mostly do (according to our current understanding). So it seems
wecouldequally saythatbrainsaresyntactic.
Assuming we are generous in interpreting the axioms, then the conclusion—that pro-
grams are not sufficient for minds—does follow. But the conclusion is unsatisfactory—all
Searlehasshownisthatifyouexplicitly denyfunctionalism (thatiswhathisaxiom3does),
then you can’t necessarily conclude that non-brains are minds. This is reasonable enough—
almost tautological—so the whole argument comes down to whether axiom 3 can be ac-
cepted. AccordingtoSearle,thepointoftheChineseRoomargumentistoprovideintuitions
for axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett
(1991) calls an intuition pump: it amplifies one’s prior intuitions, so biological naturalists
INTUITIONPUMP
are more convinced of their positions, and functionalists are convinced only that axiom 3 is
unsupported, or that in general Searle’s argument is unconvincing. The argument stirs up
combatants, but has done little to change anyone’s opinion. Searle remains undeterred, and
has recently started calling the Chinese Room a“refutation” ofstrong AI rather than just an
“argument” (Snell,2008).
Eventhosewhoaccept axiom 3,andthus acceptSearle’s argument, haveonly theirin-
tuitionstofallbackonwhendecidingwhatentitiesareminds. Theargumentpurportstoshow
thattheChineseRoomisnotamindbyvirtueofrunningtheprogram,buttheargumentsays
nothing about how to decide whether the room (ora computer, some other type of machine,
oranalien)isamindbyvirtueofsomeotherreason. Searlehimselfsaysthatsomemachines
do have minds: humans are biological machines with minds. According to Searle, human
brains may or may not be running something like an AI program, but if they are, that is not
the reason they are minds. It takes more to make a mind—according to Searle, something
equivalent tothecausal powersof individual neurons. What these powersare isleftunspec-
ified. Itshould benoted, however, that neurons evolved to fulfill functional roles—creatures
Section26.2. StrongAI:CanMachines ReallyThink? 1033
withneuronswerelearninganddecidinglongbeforeconsciousness appearedonthescene. It
wouldbearemarkable coincidence ifsuchneurons justhappened togenerate consciousness
because of some causal powers that are irrelevant to their functional capabilities; after all, it
isthefunctional capabilities thatdictate survivaloftheorganism.
In the case of the Chinese Room, Searle relies on intuition, not proof: just look at the
room; what’s there to be a mind? But one could make the same argument about the brain:
just look at this collection of cells (or of atoms), blindly operating according to the laws of
biochemistry(orofphysics)—what’stheretobeamind? Whycanahunkofbrainbeamind
whileahunkoflivercannot? Thatremainsthegreatmystery.
26.2.4 Consciousness,qualia, and theexplanatory gap
Running through all the debates about strong AI—the elephant in the debating room, so
to speak—is the issue of consciousness. Consciousness is often broken down into aspects
CONSCIOUSNESS
such as understanding and self-awareness. The aspect we will focus on is that of subjective
experience: whyitisthatitfeelslikesomethingtohavecertainbrainstates(e.g.,whileeating
ahamburger), whereasitpresumablydoesnotfeellikeanythingtohaveotherphysicalstates
(e.g.,whilebeingarock). Thetechnicaltermfortheintrinsicnatureofexperiences isqualia
QUALIA
(fromtheLatinwordmeaning,roughly, “suchthings”).
Qualia present a challenge for functionalist accounts of the mind because different
qualia could be involved in what are otherwise isomorphic causal processes. Consider, for
INVERTED example,theinvertedspectrumthoughtexperiment,whichthesubjectiveexperienceofper-
SPECTRUM
son X when seeing red objects is the same experience that the rest of us experience when
seeinggreenobjects,andviceversa. X stillcallsredobjects“red,”stopsforredtrafficlights,
and agrees that the redness of red traffic lights is a more intense red than the redness of the
settingsun. Yet,X’ssubjective experience isjustdifferent.
Qualiaarechallengingnotjustforfunctionalism butforallofscience. Suppose,forthe
sakeofargument,thatwehavecompletedtheprocessofscientificresearchonthebrain—we
have found that neural process P in neuron N transforms molecule Ainto molecule B,
12 177
and so on, and on. There is simply no currently accepted form of reasoning that would lead
from such findings to the conclusion that the entity owning those neurons has any particular
subjective experience. This explanatory gap has led some philosophers to conclude that
EXPLANATORYGAP
humansaresimplyincapable offorming aproperunderstanding oftheirownconsciousness.
Others, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia,
attributing themtoaphilosophical confusion.
Turinghimselfconcedesthatthequestionofconsciousness isadifficultone,butdenies
that it has much relevance to the practice of AI: “I do not wish to give the impression that I
think there is no mystery about consciousness ... But I do not think these mysteries neces-
sarily need to be solved before we can answer the question with which we are concerned in
thispaper.” WeagreewithTuring—weareinterested increating programs thatbehave intel-
ligently. Theadditional project ofmaking themconscious isnotonethatweareequipped to
takeon,noronewhosesuccesswewouldbeabletodetermine.
1034 Chapter 26. Philosophical Foundations
26.3 THE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE
So far, we have concentrated on whether we can develop AI, but we must also consider
whetherweshould. IftheeffectsofAItechnologyaremorelikelytobenegativethanpositive,
then it would be the moral responsibility of workers in the field to redirect their research.
Many new technologies have had unintended negative side effects: nuclear fission brought
Chernobyl and the threat of global destruction; the internal combustion engine brought air
pollution, global warming, and the paving-over of paradise. In a sense, automobiles are
robotsthathaveconquered theworldbymakingthemselves indispensable.
All scientists and engineers face ethical considerations of how they should act on the
job, what projects should or should not be done, and how they should be handled. See the
handbook on the Ethics of Computing (Berleur and Brunnstein, 2001). AI, however, seems
toposesomefreshproblemsbeyondthatof,say,building bridgesthatdon’tfalldown:
• Peoplemightlosetheirjobstoautomation.
• Peoplemighthavetoomuch(ortoolittle)leisuretime.
• Peoplemightlosetheirsenseofbeingunique.
• AIsystemsmightbeusedtowardundesirable ends.
• TheuseofAIsystemsmightresultinalossofaccountability.
• ThesuccessofAImightmeantheendofthehumanrace.
Wewilllookateachissueinturn.
Peoplemightlose theirjobsto automation. Themodern industrial economy has be-
comedependent oncomputers ingeneral,andselectAIprogramsinparticular. Forexample,
much of the economy, especially in the United States, depends on the availability of con-
sumer credit. Credit card applications, charge approvals, and fraud detection are now done
by AI programs. One could say that thousands of workers have been displaced by these AI
programs, but in fact if you took away the AI programs these jobs would not exist, because
human laborwould addanunacceptable costtothetransactions. Sofar, automation through
information technology in general and AI in particular has created more jobs than it has
eliminated, and has created more interesting, higher-paying jobs. Nowthat thecanonical AI
program isan“intelligent agent” designed toassistahuman,lossofjobsislessofaconcern
than it was when AI focused on “expert systems” designed to replace humans. But some
researchersthinkthatdoingthecompletejobistherightgoalforAI.Inreflectingonthe25th
AnniversaryoftheAAAI,NilsNilsson(2005)setasachallenge thecreation ofhuman-level
AIthat could pass theemployment testrather than the Turing Test—arobot that could learn
todoanyoneofarangeofjobs. Wemayendupinafuturewhereunemploymentishigh,but
eventheunemployed serveasmanagers oftheirowncadreofrobotworkers.
Peoplemighthavetoomuch(ortoolittle)leisuretime. AlvinTofflerwroteinFuture
Shock (1970), “The work week has been cut by 50 percent since the turn of the century. It
is not out of the way to predict that it will be slashed in half again by 2000.” Arthur C.
Clarke (1968b) wrote that people in 2001 might be “faced with a future of utter boredom,
wherethemainproblem inlifeisdeciding whichofseveral hundred TVchannels toselect.”
Section26.3. TheEthicsandRisksofDeveloping ArtificialIntelligence 1035
The only one of these predictions that has come close to panning out is the number of TV
channels. Instead, people working inknowledge-intensive industries have found themselves
partofanintegratedcomputerized systemthatoperates24hoursaday;tokeepup,theyhave
beenforcedtoworklongerhours. Inanindustrialeconomy,rewardsareroughlyproportional
to the time invested; working 10% more would tend to mean a 10% increase in income. In
an information economy marked by high-bandwidth communication and easy replication of
intellectualproperty(whatFrankandCook(1996)callthe“Winner-Take-AllSociety”),there
isalargerewardforbeingslightlybetterthanthecompetition;working10%morecouldmean
a 100% increase in income. So there is increasing pressure on everyone to work harder. AI
increases the pace of technological innovation and thus contributes to this overall trend, but
AIalso holds the promise ofallowing ustotake some timeoffand let ourautomated agents
handlethingsforawhile. TimFerriss(2007)recommendsusingautomationandoutsourcing
toachieveafour-hour workweek.
Peoplemightlosetheirsenseofbeingunique. InComputer PowerandHumanRea-
son, Weizenbaum (1976), theauthorofthe ELIZA program, points outsome ofthe potential
threatsthatAIposestosociety. OneofWeizenbaum’sprincipalargumentsisthatAIresearch
makespossibletheideathathumansareautomata—anideathatresultsinalossofautonomy
orevenofhumanity. WenotethattheideahasbeenaroundmuchlongerthanAI,goingback
at least to L’Homme Machine (La Mettrie, 1748). Humanity has survived other setbacks to
our sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved
the Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put
Homosapiens atthesamelevelasotherspecies. AI,ifwidelysuccessful, maybeatleast as
threatening tothemoralassumptions of21st-century societyasDarwin’stheoryofevolution
wastothoseofthe19thcentury.
AI systems might be used toward undesirable ends. Advanced technologies have
oftenbeenusedbythepowerfultosuppress theirrivals. Asthenumbertheorist G.H.Hardy
wrote(Hardy,1940),“Ascienceissaidtobeusefulifitsdevelopmenttendstoaccentuatethe
existing inequalities in the distribution of wealth, or more directly promotes the destruction
of human life.” Thisholds forall sciences, AIbeing no exception. Autonomous AI systems
are now commonplace onthe battlefield; the U.S.military deployed over5,000 autonomous
aircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009). One moral theory
holds thatmilitary robots arelikemedieval armortaken toitslogical extreme: noonewould
have moral objections to a soldier wanting to wear a helmet when being attacked by large,
angry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor. On
the other hand, robotic weapons pose additional risks. To the extent that human decision
making is taken out of the firing loop, robots may end up making decisions that lead to the
killing of innocent civilians. At a larger scale, the possession of powerful robots (like the
possession ofsturdyhelmets)maygiveanationoverconfidence, causingittogotowarmore
recklessly than necessary. In most wars, at least one party is overconfident in its military
abilities—otherwise theconflictwouldhavebeenresolvedpeacefully.
Weizenbaum (1976) also pointed out that speech recognition technology could lead to
widespread wiretapping, andhencetoalossofcivilliberties. Hedidn’tforesee aworldwith
terroristthreatsthatwouldchangethebalanceofhowmuchsurveillancepeoplearewillingto
1036 Chapter 26. Philosophical Foundations
accept, buthedidcorrectly recognize thatAIhasthepotential tomass-produce surveillance.
Hisprediction has inpart come true: the U.K.now has anextensive network ofsurveillance
cameras, andothercountries routinely monitorWebtrafficandtelephone calls. Someaccept
thatcomputerization leads toaloss ofprivacy—Sun Microsystems CEOScottMcNealyhas
said “You have zero privacy anyway. Get over it.” David Brin (1998) argues that loss of
privacy is inevitable, and the way to combat the asymmetry of power of the state over the
individual is to make the surveillance accessible to all citizens. Etzioni (2004) argues for a
balancing ofprivacyandsecurity; individual rightsandcommunity.
TheuseofAIsystemsmightresultinalossofaccountability. Inthelitigious atmo-
sphere that prevails in the United States, legal liability becomes an important issue. When a
physicianreliesonthejudgmentofamedicalexpertsystemforadiagnosis, whoisatfaultif
thediagnosisiswrong? Fortunately,dueinparttothegrowinginfluenceofdecision-theoretic
methods in medicine, it is now accepted that negligence cannot be shown if the physician
performsmedicalproceduresthathavehigh expectedutility,eveniftheactualresultiscatas-
trophic for the patient. The question should therefore be “Who is at fault if the diagnosis is
unreasonable?” So far, courts have held that medical expert systems play the same role as
medicaltextbooksandreferencebooks;physiciansareresponsibleforunderstanding therea-
soning behind any decision and for using their own judgment in deciding whether to accept
the system’s recommendations. In designing medical expert systems as agents, therefore,
the actions should be thought of not as directly affecting the patient but as influencing the
physician’sbehavior. Ifexpertsystemsbecomereliablymoreaccuratethanhumandiagnosti-
cians,doctorsmightbecomelegallyliableiftheydon’tusetherecommendationsofanexpert
system. AtulGawande(2002)exploresthispremise.
SimilarissuesarebeginningtoariseregardingtheuseofintelligentagentsontheInter-
net. Someprogress hasbeen madeinincorporating constraints intointelligent agents sothat
theycannot,forexample,damagethefilesofotherusers(WeldandEtzioni,1994). Theprob-
lem is magnified when money changes hands. If monetary transactions are made “on one’s
behalf” byan intelligent agent, isone liable forthedebts incurred? Would itbepossible for
an intelligent agent to have assets itself and to perform electronic trades on its own behalf?
So far, these questions do not seem to be well understood. To our knowledge, no program
has been granted legal status as an individual for the purposes of financial transactions; at
present, it seems unreasonable to do so. Programs are also not considered to be “drivers”
forthepurposes ofenforcing trafficregulations onrealhighways. InCalifornia law,atleast,
there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding
thespeedlimits,althoughthedesignerofthevehicle’s controlmechanismwouldbeliablein
thecase ofanaccident. Aswithhuman reproductive technology, the lawhasyet tocatch up
withthenewdevelopments.
The success of AI might mean the end of the human race. Almost any technology
hasthepotentialtocauseharminthewronghands,butwithAIandrobotics,wehavethenew
problemthatthewronghandsmightbelongtothetechnologyitself. Countlesssciencefiction
stories have warned about robots or robot–human cyborgs running amok. Early examples
Section26.3. TheEthicsandRisksofDeveloping ArtificialIntelligence 1037
include MaryShelley’s Frankenstein, ortheModernPrometheus(1818)5 andKarelCapek’s
play R.U.R.(1921), in which robots conquer the world. In movies, we have The Terminator
(1984), which combines the cliches of robots-conquer-the-world with time travel, and The
Matrix(1999), whichcombinesrobots-conquer-the-world withbrain-in-a-vat.
Itseemsthat robots are theprotagonists ofso manyconquer-the-world stories because
theyrepresent theunknown, justlikethewitches andghosts oftales fromearliereras, orthe
Martians from The War of the Worlds (Wells, 1898). The question is whether an AI system
posesabiggerriskthantraditional software. Wewilllookatthreesourcesofrisk.
First, the AI system’s state estimation may be incorrect, causing it to do the wrong
thing. Forexample,anautonomous carmightincorrectly estimatethepositionofacarinthe
adjacent lane, leading toan accident that might kill the occupants. Moreseriously, amissile
defense system might erroneously detect an attack and launch a counterattack, leading to
the death of billions. These risks are not really risks of AI systems—in both cases the same
mistakecouldjustaseasilybemadebyahumanasbyacomputer. Thecorrectwaytomitigate
these risks is to design a system with checks and balances so that a single state-estimation
errordoesnotpropagatethrough thesystemunchecked.
Second, specifying the right utility function for an AI system to maximize is not so
easy. Forexample,wemightproposeautilityfunctiondesignedtominimizehumansuffering,
expressed as anadditive reward function overtimeasinChapter17. Giventhewayhumans
are, however, we’ll always find a way to suffer even in paradise; so the optimal decision for
the AI system is toterminate thehuman race as soon as possible—no humans, no suffering.
With AI systems, then, we need to be very careful what we ask for, whereas humans would
have no trouble realizing that the proposed utility function cannot be taken literally. On the
otherhand,computersneednotbetaintedbytheirrationalbehaviorsdescribedinChapter16.
Humans sometimes use their intelligence in aggressive ways because humans have some
innately aggressive tendencies, due tonatural selection. Themachines webuild need not be
innately aggressive, unless we decide to build them that way (or unless they emerge as the
end product ofamechanism design that encourages aggressive behavior). Fortunately, there
aretechniques, suchasapprenticeship learning, thatallowsustospecifyautilityfunctionby
example. One can hope that a robot that is smart enough to figure out how to terminate the
humanraceisalsosmartenoughtofigureoutthatthatwasnottheintended utilityfunction.
Third, the AI system’s learning function may cause it to evolve into a system with
unintended behavior. This scenario is the most serious, and is unique to AI systems, so we
willcoveritinmoredepth. I.J.Goodwrote(1965),
ULTRAINTELLIGENT Let an ultraintelligent machine be defined as a machine that can far surpass all the
MACHINE
intellectualactivitiesofanymanhoweverclever. Sincethedesignofmachinesisoneof
theseintellectualactivities,anultraintelligentmachinecoulddesignevenbettermachines;
there would then unquestionablybe an “intelligence explosion,” and the intelligence of
manwouldbeleftfarbehind. Thusthefirstultraintelligentmachineisthelastinvention
that man need evermake, providedthat the machine is docile enoughto tell us how to
keepitundercontrol.
5 Asayoungman,CharlesBabbagewasinfluencedbyreadingFrankenstein.
1038 Chapter 26. Philosophical Foundations
TECHNOLOGICAL The “intelligence explosion” has also been called the technological singularity by mathe-
SINGULARITY
matics professor and science fiction author Vernor Vinge, who writes (1993), “Within thirty
years,wewillhavethetechnological meanstocreatesuperhuman intelligence. Shortlyafter,
thehumanerawillbeended.” GoodandVinge(andmanyothers)correctlynotethatthecurve
of technological progress (on many measures) is growing exponentially at present (consider
Moore’sLaw). However,itisaleaptoextrapolatethatthecurvewillcontinuetoasingularity
ofnear-infinitegrowth. Sofar,everyothertechnologyhasfollowedanS-shapedcurve,where
the exponential growth eventually tapers off. Sometimes new technologies step in when the
old ones plateau; sometimes wehit hard limits. With less than acentury ofhigh-technology
historytogoon,itisdifficulttoextrapolate hundreds ofyearsahead.
Note that the concept of ultraintelligent machines assumes that intelligence is an es-
pecially important attribute, and if you have enough of it, all problems can be solved. But
we know there are limits on computability and computational complexity. If the problem
ofdefining ultraintelligent machines (orevenapproximations tothem) happens to fallin the
class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then
even exponential progress in technology won’t help—the speed of light puts a strict upper
bound on how much computing can bedone; problems beyond that limitwill not besolved.
Westilldon’tknowwherethoseupperbounds are.
Vinge is concerned about the coming singularity, but some computer scientists and
futurists relish it. HansMoravec (2000) encourages ustogiveeveryadvantage toour“mind
children,” the robots we create, which may surpass us in intelligence. There is even a new
word—transhumanism—fortheactivesocialmovementthatlooksforwardtothisfuturein
TRANSHUMANISM
which humans are merged with—or replaced by—robotic and biotech inventions. Suffice it
tosaythatsuchissuespresentachallengeformostmoraltheorists,whotakethepreservation
ofhuman lifeand thehuman species tobe agood thing. RayKurzweiliscurrently themost
visibleadvocate forthesingularity view,writingin TheSingularity isNear(2005):
TheSingularitywillallowustotranscendtheselimitationsofourbiologicalbodiesand
brain. We will gainpoweroverourfates. Ourmortality will be in ourown hands. We
willbeabletoliveaslongaswewant(asubtlydifferentstatementfromsayingwewill
liveforever).Wewillfullyunderstandhumanthinkingandwillvastlyextendandexpand
itsreach. Bytheendofthiscentury,thenonbiologicalportionofourintelligencewillbe
trillionsoftrillionsoftimesmorepowerfulthanunaidedhumanintelligence.
Kurzweil also notes the potential dangers, writing “Butthe Singularity will also amplify the
abilitytoactonourdestructive inclinations, soitsfullstoryhasnotyetbeenwritten.”
If ultraintelligent machines are a possibility, we humans would do well to make sure
that wedesign their predecessors in such a way that they design themselves to treat us well.
Science fiction writer Isaac Asimov (1942) was the first to address this issue, with his three
lawsofrobotics:
1. A robot may not injure a human being or, through inaction, allow a human being to
cometoharm.
2. Arobotmustobeyordersgiventoitbyhumanbeings,except wheresuchorderswould
conflictwiththeFirstLaw.
Section26.3. TheEthicsandRisksofDeveloping ArtificialIntelligence 1039
3. Arobotmustprotectitsownexistenceaslongassuchprotection doesnotconflictwith
theFirstorSecondLaw.
Theselawsseemreasonable, atleast toushumans.6 Butthetrickishowtoimplement these
laws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the
robot isfound wandering inacircle around theselenium source. Everytimeitheads toward
the source, it senses a danger, and the third law causes it to veer away. But every time it
veers away, the danger recedes, and the power of the second law takes over, causing it to
veer back towards the selenium. The set of points that define the balancing point between
thetwolawsdefinesacircle. Thissuggests thatthelawsarenotlogical absolutes, butrather
are weighed against each other, with a higher weighting for the earlier laws. Asimov was
probably thinking of an architecture based on control theory—perhaps a linear combination
offactors—while todaythemostlikelyarchitecture wouldbeaprobabilistic reasoning agent
that reasons over probability distributions of outcomes, and maximizes utility as defined by
the three laws. But presumably we don’t want our robots to prevent a human from crossing
the street because of the nonzero chance of harm. That means that the negative utility for
harm to a human must be much greater than for disobeying, but that each of the utilities is
finite,notinfinite.
Yudkowsky(2008)goesintomoredetailabouthowtodesignaFriendlyAI.Heasserts
FRIENDLYAI
that friendliness (adesire notto harm humans) should bedesigned infrom the start, but that
thedesigners shouldrecognize boththattheirowndesigns maybeflawed,andthattherobot
willlearnandevolveovertime. Thusthechallenge isoneofmechanism design—to definea
mechanism for evolving AI systems under a system of checks and balances, and to give the
systemsutilityfunctions thatwillremainfriendly inthefaceofsuchchanges.
Wecan’tjustgiveaprogramastaticutilityfunction,becausecircumstances,andourde-
sired responses tocircumstances, change overtime. Forexample, iftechnology hadallowed
us to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of
thetime, itwouldbefighting today toreestablish slavery andabolish women’s right tovote.
Ontheotherhand,ifwebuild anAIagenttodayandtellittoevolveitsutility function, how
canweassure that itwon’treason that“Humans think itismoraltokill annoying insects, in
part because insect brains are so primitive. But human brains are primitive compared to my
powers,soitmustbemoralformetokillhumans.”
Omohundro (2008) hypothesizes that even an innocuous chess program could pose a
risk to society. Similarly, Marvin Minsky once suggested that an AI program designed to
solve the Riemann Hypothesis might end up taking over all the resources of Earth to build
more powerful supercomputers to help achieve its goal. The moral is that even if you only
want your program to play chess or prove theorems, if you give it the capability to learn
and alter itself, you need safeguards. Omohundro concludes that “Social structures which
cause individuals to bearthecost oftheirnegative externalities would go along way toward
ensuringastableandpositivefuture,”Thisseemstobeanexcellentideaforsocietyingeneral,
regardlessofthepossibility ofultraintelligent machines.
6 Arobotmightnoticetheinequitythatahumanisallowedtokillanotherinself-defense,butarobotisrequired
tosacrificeitsownlifetosaveahuman.
1040 Chapter 26. Philosophical Foundations
We should note that the idea of safeguards against change in utility function is not a
newone. IntheOdyssey,Homer(ca. 700B.C.) describedUlysses’encounterwiththesirens,
whose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it
would have that effect on him, Ulysses ordered his crew to bind him to the mast so that he
could not perform the self-destructive act. It is interesting to think how similar safeguards
couldbebuiltintoAIsystems.
Finally, let us consider the robot’s point of view. If robots become conscious, then to
treat them as mere “machines” (e.g., to take them apart) might be immoral. Science fiction
writershaveaddressed theissueofrobotrights. Themovie A.I.(Spielberg, 2001) wasbased
on a story by Brian Aldiss about an intelligent robot who was programmed to believe that
he was human and fails to understand his eventual abandonment by his owner–mother. The
story(andthemovie)arguefortheneedforacivilrightsmovementforrobots.
26.4 SUMMARY
Thischapterhasaddressed thefollowingissues:
• Philosophers use the term weak AI for the hypothesis that machines could possibly
behave intelligently, andstrong AIforthehypothesis thatsuch machines would count
ashavingactualminds(asopposed tosimulatedminds).
• Alan Turing rejected the question “Can machines think?” and replaced it with a be-
havioral test. He anticipated many objections to the possibility of thinking machines.
Few AI researchers pay attention to the Turing Test, preferring to concentrate on their
systems’performance onpractical tasks,ratherthantheabilitytoimitatehumans.
• Thereisgeneralagreementinmoderntimesthatmentalstatesarebrainstates.
• Argumentsforandagainststrong AIareinconclusive. Fewmainstream AIresearchers
believethatanything significanthingesontheoutcomeofthedebate.
• Consciousness remainsamystery.
• We identified six potential threats to society posed by AI and related technology. We
concluded that someofthethreats areeitherunlikely ordifferlittle from threats posed
by “unintelligent” technologies. One threat in particular is worthy of further consider-
ation: that ultraintelligent machines might lead to a future that is very different from
today—we may not like it, and at that point we may not have a choice. Such consid-
erations lead inevitably to the conclusion that we must weigh carefully, and soon, the
possibleconsequences ofAIresearch.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Sources for the various responses to Turing’s 1950 paper and for the main critics of weak
AIweregiveninthechapter. Although itbecamefashionable inthepost-neural-network era
Bibliographical andHistorical Notes 1041
to deride symbolic approaches, not all philosophers are critical of GOFAI. Someare, in fact,
ardent advocates and even practitioners. Zenon Pylyshyn (1984) has argued that cognition
can best be understood through a computational model, not only in principle but also as a
way of conducting research at present, and has specifically rebutted Dreyfus’s criticisms of
the computational model of human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in
analyzingbeliefrevision,makesconnections withAIresearchontruthmaintenancesystems.
MichaelBratmanhasappliedhis“belief-desire-intention”modelofhumanpsychology(Brat-
man, 1987) to AI research on planning (Bratman, 1992). At the extreme end of strong AI,
Aaron Sloman (1978, p. xiii) has even described as “racialist” the claim by Joseph Weizen-
baum(1976)thatintelligent machinescanneverberegarded aspersons.
Proponents of the importance of embodiment in cognition include the philosophers
Merleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the
bodyandthesubjectiveinterpretationofrealityaffordedbyoursenses,andHeidegger,whose
Being and Time(1927) asked what it means to actually be an agent, and criticized all of the
historyofphilosophyfortakingthisnotionforgranted. Inthecomputerage,AlvaNoe(2009)
and Andy Clark (1998, 2008) propose that our brains form a rather minimal representation
of the world, use the world itself in ajust-in-time basis to maintain the illusion of adetailed
internal model, use props in the world (such as paper and pencil as well as computers) to
increase the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)
presentarguments forhowthebodyhelpsshapecognition.
The nature of the mind has been a standard topic of philosophical theorizing from an-
cient times to the present. In the Phaedo, Plato specifically considered and rejected the idea
that the mind could be an “attunement” orpattern of organization of the parts of the body, a
viewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He
decided instead that the mind had to be an immortal, immaterial soul, separable from the
body anddifferent insubstance—the viewpoint ofdualism. Aristotle distinguished avariety
ofsouls(Greekψυχη)inlivingthings,someofwhich,atleast,hedescribedinafunctionalist
manner. (SeeNussbaum(1978)formoreonAristotle’s functionalism.)
Descartesisnotoriousforhisdualisticviewofthehumanmind,butironicallyhishistor-
icalinfluencewastowardmechanismandphysicalism. Heexplicitlyconceivedofanimalsas
automata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine]
should produce different arrangements of words so as to give an appropriately meaningful
answer to whatever is said in its presence, as even the dullest of men can do” (Descartes,
1637). Descartes’s spirited defense of the animals-as-automata viewpoint actually had the
effectofmakingiteasiertoconceiveofhumansasautomataaswell,eventhoughhehimself
did not take this step. The book L’Homme Machine (La Mettrie, 1748) did explicitly argue
thathumansareautomata.
Modernanalyticphilosophyhastypicallyacceptedphysicalism,butthevarietyofviews
on the content of mental states is bewildering. Theidentification of mental states with brain
states is usually attributed to Place (1956) and Smart (1959). The debate between narrow-
contentandwide-contentviewsofmentalstateswastriggeredbyHilaryPutnam(1975),who
introduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a
TWINEARTHS
devicetogenerate identical brainstateswithdifferent (wide)content.
1042 Chapter 26. Philosophical Foundations
Functionalism isthephilosophy ofmindmostnaturally suggested byAI.Theideathat
mental states correspond to classes of brain states defined functionally is due to Putnam
(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functional-
ism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,
1991)hasattractedmanyattemptedrebuttals. Metzinger(2009)arguesthereisnosuchthing
asanobjective self, thatconsciousness isthesubjective appearance ofaworld. Theinverted
spectrum argument concerning qualia was introduced by John Locke (1690). Frank Jack-
son (1982) designed aninfluential thought experiment involving Mary, acolor scientist who
has been brought up in an entirely black-and-white world. There’s Something About Mary
(Ludlowetal.,2004)collectsseveralpapersonthistopic.
Functionalism hascomeunderattack from authors whoclaim thattheydonotaccount
for the qualia or “what it’s like” aspect of mental states (Nagel, 1974). Searle has focused
instead on the alleged inability of functionalism to account for intentionality (Searle, 1980,
1984, 1992). Churchland and Churchland (1982) rebut both these types of criticism. The
Chinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).
We’ll just mention here a related work: Terry Bisson’s (1990) science fiction story They’re
Made out of Meat, in which alien robotic explorers who visit earth are incredulous to find
thinking humanbeings whosemindsaremadeofmeat. Presumably, therobotic alien equiv-
alentofSearlebelieves thathecanthink duetothespecial causal powersofrobotic circuits;
causalpowersthatmeremeat-brains donotpossess.
Ethical issues in AI predate the existence of the field itself. I. J. Good’s (1965) ul-
traintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).
Written four years after the publication of Darwin’s On the Origins of Species and at a time
whenthemostsophisticatedmachinesweresteamengines,Butler’sarticleonDarwinAmong
theMachinesenvisioned“theultimatedevelopmentofmechanicalconsciousness” bynatural
selection. Thethemewasreiterated byGeorgeDyson(1998)inabookofthesametitle.
Thephilosophical literature onminds,brains, andrelated topicsislargeanddifficultto
read without training in the terminology and methods of argument employed. The Encyclo-
pedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in
this process. The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more
accessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent
articles and up-to-date references. The MITEncyclopedia of Cognitive Science (Wilson and
Keil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.
There are several general introductions to the philosophical “AI question” (Boden, 1990;
Haugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007). The Behavioral and
Brain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientific
debates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in
thejournals AIandSociety andJournal ofArtificialIntelligence andLaw.
Exercises 1043
EXERCISES
26.1 Gothrough Turing’s listofalleged “disabilities” ofmachines, identifying whichhave
been achieved, which are achievable in principle by aprogram, andwhich are still problem-
aticbecausetheyrequire conscious mentalstates.
26.2 Findandanalyze anaccount inthepopular mediaofoneormore ofthearguments to
theeffectthatAIisimpossible.
26.3 In the brain replacement argument, it is important to be able to restore the subject’s
brain to normal, such that its external behavior is as it would have been if the operation had
not taken place. Can the skeptic reasonably object that this would require updating those
neurophysiological properties of the neurons relating to conscious experience, as distinct
fromthoseinvolved inthefunctional behavioroftheneurons?
26.4 Suppose that a Prolog program containing many clauses about the rules of British
citizenship is compiled and run on an ordinary computer. Analyze the “brain states” of the
computerunderwideandnarrowcontent.
26.5 AlanPerlis(1982)wrote,“Ayearspentinartificialintelligenceisenoughtomakeone
believe in God”. Healso wrote, in a letter to Philip Davis, that one of the central dreams of
computer science is that “through theperformance of computers and their programs wewill
remove all doubt that there is only a chemical distinction between the living and nonliving
world.” To what extent does the progress made so far in artificial intelligence shed light on
theseissues? Supposethatatsomefuturedate,theAIendeavorhasbeencompletelysuccess-
ful;thatis,wehavebuildintelligent agentscapableofcarryingoutanyhumancognitivetask
athumanlevelsofability. Towhatextentwouldthatshedlightontheseissues?
26.6 Comparethesocialimpactofartificialintelligenceinthelastfiftyyearswiththesocial
impact of the introduction of electric appliances and the internal combustion engine in the
fiftyyearsbetween1890and1940.
26.7 I. J. Good claims that intelligence is the most important quality, and that building
ultraintelligent machines will change everything. A sentient cheetah counters that “Actually
speedismoreimportant;ifwecouldbuildultrafastmachines,thatwouldchangeeverything,”
and a sentient elephant claims “You’re both wrong; what we need is ultrastrong machines.”
Whatdoyouthinkofthesearguments?
26.8 AnalyzethepotentialthreatsfromAItechnologytosociety. Whatthreatsaremostse-
rious,andhowmighttheybecombated? Howdotheycomparetothepotentialbenefits?
26.9 HowdothepotentialthreatsfromAItechnology comparewiththosefromothercom-
putersciencetechnologies, andtobio-,nano-, andnuclear technologies?
26.10 Some critics object that AI is impossible, while others object that it is too possible
and that ultraintelligent machines pose a threat. Which of these objections do you think is
morelikely? Woulditbeacontradiction forsomeonetoholdbothpositions?
27
AI: THE PRESENT AND
FUTURE
Inwhichwetakestockofwhereweareandwherewearegoing,thisbeingagood
thingtodobeforecontinuing.
In Chapter 2, we suggested that it would be helpful to view the AI task as that of designing
rational agents—that is, agents whose actions maximize their expected utility given their
percept histories. We showed that the design problem depends on the percepts and actions
available to the agent, the utility function that the agent’s behavior should satisfy, and the
nature of the environment. A variety of different agent designs are possible, ranging from
reflex agents to fully deliberative, knowledge-based, decision-theoretic agents. Moreover,
thecomponentsofthesedesignscanhaveanumberofdifferentinstantiations—for example,
logicalorprobabilisticreasoning,andatomic,factored, orstructuredrepresentationsofstates.
Theintervening chapterspresented theprinciples bywhich thesecomponents operate.
Forall the agent designs and components, there has been tremendous progress both in
our scientific understanding and in our technological capabilities. In this chapter, we stand
back from the details and ask, “Will all this progress lead to a general-purpose intelligent
agent that can perform well in a wide variety of environments?” Section 27.1 looks at the
components ofanintelligent agent toassess what’s knownandwhat’smissing. Section 27.2
doesthesamefortheoverallagentarchitecture. Section27.3askswhetherdesigningrational
agents is the right goal in the first place. (The answer is, “Not really, but it’s OK for now.”)
Finally,Section27.4examinestheconsequences ofsuccessinourendeavors.
27.1 AGENT COMPONENTS
Chapter 2 presented several agent designs and their components. To focus our discussion
here, wewilllook attheutility-based agent, whichweshow againinFigure 27.1. Whenen-
dowedwithalearningcomponent(Figure2.15),thisisthemostgeneralofouragentdesigns.
Let’sseewherethestateoftheartstandsforeachofthecomponents.
Interaction with the environment through sensors and actuators: For much of the
history of AI, this has been a glaring weak point. With a few honorable exceptions, AI sys-
temswerebuiltinsuchawaythathumanshadtosupplytheinputsandinterpret theoutputs,
1044
Section27.1. AgentComponents 1045
Agent
Environment
Sensors
State
What the world
How the world evolves is like now
What it will be like
What my actions do if I do action A
How happy I will be
Utility
in such a state
What action I
should do now
Actuators
Figure27.1 Amodel-based,utility-basedagent,asfirstpresentedinFigure2.14.
while robotic systems focused on low-level tasks in which high-level reasoning and plan-
ning were largely absent. This was due in part to the great expense and engineering effort
required to get real robots to work at all. The situation has changed rapidly in recent years
with the availability of ready-made programmable robots. These, in turn, have benefited
fromsmall,cheap,high-resolution CCDcamerasandcompact,reliablemotordrives. MEMS
(micro-electromechanicalsystems)technologyhassuppliedminiaturizedaccelerometers,gy-
roscopes, and actuators for an artificial flying insect (Floreano et al., 2009). It may also be
possible tocombinemillionsofMEMSdevicestoproduce powerfulmacroscopic actuators.
Thus, we see that AI systems are at the cusp of moving from primarily software-only
systems to embedded robotic systems. The state of robotics today is roughly comparable to
the state of personal computers in about 1980: at that time researchers and hobbyists could
experimentwithPCs,butitwouldtakeanotherdecadebefore theybecamecommonplace.
Keepingtrack of thestate of the world: This is one of the core capabilities required
foran intelligent agent. It requires both perception and updating of internal representations.
Chapter 4 showed how to keep track of atomic state representations; Chapter 7 described
how to do it for factored (propositional) state representations; Chapter 12 extended this to
first-orderlogic;andChapter15described filteringalgorithms forprobabilistic reasoning in
uncertainenvironments. Currentfilteringandperception algorithmscanbecombinedtodoa
reasonable job of reporting low-level predicates such as “the cup is on the table.” Detecting
higher-level actions, such as “Dr. Russell is having a cup of tea with Dr. Norvig while dis-
cussingplansfornextweek,”ismoredifficult. Currentlyit canbedone(seeFigure24.25on
page961)onlywiththehelpofannotated examples.
Anotherproblemisthat,althoughtheapproximatefiltering algorithmsfromChapter15
can handle quite large environments, they are still dealing with a factored representation—
theyhaverandomvariables,butdonotrepresentobjectsand relationsexplicitly. Section14.6
explained how probability and first-order logic can be combined to solve this problem, and
1046 Chapter 27. AI:ThePresentandFuture
Section14.6.3showedhowwecanhandleuncertaintyabouttheidentityofobjects. Weexpect
thattheapplicationoftheseideasfortrackingcomplexenvironmentswillyieldhugebenefits.
However, weare still faced withadaunting task ofdefining general, reusable representation
schemesforcomplexdomains. AsdiscussedinChapter12,wedon’tyetknowhowtodothat
ingeneral; only forisolated, simple domains. Itispossible that anewfocus onprobabilistic
ratherthanlogicalrepresentationcoupledwithaggressivemachinelearning(ratherthanhand-
encoding ofknowledge) willallowforprogress.
Projecting,evaluating,andselectingfuturecoursesofaction: Thebasicknowledge-
representation requirements hereare thesameasforkeeping track ofthe world; theprimary
difficulty is coping with courses of action—such as having a conversation or a cup of tea—
that consist eventually of thousands ormillions of primitive steps fora real agent. It is only
by imposing hierarchical structure on behavior that we humans cope at all. We saw in
Section 11.2 how to use hierarchical representations to handle problems of this scale; fur-
thermore, work in hierarchical reinforcement learning has succeeded in combining some
ofthese ideas withthe techniques fordecision making under uncertainty described inChap-
ter 17. As yet, algorithms for the partially observable case (POMDPs) are using the same
atomicstaterepresentation weusedforthesearchalgorithmsofChapter3. Thereisclearlya
greatdealofworktodohere,butthetechnical foundations arelargelyinplace. Section27.2
discusses thequestion ofhowthesearchforeffectivelong-range plansmightbecontrolled.
Utility asan expression ofpreferences: Inprinciple, basing rational decisions on the
maximization of expected utility is completely general and avoids many of the problems of
purely goal-based approaches, such as conflicting goals and uncertain attainment. As yet,
however,therehasbeenverylittleworkonconstructing realistic utilityfunctions—imagine,
forexample,thecomplexwebofinteractingpreferencesthatmustbeunderstoodbyanagent
operating asan officeassistant forahuman being. Ithasproven verydifficult todecompose
preferences over complex states in the same way that Bayes nets decompose beliefs over
complex states. One reason may be that preferences over states are really compiled from
preferences overstate histories, which are described by reward functions (see Chapter 17).
Eveniftherewardfunctionissimple,thecorrespondingutilityfunctionmaybeverycomplex.
Thissuggests that wetake seriously the task ofknowledge engineering forreward functions
asawayofconveying toouragentswhatitisthatwewantthemtodo.
Learning: Chapters 18 to 21 described how learning in an agent can be formulated as
inductive learning (supervised, unsupervised, or reinforcement-based) of the functions that
constitute the various components of the agent. Very powerful logical and statistical tech-
niques have been developed that can cope with quite large problems, reaching or exceeding
human capabilities in many tasks—as long as we are dealing with a predefined vocabulary
of features and concepts. On the other hand, machine learning has made very little progress
on the important problem of constructing new representations at levels of abstraction higher
than theinput vocabulary. Incomputer vision, forexample, learning complex concepts such
asClassroom andCafeteria would bemadeunnecessarily difficult iftheagent wereforced
to work from pixels as the input representation; instead, the agent needs to be able to form
intermediate concepts first, such as Desk and Tray, without explicit human supervision.
Similar considerations apply to learning behavior: HavingACupOfTea is a very important
Section27.2. AgentArchitectures 1047
high-levelstepinmanyplans,buthowdoesitgetintoanactionlibrarythatinitiallycontains
much simpler actions such as RaiseArm and Swallow? Perhaps this will incorporate some
DEEPBELIEF oftheideasofdeepbeliefnetworks—Bayesiannetworksthathavemultiplelayersofhidden
NETWORKS
variables, asintheworkofHinton etal.(2006), HawkinsandBlakeslee (2004), andBengio
andLeCun(2007).
The vast majority of machine learning research today assumes a factored representa-
tion, learning afunction h : Rn → R for regression and h : Rn → {0,1} forclassification.
Learning researchers will need to adapt their very successful techniques for factored repre-
sentations to structured representations, particularly hierarchical representations. The work
oninductive logicprogramming inChapter19isafirststepin thisdirection; thelogicalnext
stepistocombinetheseideaswiththeprobabilistic languages ofSection14.6.
Unless weunderstand such issues, weare faced withthe daunting task ofconstructing
large commonsense knowledge bases by hand, an approach that has not fared well to date.
There is great promise in using the Web as a source of natural language text, images, and
videos to serve as a comprehensive knowledge base, but so far machine learning algorithms
arelimitedintheamountoforganized knowledge theycanextractfromthesesources.
27.2 AGENT ARCHITECTURES
It is natural to ask, “Which of the agent architectures in Chapter 2 should an agent use?”
The answer is, “All of them!” We have seen that reflex responses are needed for situations
in which time is of the essence, whereas knowledge-based deliberation allows the agent to
HYBRID plan ahead. A complete agent must be able to do both, using a hybrid architecture. One
ARCHITECTURE
important property of hybrid architectures is that the boundaries between different decision
components are not fixed. For example, compilation continually converts declarative in-
formationatthedeliberativelevelintomoreefficientrepresentations, eventuallyreachingthe
reflexlevel—seeFigure27.2. (Thisisthepurposeofexplanation-basedlearning,asdiscussed
in Chapter 19.) Agent architectures such as SOAR (Laird et al., 1987) and THEO (Mitchell,
1990) have exactly this structure. Every time they solve a problem by explicit deliberation,
they save away a generalized version of the solution for use by the reflex component. A
less studied problem is the reversal of this process: when the environment changes, learned
reflexes may no longer be appropriate and the agent must return to the deliberative level to
produce newbehaviors.
Agents also need ways to control their own deliberations. They must be able to cease
deliberating when action is demanded, and they must be able to use the time available for
deliberation to execute the most profitable computations. For example, a taxi-driving agent
that sees an accident ahead must decide in a split second either to brake or to take evasive
action. It should also spend that split second thinking about the most important questions,
such as whether the lanes to the left and right are clear and whether there is a large truck
close behind, rather than worrying about wear and tear on the tires or where to pick up the
next passenger. These issues are usually studied under the heading of real-time AI. As AI
REAL-TIMEAI
1048 Chapter 27. AI:ThePresentandFuture
Percepts
Compilation
Knowledge-based
deliberation
Reflex system Actions
Figure 27.2 Compilation serves to convertdeliberative decision making into more effi-
cient,reflexivemechanisms.
systems move into more complex domains, all problems will become real-time, because the
agentwillneverhavelongenough tosolvethedecision problem exactly.
Clearly, thereisapressing needfor general methodsofcontrolling deliberation, rather
than specific recipes forwhattothink about ineach situation. The firstuseful idea isto em-
ANYTIME ploy anytimealgorithms (Deanand Boddy, 1988; Horvitz, 1987). Ananytimealgorithm is
ALGORITHM
an algorithm whose output quality improves gradually over time, so that it has a reasonable
decision readywheneveritisinterrupted. Suchalgorithms arecontrolled bya metalevel de-
cisionprocedurethatassesseswhetherfurthercomputationisworthwhile. (SeeSection3.5.4
for a brief description of metalevel decision making.) Example of an anytime algorithms
includeiterativedeepening ingame-treesearchandMCMCin Bayesiannetworks.
DECISION-
Thesecondtechniqueforcontrollingdeliberationisdecision-theoreticmetareasoning
THEORETIC
METAREASONING
(Russell and Wefald, 1989, 1991; Horvitz, 1989; Horvitz and Breese, 1996). This method
applies the theory of information value (Chapter 16) to the selection of individual computa-
tions. Thevalue ofacomputation depends onboth its cost (interms ofdelaying action) and
itsbenefits(intermsofimproveddecisionquality). Metareasoning techniquescanbeusedto
design better search algorithms and to guarantee that the algorithms have the anytime prop-
erty. Metareasoning isexpensive, ofcourse, andcompilation methods can beapplied sothat
theoverhead issmallcompared tothecostsofthecomputations being controlled. Metalevel
reinforcement learning mayprovide anotherwaytoacquire effective policies forcontrolling
deliberation: inessence,computationsthatleadtobetterdecisionsarereinforced,whilethose
that turn out to have no effect are penalized. This approach avoids the myopia problems of
thesimplevalue-of-information calculation.
REFLECTIVE Metareasoning is one specific example of a reflective architecture—that is, an archi-
ARCHITECTURE
tecturethatenablesdeliberationaboutthecomputationalentitiesandactionsoccurringwithin
the architecture itself. A theoretical foundation for reflective architectures can be built by
definingajointstatespacecomposedfromtheenvironment stateandthecomputational state
of the agent itself. Decision-making and learning algorithms can be designed that operate
over this joint state space and thereby serve to implement and improve the agent’s compu-
tational activities. Eventually, we expect task-specific algorithms such as alpha–beta search
andbackwardchainingtodisappearfromAIsystems,tobereplacedbygeneralmethodsthat
directtheagent’scomputations towardtheefficientgeneration ofhigh-quality decisions.
Section27.3. AreWeGoingintheRightDirection? 1049
27.3 ARE WE GOING IN THE RIGHT DIRECTION?
Theprecedingsectionlistedmanyadvancesandmanyopportunitiesforfurtherprogress. But
where is this all leading? Dreyfus (1992) gives the analogy of trying to get to the moon by
climbing a tree; one can report steady progress, all the way to the top of the tree. In this
section, weconsiderwhetherAI’scurrentpathismorelikea treeclimborarockettrip.
InChapter1,wesaidthatourgoalwastobuildagentsthatactrationally. However,we
alsosaidthat
...achievingperfectrationality—alwaysdoingtherightthing—isnotfeasibleincompli-
catedenvironments.Thecomputationaldemandsarejusttoohigh.Formostofthebook,
however,wewilladopttheworkinghypothesisthatperfectrationalityisagoodstarting
pointforanalysis.
Nowitistimetoconsider againwhatexactly thegoalofAIis. Wewanttobuild agents, but
withwhatspecification inmind? Herearefourpossibilities:
PERFECT Perfect rationality. A perfectly rational agent acts at every instant in such a way as to
RATIONALITY
maximizeitsexpectedutility,giventheinformationithasacquiredfromtheenvironment. We
have seen thatthe calculations necessary toachieve perfect rationality inmostenvironments
aretootimeconsuming, soperfectrationality isnotarealisticgoal.
CALCULATIVE Calculative rationality. This is the notion of rationality that we have used implicitly in de-
RATIONALITY
signinglogicalanddecision-theoretic agents,andmostoftheoreticalAIresearchhasfocused
on this property. A calculatively rational agent eventually returns what would have been the
rationalchoiceatthebeginningofitsdeliberation. Thisisaninterestingpropertyforasystem
to exhibit, but in most environments, the right answer at the wrong time is of no value. In
practice, AIsystemdesignersareforcedtocompromiseondecisionqualitytoobtainreason-
able overall performance; unfortunately, the theoretical basis of calculative rationality does
notprovideawell-founded waytomakesuchcompromises.
BOUNDED Boundedrationality. Herbert Simon(1957) rejected the notion ofperfect (oreven approx-
RATIONALITY
imately perfect) rationality and replaced it with bounded rationality, a descriptive theory of
decision makingbyrealagents. Hewrote,
Thecapacityofthe humanmindforformulatingandsolvingcomplexproblemsis very
smallcomparedwiththesizeoftheproblemswhosesolutionisrequiredforobjectively
rationalbehaviorintherealworld—orevenforareasonable approximationtosuchob-
jectiverationality.
He suggested that bounded rationality works primarily by satisficing—that is, deliberating
only long enough to come up with an answer that is “good enough.” Simon won the Nobel
Prizein economics forthis workand has written about itin depth (Simon, 1982). Itappears
to be a useful model of human behaviors in many cases. It is not a formal specification
for intelligent agents, however, because the definition of “good enough” is not given by the
theory. Furthermore,satisficingseemstobejustoneofalargerangeofmethodsusedtocope
withbounded resources.
1050 Chapter 27. AI:ThePresentandFuture
BOUNDED Bounded optimality (BO). A bounded optimal agent behaves as well as possible, given its
OPTIMALITY
computational resources. That is, the expected utility of the agent program for a bounded
optimalagentisatleastashighastheexpectedutilityofanyotheragentprogramrunningon
thesamemachine.
Ofthesefourpossibilities, boundedoptimalityseemstoofferthebesthopeforastrong
theoreticalfoundationforAI.Ithastheadvantageofbeingpossibletoachieve: thereisalways
atleast onebestprogram—something thatperfect rationality lacks. Bounded optimal agents
areactuallyusefulintherealworld,whereascalculativelyrationalagentsusuallyarenot,and
satisficingagentsmightormightnotbe,depending onhowambitioustheyare.
The traditional approach in AI has been to start with calculative rationality and then
makecompromises tomeetresource constraints. Ifthe problems imposed bytheconstraints
are minor, one would expect the final design to be similar to a BO agent design. But as the
resource constraints become more critical—for example, as the environment becomes more
complex—onewouldexpectthetwodesignstodiverge. Inthetheoryofboundedoptimality,
theseconstraints canbehandledinaprincipled fashion.
As yet, little is known about bounded optimality. It is possible to construct bounded
optimal programs for very simple machines and for somewhat restricted kinds of environ-
ments (Etzioni, 1989; Russell et al., 1993), but as yet we have no idea what BO programs
are like for large, general-purpose computers in complex environments. If there is to be a
constructive theory of bounded optimality, we have to hope that the design of bounded op-
timal programs does not depend too strongly on the details of the computer being used. It
would make scientific research very difficult if adding afew kilobytes of memory to a giga-
byte machine made a significant difference to the design of the BO program. One way to
make sure this cannot happen is to be slightly more relaxed about the criteria for bounded
optimality. Byanalogy with the notion of asymptotic complexity (Appendix A), wecan de-
ASYMPTOTIC
fine asymptotic bounded optimality (ABO) as follows (Russell and Subramanian, 1995).
BOUNDED
OPTIMALITY
Suppose a program P is bounded optimal for a machine M in a class of environments E,
(cid:2)
where the complexity of environments in E is unbounded. Then program P is ABOfor M
in E if it can outperform P by running on a machine kM that is k times faster (or larger)
than M. Unless k were enormous, we would be happy with a program that was ABO for
a nontrivial environment on a nontrivial architecture. There would be little point in putting
enormous effort into finding BO rather than ABO programs, because the size and speed of
available machinestendstoincrease byaconstant factorinafixedamountoftimeanyway.
Wecan hazard a guess that BO orABO programs for powerful computers in complex
environments willnotnecessarilyhaveasimple,elegantstructure. Wehavealreadyseenthat
general-purposeintelligencerequiressomereflexcapabilityandsomedeliberativecapability;
avarietyofformsofknowledgeanddecision making;learning andcompilationmechanisms
forallofthoseforms;methodsforcontrollingreasoning;andalargestoreofdomain-specific
knowledge. Abounded optimal agent must adapt tothe environment inwhich it findsitself,
so that eventually its internal organization will reflect optimizations that are specific to the
particular environment. This is only to be expected, and it is similar to the way in which
racing cars restricted by engine capacity have evolved into extremely complex designs. We
Section27.4. WhatIfAIDoesSucceed? 1051
suspect that a science of artificial intelligence based on bounded optimality will involve a
good deal of study of the processes that allow an agent program to converge to bounded
optimality andperhapslessconcentration onthedetailsof themessyprograms thatresult.
Insum,theconcept ofbounded optimality isproposed asaformaltaskforAIresearch
thatisbothwelldefinedandfeasible. Bounded optimality specifies optimal programs rather
than optimal actions. Actions are, after all, generated by programs, and it is over programs
thatdesigners havecontrol.
27.4 WHAT IF AI DOES SUCCEED?
InDavidLodge’sSmallWorld(1984),anovelabouttheacademicworldofliterarycriticism,
the protagonist causes consternation by asking a panel of eminent but contradictory literary
theorists the following question: “What if you were right?” None of the theorists seems to
haveconsideredthisquestionbefore,perhapsbecausedebatingunfalsifiabletheoriesisanend
initself. Similarconfusion canbeevoked byaskingAIresearchers, “Whatifyousucceed?”
As Section 26.3 relates, there are ethical issues to consider. Intelligent computers are
morepowerfulthandumbones,butwillthatpowerbeusedforgoodorill? Thosewhostrive
todevelopAIhavearesponsibility toseethattheimpactoftheirworkisapositiveone. The
scopeoftheimpactwilldependonthedegreeofsuccessofAI.EvenmodestsuccessesinAI
havealreadychangedthewaysinwhichcomputerscienceistaught(Stein,2002)andsoftware
development ispracticed. AIhasmadepossible newapplications suchasspeechrecognition
systems,inventory controlsystems,surveillance systems,robots, andsearchengines.
We can expect that medium-level successes in AI would affect all kinds of people in
theirdaily lives. Sofar, computerized communication networks, such ascell phones and the
Internet,havehadthiskindofpervasiveeffectonsociety,butAIhasnot. AIhasbeenatwork
behind the scenes—for example, in automatically approving or denying credit card transac-
tionsforeverypurchasemadeontheWeb—buthasnotbeenvisibletotheaverageconsumer.
Wecan imagine that truly useful personal assistants forthe office orthe home would have a
large positive impact on people’s lives, although they might cause some economic disloca-
tion in the short term. Automated assistants fordriving could prevent accidents, saving tens
of thousands of lives per year. A technological capability at this level might also be applied
to the development of autonomous weapons, which many view as undesirable. Some of the
biggestsocietal problemswefacetoday—such astheharnessing ofgenomicinformation for
treatingdisease,theefficientmanagementofenergyresources,andtheverificationoftreaties
concerning nuclearweapons—are beingaddressed withthehelpofAItechnologies.
Finally,itseemslikelythatalarge-scalesuccessinAI—thecreationofhuman-levelin-
telligence andbeyond—would change thelivesofamajorityofhumankind. Theverynature
ofourworkandplaywouldbealtered,aswouldourviewofintelligence, consciousness, and
thefuturedestinyofthehumanrace. AIsystemsatthislevelofcapability couldthreatenhu-
manautonomy,freedom,andevensurvival. Forthesereasons,wecannotdivorceAIresearch
fromitsethicalconsequences (seeSection26.3).
1052 Chapter 27. AI:ThePresentandFuture
Which waywillthe future go? Science fiction authors seem tofavor dystopian futures
over utopian ones, probably because they make for more interesting plots. But so far, AI
seemstofitinwithotherrevolutionarytechnologies(printing,plumbing,airtravel,telephony)
whosenegativerepercussions areoutweighedbytheirpositiveaspects.
In conclusion, wesee that AI has made great progress in its short history, but the final
sentence of Alan Turing’s (1950) essay on Computing Machinery and Intelligence is still
validtoday:
Wecanseeonlyashortdistance ahead,
butwecanseethatmuchremainstobedone.
A
MATHEMATICAL
BACKGROUND
A.1 COMPLEXITY ANALYSIS AND O() NOTATION
Computer scientists are often faced with the task of comparing algorithms to see how fast
theyrunorhowmuchmemorytheyrequire. Therearetwoapproaches tothistask. Thefirst
is benchmarking—running the algorithms on a computer and measuring speed in seconds
BENCHMARKING
and memory consumption in bytes. Ultimately, this is what really matters, but a benchmark
can be unsatisfactory because it is so specific: it measures the performance of a particular
program written inaparticular language, running onaparticular computer, withaparticular
compiler and particular input data. From the single result that the benchmark provides, it
can be difficult to predict how well the algorithm would do on a different compiler, com-
ANALYSISOF puter, or data set. The second approach relies on a mathematical analysis of algorithms,
ALGORITHMS
independently oftheparticularimplementation andinput, asdiscussed below.
A.1.1 Asymptoticanalysis
We will consider algorithm analysis through the following example, a program to compute
thesumofasequence ofnumbers:
functionSUMMATION(sequence)returnsanumber
sum←0
fori =1toLENGTH(sequence)do
sum←sum +sequence[i]
returnsum
The first step in the analysis is to abstract overthe input, in order to find some parameter or
parameters that characterize the size of the input. In this example, the input can be charac-
terized by the length of the sequence, which we will call n. The second step is to abstract
overtheimplementation, tofindsomemeasurethatreflectstherunningtimeofthealgorithm
butisnottiedtoaparticularcompilerorcomputer. FortheSUMMATION program,thiscould
be just the number of lines of code executed, or it could be more detailed, measuring the
numberofadditions, assignments, arrayreferences, andbranches executed bythealgorithm.
1053
1054 Appendix A. Mathematical background
Eitherwaygives usacharacterization ofthe total numberof steps taken by thealgorithm as
afunction ofthe size of the input. Wewill call this characterization T(n). If wecount lines
ofcode, wehaveT(n)=2n+2forourexample.
If all programs were as simple as SUMMATION, the analysis of algorithms would be a
trivial field. Buttwoproblems make itmorecomplicated. First, itisrare tofindaparameter
like n that completely characterizes the number of steps taken by an algorithm. Instead, the
best we can usually do is compute the worst case T (n) or the average case T (n).
worst avg
Computinganaverage meansthattheanalystmustassumesomedistribution ofinputs.
The second problem is that algorithms tend to resist exact analysis. In that case, it is
necessarytofallbackonanapproximation. WesaythattheSUMMATION algorithmisO(n),
meaning that its measure is at most a constant times n, with the possible exception of a few
smallvaluesofn. Moreformally,
T(n)isO(f(n))ifT(n)≤ kf(n)forsomek, foralln > n .
0
ASYMPTOTIC TheO()notation gives uswhatis called an asymptotic analysis. Wecan say without ques-
ANALYSIS
tionthat, asnasymptotically approaches infinity, anO(n)algorithm isbetterthanan O(n2)
algorithm. Asinglebenchmarkfigurecouldnotsubstantiate suchaclaim.
TheO()notation abstracts overconstant factors, whichmakes it easierto use, but less
precise, than the T() notation. Forexample, an O(n2) algorithm will always be worse than
anO(n)inthelong run, butifthetwoalgorithms are T(n2+1)and T(100n+1000),then
theO(n2)algorithm isactuallybetterfor n < 110.
Despite this drawback, asymptotic analysis is the most widely used tool for analyzing
algorithms. Itisprecisely because theanalysis abstracts overboththeexactnumberofoper-
ations (by ignoring the constant factor k) and the exact content of the input (by considering
onlyitssizen)thattheanalysisbecomesmathematicallyfeasible. TheO()notationisagood
compromisebetweenprecision andeaseofanalysis.
A.1.2 NP andinherently hardproblems
The analysis of algorithms and the O() notation allow us to talk about the efficiency of a
particularalgorithm. However,theyhavenothingtosayaboutwhethertherecouldbeabetter
COMPLEXITY algorithmfortheproblemathand. Thefieldofcomplexityanalysisanalyzesproblemsrather
ANALYSIS
thanalgorithms. Thefirstgrossdivisionisbetweenproblemsthatcanbesolvedinpolynomial
time and problems that cannot be solved in polynomial time, no matter what algorithm is
used. Theclassofpolynomialproblems—those whichcanbesolvedintimeO(nk)forsome
k—iscalledP.Thesearesometimescalled“easy”problems,becausetheclasscontainsthose
problems with running times like O(logn) and O(n). But it also contains those with time
O(n1000),sothename“easy”shouldnotbetakentooliterally.
Another important class of problems is NP, the class of nondeterministic polynomial
problems. Aproblem isinthis class ifthere issome algorithm thatcan guess asolution and
then verify whether the guess is correct in polynomial time. The idea is that if you have an
arbitrarily large numberofprocessors, sothat you can try allthe guesses atonce, oryou are
very lucky and always guess right the first time, then the NP problems become P problems.
Oneof thebiggest open questions in computer science is whetherthe class NPis equivalent
SectionA.2. Vectors,Matrices,andLinearAlgebra 1055
to the class P when one does not have the luxury of an infinite number of processors or
omniscientguessing. Mostcomputerscientistsareconvinced thatP(cid:7)=NP;thatNPproblems
areinherentlyhardandhavenopolynomial-time algorithms. Butthishasneverbeenproven.
ThosewhoareinterestedindecidingwhetherP=NPlookatasubclassofNPcalledthe
NP-complete problems. The word “complete” is used here in the sense of “most extreme”
NP-COMPLETE
and thus refers to the hardest problems in the class NP. It has been proven that either all
the NP-complete problems are in P or none of them is. This makes the class theoretically
interesting, but the class is also of practical interest because many important problems are
known to be NP-complete. An example is the satisfiability problem: given a sentence of
propositional logic, is there an assignment of truth values to the proposition symbols of the
sentence that makes it true? Unless a miracle occurs and P = NP, there can be no algorithm
that solves all satisfiability problems in polynomial time. However, AI is more interested in
whether there are algorithms that perform efficiently on typical problems drawn from a pre-
determineddistribution; aswesawinChapter7,therearealgorithmssuchasWALKSAT that
doquitewellonmanyproblems.
Theclassco-NPisthecomplementofNP,inthesensethat,foreverydecision problem
CO-NP
inNP,thereisacorresponding problem inco-NPwiththe“yes” and“no” answers reversed.
We know that P is a subset of both NP and co-NP, and it is believed that there are problems
inco-NPthatarenotinP.Theco-NP-completeproblemsarethehardestproblemsinco-NP.
CO-NP-COMPLETE
Theclass #P (pronounced “sharp P”)is the set of counting problems corresponding to
thedecision problems inNP.Decision problems haveayes-or-no answer: isthere asolution
tothis 3-SATformula? Counting problems have aninteger answer: how many solutions are
there to this 3-SAT formula? In some cases, the counting problem is much harder than the
decision problem. For example, deciding whether a bipartite graph has a perfect matching
canbedone intimeO(VE)(where thegraph has V vertices and E edges), butthecounting
problem“howmanyperfectmatchesdoesthisbipartitegraph have”is#P-complete,meaning
thatitishardasanyproblem in#PandthusatleastashardasanyNPproblem.
AnotherclassistheclassofPSPACEproblems—thosethatrequireapolynomialamount
ofspace,evenonanondeterministic machine. Itisbelieved thatPSPACE-hardproblemsare
worse than NP-complete problems, although it could turn out that NP = PSPACE,just as it
couldturnoutthatP=NP.
A.2 VECTORS, MATRICES, AND LINEAR ALGEBRA
Mathematicians define a vector as a memberof a vector space, but wewilluse amore con-
VECTOR
crete definition: avectorisanordered sequence ofvalues. Forexample, intwo-dimensional
space, we have vectors such as x=(cid:16)3,4(cid:17) and y=(cid:16)0,2(cid:17). We follow the convention of bold-
face characters for vector names, although some authors use arrows or bars over the names:
(cid:23)xory¯. Theelements ofavectorcan beaccessed using subscripts: z=(cid:16)z ,z ,...,z (cid:17). One
1 2 n
confusing point: this book is synthesizing work from many subfields, which variously call
theirsequencesvectors,lists,ortuples,andvariouslyusethenotations(cid:16)1,2(cid:17),[1,2],or(1,2).
1056 Appendix A. Mathematical background
The two fundamental operations on vectors are vector addition and scalar multiplica-
tion. Thevectoradditionx+yistheelementwisesum: x+y=(cid:16)3+0,4+2(cid:17)=(cid:16)3,6(cid:17). Scalar
multiplication multiplieseachelementbyaconstant: 5x=(cid:16)5×3,5×4(cid:17)=(cid:16)15,20(cid:17).
The length of a vector is denoted |x(cid:10) | and is computed by taking the square root of the
sumofthesquares oftheelements: |x|= (32 +42)=5. Thedotproduct x·y(also called
scalar product) of two vectors is the sum of the products of corresponding elements, that is,
(cid:2)
x·y= x y ,orinourparticularcase, x·y=3×0+4×2=8.
i i i
Vectors are often interpreted as directed line segments (arrows) in an n-dimensional
Euclidean space. Vector addition is then equivalent to placing the tail of one vector at the
head of the other, and the dot product x · y is equal to |x| |y| cosθ, where θ is the angle
betweenxandy.
A matrix is a rectangular array of values arranged into rows and columns. Here is a
MATRIX
matrixAofsize3×4:
⎛ ⎞
A A A A
1,1 1,2 1,3 1,4
⎝ ⎠
A A A A
2,1 2,2 2,3 2,4
A A A A
3,1 3,2 3,3 3,4
The first index of A specifies the row and the second the column. In programming lan-
i,j
guages, A isoftenwritten A[i,j]orA[i][j].
i,j
Thesumoftwomatricesisdefinedbyaddingtheircorrespondingelements;forexample
(A+B) =A +B . (ThesumisundefinedifAandBhavedifferentsizes.) Wecanalso
i,j i,j i,j
define the multiplication of a matrix by a scalar: (cA) =cA . Matrix multiplication (the
i,j i,j
productoftwomatrices)ismorecomplicated. Theproduct ABisdefinedonlyifAisofsize
a×band Bisofsizeb×c(i.e., thesecond matrix hasthe samenumberofrowsasthefirst
hascolumns);theresultisamatrixofsize a×c. Ifthematricesareofappropriate size,then
theresultis
(cid:12)
(AB) = A B .
i,k i,j j,k
j
Matrix multiplication is not commutative, even for square matrices: AB (cid:7)= BA in general.
It is, however, associative: (AB)C = A(BC). Notethat the dot product can be expressed in
termsofatranspose andamatrixmultiplication:x·y = x (cid:12) y.
TheidentitymatrixIhaselementsI equalto1wheni=j andequalto0otherwise.
IDENTITYMATRIX i,j
(cid:12)
It has the property that AI=A for all A. The transpose of A, written A is formed by
TRANSPOSE
(cid:12)
turningrowsintocolumnsandviceversa,or,moreformally, byA =A . Theinverseof
INVERSE i,j j,i
asquare matrix A is another square matrix A
−1
such that A
−1A=I.
Fora singular matrix,
SINGULAR
theinversedoesnotexist. Foranonsingular matrix,itcanbecomputedinO(n3)time.
Matricesareusedtosolvesystemsoflinearequations inO(n3)time;thetimeisdomi-
natedbyinvertingamatrixofcoefficients. Considerthefollowingsetofequations,forwhich
wewantasolution inx,y,andz:
+2x+y−z = 8
−3x−y+2z = −11
−2x+y+2z = −3.
SectionA.3. Probability Distributions 1057
Wecanrepresentthissystemasthematrixequation Ax= b,where
⎛ ⎞ ⎛ ⎞ ⎛ ⎞
2 1 −1 x 8
A = ⎝ −3 −1 2 ⎠ , x= ⎝ y ⎠ , b = ⎝ −11 ⎠ .
−2 1 2 z −3
TosolveAx = bwemultiplybothsidesbyA
−1,
yieldingA
−1Ax
= A
−1b,whichsimplifies
tox = A
−1b.
Afterinverting Aandmultiplying byb,wegettheanswer
⎛ ⎞ ⎛ ⎞
x 2
⎝ ⎠ ⎝ ⎠
x = y = 3 .
z −1
A.3 PROBABILITY DISTRIBUTIONS
Aprobability isameasureoverasetofeventsthatsatisfiesthreeaxioms:
1. The measure of each event is between 0 and 1. Wewrite this as 0 ≤ P(X=x ) ≤ 1,
i
where X is a random variable representing an event and x are the possible values of
i
X. In general, random variables are denoted by uppercase letters and their values by
lowercaseletters.
(cid:2)
2. Themeasureofthewholesetis1;thatis, n P(X=x )=1.
i=1 i
3. Theprobability of aunion of disjoint events isthe sum ofthe probabilities ofthe indi-
vidualevents;thatis,P(X=x ∨X=x )=P(X=x )+P(X=x ),wherex and
1 2 1 2 1
x aredisjoint.
2
A probabilistic modelconsists of a sample space of mutually exclusive possible outcomes,
togetherwithaprobabilitymeasureforeachoutcome. Forexample,inamodeloftheweather
tomorrow, the outcomes might be sunny, cloudy, rainy, and snowy. A subset of these out-
comesconstitutesanevent. Forexample,theeventofprecipitation isthesubsetconsistingof
{rainy,snowy}.
We use P(X) to denote the vector of values (cid:16)P(X=x ),...,P(X=x )(cid:17). We also
(cid:2) 1(cid:2) n
useP(x )asanabbreviation forP(X=x )and P(x)for n P(X=x ).
i i x i=1 i
TheconditionalprobabilityP(B|A)isdefinedasP(B∩A)/P(A). AandBarecondi-
tionally independent ifP(B|A)=P(B)(orequivalently, P(A|B)=P(A)). Forcontinuous
variables, thereareaninfinitenumberofvalues,andunless therearepointspikes, theproba-
PROBABILITY bility ofanyone value is0. Therefore, wedefinea probability densityfunction, which we
DENSITYFUNCTION
also denote as P(·), but which has aslightly different meaning from thediscrete probability
function. Thedensity function P(x)forarandom variable X, whichmightbethought ofas
P(X=x), is intuitively defined as the ratio of the probability that X falls into an interval
around x,dividedbythewidthoftheinterval, astheinterval widthgoestozero:
P(x)= lim P(x ≤ X ≤ x+dx)/dx.
dx→0
1058 Appendix A. Mathematical background
Thedensityfunction mustbenonnegative forallxandmusthave
(cid:26)
∞
P(x)dx=1.
−∞
CUMULATIVE We can also define a cumulative probability density function F (x), which is the proba-
PROBABILITY X
DENSITYFUNCTION
bilityofarandom variablebeinglessthan x:
(cid:26)
x
F (x) = P(X ≤ x) = P(u)du.
X
−∞
Notethattheprobability densityfunctionhasunits,whereasthediscreteprobability function
isunitless. Forexample,ifvaluesofX aremeasuredinseconds,thenthedensityismeasured
in Hz (i.e., 1/sec). If values of X are points in three-dimensional space measured in meters,
thendensity ismeasuredin1/m3.
GAUSSIAN Oneof the most important probability distributions is the Gaussian distribution, also
DISTRIBUTION
knownasthenormaldistribution. AGaussian distribution withmean μandstandard devi-
ationσ (andtherefore variance σ2)isdefinedas
1
P(x)= √ e −(x−μ)2/(2σ2) ,
σ 2π
where x is a continuous variable ranging from −∞ to +∞. With mean μ=0 and variance
STANDARDNORMAL σ2=1,wegetthespecialcaseofthestandardnormaldistribution. Foradistribution over
DISTRIBUTION
MULTIVARIATE avectorxinndimensions, thereisthemultivariateGaussiandistribution:
GAUSSIAN “ ”
P(x)= (cid:10) 1 e −1 2 (x−μ )(cid:4)Σ−1 (x−μ ) ,
(2π)n|Σ|
whereμisthemeanvectorandΣisthecovariance matrix(seebelow).
CUMULATIVE In one dimension, we can define the cumulative distribution function F(x) as the
DISTRIBUTION
probability thatarandomvariable willbelessthan x. Forthenormaldistribution, thisis
(cid:26)x
1 z−μ
F(x)= P(z)dz= (1+erf( √ )),
2 σ 2
−∞
whereerf(x)istheso-called errorfunction,whichhasnoclosed-form representation.
CENTRALLIMIT The central limit theorem states that the distribution formed by sampling n indepen-
THEOREM
dent random variables and taking their mean tends to a normal distribution as n tends to
infinity. Thisholdsforalmostanycollectionofrandomvariables, eveniftheyarenotstrictly
independent, unlessthevarianceofanyfinitesubsetofvariables dominatestheothers.
The expectation of a random variable, E(X), is the mean or average value, weighted
EXPECTATION
bytheprobability ofeachvalue. Foradiscrete variableitis:
(cid:12)
E(X)= x P(X=x ).
i i
i
Foracontinuousvariable,replacethesummationwithanintegralovertheprobabilitydensity
function, P(x):
(cid:26)∞
E(X)= xP(x)dx,
−∞
Bibliographical andHistorical Notes 1059
Therootmeansquare,RMS,ofasetofvalues(oftensamplesofarandomvariable)is
ROOTMEANSQUARE
thesquarerootofthemeanofthesquares ofthevalues,
(cid:8)
x2+...+x2
RMS(x ,...,x )= 1 n .
1 n
n
Thecovarianceoftworandomvariablesistheexpectationoftheproductoftheirdifferences
COVARIANCE
fromtheirmeans:
cov(X,Y) = E((X −μ )(Y −μ )).
X Y
The covariance matrix, often denoted Σ, is a matrix of covariances between elements of a
COVARIANCEMATRIX
vector of random variables. Given X = (cid:16)X ,...X (cid:17)(cid:12) , the entries of the covariance matrix
1 n
areasfollows:
Σ = cov(X ,X ) = E((X −μ )(X −μ )).
i,j i j i i j j
Afew more miscellaneous points: weuse log(x)forthe natural logarithm, log (x). Weuse
e
argmax f(x)forthevalueofxforwhichf(x)ismaximal.
x
BIBLIOGRAPHICAL AND HISTORICAL NOTES
TheO()notationsowidelyusedincomputersciencetodaywasfirstintroducedinthecontext
of numbertheory by the Germanmathematician P.G.H. Bachmann (1894). Theconcept of
NP-completeness was invented by Cook (1971), and the modern method for establishing a
reduction fromoneproblem toanotherisduetoKarp(1972). CookandKarphavebothwon
theTuringaward,thehighesthonorincomputerscience, for theirwork.
Classic works on the analysis and design ofalgorithms include those by Knuth (1973)
and Aho, Hopcroft, and Ullman (1974); more recent contributions are by Tarjan (1983) and
Cormen, Leiserson, and Rivest (1990). These books place an emphasis on designing and
analyzing algorithms to solve tractable problems. For the theory of NP-completeness and
other forms of intractability, see Garey and Johnson (1979) or Papadimitriou (1994). Good
textsonprobability include Chung(1979), Ross(1988), andBertsekasandTsitsiklis (2008).
B
NOTES ON LANGUAGES
AND ALGORITHMS
B.1 DEFINING LANGUAGES WITH BACKUS–NAUR FORM (BNF)
In this book, we define several languages, including the languages of propositional logic
(page 243), first-order logic (page 293), and a subset of English (page 899). A formal lan-
guageisdefinedasasetofstringswhereeachstringisasequenceofsymbols. Thelanguages
weare interested inconsist of aninfinite set of strings, soweneed a concise way tocharac-
terizetheset. Wedothatwithagrammar. Theparticulartypeofgrammarweuseiscalleda
CONTEXT-FREE context-freegrammar,becauseeachexpressionhasthesameforminanycontext. Wewrite
GRAMMAR
BACKUS–NAUR ourgrammarsinaformalism called Backus–Naurform(BNF).Therearefourcomponents
FORM(BNF)
toaBNFgrammar:
• Asetofterminalsymbols. Thesearethesymbolsorwordsthatmakeupthestringsof
TERMINALSYMBOL
thelanguage. Theycouldbeletters(A,B,C,...)orwords(a,aardvark,abacus,...),
orwhateversymbolsareappropriate forthedomain.
NONTERMINAL • A set of nonterminal symbols that categorize subphrases of the language. Forexam-
SYMBOL
ple, the nonterminal symbol NounPhrase in English denotes an infinite set of strings
including “you”and“thebigslobberydog.”
• A start symbol, which is the nonterminal symbol that denotes the complete set of
STARTSYMBOL
strings of the language. InEnglish, this is Sentence; forarithmetic, it might be Expr,
andforprogramming languages itis Program.
• A set of rewrite rules, of the form LHS → RHS, where LHS is a nonterminal
symboland RHS isasequence ofzero ormoresymbols. These canbeeitherterminal
ornonterminal symbols, orthesymbol (cid:2),whichisusedtodenotetheemptystring.
Arewriteruleoftheform
Sentence → NounPhrase VerbPhrase
means that whenever wehave twostrings categorized asa NounPhrase and a VerbPhrase,
we can append them together and categorize the result as a Sentence. As an abbreviation,
thetworules(S → A)and(S → B)canbewritten(S → A|B).
1060
SectionB.2. DescribingAlgorithmswithPseudocode 1061
HereisaBNFgrammarforsimplearithmetic expressions:
Expr → Expr Operator Expr | ( Expr ) | Number
Number → Digit | Number Digit
Digit → 0|1| 2| 3|4| 5| 6|7| 8| 9
Operator → + | − | ÷ | ×
We cover languages and grammars in more detail in Chapter 22. Be aware that other books
useslightlydifferentnotationsforBNF;forexample,youmightsee(cid:16)Digit(cid:17)insteadofDigit
foranonterminal, ‘word’insteadof wordforaterminal, or::=instead of → inarule.
B.2 DESCRIBING ALGORITHMS WITH PSEUDOCODE
Thealgorithms inthisbookaredescribed inpseudocode. Mostofthepseudocode should be
familiar to users of languages like Java, C++, or Lisp. In some places we use mathematical
formulasorordinaryEnglishtodescribepartsthatwouldotherwisebemorecumbersome. A
fewidiosyncrasies shouldbenoted.
• Persistent variables: Weuse thekeyword persistent tosaythat avariable isgiven an
initialvaluethefirsttimeafunctioniscalledandretainsthatvalue(orthevaluegivento
itbyasubsequent assignment statement) onallsubsequent calls tothe function. Thus,
persistent variables are like global variables in that they outlive a single call to their
function, but they are accessible only within the function. The agent programs in the
book use persistent variables for memory. Programs with persistent variables can be
implemented as objects in object-oriented languages such as C++, Java, Python, and
Smalltalk. In functional languages, they can be implemented by functional closures
overanenvironment containing therequiredvariables.
• Functionsas values: Functions and procedures have capitalized names, and variables
have lowercase italic names. So most of the time, a function call looks like FN(x).
However,weallowthevalueofavariabletobeafunction; for example, ifthevalueof
thevariable f isthesquarerootfunction, then f(9)returns 3.
• for each: The notation “for each x in c do” means that the loop is executed with the
variable xboundtosuccessive elementsofthecollection c.
• Indentation is significant: Indentation is used to mark the scope of a loop or condi-
tional,asinthelanguagePython,andunlikeJavaandC++(whichusebraces)orPascal
andVisualBasic(whichuseend).
• Destructuringassignment: Thenotation“x,y←pair”meansthattheright-handside
must evaluate to a two-element tuple, and the first element is assigned to x and the
second to y. The same idea is used in “for each x,y in pairs do” and can be used to
swaptwovariables: “x,y←y,x”
• Generators and yield: the notation “generator G(x) yields numbers” defines G as a
generatorfunction. Thisisbestunderstoodbyanexample. Thecodefragmentshownin
1062 Appendix B. NotesonLanguages andAlgorithms
generatorPOWERS-OF-2()yieldsints
i←1
whiletrue do
yieldi
i←2 × i
forp inPOWERS-OF-2()do
PRINT(p)
FigureB.1 Exampleofageneratorfunctionanditsinvocationwithinaloop.
FigureB.1prints the numbers 1, 2, 4, ...,and neverstops. Thecallto POWERS-OF-2
returnsagenerator, whichinturnyieldsonevalueeachtime theloopcodeasksforthe
next element of the collection. Even though the collection is infinite, it is enumerated
oneelementatatime.
• Lists: [x,y,z] denotes a list of three elements. [first|rest] denotes a list formed by
addingfirst tothelistrest. InLisp,thisistheconsfunction.
• Sets: {x,y,z}denotesasetofthreeelements. {x : p(x)}denotesthesetofallelements
xforwhichp(x)istrue.
• Arrays start at 1: Unless stated otherwise, the first index of an array is 1 as in usual
mathematicalnotation, not0,asinJavaandC.
B.3 ONLINE HELP
Most of the algorithms in the book have been implemented in Java, Lisp, and Python at our
onlinecoderepository:
aima.cs.berkeley.edu
The same Web site includes instructions for sending comments, corrections, or suggestions
forimprovingthebook,andforjoiningdiscussion lists.
Bibliography
Thefollowingabbreviationsareusedforfrequentlycitedconferencesandjournals:
AAAI ProceedingsoftheAAAIConferenceonArtificialIntelligence
AAMAS ProceedingsoftheInternationalConferenceonAutonomousAgentsandMulti-agentSystems
ACL ProceedingsoftheAnnualMeetingoftheAssociationforComputationalLinguistics
AIJ ArtificialIntelligence
AIMag AIMagazine
AIPS ProceedingsoftheInternationalConferenceonAIPlanningSystems
BBS BehavioralandBrainSciences
CACM CommunicationsoftheAssociationforComputingMachinery
COGSCI ProceedingsoftheAnnualConferenceoftheCognitiveScienceSociety
COLING ProceedingsoftheInternationalConferenceonComputationalLinguistics
COLT ProceedingsoftheAnnualACMWorkshoponComputationalLearningTheory
CP ProceedingsoftheInternationalConferenceonPrinciplesandPracticeofConstraintProgramming
CVPR ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
EC ProceedingsoftheACMConferenceonElectronicCommerce
ECAI ProceedingsoftheEuropeanConferenceonArtificialIntelligence
ECCV ProceedingsoftheEuropeanConferenceonComputerVision
ECML ProceedingsoftheTheEuropeanConferenceonMachineLearning
ECP ProceedingsoftheEuropeanConferenceonPlanning
FGCS ProceedingsoftheInternationalConferenceonFifthGenerationComputerSystems
FOCS ProceedingsoftheAnnualSymposiumonFoundationsofComputerScience
ICAPS ProceedingsoftheInternationalConferenceonAutomatedPlanningandScheduling
ICASSP ProceedingsoftheInternationalConferenceonAcoustics,Speech,andSignalProcessing
ICCV ProceedingsoftheInternationalConferenceonComputerVision
ICLP ProceedingsoftheInternationalConferenceonLogicProgramming
ICML ProceedingsoftheInternationalConferenceonMachineLearning
ICPR ProceedingsoftheInternationalConferenceonPatternRecognition
ICRA ProceedingsoftheIEEEInternationalConferenceonRoboticsandAutomation
ICSLP ProceedingsoftheInternationalConferenceonSpeechandLanguageProcessing
IJAR InternationalJournalofApproximateReasoning
IJCAI ProceedingsoftheInternationalJointConferenceonArtificialIntelligence
IJCNN ProceedingsoftheInternationalJointConferenceonNeuralNetworks
IJCV InternationalJournalofComputerVision
ILP ProceedingsoftheInternationalWorkshoponInductiveLogicProgramming
ISMIS ProceedingsoftheInternationalSymposiumonMethodologiesforIntelligentSystems
ISRR ProceedingsoftheInternationalSymposiumonRoboticsResearch
JACM JournaloftheAssociationforComputingMachinery
JAIR JournalofArtificialIntelligenceResearch
JAR JournalofAutomatedReasoning
JASA JournaloftheAmericanStatisticalAssociation
JMLR JournalofMachineLearningResearch
JSL JournalofSymbolicLogic
KDD ProceedingsoftheInternationalConferenceonKnowledgeDiscoveryandDataMining
KR ProceedingsoftheInternationalConferenceonPrinciplesofKnowledgeRepresentationandReasoning
LICS ProceedingsoftheIEEESymposiumonLogicinComputerScience
NIPS AdvancesinNeuralInformationProcessingSystems
PAMI IEEETransactionsonPatternAnalysisandMachineIntelligence
PNAS ProceedingsoftheNationalAcademyofSciencesoftheUnitedStatesofAmerica
PODS ProceedingsoftheACMInternationalSymposiumonPrinciplesofDatabaseSystems
SIGIR ProceedingsoftheSpecialInterestGrouponInformationRetrieval
SIGMOD ProceedingsoftheACMSIGMODInternationalConferenceonManagementofData
SODA ProceedingsoftheAnnualACM–SIAMSymposiumonDiscreteAlgorithms
STOC ProceedingsoftheAnnualACMSymposiumonTheoryofComputing
TARK ProceedingsoftheConferenceonTheoreticalAspectsofReasoningaboutKnowledge
UAI ProceedingsoftheConferenceonUncertaintyinArtificialIntelligence
1063
1064 Bibliography
Aarup, M., Arentoft, M. M., Parrod, Y., Stader, Alekhnovich, M.,Hirsch,E.A.,andItsykson,D. Appelt,D.(1999). Introductiontoinformationex-
J., and Stokes, I. (1994). OPTIMUM-AIV: (2005). Exponentiallowerboundsfortherunning traction.CACM,12(3),161–172.
Aknowledge-basedplanningandschedulingsystem time of DPLL algorithms on satisfiable formulas.
Apt,K.R.(1999). Theessenceofconstraintprop-
forspacecraftAIV. InFox, M.andZweben, M. JAR,35(1–3),51–72.
agation. TheoreticalComputerScience,221(1–2),
(Eds.),KnowledgeBasedScheduling.MorganKauf-
Allais, M. (1953). Lecomportment de l’homme 179–210.
mann.
rationneldevantlarisque: critiquedespostulatset Apt, K.R.(2003). PrinciplesofConstraintPro-
Abney, S. (2007). Semisupervised Learning for axiomesdel’e´coleAme´ricaine. Econometrica,21, gramming.CambridgeUniversityPress.
ComputationalLinguistics.CRCPress. 503–546.
Apte´,C.,Damerau,F.,andWeiss,S.(1994). Auto-
Abramson, B.andYung,M.(1989). Divideand Allen,J.F.(1983). Maintainingknowledgeabout matedlearningofdecisionrulesfortextcategoriza-
conquerunderglobalconstraints:Asolutiontothe temporalintervals.CACM,26(11),832–843. tion.ACMTransactionsonInformationSystems,12,
N-queensproblem.J.ParallelandDistributedCom-
puting,6(3),649–662. Allen,J.F.(1984). Towardsageneraltheoryofac- 233–251.
tionandtime.AIJ,23,123–154. Arbuthnot, J. (1692). Of the Laws of Chance.
Achlioptas, D.(2009). Randomsatisfiability. In
Biere,A.,Heule,M.,vanMaaren,H.,andWalsh,T. Allen,J.F.(1991).Timeandtimeagain:Themany Motte,London. TranslationintoEnglish,withad-
(Eds.),HandbookofSatisfiability.IOSPress. waystorepresenttime.Int.J.IntelligentSystems,6, ditions,ofHuygens(1657).
341–355. Archibald, C., Altman, A., and Shoham, Y.
Achlioptas,D.,Beame,P.,andMolloy,M.(2004).
E ity xp th o r n e e s n h t o ia ld l . bo In un S d O s D fo A r -0 D 4 P . LLbelowthesatisfiabil- A Re ll a e d n i , n J g . s F i . n ,H Pl e a n n d n l i e n r, g. J. M ,a o n r d ga T n at K e, a A uf . m (E an d n s. . ).(1990). ( li 2 a 0 rd 0 s 9) p . lay A e n r. al I y n si I s JC o A f I a -0 w 9. inningcomputationalbil-
A O c n h t l h io e p m ta a s x , im D u . m , N sa a t o is r, fia A b . i , lit a y n o d f P ra e n re d s o , m Y f . or ( m 20 u 0 la 7 s ) . . A co l n li n s e , c L t . fo ( u 1 r 9 . 8 T 8 h ). eg A am k e n i o s w s l o e l d v g ed e- : b W as h e i d te a w pp in ro s. ac M h a t s o - A ed r i i t e io ly n , ). D H . a (2 rp 0 e 0 r 9 . ). PredictablyIrrational(Revised
JACM,54(2). ter’sthesis,VrijeUniv.,Amsterdam. Arkin,R.(1998). Behavior-BasedRobotics. MIT
Press.
Achlioptas,D.andPeres,Y.(2004). Thethreshold Almuallim,H.andDietterich,T.(1991). Learning
forrandomk-SATis2klog2−o(k).J.American withmanyirrelevantfeatures. InAAAI-91,Vol.2, Armando, A.,Carbone,R.,Compagna,L.,Cuel-
MathematicalSociety,17(4),947–973. pp.547–552. lar,J.,andTobarra,L.(2008). Formalanalysisof
SAML2.0webbrowsersinglesign-on: Breaking
Ackley, D.H.andLittman, M.L.(1991). Inter- ALPAC(1966). Languageandmachines: Com- theSAML-basedsinglesign-onforgoogleapps. In
actionsbetweenlearningandevolution. InLang- puters in translation and linguistics. Tech. rep. FMSE ’08: Proc. 6th ACM workshopon Formal
ton, C.,Taylor, C., Farmer, J.D., andRamussen, 1416, The Automatic Language ProcessingAdvi- methodsinsecurityengineering,pp.1–10.
S.(Eds.),ArtificialLifeII,pp.487–509.Addison- soryCommitteeofthe NationalAcademyofSci-
Wesley. ences. Arnauld,A.(1662).Lalogique,oul’artdepenser.
ChezCharlesSavreux,aupieddelaTourdeNostre
Adelson-Velsky,G.M.,Arlazarov,V.L.,Bitman, Alterman,R.(1988).Adaptiveplanning.Cognitive Dame,Paris.
A.R.,Zhivotovsky,A.A.,andUskov,A.V.(1970). Science,12,393–422.
Programming acomputer toplaychess. Russian Arora,S.(1998). Polynomialtimeapproximation
MathematicalSurveys,25,221–262. Amarel, S. (1967). An approach to heuristic schemesforEuclideantravelingsalesmanandother
problem-solvingandtheoremprovinginthepropo- geometricproblems.JACM,45(5),753–782.
Adida,B.andBirbeck,M.(2008). RDFaprimer. sitional calculus. In Hart, J. and Takasu, S.
Tech.rep.,W3C. (Eds.),SystemsandComputerScience.Universityof Arunachalam,R.andSadeh,N.M.(2005). The
TorontoPress. supplychaintradingagentcompetition. Electronic
Agerbeck,C.andHansen,M.O.(2008). Amulti- CommerceResearchandApplications,Spring,66–
agentapproachtosolving NP-completeproblems. Amarel, S. (1968). On representations of prob- 84.
Master’sthesis,TechnicalUniv.ofDenmark. lems of reasoning about actions. In Michie, D.
(Ed.),MachineIntelligence3,Vol.3,pp.131–171. Ashby,W.R.(1940).Adaptivenessandequilibrium.
Aggarwal,G.,Goel,A.,andMotwani,R.(2006). Elsevier/North-Holland. J.MentalScience,86,478–483.
Truthfulauctionsforpricingsearchkeywords. In
EC-06,pp.1–7. Amir,E.andRussell,S.J.(2003).Logicalfiltering. Ashby,W.R.(1948).Designforabrain.Electronic
Agichtein, E.andGravano, L.(2003). Querying InIJCAI-03. Engineering,December,379–383.
textdatabasesforefficientinformationextraction.In Amit, D., Gutfreund, H., and Sompolinsky, H. Ashby,W.R.(1952).DesignforaBrain.Wiley.
Proc.IEEEConferenceonDataEngineering. (1985).Spin-glassmodelsofneuralnetworks.Phys- Asimov,I.(1942).Runaround.AstoundingScience
Agmon,S.(1954). Therelaxationmethodforlin- icalReview,A32,1007–1018. Fiction,March.
earinequalities. CanadianJournalofMathematics, Andersen,S.K.,Olesen,K.G.,Jensen,F.V.,and Asimov,I.(1950).I,Robot.Doubleday.
6(3),382–392. Jensen, F. (1989). HUGIN—Ashellforbuilding
Astrom,K.J.(1965). OptimalcontrolofMarkov
Agre,P.E.andChapman,D.(1987). Pengi:anim- Bayesian belief universes for expert systems. In decisionprocesseswithincompletestateestimation.
plementationofatheoryofactivity.InIJCAI-87,pp. IJCAI-89,Vol.2,pp.1080–1085. J.Math.Anal.Applic.,10,174–205.
268–272. Anderson,J.R.(1980). CognitivePsychologyand Audi,R.(Ed.).(1999). TheCambridgeDictionary
Aho,A.V.,Hopcroft,J.,andUllman,J.D.(1974). ItsImplications.W.H.Freeman. ofPhilosophy.CambridgeUniversityPress.
T A h d e di D so e n s - ig W n es a l n e d y. AnalysisofComputerAlgorithms. A tio n n d . e H rs a o r n va , r J d .R U . n ( i 1 v 9 er 8 s 3 it ) y . P T r h e e ss A . rchitectureofCogni- B A a x s e i l c ro B d o , o R k . s. (1985). TheEvolutionofCooperation.
Aizerman, M., Braverman, E., andRozonoer, L. Andoni,A.andIndyk,P.(2006).Near-optimalhash- Baader,F.,Calvanese,D.,McGuinness,D.,Nardi,
( f to 1 u m 9 n 6 c a t 4 t i i ) o o . n n m a T n e h t d h e o R o d r e e m i t n i o c p t a e a l t C t f e o o r u n n n tr r d o e a c l t , o i 2 o g 5 n n , s it 8 i o o 2 f n 1– l t e 8 h a 3 e r 7 n . p in o g te . n A ti u a - l i h n ig g h a d lg im or e it n h s m io s ns fo . r In ap F p O ro C x S im -0 a 6 t . enearestneighborin D Lo ., g a ic nd H P a a n t d e b l- o S o c k hn (2 e n id d e e r, d P it . io (2 n 0 ). 07 C ). am Th b e rid D g e e sc U r n ip iv ti e o r n -
Andre,D.andRussell,S.J.(2002).Stateabstraction sityPress.
Al-Chang,M.,Bresina,J.,Charest,L.,Chase,A., forprogrammablereinforcementlearningagents.In Baader,F.andSnyder,W.(2001). Unificationthe-
Hsu,J.,Jonsson,A.,Kanefsky,B.,Morris,P.,Rajan, AAAI-02,pp.119–125. ory.InRobinson,J.andVoronkov,A.(Eds.),Hand-
K.,Yglesias,J.,Chafin,B.,Dias,W.,andMaldague,
P.(2004).MAPGEN:Mixed-Initiativeplanningand Anthony,M.andBartlett,P.(1999). NeuralNet- bookofAutomatedReasoning,pp.447–533.Else-
schedulingfortheMarsExplorationRovermission. work Learning: Theoretical Foundations. Cam- vier.
IEEEIntelligentSystems,19(1),8–12. bridgeUniversityPress. Bacchus,F.(1990). RepresentingandReasoning
Albus,J.S.(1975).Anewapproachtomanipulator Aoki,M.(1965). Optimalcontrolofpartiallyob- withProbabilisticKnowledge.MITPress.
control:Thecerebellarmodelarticulationcontroller servable Markov systems. J. Franklin Institute, Bacchus,F.andGrove,A.(1995).Graphicalmodels
(CMAC). J.DynamicSystems,Measurement,and 280(5),367–386. forpreferenceandutility.InUAI-95,pp.3–10.
Control,97,270–277.
Appel,K.andHaken,W.(1977).Everyplanarmap Bacchus,F.andGrove,A.(1996). Utilityindepen-
Aldous,D.andVazirani,U.(1994). “Gowiththe is fourcolorable: PartI:Discharging. Illinois J. denceinaqualitativedecisiontheory.InKR-96,pp.
winners”algorithms.InFOCS-94,pp.492–501. Math.,21,429–490. 542–552.
Bibliography 1065
Bacchus,F.,Grove,A.,Halpern,J.Y.,andKoller, Barto,A.G.,Bradtke,S.J.,andSingh,S.P.(1995). Bellman,R.E.(1961).AdaptiveControlProcesses:
D.(1992). Fromstatisticstobeliefs. InAAAI-92, Learningtoactusingreal-timedynamicprogram- AGuidedTour.PrincetonUniversityPress.
pp.602–608. ming.AIJ,73(1),81–138.
Bellman,R.E.(1965). Ontheapplicationofdy-
Bacchus,F.andvanBeek,P.(1998).Ontheconver- Barto,A.G.,Sutton,R.S.,andAnderson,C.W. namicprogrammingtothedeterminationofoptimal
sionbetweennon-binaryandbinaryconstraintsatis- (1983).Neuron-likeadaptiveelementsthatcansolve playinchessandcheckers.PNAS,53,244–246.
factionproblems.InAAAI-98,pp.311–318. difficultlearningcontrolproblems. IEEETransac-
Bellman,R.E.(1978).AnIntroductiontoArtificial
tionsonSystems,ManandCybernetics,13, 834–
Bacchus,F.andvanRun,P.(1995). Dynamicvari- Intelligence:CanComputersThink?Boyd&Fraser
846.
ableorderinginCSPs.InCP-95,pp.258–275. PublishingCompany.
Barto, A. G., Sutton, R. S., and Brouwer, P. S.
Bachmann,P.G.H.(1894).DieanalytischeZahlen- Bellman,R.E.(1984).EyeoftheHurricane.World
(1981). Associativesearchnetwork: Areinforce-
theorie.B.G.Teubner,Leipzig. Scientific.
mentlearningassociativememory. BiologicalCy-
Backus,J.W.(1996).Transcriptofquestionandan- bernetics,40(3),201–211. Bellman,R.E.andDreyfus,S.E.(1962). Applied
swersession. InWexelblat,R.L.(Ed.),Historyof DynamicProgramming.PrincetonUniversityPress.
Barwise,J.andEtchemendy,J.(1993). TheLan-
ProgrammingLanguages,p.162.AcademicPress. guageofFirst-OrderLogic: IncludingtheMacin- Bellman, R. E. (1957). Dynamic Programming.
Bagnell,J.A.andSchneider,J.(2001).Autonomous toshProgramTarski’sWorld4.0(ThirdRevisedand PrincetonUniversityPress.
helicoptercontrolusingreinforcementlearningpol- Expandededition).CenterfortheStudyofLanguage Belongie, S., Malik, J., and Puzicha, J. (2002).
icysearchmethods.InICRA-01. andInformation(CSLI). Shapematchingandobjectrecognitionusingshape
Baker, J. (1975). The Dragon system—An Barwise,J.andEtchemendy,J.(2002). Language, contexts.PAMI,24(4),509–522.
overview.IEEETransactionsonAcoustics;Speech; ProofandLogic.CSLI(Univ.ofChicagoPress). Ben-Tal,A.andNemirovski,A.(2001).Lectureson
andSignalProcessing,23,24–29. Baum,E.,Boneh,D.,andGarrett,C.(1995). On ModernConvexOptimization:Analysis,Algorithms,
Baker,J.(1979). Trainablegrammarsforspeech geneticalgorithms.InCOLT-95,pp.230–239. andEngineeringApplications. SIAM(Societyfor
recognition. InSpeechCommunicationPapersfor IndustrialandAppliedMathematics).
Baum,E.andHaussler,D.(1989). Whatsizenet
the97thMeetingoftheAcousticalSocietyofAmer- gives valid generalization? Neural Computation, Bengio, Y.andLeCun, Y.(2007). Scalinglearn-
ica,pp.547–550. 1(1),151–160. ingalgorithmstowardsAI. InBottou,L.,Chapelle,
Baldi, P., Chauvin, Y., Hunkapiller, T., and Mc- O.,DeCoste,D.,andWeston,J.(Eds.),Large-Scale
Clure,M.(1994). HiddenMarkovmodelsofbio- Baum,E.andSmith,W.D.(1997).ABayesianap- KernelMachines.MITPress.
proachtorelevanceingameplaying. AIJ,97(1–2),
logicalprimarysequenceinformation.PNAS,91(3), 195–242. Bentham,J.(1823).PrinciplesofMoralsandLegis-
1059–1063. lation.OxfordUniversityPress,Oxford,UK.Orig-
Baldwin,J.M.(1896). Anewfactorinevolution. Baum,E.andWilczek,F.(1988).Supervisedlearn- inalworkpublishedin1789.
ingofprobabilitydistributionsbyneuralnetworks.
AmericanNaturalist, 30, 441–451. Continuedon InAnderson,D.Z.(Ed.),NeuralInformationPro- Berger,J.O.(1985). StatisticalDecisionTheory
pages536–553. cessingSystems,pp.52–61.AmericanInstituteof andBayesianAnalysis.SpringerVerlag.
Ballard,B.W.(1983). The*-minimaxsearchpro- Physics. Berkson,J.(1944).Applicationofthelogisticfunc-
cedurefortreescontainingchancenodes.AIJ,21(3), Baum, L. E. and Petrie, T. (1966). Statistical tiontobio-assay.JASA,39,357–365.
327–350.
inference for probabilistic functions offinite state Berlekamp,E.R.,Conway,J.H.,andGuy,R.K.
Baluja,S.(1997). Geneticalgorithmsandexplicit Markovchains. AnnalsofMathematicalStatistics, (1982). Winning Ways, For Your Mathematical
searchstatistics.InMozer,M.C.,Jordan,M.I.,and 41. Plays.AcademicPress.
Petsche,T.(Eds.),NIPS9,pp.319–325.MITPress.
Baxter, J.andBartlett, P.(2000). Reinforcement Berlekamp,E.R.andWolfe,D.(1994).Mathemat-
Bancilhon, F., Maier, D., Sagiv, Y., andUllman, learninginPOMDP’sviadirectgradientascent. In icalGo:ChillingGetstheLastPoint.A.K.Peters.
J.D.(1986). Magicsetsandotherstrangewaysto ICML-00,pp.41–48.
implementlogicprograms.InPODS-86,pp.1–16. Berleur, J.andBrunnstein, K.(2001). Ethicsof
Bayardo, R. J. and Miranker, D. P. (1994). An Computing:Codes,SpacesforDiscussionandLaw.
Banko,M.andBrill,E.(2001).Scalingtoveryvery optimalbacktrackalgorithmfortree-structuredcon- ChapmanandHall.
largecorporafornaturallanguagedisambiguation. straintsatisfactionproblems.AIJ,71(1),159–181.
InACL-01,pp.26–33. Berliner,H.J.(1979).TheB*treesearchalgorithm:
Bayardo, R.J.andSchrag, R.C.(1997). Using Abest-firstproofprocedure.AIJ,12(1),23–40.
Banko, M., Brill, E., Dumais, S. T., and Lin, J. CSPlook-backtechniquestosolvereal-worldSAT
(2002). Askmsr: Question answering using the instances.InAAAI-97,pp.203–208. Berliner, H. J. (1980a). Backgammoncomputer
worldwideweb.InProc.AAAISpringSymposiumon programbeatsworldchampion.AIJ,14,205–220.
Bayes,T.(1763). Anessaytowardssolvingaprob-
MiningAnswersfromTextsandKnowledgeBases, leminthedoctrineofchances.PhilosophicalTrans- Berliner,H.J.(1980b). Computerbackgammon.
pp.7–9.
actionsoftheRoyalSocietyofLondon,53,370–418.
ScientificAmerican,249(6),64–72.
Banko,M.,Cafarella,M.J.,Soderland,S.,Broad- Bernardo, J. M. and Smith, A. F. M. (1994).
Beal, D.F.(1980). Ananalysisofminimax. In
head,M.,andEtzioni,O.(2007).Openinformation BayesianTheory.Wiley.
Clarke, M. R. B. (Ed.), Advances in Computer
extractionfromtheweb.InIJCAI-07.
Chess2,pp.103–109.EdinburghUniversityPress. Berners-Lee,T.,Hendler,J.,andLassila,O.(2001).
Banko, M.andEtzioni,O.(2008). Thetradeoffs Thesemanticweb.ScientificAmerican,284(5),34–
Beal,J.andWinston,P.H.(2009).Thenewfrontier
betweenopenandtraditionalrelationextraction. In 43.
ofhuman-levelartificialintelligence. IEEEIntelli-
ACL-08,pp.28–36.
gentSystems,24(4),21–23. Bernoulli, D. (1738). Specimen theoriae novae
Bar-Hillel,Y.(1954). Indexicalexpressions.Mind, de mensura sortis. Proc. St. PetersburgImperial
Beckert,B.andPosegga,J.(1995).Leantap:Lean,
63,359–379. AcademyofSciences,5,175–192.
tableau-baseddeduction.JAR,15(3),339–358.
Bar-Hillel,Y.(1960). Thepresentstatusofauto- Bernstein,A.andRoberts,M.(1958). Computer
Beeri, C., Fagin, R., Maier, D., and Yannakakis,
matictranslationoflanguages. InAlt,F.L.(Ed.), vs.chessplayer. ScientificAmerican,198(6),96–
M.(1983). Onthedesirabilityofacyclicdatabase
AdvancesinComputers,Vol.1,pp.91–163.Aca- 105.
schemes.JACM,30(3),479–513.
demicPress.
Bernstein,P.L.(1996). AgainsttheOdds:TheRe-
Bekey,G.(2008). Robotics:StateOfTheArtAnd
Bar-Shalom, Y. (Ed.). (1992). Multitarget- markableStoryofRisk.Wiley.
FutureChallenges.ImperialCollegePress.
multisensor tracking: Advanced applications.
Berrou, C., Glavieux, A., and Thitimajshima, P.
ArtechHouse. Bell,C.andTate,A.(1985). Usingtemporalcon-
(1993).NearShannonlimiterrorcontrol-correcting
straintstorestrictsearchinaplanner.InProc.Third
Bar-Shalom,Y.andFortmann,T.E.(1988).Track- codinganddecoding:Turbo-codes.1.InProc.IEEE
AlveyIKBSSIGWorkshop.
ingandDataAssociation.AcademicPress. InternationalConferenceonCommunications, pp.
Bell,J.L.andMachover,M.(1977). ACoursein 1064–1070.
Bartak,R.(2001).Theoryandpracticeofconstraint
MathematicalLogic.Elsevier/North-Holland.
propagation.InProc.ThirdWorkshoponConstraint Berry,D.A.andFristedt,B.(1985). BanditProb-
ProgrammingforDecisionandControl(CPDC-01), Bellman,R.E.(1952). Onthetheoryofdynamic lems:SequentialAllocationofExperiments. Chap-
pp.7–14. programming.PNAS,38,716–719. manandHall.
1066 Bibliography
Bertele,U.andBrioschi,F.(1972). Nonserialdy- Blazewicz, J., Ecker, K., Pesch, E., Schmidt, G., Borenstein, J., Everett, B., and Feng, L. (1996).
namicprogramming.AcademicPress. and Weglarz, J. (2007). Handbook on Schedul- NavigatingMobileRobots:SystemsandTechniques.
ing: ModelsandMethodsforAdvancedPlanning A.K.Peters,Ltd.
Bertoli, P., Cimatti, A., and Roveri, M. (2001a). (InternationalHandbooksonInformationSystems).
Heuristicsearch+symbolicmodelchecking=ef- Springer-VerlagNewYork,Inc. Borenstein,J.andKoren.,Y.(1991). Thevector
ficientconformantplanning. InIJCAI-01,pp.467– fieldhistogram—Fastobstacleavoidanceformobile
472. Blei,D.M.,Ng,A.Y.,andJordan,M.I.(2001). robots.IEEETransactionsonRoboticsandAutoma-
LatentDirichletAllocation. InNeuralInformation tion,7(3),278–288.
Bertoli,P.,Cimatti,A.,Roveri,M.,andTraverso,P. ProcessingSystems,Vol.14.
Borgida,A.,Brachman,R.J.,McGuinness,D.,and
(2001b). Planninginnondeterministicdomainsun-
Blinder, A.S.(1983). Issuesinthecoordination AlperinResnick,L.(1989).CLASSIC:Astructural
derpartialobservabilityviasymbolicmodelcheck-
ofmonetaryandfiscalpolicies. InMonetaryPolicy datamodelforobjects.SIGMODRecord,18(2),58–
ing.InIJCAI-01,pp.473–478.
Issuesinthe1980s.FederalReserveBank,Kansas 67.
Bertot, Y., Casteran, P., Huet, G., and Paulin- City,Missouri. Boroditsky, L. (2003). Linguistic relativity. In
Mohring,C.(2004). InteractiveTheoremProving Bliss,C.I.(1934). Themethodofprobits. Science, Nadel,L.(Ed.),EncyclopediaofCognitiveScience,
andProgramDevelopment.Springer. 79(2037),38–39. pp.917–921.Macmillan.
Bertsekas,D.(1987). DynamicProgramming:De- Block,H.D.,Knight,B.,andRosenblatt,F.(1962). Boser,B.,Guyon,I.,andVapnik,V.N.(1992). A
terministicandStochasticModels.Prentice-Hall. Analysisofafour-layerseries-coupledperceptron. trainingalgorithmforoptimalmarginclassifiers. In
Rev.ModernPhysics,34(1),275–282. COLT-92.
Bertsekas,D.andTsitsiklis,J.N.(1996). Neuro-
dynamicprogramming.AthenaScientific. Blum,A.L.andFurst,M.(1995). Fastplanning Bosse, M., Newman, P., Leonard, J., Soika, M.,
throughplanninggraphanalysis. InIJCAI-95,pp. Feiten, W., and Teller, S. (2004). Simultaneous
Bertsekas,D.andTsitsiklis,J.N.(2008).Introduc- 1636–1642. localizationandmapbuildinginlarge-scalecyclic
tiontoProbability(2ndedition).AthenaScientific. environments using the atlas framework. Int. J.
Blum,A.L.andFurst,M.(1997). Fastplanning
Bertsekas,D.andShreve,S.E.(2007). Stochastic throughplanninggraphanalysis.AIJ,90(1–2),281– RoboticsResearch,23(12),1113–1139.
OptimalControl: TheDiscrete-TimeCase. Athena 300. Bourzutschky, M. (2006). 7-man
Scientific. endgames with pawns. CCRL Discussion
Blum,A.L.(1996). On-linealgorithmsinmachine
Board, kirill-kryukov.com/chess/
Bessie`re, C. (2006). Constraint propagation. In learning.InProc.WorkshoponOn-LineAlgorithms,
discussion-board/viewtopic.php?t=
Rossi,F.,vanBeek,P.,andWalsh,T.(Eds.),Hand- Dagstuhl,pp.306–325.
805.
bookofConstraintProgramming.Elsevier. Blum,A.L.andMitchell,T.M.(1998). Combin-
Boutilier, C.andBrafman, R.I.(2001). Partial-
Bhar,R.andHamori,S.(2004). HiddenMarkov inglabeledandunlabeleddatawithco-training. In orderplanningwithconcurrentinteractingactions.
Models: Applications to Financial Economics. COLT-98,pp.92–100. JAIR,14,105–136.
Springer. Blumer,A.,Ehrenfeucht,A.,Haussler,D.,andWar-
Boutilier, C., Dearden, R., and Goldszmidt, M.
muth, M. (1989). Learnability and the Vapnik-
Bibel, W. (1993). Deduction: AutomatedLogic. (2000). Stochasticdynamicprogrammingwithfac-
Chervonenkisdimension.JACM,36(4),929–965.
AcademicPress. toredrepresentations.AIJ,121,49–107.
Bobrow,D.G.(1967).Naturallanguageinputfora
Biere,A.,Heule,M.,vanMaaren,H.,andWalsh, computerproblemsolvingsystem.InMinsky,M.L. Boutilier,C.,Reiter,R.,andPrice,B.(2001).Sym-
T.(Eds.).(2009). HandbookofSatisfiability. IOS (Ed.), Semantic Information Processing, pp.133– bolicdynamicprogrammingforfirst-orderMDPs.In
Press. 215.MITPress. IJCAI-01,pp.467–472.
Billings, D., Burch, N., Davidson, A., Holte, R., Bobrow,D.G.,Kaplan,R.,Kay,M.,Norman,D.A., Boutilier, C.,Friedman, N., Goldszmidt, M.,and
Schaeffer, J., Schauenberg, T., and Szafron, D. Thompson,H.,andWinograd,T.(1977). GUS,a Koller,D.(1996).Context-specificindependencein
(2003). Approximating game-theoretic optimal framedrivendialogsystem.AIJ,8,155–173. Bayesiannetworks.InUAI-96,pp.115–123.
strategiesforfull-scalepoker.InIJCAI-03. Bouzy,B.andCazenave,T.(2001). Computergo:
Boden, M. A.(1977). ArtificialIntelligence and
AnAIorientedsurvey.AIJ,132(1),39–103.
Binder,J.,Koller,D.,Russell,S.J.,andKanazawa, NaturalMan.BasicBooks.
K.(1997a). Adaptiveprobabilisticnetworkswith Boden,M.A.(Ed.).(1990). ThePhilosophyofAr- Bowerman,M.andLevinson,S.(2001). Language
hiddenvariables.MachineLearning,29,213–244. tificialIntelligence.OxfordUniversityPress. acquisitionandconceptualdevelopment.Cambridge
UniversityPress.
Binder,J.,Murphy,K.,andRussell,S.J.(1997b). Bolognesi,A.andCiancarini,P.(2003). Computer
Space-efficient inference in dynamic probabilistic programmingofkriegspielendings:ThecaseofKR Bowling,M.,Johanson,M.,Burch,N.,andSzafron,
networks.InIJCAI-97,pp.1292–1296. vs.k.InAdvancesinComputerGames10. D.(2008). Strategyevaluationinextensivegames
withimportancesampling.InICML-08.
Binford,T.O.(1971). Visualperceptionbycom- Bonet, B.(2002). Anepsilon-optimalgrid-based
Box, G.E.P.(1957). Evolutionaryoperation: A
puter. InvitedpaperpresentedattheIEEESystems algorithmforpartiallyobservableMarkovdecision
methodofincreasingindustrialproductivity.Applied
ScienceandCyberneticsConference,Miami. processes.InICML-02,pp.51–58.
Statistics,6,81–101.
Binmore, K. (1982). Essays on Foundations of Bonet, B. and Geffner, H. (1999). Planning as Box,G.E.P.,Jenkins,G.,andReinsel,G.(1994).
GameTheory.Pitman. heuristicsearch:Newresults. InECP-99,pp.360– TimeSeriesAnalysis:ForecastingandControl(3rd
372.
Bishop,C.M.(1995). NeuralNetworksforPattern edition).PrenticeHall.
Bonet, B.andGeffner,H.(2000). Planningwith
Recognition.OxfordUniversityPress. Boyan, J. A. (2002). Technical update: Least-
incompleteinformationasheuristicsearchinbelief
squares temporal difference learning. Machine
Bishop,C.M.(2007).PatternRecognitionandMa- space.InICAPS-00,pp.52–61. Learning,49(2–3),233–246.
chineLearning.Springer-Verlag. Bonet,B.andGeffner,H.(2005).Analgorithmbet-
Bisson,T.(1990).They’remadeoutofmeat.Omni
terthanAO∗?InAAAI-05. B
in
o
g
y
e
a
v
n
a
,
lu
J
a
.
ti
A
on
.
f
a
u
n
n
d
ct
M
ion
o
s
or
f
e
o
,
r
A
gl
.
ob
W
al
.
o
(
p
1
t
9
i
9
m
8
i
)
z
.
atio
L
n
ea
a
r
n
n
d
-
Magazine. Boole, G. (1847). The Mathematical Analysisof Booleansatisfiability.InAAAI-98.
Logic:BeinganEssaytowardsaCalculusofDeduc-
Bistarelli,S.,Montanari,U.,andRossi,F.(1997). Boyd,S.andVandenberghe,L.(2004).ConvexOp-
tiveReasoning.Macmillan,Barclay,andMacmillan,
Semiring-basedconstraintsatisfactionandoptimiza- timization.CambridgeUniversityPress.
Cambridge.
tion.JACM,44(2),201–236. Boyen,X.,Friedman,N.,andKoller,D.(1999).Dis-
Booth,T.L.(1969). Probabilisticrepresentationof
Bitner,J.R.andReingold,E.M.(1975).Backtrack formallanguages. InIEEEConferenceRecordof coveringthehiddenstructureofcomplexdynamic
programmingtechniques.CACM,18(11),651–656. the1969TenthAnnualSymposiumonSwitchingand systems.InUAI-99.
AutomataTheory,pp.74–81. Boyer,R.S.andMoore,J.S.(1979). AComputa-
Bizer,C.,Auer,S.,Kobilarov,G.,Lehmann,J.,and
tionalLogic.AcademicPress.
Cyganiak,R.(2007).DBPedia–queryingwikipedia Borel,E.(1921). Lathe´oriedujeuetlese´quations
likeadatabase. InDevelopersTrackPresentation inte´grales a` noyau syme´trique. Comptes Rendus Boyer,R.S.andMoore,J.S.(1984).Proofchecking
atthe16thInternationalConferenceonWorldWide HebdomadairesdesSe´ancesdel’Acade´miedesSci- theRSApublickeyencryptionalgorithm.American
Web. ences,173,1304–1308. MathematicalMonthly,91(3),181–189.
Bibliography 1067
Brachman, R. J. (1979). On the epistemologi- Bridle,J.S.(1990). Probabilisticinterpretationof Bryson,A.E.andHo,Y.-C.(1969). AppliedOpti-
calstatusofsemanticnetworks. InFindler,N.V. feedforwardclassificationnetworkoutputs,withre- malControl.Blaisdell.
(Ed.), Associative Networks: Representation and lationshipstostatisticalpatternrecognition. InFo-
Buchanan, B. G. and Mitchell, T. M. (1978).
UseofKnowledge by Computers, pp. 3–50.Aca- gelmanSoulie´,F.andHe´rault,J.(Eds.),Neurocom-
Model-directedlearningofproductionrules.InWa-
demicPress. puting:Algorithms,ArchitecturesandApplications.
terman,D.A.andHayes-Roth,F.(Eds.),Pattern-
Springer-Verlag.
Brachman,R.J.,Fikes,R.E.,andLevesque,H.J. DirectedInferenceSystems,pp.297–312.Academic
(1983). Krypton: Afunctionalapproachtoknowl- Briggs, R. (1985). Knowledge representation in Press.
edgerepresentation.Computer,16(10),67–73. Sanskritandartificialintelligence.AIMag,6(1),32– Buchanan,B.G.,Mitchell,T.M.,Smith,R.G.,and
Brachman,R.J.andLevesque,H.J.(Eds.).(1985). 39. Johnson,C.R.(1978). Modelsoflearningsystems.
Readings in Knowledge Representation. Morgan Brin,D.(1998).TheTransparentSociety.Perseus. InEncyclopediaofComputerScienceandTechnol-
Kaufmann. ogy,Vol.11.Dekker.
Brin,S.(1999). Extractingpatternsandrelations
Bradtke,S.J.andBarto,A.G.(1996).Linearleast- fromtheworldwideweb.Technicalreport1999-65, Buchanan, B. G. and Shortliffe, E. H. (Eds.).
squaresalgorithmsfortemporaldifferencelearning. StanfordInfoLab. (1984). Rule-BasedExpertSystems: TheMYCIN
MachineLearning,22,33–57. ExperimentsoftheStanfordHeuristicProgramming
Brin, S.andPage, L.(1998). Theanatomyofa Project.Addison-Wesley.
Brafman,O.andBrafman,R.(2009). Sway: The large-scalehypertextualwebsearchengine.InProc.
IrresistiblePullofIrrationalBehavior. Broadway SeventhWorldWideWebConference. Buchanan, B.G.,Sutherland, G.L.,andFeigen-
Business. baum,E.A.(1969). HeuristicDENDRAL:Apro-
Bringsjord,S.(2008). IfIwerejudge. InEpstein, gramforgeneratingexplanatoryhypothesesinor-
Brafman,R.I.andDomshlak,C.(2008).Fromone R.,Roberts,G.,andBeber,G.(Eds.),Parsingthe ganicchemistry. InMeltzer, B., Michie, D., and
tomany: Planningforlooselycoupledmulti-agent TuringTest.Springer. Swann,M.(Eds.),MachineIntelligence4,pp.209–
systems.InICAPS-08,pp.28–35. 254.EdinburghUniversityPress.
Broadbent,D.E.(1958). PerceptionandCommu-
Brafman,R.I.andTennenholtz,M.(2000).Anear nication.Pergamon. Buehler,M.,Iagnemma,K.,andSingh,S.(Eds.).
optimalpolynomialtimealgorithmforlearningin (2006). The2005DARPAGrandChallenge: The
certainclassesofstochasticgames.AIJ,121,31–47. Brooks,R.A.(1986).Arobustlayeredcontrolsys- GreatRobotRace.Springer-Verlag.
temforamobilerobot. IEEEJournalofRobotics
Braitenberg,V.(1984). Vehicles: Experimentsin andAutomation,2,14–23. Bunt,H.C.(1985). Theformalrepresentationof
SyntheticPsychology.MITPress. (quasi-)continuousconcepts. InHobbs,J.R.and
Brooks, R. A. (1989). Engineering approach to Moore,R.C.(Eds.),FormalTheoriesoftheCom-
Bransford,J.andJohnson,M.(1973). Considera- buildingcomplete,intelligentbeings. Proc.SPIE— monsenseWorld,chap.2,pp.37–70.Ablex.
tionofsomeproblemsincomprehension.InChase, the International Societyfor OpticalEngineering,
W. G.(Ed.), VisualInformation Processing.Aca- 1002,618–625. Burgard,W.,Cremers,A.B.,Fox,D.,Ha¨hnel,D.,
demicPress. Lakemeyer,G.,Schulz,D.,Steiner,W.,andThrun,
Brooks,R.A.(1991).Intelligencewithoutrepresen- S.(1999). Experienceswithaninteractivemuseum
Brants,T.,Popat,A.C.,Xu,P.,Och,F.J.,andDean, tation.AIJ,47(1–3),139–159. tour-guiderobot.AIJ,114(1–2),3–55.
J.(2007). Largelanguagemodelsinmachinetrans-
lation. InEMNLP-CoNLL-2007: Proc.2007Joint Brooks,R.A.andLozano-Perez,T.(1985). Asub- Buro,M.(1995). ProbCut: Aneffectiveselective
ConferenceonEmpiricalMethodsinNaturalLan- divisionalgorithminconfigurationspaceforfind- extensionofthealpha-betaalgorithm. J.Interna-
guageProcessingandComputationalNaturalLan- pathwithrotation. IEEETransactionsonSystems, tionalComputerChessAssociation,18(2),71–76.
ManandCybernetics,15(2),224–233.
guageLearning,pp.858–867. Buro, M. (2002). Improving heuristic mini-max
Bratko,I.(1986). PrologProgrammingforArtifi- Brown,C.,Finkelstein,L.,andPurdom,P.(1988). searchbysupervisedlearning. AIJ,134(1–2),85–
cialIntelligence(1stedition).Addison-Wesley. Backtracksearchinginthepresenceofsymmetry. 99.
InMora,T.(Ed.),AppliedAlgebra,AlgebraicAl-
Burstein,J.,Leacock,C.,andSwartz, R.(2001).
Bratko,I.(2001). PrologProgrammingforArtifi- gorithmsandError-CorrectingCodes,pp.99–110.
Automatedevaluationofessaysandshortanswers.
cialIntelligence(Thirdedition).Addison-Wesley. Springer-Verlag.
InFifthInternationalComputerAssistedAssessment
Bratman,M.E.(1987).Intention,Plans,andPrac- Brown,K.C.(1974).Anoteontheapparentbiasof (CAA)Conference.
ticalReason.HarvardUniversityPress. netrevenueestimates.J.Finance,29,1215–1216. Burton,R.(2009).OnBeingCertain:BelievingYou
Bratman,M.E.(1992). Planningandthestability Brown, P. F., Cocke, J., Della Pietra, S. A., AreRightEvenWhenYou’reNot.St.Martin’sGrif-
ofintention.MindsandMachines,2(1),1–16. DellaPietra, V.J.,Jelinek,F., Mercer,R.L.,and fin.
Breese,J.S.(1992). Constructionofbeliefandde- Roossin, P.(1988). Astatisticalapproachtolan- Buss,D.M.(2005).Handbookofevolutionarypsy-
cisionnetworks. ComputationalIntelligence,8(4), guagetranslation.InCOLING-88,pp.71–76. chology.Wiley.
624–647. Brown,P.F.,DellaPietra,S.A.,DellaPietra,V.J., Butler,S.(1863).Darwinamongthemachines.The
Breese,J.S.andHeckerman,D.(1996). Decision- andMercer,R.L.(1993). Themathematicsofsta- Press(Christchurch,NewZealand),June13.
tisticalmachinetranslation: Parameterestimation.
theoretictroubleshooting: Aframeworkforrepair ComputationalLinguistics,19(2),263–311. Bylander,T.(1992). Complexityresultsforserial
andexperiment.InUAI-96,pp.124–132. decomposability.InAAAI-92,pp.729–734.
Brownston,L.,Farrell,R.,Kant,E.,andMartin,N.
Breiman,L.(1996). Baggingpredictors. Machine Bylander,T.(1994).Thecomputationalcomplexity
(1985). ProgrammingexpertsystemsinOPS5: An
Learning,24(2),123–140. ofpropositionalSTRIPSplanning. AIJ,69, 165–
introductiontorule-basedprogramming. Addison-
204.
Breiman,L.,Friedman,J.,Olshen,R.A.,andStone, Wesley.
C.J.(1984). ClassificationandRegressionTrees. Byrd,R.H.,Lu,P.,Nocedal,J.,andZhu,C.(1995).
WadsworthInternationalGroup.
Bruce,V.,Georgeson,M.,andGreen,P.(2003).Vi-
Alimitedmemoryalgorithmforboundconstrained
sualPerception:Physiology,PsychologyandEcol- optimization.SIAMJournalonScientificandStatis-
Brelaz,D.(1979).Newmethodstocolorthevertices ogy.PsychologyPress. ticalComputing,16(5),1190–1208.
ofagraph.CACM,22(4),251–256.
Bruner, J.S., Goodnow, J.J., andAustin, G.A. Cabeza,R.andNyberg,L.(2001). Imagingcogni-
Brent, R.P.(1973). Algorithmsforminimization (1957).AStudyofThinking.Wiley. tionII:Anempiricalreviewof275PETandfMRI
withoutderivatives.Prentice-Hall. Bryant,B.D.andMiikkulainen,R.(2007).Acquir- studies.J.CognitiveNeuroscience,12,1–47.
Bresnan,J.(1982). TheMentalRepresentationof ingvisiblyintelligentbehaviorwithexample-guided Cafarella,M.J.,Halevy,A.,Zhang,Y.,Wang,D.Z.,
GrammaticalRelations.MITPress. neuroevolution.InAAAI-07. andWu,E.(2008).Webtables:Exploringthepower
Brewka, G., Dix, J., and Konolige, K. (1997). Bryce, D.andKambhampati, S.(2007). Atuto- oftablesontheweb.InVLDB-2008.
NononotonicReasoning:AnOverview.CSLIPubli- rialonplanninggraph-basedreachabilityheuristics. Calvanese, D., Lenzerini, M., and Nardi, D.
cations. AIMag,Spring,47–83. (1999). Unifying class-based representation for-
malisms.JAIR,11,199–240.
Brickley,D.andGuha,R.V.(2004). RDFvocab- Bryce, D., Kambhampati, S., and Smith, D. E.
ularydescriptionlanguage1.0:RDFschema.Tech. (2006). Planninggraphheuristicsforbeliefspace Campbell, M. S., Hoane, A. J., and Hsu, F.-H.
rep.,W3C. search.JAIR,26,35–99. (2002).DeepBlue.AIJ,134(1–2),57–83.
1068 Bibliography
Canny, J.andReif,J.(1987). Newlowerbound Charniak, E. (1996). Tree-bank grammars. In Church, A. (1936). A note on the Entschei-
techniquesforrobotmotionplanningproblems. In AAAI-96,pp.1031–1036. dungsproblem.JSL,1,40–41and101–102.
FOCS-87,pp.39–48.
Charniak, E. (1997). Statistical parsing with a Church,A.(1956). IntroductiontoMathematical
Canny,J.(1986).Acomputationalapproachtoedge context-freegrammarandwordstatistics. InAAAI- Logic.PrincetonUniversityPress.
detection.PAMI,8,679–698. 97,pp.598–603. Church,K.andPatil,R.(1982). Copingwithsyn-
Canny,J.(1988). TheComplexityofRobotMotion Charniak,E.andGoldman,R.(1992). ABayesian tacticambiguityorhowtoputtheblockintheboxon
Planning.MITPress. modelofplanrecognition.AIJ,64(1),53–79. thetable. ComputationalLinguistics,8(3–4),139–
149.
Capen, E., Clapp, R., andCampbell, W. (1971). Charniak,E.andMcDermott,D.(1985).Introduc-
Competitive bidding in high-risk situations. J. tiontoArtificialIntelligence.Addison-Wesley. Church,K.(2004). Speechandlanguageprocess-
PetroleumTechnology,23,641–653. Charniak, E., Riesbeck, C., McDermott, D., and ing: Canweusethepasttopredictthefuture. In
Proc.ConferenceonText,Speech,andDialogue.
Caprara,A.,Fischetti,M.,andToth,P.(1995). A Meehan,J.(1987). ArtificialIntelligenceProgram-
heuristicmethodforthesetcoveringproblem. Op- ming(2ndedition).LawrenceErlbaumAssociates. Church,K.andGale,W.A.(1991). Acomparison
erationsResearch,47,730–743. Charniak, E.(1991). Bayesiannetworkswithout oftheenhancedGood–Turinganddeletedestima-
tionmethodsforestimatingprobabilitiesofEnglish
Carbonell,J.G.(1983). Derivationalanalogyand tears.AIMag,12(4),50–63. bigrams.ComputerSpeechandLanguage,5,19–54.
itsroleinproblemsolving.InAAAI-83,pp.64–69. Charniak, E. and Johnson, M. (2005). Coarse-
Churchland, P.M.andChurchland, P.S.(1982).
Carbonell,J.G.,Knoblock,C.A.,andMinton,S. to-fine n-best parsing and maxent discriminative Functionalism,qualia,andintentionality. InBiro,
reranking.InACL-05.
(1989). PRODIGY:Anintegratedarchitecturefor J. I. and Shahan, R. W. (Eds.), Mind, Brain and
planningandlearning. TechnicalreportCMU-CS- Chater,N.andOaksford,M.(Eds.).(2008). The Function: Essaysin the Philosophy ofMind, pp.
89-189, Computer ScienceDepartment, Carnegie- probabilisticmind:ProspectsforBayesiancognitive 121–145.UniversityofOklahomaPress.
MellonUniversity. science.OxfordUniversityPress.
Churchland, P. S. (1986). Neurophilosophy:
Carbonell,J.R.andCollins,A.M.(1973).Natural Chatfield,C.(1989). TheAnalysisofTimeSeries: Toward a Unified Science of the Mind–Brain.
semanticsinartificialintelligence. InIJCAI-73,pp. AnIntroduction(4thedition).ChapmanandHall. MITPress.
344–351. Cheeseman,P.(1985).Indefenseofprobability.In Ciancarini,P.andWooldridge,M.(2001). Agent-
Cardano,G.(1663).Liberdeludoaleae.Lyons. IJCAI-85,pp.1002–1009. OrientedSoftwareEngineering.Springer-Verlag.
Carnap,R.(1928). DerlogischeAufbauderWelt. Cheeseman,P.(1988).Aninquiryintocomputerun- Cimatti, A., Roveri, M., andTraverso, P.(1998).
Weltkreis-verlag. TranslatedintoEnglishas(Car- derstanding. ComputationalIntelligence,4(1),58– Automatic OBDD-based generation of universal
nap,1967). 66. plansinnon-deterministicdomains.InAAAI-98,pp.
875–881.
Carnap,R.(1948). Ontheapplicationofinductive Cheeseman, P., Kanefsky, B., and Taylor, W.
logic.PhilosophyandPhenomenologicalResearch, (1991). Wherethe really hardproblems are. In Clark,A.(1998).BeingThere:PuttingBrain,Body,
8,133–148. IJCAI-91,pp.331–337. andWorldTogetherAgain.MITPress.
Carnap,R.(1950). LogicalFoundationsofProba- Cheeseman, P., Self, M., Kelly, J., and Stutz, J. Clark,A.(2008). SupersizingtheMind: Embodi-
bility.UniversityofChicagoPress. (1988).Bayesianclassification.InAAAI-88,Vol.2, ment,Action,andCognitiveExtension.OxfordUni-
pp.607–611. versityPress.
Carroll,S.(2007). TheMakingoftheFittest:DNA
andtheUltimateForensicRecordofEvolution.Nor- Cheeseman, P. and Stutz, J. (1996). Bayesian Clark,K.L.(1978).Negationasfailure.InGallaire,
ton. classification(AutoClass): Theoryandresults. In H.andMinker,J.(Eds.),LogicandDataBases,pp.
Fayyad, U., Piatesky-Shapiro, G., Smyth, P., and 293–322.Plenum.
Casati,R.andVarzi,A.(1999). Partsandplaces: Uthurusamy,R.(Eds.),AdvancesinKnowledgeDis- Clark,P.andNiblett,T.(1989).TheCN2induction
thestructuresofspatialrepresentation.MITPress. coveryandDataMining.AAAIPress/MITPress. algorithm.MachineLearning,3,261–283.
Cassandra, A.R., Kaelbling, L.P., andLittman, Chen,S.F.andGoodman,J.(1996). Anempirical Clark,S.andCurran,J.R.(2004).ParsingtheWSJ
M.L.(1994). Actingoptimallyinpartiallyobserv- studyofsmoothingtechniquesforlanguagemodel- usingCCGandlog-linearmodels. InACL-04,pp.
able stochastic domains. In AAAI-94, pp. 1023– ing.InACL-96,pp.310–318. 104–111.
1028.
Cheng,J.andDruzdzel,M.J.(2000). AIS-BN:An Clarke, A.C.(1968a). 2001: ASpaceOdyssey.
Cassandras,C.G.andLygeros,J.(2006).Stochas- adaptiveimportancesamplingalgorithmforeviden- Signet.
ticHybridSystems.CRCPress. tialreasoninginlargeBayesiannetworks.JAIR,13,
155–188. Clarke,A.C.(1968b).Theworldof2001.Vogue.
Castro,R.,Coates,M.,Liang,G.,Nowak,R.,and
Yu,B.(2004). Networktomography:Recentdevel- Cheng,J.,Greiner,R.,Kelly,J.,Bell,D.A.,and Clarke,E.andGrumberg,O.(1987). Researchon
opments.StatisticalScience,19(3),499–517. Liu,W.(2002). LearningBayesiannetworksfrom automaticverificationoffinite-stateconcurrentsys-
data: Aninformation-theorybasedapproach. AIJ, tems. AnnualReviewofComputerScience,2,269–
Cesa-Bianchi,N.andLugosi,G.(2006).Prediction, 137,43–90. 290.
learning,andGames.CambridgeUniversityPress.
Chklovski, T.andGil, Y.(2005). Improvingthe Clarke,M.R.B.(Ed.).(1977). AdvancesinCom-
Cesta,A.,Cortellessa,G.,Denis,M.,Donati,A., designofintelligentacquisitioninterfacesforcol- puterChess1.EdinburghUniversityPress.
Fratini,S.,Oddi,A.,Policella,N.,Rabenau,E.,and lectingworldknowledgefromwebcontributors. In Clearwater,S.H.(Ed.).(1996).Market-BasedCon-
Schulster,J.(2007). MEXAR2:AIsolvesmission Proc.ThirdInternationalConferenceonKnowledge trol.WorldScientific.
plannerproblems. IEEEIntelligentSystems,22(4), Capture(K-CAP).
12–19. Clocksin,W.F.andMellish,C.S.(2003).Program-
Chomsky,N.(1956).Threemodelsforthedescrip- minginProlog(5thedition).Springer-Verlag.
Chakrabarti, P. P., Ghose, S., Acharya, A., and tionoflanguage. IRETransactionsonInformation
deSarkar,S.C.(1989).Heuristicsearchinrestricted Theory,2(3),113–124. Clocksin, W.F.(2003). ClauseandEffect: Pro-
memory.AIJ,41(2),197–222. log Programming for the Working Programmer.
Chomsky,N.(1957).SyntacticStructures.Mouton. Springer.
Chandra,A.K.andHarel,D.(1980). Computable
queriesforrelationaldatabases. J.Computerand Choset,H.(1996). SensorBasedMotionPlanning: Coarfa,C.,Demopoulos,D.,Aguirre,A.,Subrama-
SystemSciences,21(2),156–178. TheHierarchicalGeneralizedVoronoiGraph.Ph.D. nian,D.,andYardi,M.(2003).Random3-SAT:The
thesis,CaliforniaInstituteofTechnology. plotthickens.Constraints,8(3),243–261.
Chang,C.-L.andLee,R.C.-T.(1973). Symbolic
LogicandMechanicalTheoremProving.Academic Choset,H.,Lynch,K.,Hutchinson,S.,Kantor,G., Coates,A.,Abbeel,P.,andNg,A.Y.(2009). Ap-
Press. Burgard, W., Kavraki, L., and Thrun, S. (2004). prenticeshiplearningforhelicoptercontrol. JACM,
PrinciplesofRoboticMotion: Theory,Algorithms, 52(7),97–105.
Chapman, D. (1987). Planning for conjunctive andImplementation.MITPress.
Cobham, A.(1964). Theintrinsiccomputational
goals.AIJ,32(3),333–377.
Chung, K. L. (1979). Elementary Probability difficultyoffunctions. InProc.1964International
Charniak,E.(1993). StatisticalLanguageLearn- Theory with Stochastic Processes (3rd edition). CongressforLogic,Methodology,andPhilosophyof
ing.MITPress. Springer-Verlag. Science,pp.24–30.
Bibliography 1069
Cohen,P.R.(1995). Empiricalmethodsforartifi- Copernicus (1543). De Revolutionibus Orbium Cross,S.E.andWalker,E.(1994). DART:Apply-
cialintelligence.MITPress. Coelestium.ApudIoh.Petreium,Nuremberg. ingknowledgebasedplanningandschedulingtocri-
sisactionplanning. InZweben,M.andFox,M.S.
Cohen,P.R.andLevesque,H.J.(1990). Intention Cormen, T. H., Leiserson, C. E., and Rivest, R.
(Eds.),IntelligentScheduling,pp.711–729.Morgan
ischoicewithcommitment.AIJ,42(2–3),213–261. (1990).IntroductiontoAlgorithms.MITPress.
Kaufmann.
Cohen,P.R.,Morgan,J.,andPollack,M.E.(1990). Cortes,C.andVapnik,V.N.(1995).Supportvector
Cruse,D.A.(1986).LexicalSemantics.Cambridge
IntentionsinCommunication.MITPress. networks.MachineLearning,20,273–297.
UniversityPress.
Cohen,W.W.andPage,C.D.(1995). Learnabil- Cournot, A. (Ed.). (1838). Recherches sur les
Culberson,J.andSchaeffer,J.(1996). Searching
ityininductivelogic programming: Methodsand principesmathe´matiquesdelathe´oriedesrichesses.
with pattern databases. In Advances in Artificial
results. NewGenerationComputing,13(3–4),369– L.Hachette,Paris.
Intelligence(LectureNotesinArtificialIntelligence
409.
Cover,T.andThomas,J.(2006).ElementsofInfor- 1081),pp.402–416.Springer-Verlag.
Cohn,A.G.,Bennett,B.,Gooday,J.M.,andGotts, mationTheory(2ndedition).Wiley.
Culberson, J. and Schaeffer, J. (1998). Pattern
N.(1997).RCC:Acalculusforregionbasedqualita-
tivespatialreasoning.GeoInformatica,1,275–316. Cowan,J.D.andSharp,D.H.(1988a).Neuralnets. databases. ComputationalIntelligence,14(4),318–
QuarterlyReviewsofBiophysics,21,365–427. 334.
Collin, Z., Dechter, R., and Katz, S. (1999).
Self-stabilizing distributed constraint satisfaction. Cowan,J.D.andSharp,D.H.(1988b).Neuralnets Cullingford, R. E. (1981). Integrating knowl-
ChicagoJournalofTheoreticalComputerScience, andartificialintelligence.Daedalus,117,85–121. edge sources for computer “understanding” tasks.
IEEETransactionsonSystems,ManandCybernet-
1999(115). Cowell,R.,Dawid,A.P.,Lauritzen,S.,andSpiegel-
ics(SMC),11.
Collins,F.S.,Morgan,M.,andPatrinos,A.(2003). halter,D.J.(2002). ProbabilisticNetworksandEx-
The human genome project: Lessonsfrom large- pertSystems.Springer. Cummins,D.andAllen,C.(1998). TheEvolution
ofMind.OxfordUniversityPress.
scalebiology.Science,300(5617),286–290. Cox,I.(1993). Areviewofstatisticaldataassoci-
Collins,M.(1999). Head-drivenStatisticalModels ationtechniquesformotioncorrespondence. IJCV, Cushing,W.,Kambhampati,S.,Mausam,andWeld,
forNaturalLanguageProcessing.Ph.D.thesis,Uni- 10,53–66. D.S.(2007).Whenistemporalplanningreallytem-
poral?InIJCAI-07.
versityofPennsylvania. Cox,I.andHingorani,S.L.(1994).Anefficientim-
Collins,M.andDuffy,K.(2002).Newrankingalgo- plementationandevaluationofReid’smultiplehy- Cybenko,G.(1988).Continuousvaluedneuralnet-
rithmsforparsingandtagging:Kernelsoverdiscrete pothesistrackingalgorithmforvisualtracking. In workswithtwohiddenlayersaresufficient.Techni-
structures,andthevotedperceptron.InACL-02. ICPR-94,Vol.1,pp.437–442. calreport,DepartmentofComputerScience,Tufts
University.
Colmerauer,A.andRoussel,P.(1993). Thebirth Cox, I. and Wilfong, G. T. (Eds.). (1990). Au-
ofProlog.SIGPLANNotices,28(3),37–52. tonomousRobotVehicles.SpringerVerlag. Cybenko,G.(1989). Approximationbysuperposi-
tionsofasigmoidalfunction. MathematicsofCon-
Colmerauer,A.(1975). Lesgrammairesdemeta- Cox,R.T.(1946). Probability,frequency,andrea- trols,Signals,andSystems,2,303–314.
morphose. Tech.rep.,Grouped’IntelligenceArtifi- sonableexpectation. AmericanJournalofPhysics,
cielle,Universite´deMarseille-Luminy. 14(1),1–13. Daganzo,C.(1979).Multinomialprobit:Thetheory
anditsapplicationtodemandforecasting.Academic
Colmerauer, A., Kanoui, H., Pasero, R., and Craig,J.(1989).IntroductiontoRobotics:Mechan- Press.
Roussel, P. (1973). Un syste´me de communi- icsandControl(2ndedition).Addison-WesleyPub-
cation homme–machine en Franc¸ais. Rapport, lishing,Inc. Dagum, P.andLuby, M.(1993). Approximating
probabilisticinferenceinBayesianbeliefnetworks
Grouped’IntelligenceArtificielle,Universite´d’Aix- Craik, K.J.(1943). TheNatureofExplanation. isNP-hard.AIJ,60(1),141–153.
MarseilleII. CambridgeUniversityPress.
Dalal,N.andTriggs,B.(2005). Histogramsofori-
Condon, J.H. andThompson, K. (1982). Belle Craswell, N.,Zaragoza,H.,andRobertson, S.E. entedgradientsforhumandetection. InCVPR,pp.
chesshardware.InClarke,M.R.B.(Ed.),Advances (2005). Microsoftcambridgeattrec-14:Enterprise 886–893.
inComputerChess3,pp.45–54.Pergamon. track. InProc.FourteenthTextREtrievalConfer-
Congdon,C.B.,Huber,M.,Kortenkamp,D.,Bid- ence. Dantzig,G.B.(1949).Programmingofinterdepen-
dentactivities:II.Mathematicalmodel. Economet-
lack,C.,Cohen,C.,Huffman,S.,Koss,F.,Raschke, Crauser,A.,Mehlhorn,K.,Meyer,U.,andSanders, rica,17,200–211.
U., andWeymouth, T.(1992). CARMELversus P.(1998). AparallelizationofDijkstra’sshortest
Flakey:Acomparisonoftworobots. Tech.rep.Pa- pathalgorithm. InProc.23rdInternationalSym- Darwiche,A.(2001). Recursiveconditioning. AIJ,
persfromtheAAAIRobotCompetition,RC-92-01, posiumonMathematicalFoundationsofComputer 126,5–41.
AmericanAssociationforArtificialIntelligence. Science,,pp.722–731. Darwiche,A.andGinsberg,M.L.(1992). Asym-
Conlisk,J.(1989). ThreevariantsontheAllaisex- Craven,M.,DiPasquo,D.,Freitag,D.,McCallum, bolicgeneralizationofprobabilitytheory. InAAAI-
ample.AmericanEconomicReview,79(3),392–407. A., Mitchell, T. M., Nigam, K., and Slattery, S. 92,pp.622–627.
Connell,J.(1989).AColonyArchitectureforanAr- (2000).Learningtoconstructknowledgebasesfrom Darwiche,A.(2009).Modelingandreasoningwith
tificialCreature.Ph.D.thesis,ArtificialIntelligence theWorldWideWeb.AIJ,118(1/2),69–113. Bayesiannetworks.CambridgeUniversityPress.
Laboratory,MIT.AlsoavailableasAITechnicalRe- Crawford,J.M.andAuton,L.D.(1993). Experi- Darwin,C.(1859). OnTheOriginofSpeciesby
port1151. mentalresultsonthecrossoverpointinsatisfiability MeansofNaturalSelection.J.Murray,London.
Consortium,T.G.O.(2008). Thegeneontology problems.InAAAI-93,pp.21–27.
Darwin,C.(1871).DescentofMan.J.Murray.
projectin2008.NucleicAcidsResearch,36. Cristianini,N.andHahn,M.(2007). Introduction
Cook, S.A.(1971). Thecomplexityoftheorem- toComputational Genomics: A CaseStudies Ap- Dasgupta,P.,Chakrabarti,P.P.,anddeSarkar,S.C.
provingprocedures.InSTOC-71,pp.151–158. proach.CambridgeUniversityPress. (1994).Agentsearchinginatreeandtheoptimality
ofiterativedeepening.AIJ,71,195–208.
Cook,S.A.andMitchell,D.(1997). Findinghard Cristianini,N.andScho¨lkopf,B.(2002). Support
instancesofthesatisfiabilityproblem:Asurvey. In vectormachinesandkernelmethods:Thenewgen-
Davidson,D.(1980).EssaysonActionsandEvents.
OxfordUniversityPress.
Du,D.,Gu,J.,andPardalos,P.(Eds.),Satisfiabil- erationoflearningmachines.AIMag,23(3),31–41.
ity problems: Theory andapplications. American Davies,T.R.(1985). Analogy. InformalnoteIN-
Cristianini, N. andShawe-Taylor, J.(2000). An
MathematicalSociety. CSLI-85-4,CenterfortheStudyofLanguageand
introductiontosupportvectormachinesandother
Information(CSLI).
Cooper,G.(1990). Thecomputationalcomplexity kernel-basedlearningmethods.CambridgeUniver-
ofprobabilisticinferenceusingBayesianbeliefnet- sityPress. Davies,T.R.andRussell,S.J.(1987).Alogicalap-
works.AIJ,42,393–405. proachtoreasoningbyanalogy.InIJCAI-87,Vol.1,
Crockett,L.(1994).TheTuringTestandtheFrame
pp.264–270.
Cooper,G.andHerskovits,E.(1992). ABayesian Problem: AI’sMistakenUnderstandingofIntelli-
methodfortheinductionofprobabilisticnetworks gence.Ablex. Davis,E.(1986).RepresentingandAcquiringGeo-
fromdata.MachineLearning,9,309–347. graphicKnowledge.PitmanandMorganKaufmann.
Croft, B., Metzler, D., and Stroham, T. (2009).
Copeland, J. (1993). Artificial Intelligence: SearchEngines: InformationretrievalinPractice. Davis,E.(1990). RepresentationsofCommonsense
APhilosophicalIntroduction.Blackwell. AddisonWesley. Knowledge.MorganKaufmann.
1070 Bibliography
Davis,E.(2005). Knowledgeandcommunication: deMarcken, C.(1996). UnsupervisedLanguage Dechter,R.(1999).Bucketelimination:Aunifying
Afirst-ordertheory. AIJ,166,81–140. Acquisition.Ph.D.thesis,MIT. frameworkforreasoning.AIJ,113,41–85.
Davis,E.(2006). Theexpressivityofquantifying DeMorgan,A.(1864). Onthesyllogism,No.IV, Dechter, R. and Pearl, J. (1985). Generalized
overregions. J.LogicandComputation,16,891– andonthe logic ofrelations. Transactionofthe best-firstsearchstrategiesandtheoptimalityofA*.
916. CambridgePhilosophicalSociety,X,331–358. JACM,32(3),505–536.
Davis,E.(2007).Physicalreasoning.InvanHarme- DeRaedt,L.(1992). InteractiveTheoryRevision: Dechter,R.andPearl, J.(1987). Network-based
lan,F.,Lifschitz,V.,andPorter,B.(Eds.),TheHand- AnInductiveLogicProgrammingApproach. Aca- heuristicsforconstraint-satisfactionproblems. AIJ,
bookofKnowledgeRepresentation,pp.597–620.El- demicPress. 34(1),1–38.
sevier. Dechter,R.andPearl,J.(1989).Treeclusteringfor
deSalvoBraz,R.,Amir,E.,andRoth,D.(2007).
Davis,E.(2008). Pouringliquids:Astudyincom- Liftedfirst-orderprobabilisticinference. InGetoor, constraintnetworks.AIJ,38(3),353–366.
monsensephysicalreasoning.AIJ,172(1540–1578). L.andTaskar,B.(Eds.),IntroductiontoStatistical Dechter,R.(2003).ConstraintProcessing.Morgan
Davis,E.andMorgenstern,L.(2004).Introduction: RelationalLearning.MITPress. Kaufmann.
Progressinformalcommonsensereasoning. AIJ, Deacon,T.W.(1997). Thesymbolicspecies: The Dechter,R.andFrost,D.(2002). Backjump-based
153,1–12. co-evolutionoflanguageandthebrain.W.W.Nor- backtracking for constraint satisfaction problems.
Davis,E.andMorgenstern,L.(2005). Afirst-order ton. AIJ,136(2),147–188.
t L h o e g o i r c y a o n f d c C o o m m m p u u n ta ic ti a o ti n o , n 15 a ( n 5 d ), m 7 u 0 l 1 ti – - 7 a 4 g 9 en . tplans. J. Deale, M.,Yvanovich, M.,Schnitzius,D.,Kautz, D se e a c rc h h te s r p , a R ce . s a f n o d rg M ra a p te h e ic s a c l u, m R o . de ( l 2 s 0 . 0 A 7) I . J,1 A 7 N 1( D 2 / – O 3 R ),
D.,Carpenter,M.,Zweben,M.,Davis,G.,andDaun,
Davis,K.H.,Biddulph,R.,andBalashek,S.(1952). B. (1994). The space shuttle ground processing 73–106.
Automaticrecognitionofspokendigits. J.Acousti- scheduling system. In Zweben, M. andFox, M. DeCoste,D.andScho¨lkopf,B.(2002).Trainingin-
calSocietyofAmerica,24(6),637–642. (Eds.),IntelligentScheduling,pp.423–449.Morgan variantsupportvectormachines.MachineLearning,
Davis,M.(1957). AcomputerprogramforPres- Kaufmann. 46(1),161–190.
burger’salgorithm. InProvingTheorems(asDone Dean, T.,Basye,K., Chekaluk, R.,andHyun, S. Dedekind,R.(1888). Wassindundwassollendie
byMan,Logician,orMachine),pp.215–233.Proc. (1990).Copingwithuncertaintyinacontrolsystem Zahlen.Braunschweig,Germany.
SummerInstituteforSymbolicLogic.Secondedi- fornavigationandexploration. InAAAI-90,Vol.2, Deerwester,S.C.,Dumais,S.T.,Landauer,T.K.,
tion;publicationdateis1960. pp.1010–1015. Furnas,G.W.,andHarshman,R.A.(1990). Index-
Davis,M.,Logemann,G.,andLoveland,D.(1962). Dean,T.andBoddy,M.(1988).Ananalysisoftime- ingbylatentsemanticanalysis.J.AmericanSociety
Amachineprogramfortheorem-proving.CACM,5, dependentplanning.InAAAI-88,pp.49–54. forInformationScience,41(6),391–407.
394–397.
DeGroot,M.H.(1970). OptimalStatisticalDeci-
Dean,T.,Firby,R.J.,andMiller,D.(1990).Hierar-
Davis,M.andPutnam,H.(1960).Acomputingpro- sions.McGraw-Hill.
chicalplanninginvolvingdeadlines,traveltime,and
cedureforquantificationtheory. JACM,7(3),201–
resources. ComputationalIntelligence,6(1),381– DeGroot,M.H.andSchervish,M.J.(2001).Prob-
215.
398. abilityandStatistics(3rdedition).AddisonWesley.
D B H a a il s v l e . i d s, S R ys . te a m nd si L n en A a r t t , ifi D ci . al B I . nt ( e 1 l 9 li 8 g 2 e ) n . ce K . n M ow cG le r d a g w e - - D so e n a , n A , . T (1 ., 9 K 93 a ) e . lb P l l i a n n g n , i L ng .P w ., it K h i d r e m a a d n li , n J e . s , i a n n s d to N ch ic a h s o ti l c - D na e t J io o n n s g . , I G n . I ( J 1 C 9 A 8 I 1 - ) 8 . 1 G ,p e p n . er 6 a 7 li – z 6 a 9 ti . onsbasedonexpla-
domains.InAAAI-93,pp.574–579. DeJong, G.(1982). AnoverviewoftheFRUMP
Dayan,P.(1992). TheconvergenceofTD(λ)for system.InLehnert,W.andRingle,M.(Eds.),Strate-
generalλ.MachineLearning,8(3–4),341–362. D
pr
e
o
a
je
n
c
,
ti
T
o
.
n
a
a
n
n
d
d
K
ac
a
t
n
io
az
n
a
.
w
In
a,
IJ
K
C
.
A
(1
I-
9
8
8
9
9
,
a
p
)
p
.
.
A
985
m
–
o
9
d
9
e
0
l
.
for
giesforNaturalLanguageProcessing,pp.149–176.
Dayan,P.andAbbott,L.F.(2001).TheoreticalNeu- LawrenceErlbaum.
roscience: ComputationalandMathematicalMod- Dean,T.andKanazawa,K.(1989b). Amodelfor DeJong,G.andMooney,R.(1986). Explanation-
elingofNeuralSystems.MITPress. reasoningaboutpersistenceandcausation. Compu- basedlearning:Analternativeview.MachineLearn-
tationalIntelligence,5(3),142–150.
Dayan,P.andNiv,Y.(2008).Reinforcementlearn- ing,1,145–176.
ingandthebrain: Thegood,thebadandtheugly. Dean,T.,Kanazawa,K.,andShewchuk,J.(1990). DelMoral,P.,Doucet,A.,andJasra,A.(2006).Se-
CurrentOpinioninNeurobiology,18(2),185–196. Prediction, observationandestimationinplanning quentialMonteCarlosamplers. J.RoyalStatistical
deDombal, F.T.,Leaper, D.J.,Horrocks, J.C., andcontrol. In5thIEEEInternationalSymposium Society,SeriesB,68(3),411–436.
onIntelligentControl,Vol.2,pp.645–650.
andStaniland,J.R.(1974). Humanandcomputer- DelMoral,P.(2004).Feynman–KacFormulae,Ge-
aideddiagnosisofabdominalpain: Furtherreport Dean,T.andWellman,M.P.(1991). Planningand nealogicalandInteractingParticleSystemswithAp-
withemphasisonperformanceofclinicians.British Control.MorganKaufmann. plications.Springer-Verlag.
MedicalJournal,1,376–380.
Dearden,R.,Friedman,N.,andAndre,D.(1999). Delgrande,J.andSchaub,T.(2003).Ontherelation
deDombal,F.T.,Staniland,J.R.,andClamp,S.E. Model-basedBayesianexploration.InUAI-99. betweenReiter’sdefaultlogicandits(major)vari-
(1981). Geographicalvariationindiseasepresenta- ants. InSeventhEuropeanConferenceonSymbolic
tion.MedicalDecisionMaking,1,59–69. Dearden, R., Friedman, N., and Russell, S. J. andQuantitativeApproachestoReasoningwithUn-
de Finetti, B. (1937). Le pre´vision: ses lois
(1998).Bayesianq-learning.InAAAI-98.
certainty,pp.452–463.
logiques, ses sources subjectives. Ann. Inst. Debevec,P.,Taylor,C.,andMalik,J.(1996). Mod- Dempster, A. P. (1968). A generalization of
Poincare´,7,1–68. elingandrenderingarchitecturefromphotographs: Bayesian inference. J. Royal Statistical Society,
deFinetti,B.(1993). Onthesubjectivemeaning Ahybridgeometry-andimage-basedapproach. In 30(SeriesB),205–247.
Proc.23rdAnnualConferenceonComputerGraph-
ofprobability. InMonari,P.andCocchi,D.(Eds.), ics(SIGGRAPH),pp.11–20. Dempster,A.P.,Laird,N.,andRubin,D.(1977).
ProbabilitaeInduzione,pp.291–321.Clueb. Maximumlikelihoodfromincompletedataviathe
deFreitas,J.F.G.,Niranjan,M.,andGee,A.H. Debreu,G.(1960).Topologicalmethodsincardinal EMalgorithm. J.RoyalStatisticalSociety,39(Se-
(2000). SequentialMonteCarlomethodstotrain utilitytheory. InArrow,K.J.,Karlin,S.,andSup- riesB),1–38.
neuralnetworkmodels.NeuralComputation,12(4), pes,P.(Eds.),MathematicalMethodsintheSocial Deng,X.andPapadimitriou,C.H.(1990). Explor-
933–953. Sciences,1959.StanfordUniversityPress. inganunknowngraph.InFOCS-90,pp.355–361.
de Kleer, J. (1975). Qualitative andquantitative Dechter,R.(1990a).Enhancementschemesforcon- Denis, F. (2001). Learning regular languages
knowledgeinclassicalmechanics. Tech.rep.AI- straintprocessing:Backjumping,learningandcutset fromsimplepositiveexamples. MachineLearning,
TR-352,MITArtificialIntelligenceLaboratory. decomposition.AIJ,41,273–312. 44(1/2),37–66.
deKleer,J.(1989). AcomparisonofATMSand Dechter,R.(1990b). Ontheexpressivenessofnet- Dennett,D.C.(1984).Cognitivewheels:theframe
CSPtechniques.InIJCAI-89,Vol.1,pp.290–296. workswithhiddenvariables. InAAAI-90,pp.379– problemofAI. InHookway,C.(Ed.),Minds,Ma-
deKleer,J.andBrown,J.S.(1985). Aqualitative 385. chines, andEvolution: PhilosophicalStudies, pp.
physicsbasedonconfluences. InHobbs,J.R.and Dechter, R. (1992). Constraint networks. In 129–151.CambridgeUniversityPress.
Moore,R.C.(Eds.),FormalTheoriesoftheCom- Shapiro,S.(Ed.),EncyclopediaofArtificialIntelli- Dennett,D.C.(1991). ConsciousnessExplained.
monsenseWorld,chap.4,pp.109–183.Ablex. gence(2ndedition).,pp.276–285.WileyandSons. PenguinPress.
Bibliography 1071
Denney,E.,Fischer,B.,andSchumann,J.(2006). Doorenbos,R.(1994).Combiningleftandrightun- Dunn,H.L.(1946).Recordlinkage”.Am.J.Public
An empirical evaluation of automated theorem linkingformatchingalargenumberoflearnedrules. Health,36(12),1412–1416.
proversinsoftwarecertification. Int.J.AITools, InAAAI-94.
Durfee,E.H.andLesser,V.R.(1989). Negotiat-
15(1),81–107.
Doran,J.andMichie,D.(1966).Experimentswith ingtaskdecompositionandallocationusingpartial
Descartes,R.(1637).Discourseonmethod.InCot- thegraphtraverserprogram. Proc.RoyalSocietyof globalplanning.InHuhns,M.andGasser,L.(Eds.),
tingham,J.,Stoothoff,R.,andMurdoch,D.(Eds.), London,294,SeriesA,235–259. DistributedAI,Vol.2.MorganKaufmann.
The Philosophical Writings of Descartes, Vol. I.
Dorf,R.C.andBishop,R.H.(2004).ModernCon- Durme,B.V.andPasca,M.(2008). Findingcars,
CambridgeUniversityPress,Cambridge,UK.
trolSystems(10thedition).Prentice-Hall. goddessesandenzymes:Parametrizableacquisition
Descartes,R.(1641). Meditationsonfirstphiloso- of labeled instances for open-domain information
Doucet, A. (1997). Monte Carlo methods for
phy.InCottingham,J.,Stoothoff,R.,andMurdoch, extraction.InAAAI-08,pp.1243–1248.
BayesianestimationofhiddenMarkovmodels:Ap-
D.(Eds.),ThePhilosophicalWritingsofDescartes,
plicationtoradiationsignals. Ph.D.thesis,Univer- Dyer, M. (1983). In-Depth Understanding.
Vol. II. Cambridge University Press, Cambridge,
site´deParis-Sud. MITPress.
UK.
Descotte, Y. and Latombe, J.-C. (1985). Mak- Doucet, A., de Freitas, N., and Gordon, N. Dyson,G.(1998).Darwinamongthemachines:the
ingcompromisesamongantagonistconstraintsina (2001). SequentialMonteCarloMethodsinPrac- evolutionofglobalintelligence.PerseusBooks.
planner.AIJ,27,183–217. tice.Springer-Verlag. Duzeroski,S.,Muggleton,S.H.,andRussell,S.J.
Detwarasiti,A.andShachter,R.D.(2005). Influ- Doucet,A.,deFreitas,N.,Murphy,K.,andRussell, (1992). PAC-learnabilityofdeterminatelogicpro-
encediagramsforteamdecisionanalysis. Decision S.J.(2000). Rao-blackwellisedparticlefilteringfor grams.InCOLT-92,pp.128–135.
Analysis,2(4),207–228. dynamicbayesiannetworks.InUAI-00. Earley,J.(1970). Anefficientcontext-freeparsing
Devroye,L.(1987). Acourseindensityestimation. Dowling,W.F.andGallier,J.H.(1984). Linear- algorithm.CACM,13(2),94–102.
Birkhauser. timealgorithmsfortestingthesatisfiabilityofpropo- Edelkamp,S.(2009).Scalingsearchwithsymbolic
sitionalHornformulas. J.LogicProgramming,1, patterndatabases.InModelCheckingandArtificial
Dickmanns, E. D. and Zapp, A. (1987). Au- 267–284. Intelligence(MOCHART),pp.49–65.
tonomous high speed road vehicle guidance by
computer vision. In Automatic Control—World Dowty,D.,Wall,R.,andPeters,S.(1991). Intro- Edmonds, J. (1965). Paths, trees, and flowers.
Congress,1987:SelectedPapersfromthe10thTri- ductiontoMontagueSemantics.D.Reidel. CanadianJournalofMathematics,17,449–467.
ennialWorldCongressoftheInternationalFedera- Doyle,J.(1979). Atruthmaintenancesystem. AIJ, Edwards,P.(Ed.).(1967).TheEncyclopediaofPhi-
tionofAutomaticControl,pp.221–226. 12(3),231–272. losophy.Macmillan.
Dietterich,T.(1990). Machinelearning. Annual Doyle,J.(1983). Whatisrationalpsychology?To- Een, N. and So¨rensson, N. (2003). An extensi-
ReviewofComputerScience,4,255–306. wardamodernmentalphilosophy.AIMag,4(3),50– bleSAT-solver. InGiunchiglia,E.andTacchella,
Dietterich, T.(2000). Hierarchicalreinforcement 53. A.(Eds.),TheoryandApplicationsofSatisfiability
learningwiththeMAXQvaluefunctiondecompo- Doyle,J.andPatil,R.(1991).Twothesesofknowl- Testing: 6thInternationalConference(SAT2003).
sition.JAIR,13,227–303. edge representation: Language restrictions, taxo- Springer-Verlag.
Dijkstra,E.W.(1959). Anoteontwoproblemsin nomicclassification,andtheutilityofrepresentation Eiter, T., Leone, N., Mateis, C., Pfeifer, G., and
connexionwithgraphs. NumerischeMathematik,1, services.AIJ,48(3),261–297. Scarcello,F.(1998). TheKRsystemdlv: Progress
269–271. Drabble,B.(1990). Missionschedulingforspace- report,comparisonsandbenchmarks.InKR-98,pp.
Dijkstra,E.W.(1984). Thethreatstocomputing craft:DiariesofT-SCHED. InExpertPlanningSys- 406–417.
science. InACMSouthCentralRegionalConfer- tems,pp.76–81.InstituteofElectricalEngineers. Elio,R.(Ed.).(2002). CommonSense,Reasoning,
ence. Dredze,M.,Crammer,K.,andPereira,F.(2008). andRationality.OxfordUniversityPress.
Dillenburg,J.F.andNelson,P.C.(1994).Perimeter Confidence-weightedlinearclassification.InICML- Elkan,C.(1993).Theparadoxicalsuccessoffuzzy
search.AIJ,65(1),165–178. 08,pp.264–271. logic.InAAAI-93,pp.698–703.
Dinh,H.,Russell,A.,andSu,Y.(2007). Onthe Dreyfus,H.L.(1972). WhatComputersCan’tDo: Elkan, C. (1997). Boosting and naive Bayesian
valueofgoodadvice: ThecomplexityofA*with ACritiqueofArtificialReason.HarperandRow. learning. Tech.rep.,DepartmentofComputerSci-
accurateheuristics.InAAAI-07. Dreyfus,H.L.(1992). WhatComputersStillCan’t enceandEngineering,UniversityofCalifornia,San
Dissanayake,G.,Newman,P.,Clark,S.,Durrant- Do:ACritiqueofArtificialReason.MITPress. Diego.
Whyte,H.,andCsorba,M.(2001).Asolutiontothe Dreyfus,H.L.andDreyfus,S.E.(1986).Mindover Ellsberg,D.(1962).Risk,Ambiguity,andDecision.
simultaneouslocalisationandmapbuilding(SLAM) Machine:ThePowerofHumanIntuitionandExper- Ph.D.thesis,HarvardUniversity.
problem. IEEETransactionsonRoboticsandAu- tiseintheEraoftheComputer.Blackwell. Elman,J.,Bates,E.,Johnson,M.,Karmiloff-Smith,
tomation,17(3),229–241.
Dreyfus, S. E. (1969). An appraisal of some A.,Parisi,D.,andPlunkett,K.(1997). Rethinking
Do,M.B.andKambhampati,S.(2001). Sapa: A shortest-pathsalgorithms.OperationsResearch,17, Innateness.MITPress.
domain-independentheuristicmetrictemporalplan- 395–412. Empson,W.(1953).SevenTypesofAmbiguity.New
ner.InECP-01.
Dubois,D.andPrade,H.(1994).Asurveyofbelief Directions.
Do,M.B.andKambhampati,S.(2003).Planningas revisionandupdating rules invarious uncertainty Enderton,H.B.(1972). AMathematicalIntroduc-
constraintsatisfaction: solvingtheplanninggraph models.Int.J.IntelligentSystems,9(1),61–100. tiontoLogic.AcademicPress.
bycompilingitintoCSP.AIJ,132(2),151–182.
Duda,R.O.,Gaschnig,J.,andHart,P.E.(1979). Epstein, R., Roberts, G., and Beber, G. (Eds.).
Doctorow,C.(2001).Metacrap:Puttingthetorchto ModeldesignintheProspectorconsultantsystem (2008).ParsingtheTuringTest.Springer.
sevenstraw-menofthemeta-utopia. www.well. formineralexploration. InMichie, D.(Ed.), Ex-
com/˜doctorow/metacrap.htm. pertSystemsintheMicroelectronicAge,pp.153– Erdmann,M.A.andMason,M.(1988).Anexplo-
Domingos,P.andPazzani,M.(1997). Ontheopti- 167.EdinburghUniversityPress. rationofsensorlessmanipulation. IEEEJournalof
RoboticsandAutomation,4(4),369–379.
malityofthesimpleBayesianclassifierunderzero–
Duda,R.O.andHart,P.E.(1973).Patternclassifi-
oneloss.MachineLearning,29,103–30. Ernst,H.A.(1961). MH-1,aComputer-Operated
cationandsceneanalysis.Wiley.
MechanicalHand. Ph.D.thesis,MassachusettsIn-
Domingos,P.andRichardson,M.(2004). Markov
Duda,R.O.,Hart,P.E.,andStork,D.G.(2001). stituteofTechnology.
logic:Aunifyingframeworkforstatisticalrelational
PatternClassification(2ndedition).Wiley.
learning.InProc.ICML-04WorkshoponStatistical Ernst,M.,Millstein,T.,andWeld,D.S.(1997).Au-
RelationalLearning. Dudek,G.andJenkin,M.(2000). Computational tomaticSAT-compilationofplanningproblems. In
PrinciplesofMobileRobotics. CambridgeUniver- IJCAI-97,pp.1169–1176.
Donninger,C.andLorenz,U.(2004). Thechess
sityPress.
monsterhydra. In Proc.14thInternational Con- Erol,K.,Hendler,J.,andNau,D.S.(1994). HTN
ferenceonField-ProgrammableLogicandApplica- Duffy,D.(1991).PrinciplesofAutomatedTheorem planning:Complexityandexpressivity.InAAAI-94,
tions,pp.927–932. Proving.JohnWiley&Sons. pp.1123–1128.
1072 Bibliography
Erol,K.,Hendler,J.,andNau,D.S.(1996). Com- Felzenszwalb,P.andHuttenlocher,D.(2000).Effi- Forgy,C.(1981). OPS5user’smanual. Technical
plexityresultsforHTNplanning.AIJ,18(1),69–93. cientmatchingofpictorialstructures.InCVPR. reportCMU-CS-81-135,ComputerScienceDepart-
ment,Carnegie-MellonUniversity.
Etzioni,A.(2004). FromEmpiretoCommunity:A Felzenszwalb,P.andMcAllester,D.A.(2007).The
NewApproachtoInternationalRelation. Palgrave generalizedA*architecture.JAIR. Forgy,C.(1982). Afastalgorithmforthemany
Macmillan. patterns/manyobjectsmatchproblem. AIJ,19(1),
Ferguson,T.(1992). Matewithknightandbishop
17–37.
Etzioni,O.(1989).Tractabledecision-analyticcon- inkriegspiel. TheoreticalComputerScience,96(2),
trol. In Proc. First International Conference on 389–403. Forsyth,D.andPonce,J.(2002).ComputerVision:
KnowledgeRepresentationandReasoning,pp.114– AModernApproach.PrenticeHall.
Ferguson,T.(1995). Matewiththetwobishopsin
125.
kriegspiel.www.math.ucla.edu/˜tom/papers. Fourier, J. (1827). Analyse des travaux de
Etzioni, O.,Banko, M.,Soderland, S.,andWeld, l’Acade´mie Royale desSciences, pendantl’anne´e
Ferguson, T.(1973). Bayesiananalysis ofsome
D.S.(2008). Openinformationextractionfromthe 1824;partiemathe´matique. Histoiredel’Acade´mie
nonparametricproblems. AnnalsofStatistics,1(2),
web.CACM,51(12). RoyaledesSciencesdeFrance,7,xlvii–lv.
209–230.
Etzioni, O., Hanks, S., Weld, D. S., Draper, D., Fox,C.andTversky,A.(1995). Ambiguityaver-
Ferraris,P.andGiunchiglia,E.(2000).Planningas
Lesh,N.,andWilliamson,M.(1992). Anapproach sionandcomparativeignorance. QuarterlyJournal
satisabilityinnondeterministicdomains. InAAAI-
toplanningwithincompleteinformation.InKR-92. ofEconomics,110(3),585–603.
00,pp.748–753.
Etzioni,O.andWeld,D.S.(1994).Asoftbot-based
Ferriss,T.(2007).The4-HourWorkweek.Crown.
Fox,D., Burgard, W.,Dellaert, F., andThrun, S.
interfacetotheInternet.CACM,37(7),72–76. (1999). Montecarlolocalization:Efficientposition
Fikes,R.E.,Hart,P.E.,andNilsson,N.J.(1972). estimationformobilerobots.InAAAI-99.
Etzioni,O.,Banko,M.,andCafarella,M.J.(2006). Learningandexecutinggeneralizedrobotplans.AIJ,
Machinereading.InAAAI-06. 3(4),251–288. Fox,M.S.(1990). Constraint-guidedscheduling:
AshorthistoryofresearchatCMU. Computersin
Etzioni,O.,Cafarella,M.J.,Downey,D.,Popescu, Fikes,R.E.andNilsson,N.J.(1971). STRIPS:A Industry,14(1–3),79–88.
A.-M.,Shaked,T.,Soderland,S.,Weld,D.S.,and newapproachtotheapplicationoftheoremproving
Yates, A.(2005). Unsupervisednamed-entityex- toproblemsolving.AIJ,2(3–4),189–208. Fox,M.S.,Allen,B.,andStrohm,G.(1982). Job
tractionfromtheweb:Anexperimentalstudy. AIJ, shop scheduling: An investigation in constraint-
165(1),91–134. Fikes,R.E.andNilsson,N.J.(1993). STRIPS,a directedreasoning.InAAAI-82,pp.155–158.
retrospective.AIJ,59(1–2),227–232.
Evans,T.G.(1968). Aprogramforthesolutionof Fox,M.S.andLong,D.(1998). Theautomaticin-
aclassofgeometric-analogyintelligence-testques- Fine,S.,Singer,Y.,andTishby,N.(1998).Thehier- ferenceofstateinvariantsinTIM.JAIR,9,367–421.
tions.InMinsky,M.L.(Ed.),SemanticInformation archicalhiddenmarkovmodel:Analysisandappli- Franco,J.andPaull,M.(1983). Probabilisticanal-
Processing,pp.271–353.MITPress. cations.MachineLearning,32(41–62). ysisoftheDavisPutnamprocedureforsolvingthe
Fagin,R.,Halpern,J.Y.,Moses,Y.,andVardi,M.Y. Finney,D.J.(1947). Probitanalysis:Astatistical satisfiabilityproblem. DiscreteAppliedMathemat-
(1995).ReasoningaboutKnowledge.MITPress. treatmentofthesigmoidresponsecurve.Cambridge ics,5,77–87.
UniversityPress.
Fahlman,S.E.(1974).Aplanningsystemforrobot Frank,I.,Basin,D.A.,andMatsubara,H.(1998).
constructiontasks.AIJ,5(1),1–49. Firth,J.(1957).PapersinLinguistics.OxfordUni- Findingoptimalstrategiesforimperfectinformation
versityPress. games.InAAAI-98,pp.500–507.
Faugeras,O.(1993).Three-DimensionalComputer
Vision:AGeometricViewpoint.MITPress. Fisher,R.A.(1922). Onthemathematicalfounda- Frank,R.H.andCook,P.J.(1996). TheWinner-
tionsoftheoreticalstatistics.PhilosophicalTransac- Take-AllSociety.Penguin.
Faugeras,O.,Luong, Q.-T.,andPapadopoulo, T. tionsoftheRoyalSocietyofLondon,SeriesA222,
(2001). The GeometryofMultiple Images. MIT 309–368. Franz,A.(1996). AutomaticAmbiguityresolution
Press. inNaturalLanguageProcessing:AnEmpiricalAp-
Fix, E. and Hodges, J. L. (1951). Discrimina- proach.Springer.
Fearing,R.S.andHollerbach,J.M.(1985). Basic toryanalysis—Nonparametricdiscrimination:Con-
solidmechanicsfortactilesensing. Int.J.Robotics sistencyproperties. Tech.rep. 21-49-004, USAF Franz,A.andBrants,T.(2006).Allourn-gramare
Research,4(3),40–54. SchoolofAviationMedicine. belongtoyou.Blogposting.
Featherstone, R.(1987). RobotDynamicsAlgo- Floreano,D.,Zufferey,J.C.,Srinivasan,M.V.,and Frege, G.(1879). Begriffsschrift, eineder arith-
rithms.KluwerAcademicPublishers. Ellington, C.(2009). FlyingInsectsand Robots. metischennachgebildeteFormelsprachedesreinen
Denkens.Halle,Berlin. Englishtranslationappears
Feigenbaum,E.A.(1961). Thesimulationofver- Springer. invanHeijenoort(1967).
ballearningbehavior.Proc.WesternJointComputer Fogel, D. B.(2000). Evolutionary Computation:
Conference,19,121–131. TowardaNewPhilosophyofMachineIntelligence. Freitag,D.andMcCallum,A.(2000). Information
extractionwithhmmstructureslearnedbystochastic
Feigenbaum,E.A.,Buchanan,B.G.,andLeder- IEEEPress. optimization.InAAAI-00.
berg, J.(1971). Ongeneralityandproblemsolv- Fogel,L.J.,Owens,A.J.,andWalsh,M.J.(1966).
ing: AcasestudyusingtheDENDRALprogram. ArtificialIntelligencethroughSimulatedEvolution. Freuder,E.C.(1978). Synthesizingconstraintex-
InMeltzer,B.andMichie,D.(Eds.),MachineIntel- Wiley. pressions.CACM,21(11),958–966.
ligence6,pp.165–190.EdinburghUniversityPress. Freuder,E.C.(1982). Asufficientconditionfor
Foo, N.(2001). Whyengineeringmodelsdonot
Feldman,J.andSproull,R.F.(1977).Decisionthe- haveaframeproblem. InDiscreteeventmodeling backtrack-freesearch.JACM,29(1),24–32.
oryandartificialintelligenceII:Thehungrymon- andsimulationtechnologies: atapestryofsystems Freuder,E.C.(1985). Asufficientconditionfor
key. Technicalreport, ComputerScienceDepart- andAI-basedtheoriesandmethodologies.Springer. backtrack-boundedsearch.JACM,32(4),755–761.
ment,UniversityofRochester.
Forbes,J.(2002).LearningOptimalControlforAu- Freuder,E.C.andMackworth,A.K.(Eds.).(1994).
Feldman,J.andYakimovsky,Y.(1974). Decision tonomousVehicles.Ph.D.thesis,UniversityofCali- Constraint-basedreasoning.MITPress.
theoryandartificialintelligenceI:Semantics-based fornia.
regionanalyzer.AIJ,5(4),349–371. Freund,Y.andSchapire,R.E.(1996).Experiments
Forbus,K.D.(1985). Qualitativeprocesstheory. withanewboostingalgorithm.InICML-96.
Fellbaum,C.(2001). Wordnet:AnElectronicLexi- InBobrow,D.(Ed.),QualitativeReasoningAbout
Freund,Y.andSchapire,R.E.(1999).Largemargin
calDatabase.MITPress. PhysicalSystems,pp.85–186.MITPress.
classificationusingtheperceptronalgorithm. Ma-
Fellegi,I.andSunter,A.(1969).Atheoryforrecord Forbus, K.D.anddeKleer, J.(1993). Building chineLearning,37(3),277–296.
linkage”.JASA,64,1183–1210. ProblemSolvers.MITPress.
Friedberg, R. M. (1958). A learning machine:
Felner,A.,Korf,R.E.,andHanan,S.(2004).Addi- Ford,K.M.andHayes,P.J.(1995). TuringTest PartI. IBMJournalofResearchandDevelopment,
tivepatterndatabaseheuristics.JAIR,22,279–318. consideredharmful.InIJCAI-95,pp.972–977. 2,2–13.
Felner,A.,Korf,R.E.,Meshulam,R.,andHolte, Forestier,J.-P.andVaraiya,P.(1978). Multilayer Friedberg, R. M., Dunham, B., and North, T.
R.(2007).Compressedpatterndatabases.JAIR,30, controloflargeMarkovchains. IEEETransactions (1959).Alearningmachine:PartII.IBMJournalof
213–247. onAutomaticControl,23(2),298–304. ResearchandDevelopment,3(3),282–287.
Bibliography 1073
Friedgut,E.(1999). Necessaryandsufficientcon- Gaschnig,J.(1979).Performancemeasurementand Gentner,D.(1983). Structuremapping:Atheoret-
ditionsforsharpthresholdsofgraphproperties,and analysisofcertainsearchalgorithms. Technicalre- icalframeworkforanalogy. CognitiveScience,7,
thek-SATproblem. J.AmericanMathematicalSo- port CMU-CS-79-124, Computer Science Depart- 155–170.
ciety,12,1017–1054. ment,Carnegie-MellonUniversity.
Gentner,D.andGoldin-Meadow,S.(Eds.).(2003).
Friedman,G.J.(1959). Digitalsimulationofan Gasser,R.(1995). Efficientlyharnessingcomputa- Languageinmind: Advancesinthestudyoflan-
evolutionaryprocess.GeneralSystemsYearbook,4, tionalresourcesforexhaustivesearch.Ph.D.thesis, guageandthough.MITPress.
171–184. ETHZu¨rich. Gerevini,A.andLong,D.(2005). Planconstraints
Friedman,J.,Hastie,T.,andTibshirani,R.(2000). Gasser,R.(1998). Solvingninemen’smorris. In andpreferencesinPDDL3.Tech.rep.,Dept.ofElec-
Additive logistic regression: A statisticalview of Nowakowski,R.(Ed.),GamesofNoChance.Cam- tronicsforAutomation,UniversityofBrescia,Italy.
boosting.AnnalsofStatistics,28(2),337–374. bridgeUniversityPress. Gerevini,A.andSerina,I.(2002). LPG:Aplan-
Friedman,N.(1998). TheBayesianstructuralEM Gat,E.(1998). Three-layeredarchitectures.InKo- nerbasedonplanninggraphswithactioncosts. In
algorithm.InUAI-98. rtenkamp,D.,Bonasso,R.P.,andMurphy,R.(Eds.), ICAPS-02,pp.281–290.
Friedman,N.andGoldszmidt,M.(1996).Learning AI-basedMobileRobots:CaseStudiesofSuccessful Gerevini, A. and Serina, I. (2003). Planning as
Bayesiannetworkswithlocalstructure. InUAI-96, RobotSystems,pp.195–210.MITPress. propositionalCSP:fromwalksattolocalsearchfor
pp.252–262. actiongraphs.Constraints,8,389–413.
Gauss, C. F. (1809). Theoria Motus Corporum
Friedman, N. and Koller, D. (2003). Be- CoelestiuminSectionibus ConicisSolem Ambien- Gershwin,G.(1937).Let’scallthewholethingoff.
ing Bayesian about Bayesian network structure: tium.SumtibusF.PerthesetI.H.Besser,Hamburg. Song.
A Bayesian approach to structure discovery in
Bayesiannetworks.MachineLearning,50,95–125. Gauss, C. F. (1829). Beitra¨ge zur theorie der Getoor,L.andTaskar,B.(Eds.).(2007). Introduc-
algebraischen gleichungen. Collected in Werke, tiontoStatisticalRelationalLearning.MITPress.
Friedman, N., Murphy, K., and Russell, S. J. Vol.3,pages71–102.K.GesellschaftWissenschaft, Ghahramani,Z.andJordan,M.I.(1997). Facto-
(1998). Learningthestructureofdynamicproba- Go¨ttingen,Germany,1876. rialhiddenMarkovmodels. MachineLearning,29,
bilisticnetworks.InUAI-98.
Gawande,A.(2002). Complications:ASurgeon’s 245–274.
Friedman,N.(2004). Inferringcellularnetworks NotesonanImperfectScience.MetropolitanBooks. Ghahramani, Z. (1998). Learning dynamic
using probabilistic graphical models. Science,
303(5659),799–805. Geiger,D.,Verma,T.,andPearl,J.(1990).Identify- bayesiannetworks. InAdaptiveProcessingofSe-
ingindependenceinBayesiannetworks. Networks, quencesandDataStructures,pp.168–197.
Fruhwirth,T.andAbdennadher,S.(2003). Essen- 20(5),507–534. Ghahramani,Z.(2005).Tutorialonnonparametric
tialsofconstraintprogramming.CambridgeUniver-
Bayesianmethods. TutorialpresentationattheUAI
sityPress. Geisel, T. (1955). On Beyond Zebra. Random
Conference.
House.
Fuchs,J.J.,Gasquet,A.,Olalainty,B.,andCurrie,
Ghallab,M.,Howe,A.,Knoblock,C.A.,andMc-
K.W.(1990). PlanERS-1:Anexpertplanningsys- Gelb, A. (1974). Applied Optimal Estimation. Dermott,D.(1998). PDDL—Theplanningdomain
temforgeneratingspacecraftmissionplans.InFirst MITPress. definitionlanguage. Tech.rep.DCSTR-1165,Yale
InternationalConferenceonExpertPlanningSys-
tems,pp.70–75.InstituteofElectricalEngineers. Gelernter,H.(1959). Realizationofageometry- CenterforComputationalVisionandControl.
theorem proving machine. In Proc. an Interna- Ghallab,M.andLaruelle,H.(1994). Representa-
Fudenberg,D.andTirole,J.(1991). Gametheory. tional Conference on Information Processing, pp. tionandcontrolinIxTeT,atemporalplanner. In
MITPress. 273–282.UNESCOHouse. AIPS-94,pp.61–67.
Fukunaga,A.S.,Rabideau,G.,Chien,S.,andYan, Gelfond,M.andLifschitz,V.(1988).Compilingcir- Ghallab,M.,Nau,D.S.,andTraverso,P.(2004).
D. (1997). ASPEN: A framework for automated cumscriptivetheoriesintologicprograms. InNon- AutomatedPlanning:Theoryandpractice.Morgan
planningandschedulingofspacecraftcontroland MonotonicReasoning:2ndInternationalWorkshop Kaufmann.
operations.InProc.InternationalSymposiumonAI, Proceedings,pp.74–99.
RoboticsandAutomationinSpace,pp.181–187. Gibbs,R.W.(2006).Metaphorinterpretationasem-
Gelfond,M.(2008).Answersets.InvanHarmelan, bodiedsimulation.Mind,21(3),434–458.
Fung, R. and Chang, K. C. (1989). Weighting
F.,Lifschitz,V.,andPorter,B.(Eds.),Handbookof
andintegratingevidenceforstochasticsimulationin Gibson,J.J.(1950). ThePerceptionoftheVisual
KnowledgeRepresentation,pp.285–316.Elsevier.
Bayesiannetworks.InUAI-98,pp.209–220. World.HoughtonMifflin.
Gaddum,J.H.(1933). Reportsonbiologicalstan- Gelly,S.andSilver,D.(2008). Achievingmaster Gibson,J.J.(1979). TheEcologicalApproachto
dardIII:Methodsofbiologicalassaydependingona levelplayin9x9computergo. InAAAI-08,pp. VisualPerception.HoughtonMifflin.
quantalresponse. Specialreportseriesofthemedi- 1537–1540.
Gilks,W.R.,Richardson,S.,andSpiegelhalter,D.J.
calresearchcouncil183,MedicalResearchCouncil. Gelman,A.,Carlin,J.B.,Stern,H.S.,andRubin, (Eds.).(1996). MarkovchainMonteCarloinprac-
Gaifman,H.(1964). Concerningmeasuresinfirst D.(1995). BayesianDataAnalysis. Chapman& tice.ChapmanandHall.
ordercalculi. IsraelJournalofMathematics,2,1– Hall.
Gilks,W.R.,Thomas,A.,andSpiegelhalter,D.J.
18. Geman,S.andGeman,D.(1984).Stochasticrelax- (1994). A language and program for complex
Gallaire,H.andMinker,J.(Eds.).(1978). Logic ation,Gibbsdistributions,andBayesianrestoration Bayesianmodelling.TheStatistician,43,169–178.
andDatabases.Plenum. ofimages.PAMI,6(6),721–741.
Gilmore,P.C.(1960).Aproofmethodforquantifi-
Gallier,J.H.(1986). LogicforComputerScience: Genesereth,M.R.(1984). Theuseofdesignde- cationtheory: Itsjustificationandrealization. IBM
FoundationsofAutomaticTheoremProving.Harper scriptions in automated diagnosis. AIJ, 24(1–3), JournalofResearchandDevelopment,4,28–35.
andRow. 411–436.
Ginsberg,M.L.(1993). EssentialsofArtificialIn-
Gamba,A.,Gamberini,L.,Palmieri,G.,andSanna, Genesereth,M.R.andNilsson,N.J.(1987). Log- telligence.MorganKaufmann.
R.(1961). FurtherexperimentswithPAPA. Nuovo icalFoundationsofArtificialIntelligence. Morgan Ginsberg, M. L. (1999). GIB: Steps toward an
CimentoSupplemento,20(2),221–231. Kaufmann. expert-levelbridge-playingprogram. InIJCAI-99,
Garding,J.(1992). Shapefromtextureforsmooth Genesereth, M. R. and Nourbakhsh, I. (1993). pp.584–589.
curvedsurfacesinperspectiveprojection. J.Mathe- Time-savingtipsforproblemsolvingwithincom- Ginsberg,M.L.,Frank,M.,Halpin,M.P.,andTor-
maticalImagingandVision,2(4),327–350. pleteinformation.InAAAI-93,pp.724–730. rance,M.C.(1990). Searchlessonslearnedfrom
Gardner, M.(1968). LogicMachines,Diagrams Genesereth,M.R.andSmith,D.E.(1981). Meta- crosswordpuzzles.InAAAI-90,Vol.1,pp.210–215.
andBooleanAlgebra.Dover. levelarchitecture. MemoHPP-81-6,ComputerSci- Ginsberg,M.L.(2001). GIB:Imperfectinfoorma-
Garey,M.R.andJohnson,D.S.(1979).Computers enceDepartment,StanfordUniversity. tioninacomputationallychallenginggame. JAIR,
andIntractability.W.H.Freeman. 14,303–358.
Gent,I.,Petrie,K.,andPuget,J.-F.(2006). Sym-
Gaschnig,J.(1977).Ageneralbacktrackalgorithm metryinconstraintprogramming. InRossi,F.,van Gionis,A.,Indyk,P.,andMotwani,R.(1999).Simi-
thateliminatesmostredundanttests.InIJCAI-77,p. Beek,P.,andWalsh,T.(Eds.),HandbookofCon- laritysearchinhighdimensionsvishashing.InProc.
457. straintProgramming.Elsevier. 25thVeryLargeDatabase(VLDB)Conference.
1074 Bibliography
Gittins,J.C.(1989).Multi-ArmedBanditAllocation Goodman,J.andHeckerman,D.(2004). Fighting Grinstead,C.andSnell,J.(1997). Introductionto
Indices.Wiley. spamwithstatistics. Significance,theMagazineof Probability.AMS.
theRoyalStatisticalSociety,1,69–72.
Glanc,A.(1978). Ontheetymologyoftheword Grove,W.andMeehl,P.(1996). Comparativeeffi-
“robot”.SIGARTNewsletter,67,12. Goodman,N.(1954). Fact,FictionandForecast. ciencyofinformal(subjective,impressionistic)and
UniversityofLondonPress. formal(mechanical,algorithmic)predictionproce-
Glover, F. andLaguna, M.(Eds.).(1997). Tabu
search.Kluwer. Goodman,N.(1977).TheStructureofAppearance dures:Theclinicalstatisticalcontroversy. Psychol-
(3rdedition).D.Reidel. ogy,PublicPolicy,andLaw,2,293–323.
Go¨del, K. (1930). U¨ber die Vollsta¨ndigkeit des
Logikkalku¨ls.Ph.D.thesis,UniversityofVienna. Gopnik,A.andGlymour,C.(2002). Causalmaps Gruber,T.(2004). InterviewofTomGruber. AIS
andbayesnets: Acognitiveandcomputationalac- SIGSEMISBulletin,1(3).
Go¨del, K. (1931). U¨ber formal unentscheidbare countoftheory-formation. InCaruthers,P.,Stich, Gu,J.(1989).ParallelAlgorithmsandArchitectures
Sa¨tze der Principia mathematica und verwandter S.,andSiegal,M.(Eds.),TheCognitiveBasisofSci- forVeryFastAISearch. Ph.D.thesis,Universityof
SystemeI.Monatsheftefu¨rMathematikundPhysik, ence.CambridgeUniversityPress. Utah.
38,173–198.
Gordon,D.M.(2000).AntsatWork.Norton. Guard,J.,Oglesby,F.,Bennett,J.,andSettle,L.
Goebel,J.,Volk,K.,Walker,H.,andGerbault,F.
(1989).Automaticclassificationofspectrafromthe Gordon,D.M.(2007). Controlwithouthierarchy. (1969). Semi-automatedmathematics. JACM,16,
infraredastronomicalsatellite(IRAS). Astronomy Nature,446(8),143. 49–62.
andAstrophysics,222,L5–L8. Gordon,M.J.,Milner,A.J.,andWadsworth,C.P. Guestrin,C.,Koller,D.,Gearhart,C.,andKanodia,
Goertzel,B.andPennachin,C.(2007). Artificial
(1979).EdinburghLCF.Springer-Verlag. N.(2003a).Generalizingplanstonewenvironments
inrelationalMDPs.InIJCAI-03.
GeneralIntelligence.Springer. Gordon,N.(1994).Bayesianmethodsfortracking.
Ph.D.thesis,ImperialCollege. Guestrin,C.,Koller,D.,Parr,R.,andVenkatara-
Gold,B.andMorgan,N.(2000).SpeechandAudio
man,S.(2003b). Efficientsolutionalgorithmsfor
SignalProcessing.Wiley. Gordon,N.,Salmond,D.J.,andSmith,A.F.M.
factoredMDPs.JAIR,19,399–468.
(1993). Novelapproachtononlinear/non-Gaussian
Gold,E.M.(1967). Languageidentificationinthe
Bayesian state estimation. IEE Proceedings F Guestrin, C., Lagoudakis, M. G., and Parr, R.
limit.InformationandControl,10,447–474.
(RadarandSignalProcessing),140(2),107–113. (2002). Coordinated reinforcement learning. In
Goldberg, A.V.,Kaplan,H.,andWerneck,R.F. Gorry,G.A.(1968). Strategiesforcomputer-aided ICML-02,pp.227–234.
(2006).Reachfora*:Efficientpoint-to-pointshort- diagnosis. MathematicalBiosciences,2(3–4),293– Guibas,L.J.,Knuth,D.E.,andSharir,M.(1992).
estpathalgorithms.InWorkshoponalgorithmengi- 318. RandomizedincrementalconstructionofDelaunay
neeringandexperiments,pp.129–143.
Gorry, G. A., Kassirer, J. P., Essig, A., and andVoronoidiagrams. Algorithmica,7,381–413.
Goldman, R.andBoddy, M.(1996). Expressive Schwartz, W.B.(1973). Decisionanalysisasthe Seealso17thInt.Coll.onAutomata,Languagesand
planningandexplicitknowledge. InAIPS-96,pp. basisforcomputer-aidedmanagementofacuterenal Programming,1990,pp.414–431.
110–117. failure.AmericanJournalofMedicine,55,473–484. Gumperz,J.andLevinson,S.(1996). Rethinking
Goldszmidt, M.andPearl, J.(1996). Qualitative Gottlob,G.,Leone,N.,andScarcello,F.(1999a).A LinguisticRelativity.CambridgeUniversityPress.
probabilitiesfordefaultreasoning, beliefrevision, comparisonofstructuralCSPdecompositionmeth- Guyon,I.andElisseeff,A.(2003).Anintroduction
andcausalmodeling.AIJ,84(1–2),57–112. ods.InIJCAI-99,pp.394–399. tovariableandfeatureselection. JMLR,pp.1157–
Golomb,S.andBaumert,L.(1965).Backtrackpro- Gottlob,G.,Leone,N.,andScarcello,F.(1999b). 1182.
ramming.JACM,14,516–524. Hypertreedecompositionsandtractablequeries. In Hacking,I.(1975). TheEmergenceofProbability.
Golub,G.,Heath,M.,andWahba,G.(1979). Gen- PODS-99,pp.21–32. CambridgeUniversityPress.
eralizedcross-validationasamethodforchoosinga Graham,S.L.,Harrison,M.A.,andRuzzo,W.L. Haghighi, A. and Klein, D. (2006). Prototype-
goodridgeparameter.Technometrics,21(2). (1980). Animprovedcontext-freerecognizer. ACM drivengrammarinduction.InCOLING-06.
Gomes,C.,Selman,B.,Crato,N.,andKautz,H. TransactionsonProgrammingLanguagesandSys-
tems,2(3),415–462. Hald,A.(1990).AHistoryofProbabilityandStatis-
(2000).Heavy-tailedphenomenainsatisfiabilityand
ticsandTheirApplicationsbefore1750.Wiley.
constrainprocessing.JAR,24,67–100. Grama,A.andKumar,V.(1995).Asurveyofparal-
lelsearchalgorithmsfordiscreteoptimizationprob- Halevy,A.(2007). Dataspaces: Anewparadigm
Gomes,C.,Kautz,H.,Sabharwal,A.,andSelman,
lems.ORSAJournalofComputing,7(4),365–385. for data integration. In Brazilian Symposium on
B.(2008). Satisfiabilitysolvers. InvanHarmelen,
Databases.
F.,Lifschitz,V.,andPorter,B.(Eds.),Handbookof Grassmann,H.(1861). LehrbuchderArithmetik.
KnowledgeRepresentation.Elsevier. Th.Chr.Fr.Enslin,Berlin. Halevy,A.,Norvig,P.,andPereira,F.(2009). The
unreasonableeffectivenessofdata.IEEEIntelligent
Gomes,C.andSelman,B.(2001). Algorithmport- Grayson, C. J. (1960). Decisions under uncer-
Systems,March/April,8–12.
folios.AIJ,126,43–62. tainty: Drillingdecisionsbyoilandgasoperators.
Tech.rep.,DivisionofResearch,HarvardBusiness Halpern,J.Y.(1990).Ananalysisoffirst-orderlog-
Gomes, C., Selman, B., and Kautz, H. (1998). School. icsofprobability.AIJ,46(3),311–350.
Boostingcombinatorialsearchthroughrandomiza-
tion.InAAAI-98,pp.431–437. Green,B.,Wolf,A.,Chomsky,C.,andLaugherty, Halpern,J.Y.(1999). Technicaladdendum,Cox’s
K.(1961). BASEBALL:Anautomaticquestionan- theoremrevisited.JAIR,11,429–435.
Gonthier,G.(2008). Formalproof–Thefour-color swerer. InProc.WesternJointComputerConfer-
theorem.NoticesoftheAMS,55(11),1382–1393. ence,pp.219–224. Halpern,J.Y.andWeissman,V.(2008).Usingfirst-
orderlogictoreasonaboutpolicies. ACMTransac-
Good,I.J.(1961). Acausalcalculus. BritishJour- Green,C.(1969a).Applicationoftheoremproving tionsonInformationandSystemSecurity,11(4).
nalofthePhilosophyofScience,11,305–318. toproblemsolving.InIJCAI-69,pp.219–239.
Hamming,R.W.(1991).TheArtofProbabilityfor
Good,I.J.(1965).Speculationsconcerningthefirst Green, C. (1969b). Theorem-proving by resolu- ScientistsandEngineers.Addison-Wesley.
ultraintelligentmachine. InAlt,F.L.andRubinoff, tionasabasisforquestion-answeringsystems. In
M.(Eds.),AdvancesinComputers,Vol.6,pp.31– Meltzer,B.,Michie,D.,andSwann,M.(Eds.),Ma- Hammond,K.(1989).Case-BasedPlanning:View-
88.AcademicPress. chineIntelligence4,pp.183–205.EdinburghUni- ingPlanningasaMemoryTask.AcademicPress.
Good,I.J.(1983). GoodThinking: TheFounda- versityPress. Hamscher,W.,Console,L.,andKleer,J.D.(1992).
tionsofProbabilityandItsApplications.University Green, C. and Raphael, B. (1968). The use of ReadingsinModel-basedDiagnosis.MorganKauf-
ofMinnesotaPress. theorem-proving techniquesin question-answering mann.
Goodman,D.andKeene,R.(1997). Manversus systems.InProc.23rdACMNationalConference. Han,X.andBoyden,E.(2007). Multiple-colorop-
Machine:KasparovversusDeepBlue. H3Publica- Greenblatt, R. D., Eastlake, D. E., and Crocker, ticalactivation,silencing,anddesynchronizationof
tions. S.D.(1967).TheGreenblattchessprogram.InProc. neural activity, with single-spike temporal resolu-
FallJointComputerConference,pp.801–810. tion.PLoSOne,e299.
Goodman,J.(2001). Abitofprogressinlanguage
modeling. Tech.rep.MSR-TR-2001-72,Microsoft Greiner,R.(1989). Towardsaformalanalysisof Hand,D.,Mannila,H.,andSmyth,P.(2001).Prin-
Research. EBL.InICML-89,pp.450–453. ciplesofDataMining.MITPress.
Bibliography 1075
Handschin,J.E.andMayne,D.Q.(1969). Monte Hastie,T.andTibshirani,R.(1996). Discriminant Heinz,E.A.(2000). Scalablesearchincomputer
Carlotechniquestoestimatetheconditionalexpecta- adaptivenearestneighborclassificationandregres- chess.Vieweg.
tioninmulti-stagenonlinearfiltering.Int.J.Control, sion. InTouretzky,D.S.,Mozer,M.C.,andHas-
Held, M.andKarp, R.M.(1970). Thetraveling
9(5),547–559. selmo,M.E.(Eds.),NIPS8,pp.409–15.MITPress.
salesmanproblemandminimumspanningtrees.Op-
Hansen,E.(1998). SolvingPOMDPsbysearching Hastie,T.,Tibshirani,R.,andFriedman,J.(2001). erationsResearch,18,1138–1162.
inpolicyspace.InUAI-98,pp.211–219. TheElementsofStatisticalLearning:DataMining, Helmert,M.(2001).Onthecomplexityofplanning
Hansen, E.andZilberstein, S.(2001). LAO*: a InferenceandPrediction(2ndedition). Springer- intransportationdomains.InECP-01.
Verlag.
heuristicsearchalgorithmthatfindssolutionswith Helmert,M.(2003). Complexityresultsforstan-
loops.AIJ,129(1–2),35–62. Hastie,T.,Tibshirani,R.,andFriedman,J.(2009). dardbenchmarkdomainsinplanning. AIJ,143(2),
Hansen, P. andJaumard, B.(1990). Algorithms T In h f e er E en le c m e e a n n ts d o P f r S e t d a i t c is ti t o ic n al (2 L n e d ar e n d in it g io : n D ). ata Sp M ri i n n g in e g r- , 219–262.
forthemaximumsatisfiabilityproblem.Computing, Verlag. Helmert,M.(2006). Thefastdownwardplanning
44(4),279–303. system.JAIR,26,191–246.
Haugeland,J.(Ed.).(1985). ArtificialIntelligence:
Hanski,I.andCambefort,Y.(Eds.).(1991). Dung TheVeryIdea.MITPress. Helmert,M.andRichter,S.(2004).Fastdownward
BeetleEcology.PrincetonUniversityPress. –Makinguseofcausaldependenciesintheprob-
Hauk, T. (2004). Search in Trees with Chance lemrepresentation. InProc.InternationalPlanning
Hansson,O.andMayer,A.(1989).Heuristicsearch Nodes.Ph.D.thesis,Univ.ofAlberta. CompetitionatICAPS,pp.41–43.
asevidentialreasoning.InUAI5.
Haussler,D.(1989).Learningconjunctiveconcepts Helmert,M.andRo¨ger,G.(2008). Howgoodis
Hansson,O.,Mayer,A.,andYung,M.(1992).Crit- instructuraldomains. MachineLearning,4(1),7– almostperfect?InAAAI-08.
icizingsolutionstorelaxedmodelsyieldspowerful 40.
admissibleheuristics. InformationSciences,63(3), Hendler, J., Carbonell, J. G., Lenat, D. B., Mi-
207–227. Havelund, K., Lowry, M., Park, S., Pecheur, C., zoguchi,R.,andRosenbloom,P.S.(1995). VERY
Penix,J.,Visser,W.,andWhite,J.L.(2000).Formal largeknowledgebases–Architecturevsengineer-
Haralick,R.M.andElliot,G.L.(1980). Increas- analysisoftheremoteagentbeforeandafterflight. ing.InIJCAI-95,pp.2033–2036.
ingtreesearchefficiencyforconstraintsatisfaction InProc.5thNASALangleyFormalMethodsWork-
problems.AIJ,14(3),263–313. shop. Henrion,M.(1988). Propagationofuncertaintyin
Bayesiannetworksbyprobabilisticlogicsampling.
Hardin,G.(1968). Thetragedyofthecommons. Havenstein,H.(2005). SpringcomestoAIwinter. InLemmer,J.F.andKanal,L.N.(Eds.),UAI2,pp.
Science,162,1243–1248. ComputerWorld. 149–163.Elsevier/North-Holland.
Hardy,G.H.(1940). AMathematician’sApology. Hawkins,J.andBlakeslee,S.(2004). OnIntelli- Henzinger,T.A.andSastry,S.(Eds.).(1998). Hy-
CambridgeUniversityPress. gence.HenryHoltandCo. bridsystems: Computationandcontrol. Springer-
Harman,G.H.(1983).ChangeinView:Principles Hayes,P.J.(1978).Thenaivephysicsmanifesto.In Verlag.
ofReasoning.MITPress. Michie,D.(Ed.),ExpertSystemsintheMicroelec- Herbrand,J.(1930). RecherchessurlaThe´oriede
tronicAge.EdinburghUniversityPress. laDe´monstration.Ph.D.thesis,UniversityofParis.
Harris,Z.(1954). Distributionalstructure. Word,
10(2/3). Hayes,P.J.(1979).Thelogicofframes.InMetzing, Hewitt,C.(1969).PLANNER:alanguageforprov-
D.(Ed.),FrameConceptionsandTextUnderstand- ingtheoremsinrobots.InIJCAI-69,pp.295–301.
Harrison,J.R.andMarch,J.G.(1984). Decision
makingandpostdecisionsurprises. Administrative ing,pp.46–61.deGruyter. Hierholzer, C. (1873). U¨ber die Mo¨glichkeit,
ScienceQuarterly,29,26–42. Hayes,P.J.(1985a). NaivephysicsI:Ontologyfor einenLinienzugohneWiederholungundohneUn-
liquids.InHobbs,J.R.andMoore,R.C.(Eds.),For- terbrechungzuumfahren. MathematischeAnnalen,
Harsanyi,J.(1967). Gameswithincompleteinfor- malTheoriesoftheCommonsenseWorld,chap.3, 6,30–32.
mationplayedbyBayesianplayers. Management
Science,14,159–182. pp.71–107.Ablex. Hilgard,E.R.andBower,G.H.(1975).Theoriesof
Hayes,P.J.(1985b).Thesecondnaivephysicsman- Learning(4thedition).Prentice-Hall.
H f m o u a rm m rt a , c l P o b . s E a t s . p i , s a N t f h i o l s r s . s t o h IE n e , E h N E e . u T J r r . i , s a t a n i n c s d a d c R e t t i a e o p r n m h s a i o e n l n a , t B S io y . n s ( t 1 e o 9 m f 6 m s 8 S ) i . n c A i i - - i m p f p e a . s l t 1 o T – . h 3 I e 6 n o . r H A ie o b s b le b o x s f . , t J h . e R C .a o n m d m M on o s o e r n e s , e R W .C o . rl ( d E , d c s h .) a , p F . o 1 r- , H Un in iv ti e k rs k i a ty ,J P . r ( e 1 s 9 s. 62).KnowledgeandBelief.Cornell
enceandCybernetics,SSC-4(2),100–107. Hinton,G.E.andAnderson,J.A.(1981). Parallel
C H o a r r r t e , c P ti . o E n . t , o N “ i A lss f o o n rm ,N al . b J a ., si a s n f d or R t a h p e h h a e e u l, ri B st . ic (1 d 9 e 7 te 2 r ) - . H he a n y s k iv i e n, Fo S u . n ( d 2 a 0 t 0 i 8 o ) n . .P N r e e u nt r i a c l e N H e a t l w l. orks: ACompre- M As o s d o e c l i s at o e f s. AssociativeMemory. LawrenceErlbaum
minationofminimumcostpaths”.SIGARTNewslet- Hays,J.andEfros,A.A.(2007).Scenecompletion Hinton,G.E.andNowlan,S.J.(1987).Howlearn-
ter,37,28–29. Usingmillionsofphotographs. ACMTransactions ingcanguide evolution. ComplexSystems, 1(3),
onGraphics(SIGGRAPH),26(3). 495–502.
Hart, T.P.andEdwards, D.J.(1961). Thetree
prune(TP)algorithm. Artificialintelligenceproject Hearst,M.A.(1992).Automaticacquisitionofhy- Hinton,G.E.,Osindero,S.,andTeh,Y.W.(2006).
memo30,MassachusettsInstituteofTechnology. ponymsfromlargetextcorpora.InCOLING-92. Afastlearningalgorithmfordeepbeliefnets.Neural
Hartley,H.(1958).Maximumlikelihoodestimation Hearst,M.A.(2009).SearchUserInterfaces.Cam- Computation,18,1527–15554.
fromincompletedata.Biometrics,14,174–194. bridgeUniversityPress. Hinton,G.E.andSejnowski,T.(1983). Optimal
Hartley,R.andZisserman,A.(2000).Multipleview Hebb,D.O.(1949).TheOrganizationofBehavior. perceptualinference.InCVPR,pp.448–453.
geometryincomputervision.CambridgeUniversity Wiley. Hinton,G.E.andSejnowski,T.(1986). Learning
Press. Heckerman,D.(1986). Probabilisticinterpretation andrelearninginBoltzmannmachines. InRumel-
H K tio o a n e s n l o u i f g m , p , S a P t . . t , e (2 r B n 0 o 0 d t 7 e a a ) t . , ab A D a . s , o e H m h e a e l i m u n r - e i i s r n t t d , ic e M s pe . f , n o B d r e o c n n o t e s t c t , - o o B n p s . t , t i r m a u n c a d - l f a E o n l r s d e M v L ie Y e r m C /N m IN o e r ’ r t s , h- c J H e . o rt F l a l . a in n ( t d E y . ds fa .) c , to U rs A . I 2 In , p K p a . na 1 l 6 , 7 L –1 . 9 N 6. . h l M e a l I r T t D , P i D s r t e . r s ib s E . u . te a d nd Pr M oc c e C s l s e i l n l g an , d c , ha J p . . L 7 . , ( p E p d . s. 2 ), 82 P – a 3 r 1 a 7 l- .
planning.InAAAI-07,pp.1007–1012. Heckerman, D. (1991). Probabilistic Similarity Hirsh,H.(1987).Explanation-basedgeneralization
Haslum,P.andGeffner,H.(2001). Heuristicplan-
Networks.MITPress. inalogicprogrammingenvironment.InIJCAI-87.
ningwithtimeandresources. InProc.IJCAI-01 Heckerman,D.(1998). Atutorialonlearningwith Hobbs,J.R.(1990).LiteratureandCognition.CSLI
WorkshoponPlanningwithResources. Bayesiannetworks.InJordan,M.I.(Ed.),Learning Press.
ingraphicalmodels.Kluwer. Hobbs, J. R., Appelt, D., Bear, J., Israel, D.,
Haslum,P.(2006).Improvingheuristicsthroughre-
Kameyama, M., Stickel, M. E., and Tyson, M.
laxedsearch–AnanalysisofTP4andHSP*ainthe Heckerman,D.,Geiger,D.,andChickering,D.M.
(1997).FASTUS:Acascadedfinite-statetransducer
2004planningcompetition.JAIR,25,233–267. (1994). LearningBayesiannetworks: Thecombi-
for extracting information from natural-language
nationofknowledgeandstatisticaldata. Technical
Haslum,P.,Bonet,B.,andGeffner,H.(2005).New text. InRoche,E.andSchabes,Y.(Eds.),Finite-
reportMSR-TR-94-09,MicrosoftResearch.
admissibleheuristicsfordomain-independentplan- StateDevicesforNaturalLanguageProcessing,pp.
ning.InAAAI-05. Heidegger,M.(1927).BeingandTime.SCMPress. 383–406.MITPress.
1076 Bibliography
Hobbs,J.R.andMoore,R.C.(Eds.).(1985). For- Horowitz,E.andSahni,S.(1978).Fundamentalsof Huang,T.andRussell,S.J.(1998). Objectiden-
malTheoriesoftheCommonsenseWorld.Ablex. ComputerAlgorithms.ComputerSciencePress. tification: ABayesiananalysiswithapplicationto
trafficsurveillance.AIJ,103,1–17.
Hobbs,J.R.,Stickel,M.E.,Appelt,D.,andMartin, Horswill, I. (2000). Functional programming of
P.(1993).Interpretationasabduction.AIJ,63(1–2), behavior-basedsystems.AutonomousRobots,9,83– Huang,X.D.,Acero,A.,andHon,H.(2001).Spo-
69–142. 93. kenLanguageProcessing.PrenticeHall.
Hoffmann,J.(2001).FF:Thefast-forwardplanning Horvitz,E.J.(1987).Problem-solvingdesign:Rea- Hubel,D.H.(1988).Eye,Brain,andVision.W.H.
system.AIMag,22(3),57–62. soningaboutcomputationalvalue,trade-offs,andre- Freeman.
H ma o n ff t m p a la n n n n , in J g .a v n ia d h B e r u a r f i m st a ic n, fo R rw .I a . rd (2 s 0 e 0 a 6 r ) c . h: C A on n f e o w r- s r o um ur , c p es p . . I 2 n 6 P –4 ro 3 c . .SecondAnnualNASAResearchFo- H Th u e d C dl a e m st b o r n id , ge R G . r D a . mm an a d ro P f u t l h l e um E , ng G lis . h K L . an ( g 2 u 0 a 0 g 2 e ) . .
approach.AIJ,170(6–7),507–541. Horvitz,E.J.(1989). Rationalmetareasoningand CambridgeUniversityPress.
compilationforoptimizingdecisionsunderbounded Huffman,D.A.(1971).Impossibleobjectsasnon-
Hoffmann,J.andBrafman,R.I.(2005).Contingent resources. InProc.ComputationalIntelligence89. sense sentences. In Meltzer, B. and Michie, D.
planningviaheuristicforwardsearchwithimplicit AssociationforComputingMachinery. (Eds.),MachineIntelligence6,pp.295–324.Edin-
beliefstates.InICAPS-05.
Horvitz,E.J.andBarry,M.(1995). Displayofin- burghUniversityPress.
Hoffmann,J.(2005).Where“ignoringdeletelists” formationfortime-criticaldecisionmaking.InUAI- Hughes,B.D.(1995). RandomWalksandRandom
works: Localsearchtopologyinplanningbench- 95,pp.296–305. Environments,Vol.1:RandomWalks. OxfordUni-
marks.JAIR,24,685–758.
Horvitz, E.J., Breese,J.S., Heckerman, D., and versityPress.
Hoffmann,J.andNebel,B.(2001). TheFFplan- Hovel,D.(1998). TheLumiereproject: Bayesian Hughes,G.E.andCresswell,M.J.(1996). ANew
ningsystem:Fastplangenerationthroughheuristic usermodelingforinferringthegoalsandneedsof IntroductiontoModalLogic.Routledge.
search.JAIR,14,253–302. softwareusers.InUAI-98,pp.256–265.
Huhns,M.N.andSingh,M.P.(Eds.).(1998).Read-
Hoffmann, J., Sabharwal, A., and Domshlak, C. Horvitz,E.J.,Breese,J.S.,andHenrion,M.(1988). ingsinAgents.MorganKaufmann.
(2006).Friendsorfoes?AnAIplanningperspective Decisiontheoryinexpertsystemsandartificialintel-
onabstractionandsearch. InICAPS-06,pp.294– ligence.IJAR,2,247–302. Hume,D.(1739).ATreatiseofHumanNature(2nd
303. edition). RepublishedbyOxfordUniversityPress,
Horvitz,E.J.andBreese,J.S.(1996). Idealparti- 1978,Oxford,UK.
Hogan,N.(1985).Impedancecontrol:Anapproach tionofresourcesformetareasoning.InAAAI-96,pp. Humphrys,M.(2008).Howmyprogrampassedthe
tomanipulation.PartsI,II,andIII.J.DynamicSys- 1229–1234. turingtest.InEpstein,R.,Roberts,G.,andBeber,G.
tems,Measurement,andControl,107(3),1–24.
Horvitz,E.J.andHeckerman,D.(1986).Theincon- (Eds.),ParsingtheTuringTest.Springer.
Hoiem, D., Efros, A.A., andHebert, M.(2008). sistentuseofmeasuresofcertaintyinartificialintel- Hunsberger,L.andGrosz,B.J.(2000). Acom-
Puttingobjectsinperspective.IJCV,80(1). ligenceresearch. InKanal,L.N.andLemmer,J.F. binatorialauctionforcollaborativeplanning. InInt.
Holland,J.H.(1975). AdaptioninNaturalandAr- (Eds.),UAI2,pp.137–151.Elsevier/North-Holland. ConferenceonMulti-AgentSystems(ICMAS-2000).
tificialSystems.UniversityofMichiganPress. Horvitz,E.J.,Heckerman,D.,andLanglotz,C.P. Hunt,W.andBrock,B.(1992).AformalHDLand
Holland,J.H.(1995).HiddenOrder:HowAdapta- (1986). Aframeworkforcomparingalternativefor- itsuseintheFM9001verification. Philosophical
tionBuildsComplexity.Addison-Wesley. malismsforplausiblereasoning.InAAAI-86,Vol.1, TransactionsoftheRoyalSocietyofLondon,339.
pp.210–214.
Holte,R.andHernadvolgyi,I.(2001).Stepstowards Hunter,L.andStates,D.J.(1992). Bayesianclas-
theautomaticcreationofsearchheuristics. Tech. Howard,R.A.(1960).DynamicProgrammingand sificationofproteinstructure. IEEEExpert,7(4),
rep.TR04-02,CSDept.,Univ.ofAlberta. MarkovProcesses.MITPress. 67–75.
Howard,R.A.(1966). Informationvaluetheory. Hurst,M.(2000). TheInterpretationofTextinTa-
Holzmann,G.J.(1997). TheSpinmodelchecker.
IEEETransactionsonSystemsScienceandCyber- bles.Ph.D.thesis,Edinburgh.
IEEETransactionsonSoftwareEngineering,23(5),
netics,SSC-2,22–26.
279–295. Hurwicz,L.(1973). Thedesignofmechanismsfor
Howard,R.A.(1977).Riskpreference.InHoward, resourceallocation.AmericanEconomicReviewPa-
Hood, A. (1824). Case 4th—28 July 1824 (Mr. R.A.andMatheson,J.E.(Eds.),ReadingsinDe- persandProceedings,63(1),1–30.
Hood’scasesofinjuriesofthebrain). Phrenologi- cision Analysis, pp. 429–465. Decision Analysis
calJournalandMiscellany,2,82–94.
Group,SRIInternational.
Husmeier, D. (2003). Sensitivity andspecificity
ofinferringgeneticregulatoryinteractionsfrommi-
Hooker,J.(1995).Testingheuristics:Wehaveitall Howard,R.A.(1989). Microrisksformedicalde- croarray experiments with dynamic bayesian net-
wrong.J.Heuristics,1,33–42. cisionanalysis. Int. J.Technology Assessmentin works.Bioinformatics,19(17),2271–2282.
Hoos,H.andTsang,E.(2006). Localsearchmeth- HealthCare,5,357–370. Huth, M. and Ryan, M. (2004). Logic in com-
ods.InRossi,F.,vanBeek,P.,andWalsh,T.(Eds.), Howard,R.A.andMatheson,J.E.(1984). Influ- puterscience: modellingandreasoningaboutsys-
HandbookofConstraintProcessing,pp.135–168. encediagrams. InHoward, R.A.andMatheson, tems(2ndedition).CambridgeUniversityPress.
Elsevier. J.E.(Eds.),ReadingsonthePrinciplesandAppli- Huttenlocher,D.andUllman,S.(1990).Recogniz-
Hope,J.(1994). TheAuthorshipofShakespeare’s cationsofDecisionAnalysis,pp.721–762.Strategic ingsolidobjectsbyalignmentwithanimage.IJCV,
Plays.CambridgeUniversityPress. DecisionsGroup. 5(2),195–212.
Hopfield,J.J.(1982).Neuronswithgradedresponse Howe,D.(1987). Thecomputationalbehaviourof Huygens,C.(1657).Deratiociniisinludoaleae.In
havecollectivecomputationalpropertieslikethose girard’sparadox.InLICS-87,pp.205–214. vanSchooten,F.(Ed.),ExercitionumMathematico-
oftwo-stateneurons.PNAS,79,2554–2558. Hsu,F.-H.(2004). BehindDeepBlue:Buildingthe rum.Elsevirii,Amsterdam. TranslatedintoEnglish
Horn,A.(1951). Onsentenceswhicharetrueof ComputerthatDefeatedtheWorldChessChampion. byJohnArbuthnot(1692).
directunionsofalgebras.JSL,16,14–21. PrincetonUniversityPress. Huyn,N.,Dechter,R.,andPearl,J.(1980). Proba-
Horn, B.K. P. (1970). Shape from shading: A Hsu,F.-H.,Anantharaman,T.S.,Campbell,M.S., bilisticanalysisofthecomplexityofA*.AIJ,15(3),
methodforobtainingtheshapeofasmoothopaque andNowatzyk,A.(1990).Agrandmasterchessma- 241–254.
objectfromoneview. Technicalreport232,MIT chine.ScientificAmerican,263(4),44–50. Hwa,R.(1998). Anempiricalevaluationofproba-
ArtificialIntelligenceLaboratory. Hu,J.andWellman,M.P.(1998). Multiagentre- bilisticlexicalizedtreeinsertiongrammars.InACL-
Horn,B.K.P.(1986).RobotVision.MITPress. inforcementlearning:Theoreticalframeworkandan 98,pp.557–563.
algorithm.InICML-98,pp.242–250. Hwang,C.H.andSchubert,L.K.(1993).EL:Afor-
Horn,B.K.P.andBrooks,M.J.(1989).Shapefrom Hu,J.andWellman,M.P.(2003). Nashq-learning mal, yetnatural, comprehensiveknowledgerepre-
Shading.MITPress. forgeneral-sumstochasticgames. JMLR,4,1039– sentation.InAAAI-93,pp.676–682.
Horn,K.V.(2003). Constructingalogicofplausi- 1069. Ingerman,P.Z.(1967). Panini–Backusformsug-
bleinference:Aguidetocox’stheorem. IJAR,34, Huang, T., Koller, D., Malik, J., Ogasawara, G., gested.CACM,10(3),137.
3–24.
Rao,B.,Russell,S.J.,andWeber,J.(1994). Au- Inoue,K.(2001).Inverseentailmentforfullclausal
Horning,J.J.(1969).Astudyofgrammaticalinfer- tomaticsymbolictrafficsceneanalysisusingbelief theories. In LICS-2001 Workshop on Logic and
ence.Ph.D.thesis,StanfordUniversity. networks.InAAAI-94,pp.966–972. Learning.
Bibliography 1077
Intille,S.andBobick,A.(1999). Aframeworkfor Johnston,M.D.andAdorf,H.-M.(1992).Schedul- Kaelbling,L.P.andRosenschein,S.J.(1990). Ac-
recognizingmulti-agentactionfromvisualevidence. ingwithneuralnetworks: ThecaseoftheHubble tionandplanninginembeddedagents.Roboticsand
InAAAI-99,pp.518–525. space telescope. Computers and Operations Re- AutonomousSystems,6(1–2),35–48.
search,19(3–4),209–240.
Isard,M.andBlake,A.(1996). Contourtracking Kager,R.(1999). OptimalityTheory. Cambridge
bystochasticpropagationofconditionaldensity. In Jones,N.D.,Gomard,C.K.,andSestoft,P.(1993). UniversityPress.
ECCV,pp.343–356. PartialEvaluationandAutomaticProgramGenera-
tion.Prentice-Hall. Kahn,H.andMarshall,A.W.(1953). Methodsof
Iwama,K.andTamaki,S.(2004). Improvedupper reducingsamplesizeinMonteCarlocomputations.
boundsfor3-SAT.InSODA-04. Jones,R.,Laird,J.,andNielsen,P.E.(1998).Auto- OperationsResearch,1(5),263–278.
matedintelligentpilotsforcombatflightsimulation.
Jaakkola,T.andJordan,M.I.(1996). Computing InAAAI-98,pp.1047–54. Kahneman,D.,Slovic,P.,andTversky,A.(Eds.).
upperandlowerboundsonlikelihoodsinintractable (1982).JudgmentunderUncertainty:Heuristicsand
networks. InUAI-96,pp.340–348.MorganKauf- Jones,R.,McCallum,A.,Nigam,K.,andRiloff,E. Biases.CambridgeUniversityPress.
mann. (1999). Bootstrappingfortextlearningtasks. In
Proc.IJCAI-99WorkshoponTextMining:Founda- Kahneman,D.andTversky,A.(1979). Prospect
Jaakkola,T.,Singh,S.P.,andJordan,M.I.(1995). tions,Techniques,andApplications,pp.52–63. theory:Ananalysisofdecisionunderrisk. Econo-
Reinforcementlearningalgorithmforpartiallyob- metrica,pp.263–291.
servableMarkovdecisionproblems. InNIPS7,pp. Jones,T.(2007). ArtificialIntelligence: ASystems
345–352. Approach.InfinitySciencePress. Kaindl, H. and Khorsand, A. (1994). Memory-
boundedbidirectionalsearch.InAAAI-94,pp.1359–
Jackson,F.(1982). Epiphenomenalqualia. Philo- Jonsson,A.,Morris,P.,Muscettola,N.,Rajan,K., 1364.
sophicalQuarterly,32,127–136. andSmith, B.(2000). Planningininterplanetary
J p a ro ff g a ra r m ,J m .a in n g d . L In as P se ro z, c. J. F - o L u . r ( t 1 e 9 e 8 n 7 th ). A C C o M ns C tr o a n in fe t r l e o n g c ic e s 1 p 8 a 6 c . e: Theoryandpractice. InAIPS-00,pp.177– 8 K in 2 g a , l 3 m a 5 n a – d n 4 p , 6 r R . ed . i ( c 1 t 9 io 6 n 0) p . r A ob n le e m w s a . pp J. ro B a a c s h ic to E l n in g e in ar ee fi r l i t n e g r- ,
onPrinciplesofProgrammingLanguages,pp.111– Jordan,M.I.(1995). Whythelogisticfunction?
119.AssociationforComputingMachinery. atutorialdiscussiononprobabilitiesandneuralnet- Kambhampati,S.(1994). Exploitingcausalstruc-
J R a . f H fa . r C , . J ( ., 19 M 92 ic ) h . a T y h lo e v C , L S. P , (R St ) u l c a k n e g y u , a P g . e J a ., nd an s d ys Y te a m p . , w re o p r o k r s t . 95 C 03 o , m M pu a t s a s t a i c o h n u a s l e c tt o s g I n n i s ti t v it e ut s e c o ie f n T c e e ch te n c o h l n o i g c y a . l t r u eu re se t . o C c o o m n p tr u o t l at r io et n r a ie l v I a n l te a ll n i d gen re c fi e t , t 1 in 0 g ,2 d 1 u 3 r – in 2 g 44 p . lan
ACMTransactionsonProgrammingLanguagesand Jordan,M.I.(2005). Dirichletprocesses,Chinese Kambhampati,S.,Mali,A.D.,andSrivastava,B.
Systems,14(3),339–395. restaurantprocessesandallthat. Tutorialpresenta- (1998). Hybridplanningforpartiallyhierarchical
Jaynes,E.T.(2003).ProbabilityTheory:TheLogic tionattheNIPSConference. domains.InAAAI-98,pp.882–888.
ofScience.CambridgeUniv.Press. Jordan, M.I., Ghahramani, Z.,Jaakkola, T., and Kanal,L.N.andKumar,V.(1988). SearchinArti-
Jefferson,G.(1949).Themindofmechanicalman: Saul,L.K.(1998). Anintroductiontovariational ficialIntelligence.Springer-Verlag.
TheListerOrationdeliveredattheRoyalCollege methodsforgraphicalmodels.InJordan,M.I.(Ed.), Kanazawa,K.,Koller,D.,andRussell,S.J.(1995).
ofSurgeonsinEngland. BritishMedicalJournal, LearninginGraphicalModels.Kluwer. Stochasticsimulationalgorithmsfordynamicprob-
1(25),1105–1121. Jouannaud,J.-P.andKirchner,C.(1991). Solving abilisticnetworks.InUAI-95,pp.346–351.
Jeffrey,R.C.(1983). TheLogicofDecision(2nd equationsinabstractalgebras: Arule-basedsurvey Kantorovich,L.V.(1939). Mathematicalmethods
edition).UniversityofChicagoPress. ofunification.InLassez,J.-L.andPlotkin,G.(Eds.), oforganizingandplanningproduction. Publishdin
Jeffreys,H.(1948).TheoryofProbability.Oxford. ComputationalLogic,pp.257–321.MITPress. translationinManagementScience,6(4),366–422,
Judd,J.S.(1990). NeuralNetworkDesignandthe July1960.
J b e y li s n ta e t k is , ti F c . a ( l 1 m 9 e 7 t 6 h ) o . ds C . o P n r t o in c u . o IE us EE sp , e 6 e 4 c ( h 4) r , e 5 c 3 o 2 g – n 5 it 5 io 6 n . ComplexityofLearning.MITPress. Kaplan,D.andMontague,R.(1960).Aparadoxre-
Juels, A.andWattenberg, M.(1996). Stochastic gained. NotreDameJournalofFormalLogic,1(3),
Jelinek,F.(1997). StatisticalMethodsforSpeech hillclimbingasabaselinemethodforevaluatingge- 79–90.
Recognition.MITPress. neticalgorithms.InTouretzky,D.S.,Mozer,M.C., Karmarkar,N.(1984).Anewpolynomial-timeal-
Jelinek,F.andMercer,R.L.(1980). Interpolated and Hasselmo, M. E. (Eds.), NIPS 8, pp. 430–6. gorithmforlinearprogramming. Combinatorica,4,
estimationofMarkovsourceparametersfromsparse MITPress. 373–395.
d P a r t a a c . ti I c n e, P p r p o . c 3 . 8 W 1– o 3 rk 9 s 7 h . oponPatternRecognitionin Junker,U.(2003).Thelogicofilog(j)configurator: Karp,R.M.(1972). Reducibilityamongcombina-
Combiningconstraintprogrammingwithadescrip- torialproblems.InMiller,R.E.andThatcher,J.W.
Jennings,H.S.(1906). BehavioroftheLowerOr- tionlogic. InProc.IJCAI-03ConfigurationWork- (Eds.),ComplexityofComputerComputations,pp.
ganisms.ColumbiaUniversityPress. shop,pp.13–20. 85–103.Plenum.
Jenniskens,P.,Betlem,H.,Betlem,J.,andBarifaijo, Jurafsky, D. and Martin, J. H. (2000). Speech Kartam, N. A. and Levitt, R. E. (1990). A
E.(1994).TheMbalemeteoriteshower.Meteoritics, andLanguageProcessing:AnIntroductiontoNat- constraint-basedapproachtoconstructionplanning
29(2),246–254. uralLanguageProcessing,ComputationalLinguis- ofmulti-storybuildings. InExpertPlanningSys-
Jensen,F.V.(2001). BayesianNetworksandDeci- tics,andSpeechRecognition.Prentice-Hall. tems,pp.245–250.InstituteofElectricalEngineers.
sionGraphs.Springer-Verlag. Jurafsky, D. and Martin, J. H. (2008). Speech Kasami,T.(1965).Anefficientrecognitionandsyn-
J si e o n n se G n r , a F p . h V s. . S (2 p 0 r 0 in 7 g ) e . r B -V a e y r e la si g a . nNetworksandDeci- a u ti r n c a d s l , L a L n a a d n n g g S u u p a a e g g e e c e h P P R r r o e o c c c e o e s g s s s n i i n i n t g i g o : , n C A ( o n 2 m n I d n p t u e r d t o a i d t t i u i o o c n n ti ) a o . l n P L r t i e o n n g N ti u c a i e s t - - - t T R a e e x c s h e a . a n r a r c e l h y p s L . is a A b a F o l C r g a o R to r L i r t y - h 6 . m 5-7 fo 5 r 8, co A n i t r ex F t o -f r r c e e e C la a n m g b u r a i g d e g s e .
Jevons, W.S.(1874). ThePrinciplesofScience. Hall.
Routledge/ThoemmesPress,London. Kasparov, G.(1997). IBM owesmea rematch.
Kadane,J.B.andSimon,H.A.(1977). Optimal Time,149(21),66–67.
Ji,S.,Parr,R.,Li,H.,Liao,X.,andCarin,L.(2007). strategiesforaclassofconstrainedsequentialprob-
Point-basedpolicyiteration.InAAAI-07. lems.AnnalsofStatistics,5,237–255. Kaufmann, M., Manolios, P., and Moore, J. S.
(2000). Computer-AidedReasoning:AnApproach.
Jimenez,P.andTorras,C.(2000). Anefficiental- Kadane,J.B.andLarkey,P.D.(1982). Subjective Kluwer.
gorithmforsearchingimplicitAND/ORgraphswith probabilityandthetheoryofgames. Management
cycles.AIJ,124(1),1–30. Science,28(2),113–120. Kautz,H.(2006).Deconstructingplanningassatis-
fiability.InAAAI-06.
Joachims,T.(2001).Astatisticallearningmodelof Kaelbling, L.P., Littman, M.L., andCassandra,
textclassificationwithsupportvectormachines. In A.R.(1998). Planningandactionginpartiallyob- Kautz, H., McAllester, D. A., and Selman, B.
SIGIR-01,pp.128–136. servablestochasticdomains.AIJ,101,99–134. (1996). Encodingplansinpropositionallogic. In
KR-96,pp.374–384.
Johnson,W.W.andStory,W.E.(1879). Noteson Kaelbling,L.P.,Littman,M.L.,andMoore,A.W.
the“15”puzzle.AmericanJournalofMathematics, (1996). Reinforcementlearning:Asurvey. JAIR,4, Kautz,H.andSelman,B.(1992).Planningassatis-
2,397–404. 237–285. fiability.InECAI-92,pp.359–363.
1078 Bibliography
Kautz,H.andSelman,B.(1998). BLACKBOX:A Kim,J.H.andPearl,J.(1983). Acomputational Kocsis,L.andSzepesvari,C.(2006). Bandit-based
newapproachtotheapplicationoftheoremproving modelforcombinedcausalanddiagnosticreasoning Monte-Carloplanning.InECML-06.
toproblemsolving. WorkingNotesoftheAIPS-98 ininferencesystems.InIJCAI-83,pp.190–193.
Koditschek,D.(1987). Exactrobotnavigationby
WorkshoponPlanningasCombinatorialSearch.
Kim, J.-H., Lee, C.-H., Lee, K.-H., and Kup- meansofpotentialfunctions:sometopologicalcon-
Kavraki,L.,Svestka,P.,Latombe,J.-C.,andOver- puswamy,N.(2007). Evolvingpersonalityofage- siderations.InICRA-87,Vol.1,pp.1–6.
mars,M.(1996). Probabilisticroadmapsforpath neticrobotinubiquitousenvironment. InThe16th
Koehler,J.,Nebel,B.,Hoffmann,J.,andDimopou-
planninginhigh-dimensionalconfigurationspaces. IEEEInternationalSymposiumonRobotandHu-
los,Y.(1997).ExtendingplanninggraphstoanADL
IEEE Transactions on Robotics and Automation, maninteractiveCommunication,pp.848–853.
subset.InECP-97,pp.273–285.
12(4),566–580.
King,R.D.,Rowland,J.,Oliver,S.G.,andYoung,
Kay, M., Gawron, J. M., and Norvig, P. (1994). M.(2009). Theautomationofscience. Science, Koehn,P.(2009). StatisticalMachineTranslation.
Verbmobil: ATranslationSystemforFace-To-Face 324(5923),85–89. CambridgeUniversityPress.
Dialog.CSLIPress. Kirk,D.E.(2004). OptimalControlTheory: An Koenderink,J.J.(1990).SolidShape.MITPress.
Kearns,M.(1990).TheComputationalComplexity Introduction.Dover. Koenig, S. (1991). Optimal probabilistic and
ofMachineLearning.MITPress. Kirkpatrick, S.,Gelatt, C.D.,andVecchi,M.P. decision-theoreticplanningusingMarkoviandeci-
Kearns,M.,Mansour,Y.,andNg,A.Y.(2000).Ap- (1983). Optimizationbysimulatedannealing. Sci- siontheory. Master’sreport,ComputerScienceDi-
proximateplanninginlargePOMDPsviareusable ence,220,671–680. vision,UniversityofCalifornia.
trajectories.InSolla,S.A.,Leen,T.K.,andMu¨ller, Kister, J., Stein, P., Ulam, S., Walden, W., and Koenig, S. (2000). Exploring unknown environ-
K.-R.(Eds.),NIPS12.MITPress. Wells,M.(1957). Experimentsinchess. JACM,4, mentswithreal-timesearchorreinforcementlearn-
Kearns,M.andSingh,S.P.(1998). Near-optimal 174–177. ing. InSolla,S.A.,Leen,T.K.,andMu¨ller,K.-R.
reinforcement learning in polynomial time. In (Eds.),NIPS12.MITPress.
Kisynski,J.andPoole,D.(2009). Liftedaggrega-
ICML-98,pp.260–268. tionindirectedfirst-orderprobabilisticmodels. In Koenig,S.(2001). Agent-centeredsearch. AIMag,
Kearns,M.andVazirani,U.(1994).AnIntroduction IJCAI-09. 22(4),109–131.
toComputationalLearningTheory.MITPress. Kitano,H.,Asada,M.,Kuniyoshi,Y.,Noda,I.,and Koller, D., Meggido, N., and von Stengel, B.
Kearns,M.andMansour,Y.(1998).Afast,bottom- Osawa,E.(1997a). RoboCup:Therobotworldcup (1996). Efficientcomputationofequilibriaforex-
updecisiontreepruningalgorithmwithnear-optimal initiative.InProc.FirstInternationalConferenceon tensivetwo-persongames. GamesandEconomic
generalization.InICML-98,pp.269–277. AutonomousAgents,pp.340–347. Behaviour,14(2),247–259.
Kebeasy, R.M., Hussein, A.I., andDahy, S.A. Kitano,H.,Asada,M.,Kuniyoshi,Y.,Noda,I.,Os- Koller,D.andPfeffer,A.(1997). Representations
(1998).Discriminationbetweennaturalearthquakes awa,E.,andMatsubara,H.(1997b). RoboCup: A and solutions for game-theoretic problems. AIJ,
and nuclear explosions using the Aswan Seismic challengeproblemforAI.AIMag,18(1),73–85. 94(1–2),167–215.
Network.AnnalidiGeofisica,41(2),127–140.
Kjaerulff,U.(1992). Acomputationalschemefor Koller, D. and Pfeffer, A. (1998). Probabilistic
Keeney,R.L.(1974). Multiplicativeutilityfunc- reasoning in dynamic probabilistic networks. In frame-basedsystems.InAAAI-98,pp.580–587.
tions.OperationsResearch,22,22–34. UAI-92,pp.121–129.
Koller, D.andFriedman, N.(2009). Probabilis-
Keeney,R.L.andRaiffa,H.(1976).Decisionswith Klein,D.andManning,C.(2001).Parsingwithtree- ticGraphicalModels: PrinciplesandTechniques.
MultipleObjectives: PreferencesandValueTrade- bankgrammars:Empiricalbounds,theoreticalmod- MITPress.
offs.Wiley. els,andthestructureofthePenntreebank. InACL-
Koller,D.andMilch,B.(2003). Multi-agentinflu-
01.
Kemp,M.(Ed.).(1989).LeonardoonPainting:An encediagramsforrepresentingandsolvinggames.
AnthologyofWritings.YaleUniversityPress. Klein,D.andManning,C.(2003).A*parsing:Fast GamesandEconomicBehavior,45,181–221.
exactViterbiparseselection.InHLT-NAACL-03,pp.
Kephart,J.O.andChess,D.M.(2003).Thevision Koller,D.andParr,R.(2000). Policyiterationfor
119–126.
ofautonomiccomputing. IEEEComputer,36(1), factoredMDPs.InUAI-00,pp.326–334.
41–50. Klein,D.,Smarr,J.,Nguyen,H.,andManning,C.
Koller,D.andSahami,M.(1997). Hierarchically
(2003). Namedentityrecognitionwithcharacter-
Kersting,K.,Raedt,L.D.,andKramer,S.(2000). classifying documents using very few words. In
levelmodels. InConferenceonNaturalLanguage
Interpretingbayesianlogicprograms.InProc.AAAI- ICML-97,pp.170–178.
Learning(CoNLL).
2000WorkshoponLearningStatisticalModelsfrom
RelationalData. Kleinberg,J.M.(1999). Authoritativesourcesina Kolmogorov,A.N.(1941). Interpolationundex-
hyperlinkedenvironment.JACM,46(5),604–632. trapolationvonstationarenzufalligenfolgen. Bul-
Kessler,B.,Nunberg,G.,andSchu¨tze,H.(1997). letinoftheAcademyofSciencesoftheUSSR,Ser.
Automatic detection of text genre. CoRR, cmp- Klemperer,P.(2002). Whatreallymattersinauc- Math.5,3–14.
lg/9707002. tiondesign.J.EconomicPerspectives,16(1).
Kolmogorov,A.N.(1950).FoundationsoftheThe-
Keynes,J.M.(1921). ATreatiseonProbability. Kneser,R.andNey,H.(1995). Improvedbacking- oryofProbability.Chelsea.
Macmillan. offforM-gramlanguagemodeling. InICASSP-95,
pp.181–184. Kolmogorov,A.N.(1963). Ontablesofrandom
Khare,R.(2006). Microformats:Thenext(small) numbers. Sankhya,theIndianJournalofStatistics,
thingonthesemanticweb. IEEEInternetComput- Knight,K.(1999). AstatisticalMTtutorialwork- SeriesA25.
ing,10(1),68–75. book. PreparedinconnectionwiththeJohnsHop-
kinsUniversitysummerworkshop. Kolmogorov,A.N.(1965).Threeapproachestothe
Khatib,O.(1986).Real-timeobstacleavoidancefor quantitativedefinitionofinformation. Problemsin
robotmanipulatorandmobilerobots.Int.J.Robotics Knuth,D.E.(1964). Representingnumbersusing InformationTransmission,1(1),1–7.
Research,5(1),90–98. onlyone4. MathematicsMagazine,37(Nov/Dec),
308–310. Kolodner, J.(1983). Reconstructivememory: A
Khmelev,D.V.andTweedie,F.J.(2001). Using computermodel.CognitiveScience,7,281–328.
Markovchainsforidentificationofwriter. Literary Knuth,D.E.(1968).Semanticsforcontext-freelan-
andLinguisticComputing,16(3),299–307. guages. MathematicalSystemsTheory,2(2),127– Kolodner,J.(1993). Case-BasedReasoning. Mor-
145. ganKaufmann.
Kietz, J.-U.andDuzeroski, S.(1994). Inductive
logicprogrammingandlearnability. SIGARTBul- Knuth,D.E.(1973).TheArtofComputerProgram- Kondrak,G.andvanBeek,P.(1997).Atheoretical
letin,5(1),22–32. ming(secondedition).,Vol.2: FundamentalAlgo- evaluationofselectedbacktrackingalgorithms.AIJ,
rithms.Addison-Wesley. 89,365–387.
Kilgarriff, A.andGrefenstette,G.(2006). Intro-
ductiontothespecialissueonthewebascorpus. Knuth, D.E.(1975). Ananalysisofalpha–beta Konolige,K.(1997).COLBERT:Alanguageforre-
ComputationalLinguistics,29(3),333–347. pruning.AIJ,6(4),293–326. activecontrolinSaphira. InKu¨nstlicheIntelligenz:
AdvancesinArtificialIntelligence, LNAI,pp.31–
Kim,J.H.(1983). CONVINCE:AConversational Knuth, D. E.andBendix, P. B.(1970). Simple
52.
InferenceConsolidationEngine. Ph.D.thesis,De- wordproblemsinuniversalalgebras. InLeech,J.
partmentofComputerScience,UniversityofCali- (Ed.),ComputationalProblemsinAbstractAlgebra, Konolige,K.(2004). Large-scalemap-making. In
forniaatLosAngeles. pp.263–267.Pergamon. AAAI-04,pp.457–463.
Bibliography 1079
Konolige, K.(1982). A first orderformalization Koza,J.R.(1994). GeneticProgrammingII:Auto- Kyburg,H.E.andTeng,C.-M.(2006). Nonmono-
ofknowledgeandactionforamulti-agentplanning maticdiscoveryofreusableprograms.MITPress. toniclogicandstatisticalinference. Computational
system. InHayes,J.E.,Michie,D.,andPao,Y.-H. Intelligence,22(1),26–51.
Koza,J.R.,Bennett,F.H.,Andre,D.,andKeane,
(Eds.),MachineIntelligence10.EllisHorwood.
M.A.(1999).GeneticProgrammingIII:Darwinian Kyburg,H.E.(1977). Randomnessandtheright
Konolige,K.(1994).Easytobehard:Difficultprob- inventionandproblemsolving.MorganKaufmann. referenceclass.J.Philosophy,74(9),501–521.
lemsforgreedyalgorithms.InKR-94,pp.374–378.
Kraus, S., Ephrati, E., andLehmann, D.(1991). Kyburg,H.E.(1983). Thereferenceclass. Philos-
Koo,T.,Carreras,X.,andCollins,M.(2008). Sim- Negotiationinanon-cooperativeenvironment. AIJ, ophyofScience,50,374–397.
plesemi-superviseddependencyparsing.InACL-08. 3(4),255–281.
La Mettrie, J. O. (1748). L’homme machine.
Koopmans,T.C.(1972). Representationofpref- Krause,A.andGuestrin,C.(2009). Optimalvalue E.Luzac,Leyde,France.
erence orderings over time. In McGuire, C. B. ofinformationingraphicalmodels. JAIR,35,557– LaMura,P.andShoham,Y.(1999).Expectedutil-
andRadner,R.(Eds.),DecisionandOrganization. 591. itynetworks.InUAI-99,pp.366–373.
Elsevier/North-Holland.
Krause,A.,McMahan,B.,Guestrin,C.,andGupta, Laborie,P.(2003). Algorithmsforpropagatingre-
Korb,K.B.andNicholson,A.(2003). Bayesian A.(2008).Robustsubmodularobservationselection. sourceconstraints inAIplanning andscheduling.
ArtificialIntelligence.ChapmanandHall. JMLR,9,2761–2801. AIJ,143(2),151–188.
Korb,K.B.,Nicholson,A.,andJitnah,N.(1999). Kripke,S.A.(1963).Semanticalconsiderationson Ladkin,P.(1986a). Primitivesandunitsfortime
Bayesianpoker.InUAI-99. modallogic.ActaPhilosophicaFennica,16,83–94. specification.InAAAI-86,Vol.1,pp.354–359.
Korf,R.E.(1985a).Depth-firstiterative-deepening: Krogh,A.,Brown,M.,Mian,I.S.,Sjolander,K., Ladkin,P.(1986b). Timerepresentation: ataxon-
anoptimaladmissibletreesearch. AIJ,27(1),97– andHaussler,D.(1994). HiddenMarkovmodels omyofintervalrelations. InAAAI-86,Vol.1,pp.
109. incomputationalbiology: Applicationstoprotein 360–366.
Korf,R.E.(1985b).Iterative-deepeningA*:Anop- modeling.J.MolecularBiology,235,1501–1531. Lafferty,J.,McCallum,A.,andPereira,F.(2001).
timaladmissibletreesearch.InIJCAI-85,pp.1034– Ku¨bler,S.,McDonald,R.,andNivre,J.(2009).De- Conditionalrandomfields:Probabilisticmodelsfor
1036. pendencyParsing.MorganClaypool. segmentingandlabelingsequencedata.InICML-01.
Korf,R.E.(1987). Planningassearch:Aquantita- Kuhn,H.W.(1953).Extensivegamesandtheprob- Lafferty,J.andZhai,C.(2001). Probabilisticrele-
tiveapproach.AIJ,33(1),65–88. lemofinformation. InKuhn, H.W.andTucker, vancemodelsbasedondocumentandquerygenera-
Korf,R.E.(1990).Real-timeheuristicsearch.AIJ, A.W.(Eds.),ContributionstotheTheoryofGames tion.InProc.WorkshoponLanguageModelingand
42(3),189–212. II.PrincetonUniversityPress. InformationRetrieval.
Korf,R.E.(1993). Linear-spacebest-firstsearch. Kuhn, H.W.(1955). TheHungarianmethodfor Lagoudakis, M. G. and Parr, R. (2003). Least-
AIJ,62(1),41–78. theassignmentproblem. NavalResearchLogistics squarespolicyiteration.JMLR,4,1107–1149.
Quarterly,2,83–97.
Korf, R. E. (1995). Space-efficient search algo- Laird,J.,Newell,A.,andRosenbloom,P.S.(1987).
rithms.ACMComputingSurveys,27(3),337–339. Kuipers,B.J.(1985).Qualitativesimulation.InBo- SOAR:Anarchitectureforgeneralintelligence.AIJ,
brow,D.(Ed.),QualitativeReasoningAboutPhysi- 33(1),1–64.
Korf,R.E.andChickering,D.M.(1996).Best-first calSystems,pp.169–203.MITPress.
minimaxsearch.AIJ,84(1–2),299–337. Laird,J.,Rosenbloom,P.S.,andNewell,A.(1986).
Kuipers,B.J.andLevitt,T.S.(1988). Navigation ChunkinginSoar:Theanatomyofagenerallearn-
Korf,R.E.andFelner,A.(2002). Disjointpattern andmappinginlarge-scalespace.AIMag,9(2),25– ingmechanism.MachineLearning,1,11–46.
databaseheuristics.AIJ,134(1–2),9–22. 43.
Laird,J.(2008). ExtendingtheSoarcognitivear-
Korf, R.E., Reid, M., andEdelkamp, S.(2001). Kuipers,B.J.(2001). Qualitativesimulation. In chitecture.InArtificialGeneralIntelligenceConfer-
Time complexity of iterative-deepening-A*. AIJ, Meyers,R.A.(Ed.),EncyclopeidaofPhysicalSci- ence.
129,199–218. enceandTechnology.AcademicPress.
Lakoff, G.(1987). Women, Fire, andDangerous
Korf, R. E. and Zhang, W.(2000). Divide-and- Kumar,P.R.andVaraiya,P.(1986).StochasticSys- Things: WhatCategoriesRevealAbouttheMind.
conquerfrontiersearchappliedtooptimalsequence tems:Estimation,Identification,andAdaptiveCon- UniversityofChicagoPress.
t a e l l i l g i n g m en e c n e t , . p In p. A 9 m 10 er – i 9 c 1 a 6 n . AssociationforArtificialIn- trol.Prentice-Hall. Lakoff,G.andJohnson,M.(1980). MetaphorsWe
Kumar,V.(1992). Algorithmsforconstraintsatis- LiveBy.UniversityofChicagoPress.
K gr o ap rf h , s R e . a E rc . h ( . 2 J 0 A 0 C 8) M . , L 5 in 5 e (6 a ) r . -timedisk-basedimplicit factionproblems:Asurvey.AIMag,13(1),32–44. Lakoff,G.andJohnson,M.(1999). Philosophyin
Kumar, V. and Kanal, L. N. (1983). A general theFlesh:TheEmbodiedMindandItsChallengeto
Korf, R.E.andSchultze, P.(2005). Large-scale branchandboundformulationforunderstandingand WesternThought.BasicBooks.
p 13 ar 8 a 5 ll . elbreadth-firstsearch. InAAAI-05,pp.1380– s 1 y 7 n 9 t – h 1 e 9 s 8 iz . ingand/ortreesearchprocedures. AIJ,21, L su a a m ls , e J r . v a o n in d g G f r o e r e a n c s c p u a r n a , te M s . h ( o 2 o 0 t 0 in 8 g ). in Ey p e o - o in l - r h o a b n o d tic v s i- .
Kotok,A.(1962). Achessplayingprogramforthe Kumar,V.andKanal,L.N.(1988). TheCDP:A In5thCanadianConferenceonComputerandRobot
IBM7090. AIprojectmemo41,MITComputation unifyingformulationforheuristicsearch,dynamic Vision.
Center. programming, and branch-and-bound. In Kanal, Lamarck, J. B. (1809). Philosophie zoologique.
Koutsoupias,E.andPapadimitriou, C.H.(1992). L.N.andKumar,V.(Eds.),SearchinArtificialIn- ChezDentuetL’Auteur,Paris.
Onthegreedyalgorithmforsatisfiability. Informa- telligence,chap.1,pp.1–27.Springer-Verlag.
tionProcessingLetters,43(1),53–55. Landhuis, E. (2004). Lifelong debunker takes
Kumar,V.,Nau,D.S.,andKanal,L.N.(1988). A on arbiter of neutral choices: Magician-turned-
Kowalski,R.(1974). Predicatelogicasaprogram- generalbranch-and-boundformulationforAND/OR mathematicianuncoversbiasinaflipofacoin.Stan-
minglanguage. InProc.IFIPCongress,pp.569– graphandgametreesearch. InKanal,L.N.and fordReport.
574. Kumar,V.(Eds.),SearchinArtificialIntelligence,
chap.3,pp.91–130.Springer-Verlag. Langdon,W.andPoli,R.(2002). Foundationsof
Kowalski,R.(1979). LogicforProblemSolving. GeneticProgramming.Springer.
Elsevier/North-Holland. Kurien, J., Nayak, P., and Smith, D. E. (2002).
Fragment-basedconformantplanning.InAIPS-02. Langley, P., Simon, H.A., Bradshaw, G.L.,and
Kowalski,R.(1988). Theearlyyearsoflogicpro- Zytkow,J.M.(1987). ScientificDiscovery: Com-
gramming.CACM,31,38–43. Kurzweil, R.(1990). TheAgeofIntelligentMa- putationalExplorationsoftheCreativeProcesses.
chines.MITPress. MITPress.
Kowalski,R.andSergot,M.(1986). Alogic-based
calculus of events. New Generation Computing, Kurzweil, R. (2005). The Singularity is Near. Langton, C. (Ed.). (1995). Artificial Life.
4(1),67–95. Viking. MITPress.
Koza,J.R.(1992). GeneticProgramming:Onthe Kwok,C.,Etzioni,O.,andWeld,D.S.(2001).Scal- Laplace, P. (1816). Essai philosophique sur les
ProgrammingofComputersbyMeansofNaturalSe- ingquestionansweringtotheweb. InProc.10th probabilite´s (3rd edition). Courcier Imprimeur,
lection.MITPress. InternationalConferenceontheWorldWideWeb. Paris.
1080 Bibliography
Laptev,I.andPerez,P.(2007). Retrievingactions Legendre,A.M.(1805). Nouvellesme´thodespour Lighthill,J.(1973). Artificialintelligence: Agen-
inmovies.InICCV,pp.1–8. lade´terminationdesorbitesdescome`tes.. eralsurvey.InLighthill,J.,Sutherland,N.S.,Need-
ham,R.M.,Longuet-Higgins,H.C.,andMichie,D.
Lari,K.andYoung,S.J.(1990). Theestimation Lehrer,J.(2009).HowWeDecide.HoughtonMif-
(Eds.),ArtificialIntelligence: APaperSymposium.
ofstochasticcontext-freegrammarsusingtheinside- flin.
ScienceResearchCouncilofGreatBritain.
outsidealgorithm.ComputerSpeechandLanguage,
4,35–56. Lenat,D.B.(1983). EURISKO:Aprogramthat Lin,S.(1965).Computersolutionsofthetravelling
learnsnewheuristicsanddomainconcepts:Thena-
salesmanproblem. BellSystemsTechnicalJournal,
Larran˜aga,P.,Kuijpers,C.,Murga,R.,Inza,I.,and tureofheuristics, III:Programdesignandresults. 44(10),2245–2269.
Dizdarevic,S.(1999). Geneticalgorithmsforthe AIJ,21(1–2),61–98.
travellingsalesmanproblem:Areviewofrepresen- Lin,S.andKernighan,B.W.(1973). Aneffective
tationsandoperators.ArtificialIntelligenceReview, Lenat,D.B.andBrown,J.S.(1984).WhyAMand heuristicalgorithmforthetravelling-salesmanprob-
13,129–170. EURISKOappeartowork.AIJ,23(3),269–294. lem.OperationsResearch,21(2),498–516.
Larson,S.C.(1931). Theshrinkageofthecoef- Lenat,D.B.andGuha,R.V.(1990).BuildingLarge Lindley,D.V.(1956). Onameasureoftheinfor-
ficientofmultiplecorrelation. J.EducationalPsy- Knowledge-BasedSystems: RepresentationandIn- mationprovidedbyanexperiment.AnnalsofMath-
chology,22,45–55. ferenceintheCYCProject.Addison-Wesley. ematicalStatistics,27(4),986–1005.
Laskey,K.B.(2008).MEBN:Alanguageforfirst- Leonard,H.S.andGoodman,N.(1940). Thecal- Lindsay, R. K., Buchanan, B. G., Feigenbaum,
orderbayesianknowledgebases.AIJ,172,140–178. culusofindividualsanditsuses.JSL,5(2),45–55. E.A.,andLederberg,J.(1980).ApplicationsofArti-
Latombe, J.-C.(1991). RobotMotion Planning. Leonard,J.andDurrant-Whyte,H.(1992).Directed ficialIntelligenceforOrganicChemistry:TheDEN-
Kluwer. sonarsensingformobilerobotnavigation.Kluwer. DRALProject.McGraw-Hill.
Lauritzen,S.(1995).TheEMalgorithmforgraphi- Les´niewski, S. (1916). Podstawy ogo´lnej teorii Littman,M.L.(1994). Markovgamesasaframe-
calassociationmodelswithmissingdata.Computa- mnogos´ci.Moscow. work for multi-agent reinforcement learning. In
tionalStatisticsandDataAnalysis,19,191–201. ICML-94,pp.157–163.
Lettvin,J.Y.,Maturana,H.R.,McCulloch,W.S.,
Lauritzen,S.(1996). Graphicalmodels. Oxford andPitts,W.(1959). Whatthefrog’seyetellsthe Littman,M.L.,Keim,G.A.,andShazeer,N.M.
UniversityPress. frog’sbrain.Proc.IRE,47(11),1940–1951. (1999). SolvingcrosswordswithPROVERB. In
AAAI-99,pp.914–915.
Lauritzen,S.,Dawid,A.P.,Larsen,B.,andLeimer, Letz,R.,Schumann,J.,Bayerl,S.,andBibel,W.
H. (1990). Independence properties of directed (1992). SETHEO: A high-performance theorem Liu,J.S.andChen,R.(1998). SequentialMonte
Markovfields.Networks,20(5),491–505. prover.JAR,8(2),183–212. Carlo methods for dynamic systems. JASA, 93,
1022–1031.
Lauritzen,S.andSpiegelhalter,D.J.(1988).Local Levesque,H.J.andBrachman,R.J.(1987). Ex-
computationswithprobabilitiesongraphicalstruc- pressivenessandtractabilityinknowledgerepresen- Livescu,K.,Glass,J.,andBilmes,J.(2003).Hidden
turesandtheirapplicationtoexpertsystems.J.Royal tationandreasoning. ComputationalIntelligence, featuremodelingforspeechrecognitionusingdy-
StatisticalSociety,B50(2),157–224. 3(2),78–93. namicBayesiannetworks.InEUROSPEECH-2003,
pp.2529–2532.
Lauritzen,S.andWermuth,N.(1989). Graphical Levin,D.A.,Peres,Y.,andWilmer,E.L.(2008).
modelsforassociationsbetweenvariables,someof MarkovChainsandMixingTimes.AmericanMath- Livnat,A.andPippenger,N.(2006). Anoptimal
whicharequalitativeandsomequantitative. Annals ematicalSociety. braincanbecomposedofconflictingagents.PNAS,
ofStatistics,17,31–57. 103(9),3198–3202.
Levitt,G.M.(2000). TheTurk,ChessAutomaton.
LaValle, S.(2006). PlanningAlgorithms. Cam- McFarlandandCompany. Locke,J.(1690).AnEssayConcerningHumanUn-
bridgeUniversityPress. derstanding.WilliamTegg.
Levy, D. (Ed.). (1988a). Computer Chess Com-
Lavrauc,N.andDuzeroski,S.(1994). Inductive pendium.Springer-Verlag. Lodge,D.(1984).SmallWorld.PenguinBooks.
LogicProgramming: TechniquesandApplications.
EllisHorwood. Levy, D. (Ed.). (1988b). Computer Games. Loftus,E.andPalmer,J.(1974). Reconstructionof
Springer-Verlag. automobiledestruction:Anexampleoftheinterac-
Lawler,E.L.,Lenstra,J.K.,Kan,A.,andShmoys, tionbetweenlanguageandmemory.J.VerbalLearn-
D.B.(1992).TheTravellingSalesmanProblem.Wi- Levy,D.(1989).Themillionpoundbridgeprogram. ingandVerbalBehavior,13,585–589.
leyInterscience. InLevy,D.andBeal,D.(Eds.),HeuristicProgram-
minginArtificialIntelligence.EllisHorwood. Lohn, J.D., Kraus, W.F., andColombano, S.P.
Lawler,E.L.,Lenstra,J.K.,Kan,A.,andShmoys, (2001). Evolutionaryoptimizationofyagi-udaan-
D.B.(1993). Sequencingandscheduling: Algo- Levy,D.(2007).LoveandSexwithRobots.Harper. tennas.InProc.FourthInternationalConferenceon
rithms andcomplexity. InGraves, S.C., Zipkin, Lewis,D.D.(1998). NaiveBayesatforty:Thein- EvolvableSystems,pp.236–243.
P.H.,andKan,A.H.G.R.(Eds.),LogisticsofPro-
duction andInventory: Handbooks in Operations dependenceassumptionininformationretrieval. In Longley,N.andSankaran,S.(2005). TheNHL’s
ResearchandManagementScience,Volume4,pp. ECML-98,pp.4–15. overtime-lossrule: Empiricallyanalyzingtheunin-
445–522.North-Holland. Lewis,D.K.(1966). Anargumentfortheidentity tendedeffects.AtlanticEconomicJournal.
Lawler,E.L.andWood,D.E.(1966).Branch-and- theory.J.Philosophy,63(1),17–25. Longuet-Higgins,H.C.(1981). Acomputeralgo-
boundmethods: Asurvey. OperationsResearch, Lewis,D.K.(1980).MadpainandMartianpain.In rithmforreconstructingascenefromtwoprojec-
14(4),699–719. Block,N.(Ed.),ReadingsinPhilosophyofPsychol- tions.Nature,293,133–135.
Lazanas,A.andLatombe,J.-C.(1992).Landmark- ogy,Vol.1,pp.216–222.HarvardUniversityPress. Loo,B.T.,Condie,T.,Garofalakis,M.,Gay,D.E.,
basedrobotnavigation.InAAAI-92,pp.816–822. Leyton-Brown,K.andShoham,Y.(2008). Essen- Hellerstein,J.M.,Maniatis,P.,Ramakrishnan,R.,
L (1 e 9 C 89 u ) n . , Y H ., an Ja d c w k r e i l t , te L n ., di B g o it se r r e , co B g . n , i a ti n o d n: De A n p k p e l r i , ca J - . t I i n a t l r s od o u f c G ti a o m n. e M Th o e r o g r a y n : C A la C yp o o n o ci l s . e,Multidisciplinary R w SI o o G s rk M co in O e g , D : T - L . 0 , a 6 a n . n g d ua S g t e o , ic e a x , ec I u . t ( i 2 o 0 n 0 a 6 n ). do D p e ti c m la i r z a a t t i i v o e n. ne I t n -
tionsofneuralnetworkchipsandautomaticlearn- Li,C.M.andAnbulagan(1997). Heuristicsbased
ing. IEEECommunicationsMagazine,27(11),41– onunitpropagationforsatisfiabilityproblems. In Love, N., Hinrichs, T., and Genesereth, M. R.
46. IJCAI-97,pp.366–371. (2006). General game playing: Game descrip-
tionlanguagespecification.Tech.rep.LG-2006-01,
LeCun, Y., Jackel, L., Bottou, L., Brunot, A., Li,M.andVitanyi,P.M.B.(1993). AnIntroduc- StanfordUniversityComputerScienceDept.
Cortes, C., Denker, J., Drucker, H., Guyon, I., tiontoKolmogorovComplexityandItsApplications.
Muller,U.,Sackinger,E.,Simard,P.,andVapnik, Springer-Verlag. Lovejoy, W.S. (1991). A surveyof algorithmic
V.N.(1995).Comparisonoflearningalgorithmsfor methodsforpartiallyobservedMarkovdecisionpro-
handwrittendigitrecognition.InInt.Conferenceon Liberatore,P.(1997). Thecomplexityofthelan- cesses.AnnalsofOperationsResearch,28(1–4),47–
ArtificialNeuralNetworks,pp.53–60. guageA.ElectronicTransactionsonArtificialIntel- 66.
ligence,1,13–38.
Leech,G.,Rayson,P.,andWilson,A.(2001).Word Loveland,D.(1970).Alinearformatforresolution.
FrequenciesinWrittenandSpokenEnglish: Based Lifschitz,V.(2001). Answersetprogrammingand InProc.IRIASymposiumonAutomaticDemonstra-
ontheBritishNationalCorpus.Longman. plangeneration.AIJ,138(1–2),39–54. tion,pp.147–162.
Bibliography 1081
Lowe,D.(1987). Three-dimensionalobjectrecog- Mackworth,A.K.(1992). Constraintsatisfaction. Marsland, A.T.andSchaeffer, J.(Eds.).(1990).
nitionfromsingletwo-dimensionalimages.AIJ,31, InShapiro,S.(Ed.),EncyclopediaofArtificialIntel- Computers,Chess,andCognition.Springer-Verlag.
355–395. ligence(secondedition).,Vol.1,pp.285–293.Wi-
Marsland,S.(2009). MachineLearning:AnAlgo-
ley.
Lowe, D.(1999). Objectrecognition usinglocal rithmicPerspective.CRCPress.
scaleinvariantfeature.InICCV. Mahanti,A.andDaniels,C.J.(1993).ASIMDap- Martelli, A.andMontanari, U.(1973). Additive
Lowe,D.(2004). Distinctiveimagefeaturesfrom proachtoparallelheuristicsearch. AIJ,60(2),243– AND/ORgraphs.InIJCAI-73,pp.1–11.
282.
scale-invariantkeypoints.IJCV,60(2),91–110. Martelli,A.andMontanari,U.(1978). Optimizing
Lo¨wenheim,L.(1915).U¨bermo¨glichkeitenimRel- Mailath,G.andSamuelson,L.(2006). Repeated decisiontreesthroughheuristicallyguidedsearch.
ativkalku¨l.MathematischeAnnalen,76,447–470. GamesandReputations: Long-RunRelationships. CACM,21,1025–1039.
OxfordUniversityPress.
Lowerre,B.T.(1976). TheHARPYSpeechRecog- Martelli,A.(1977). Onthecomplexityofadmissi-
nitionSystem. Ph.D.thesis,ComputerScienceDe- Majercik,S.M.andLittman,M.L.(2003).Contin- blesearchalgorithms.AIJ,8(1),1–13.
gentplanningunderuncertaintyviastochasticsatis-
partment,Carnegie-MellonUniversity. fiability.AIJ,pp.119–162. Marthi,B.,Pasula,H.,Russell,S.J.,andPeres,Y.
Lowerre,B.T.andReddy,R.(1980). TheHARPY (2002). DecayedMCMCfiltering. InUAI-02,pp.
speechrecognition system. In Lea, W.A. (Ed.), Malik,J.andPerona,P.(1990).Preattentivetexture 319–326.
T H r a e l n l. dsinSpeechRecognition, chap.15.Prentice- d S i o s c c . ri A m m in . a A t , io 7 n (5 w ), it 9 h 2 e 3 a – r 9 ly 32 v . isionmechanisms.J.Opt. M C. a ( r 2 t 0 h 0 i, 5 B ). .,R C u o s n s c e u ll r , r S en . t J., h L ie a r t a h r a c m hi , c D al ., r a e n in d fo G rc u e e m str e i n n t ,
Lowry,M.(2008). Intelligentsoftwareengineering Malik, J.andRosenholtz,R.(1994). Recovering learning.InIJCAI-05.
toolsforNASA’screwexplorationvehicle.InProc. surfacecurvatureandorientationfromtexturedistor- Marthi,B.,Russell,S.J.,andWolfe,J.(2007).An-
ISMIS. tion:Aleastsquaresalgorithmandsensitivityanal- gelicsemanticsforhigh-levelactions.InICAPS-07.
ysis.InECCV,pp.353–364.
Loyd, S. (1959). Mathematical Puzzles of Sam Marthi,B.,Russell,S.J.,andWolfe,J.(2008).An-
Loyd: Selected and Edited by Martin Gardner. Malik, J.andRosenholtz, R.(1997). Computing gelichierarchicalplanning: Optimalandonlineal-
Dover. localsurfaceorientationandshapefromtexturefor gorithms.InICAPS-08.
curvedsurfaces.IJCV,23(2),149–168.
Lozano-Perez,T.(1983). Spatialplanning:Acon- Martin, D., Fowlkes, C., and Malik, J. (2004).
figurationspaceapproach. IEEETransactionson Maneva, E., Mossel, E., and Wainwright, M. J. Learningtodetectnaturalimageboundariesusing
Computers,C-32(2),108–120. (2007). Anewlookatsurveypropagationandits local brightness, color, and texture cues. PAMI,
generalizations.JACM,54(4).
Lozano-Perez, T., Mason, M., and Taylor, R. 26(5),530–549.
(1984). Automaticsynthesisoffine-motionstrate- Manna,Z.andWaldinger,R.(1971). Towardauto- Martin,J.H.(1990). AComputationalModelof
giesforrobots.Int.J.RoboticsResearch,3(1),3–24. maticprogramsynthesis.CACM,14(3),151–165. MetaphorInterpretation.AcademicPress.
Lu,F.andMilios,E.(1997). Globallyconsistent Manna,Z.andWaldinger,R.(1985). TheLogical Mason, M. (1993). Kicking the sensing habit.
rangescanalignmentforenvironmentmapping.Au- BasisforComputerProgramming: Volume1: De- AIMag,14(1),58–59.
tonomousRobots,4,333–349. ductiveReasoning.Addison-Wesley.
Mason,M.(2001). MechanicsofRoboticManipu-
Luby,M.,Sinclair,A.,andZuckerman,D.(1993). Manning,C.andSchu¨tze,H.(1999). Foundations lation.MITPress.
OptimalspeedupofLasVegasalgorithms.Informa- ofStatisticalNaturalLanguage Processing. MIT
tionProcessingLetters,47,173–180. Press. Mason,M.andSalisbury,J.(1985). Robothands
andthemechanicsofmanipulation.MITPress.
L Ph u i c l a o s s , op J. hy R , . 3 ( 6 1 . 961). Minds,machines,andGo¨del. M Int a ro n d n u in c g ti , o C n ., to Ra In g f h o a r v m a a n t , i P o . n ,a R n e d tr S i c e h va u¨ l t . ze, C H a . m (2 b 0 ri 0 d 8 g ) e . Mataric,M.J.(1997). Reinforcementlearningin
UniversityPress. themulti-robotdomain. AutonomousRobots,4(1),
Lucas,J.R.(1976). ThisGo¨deliskillingme: A 73–83.
rejoinder.Philosophia,6(1),145–148. Mannion, M. (2002). Usingfirst-order logic for Mates,B.(1953). StoicLogic. UniversityofCali-
productlinemodelvalidation. InSoftwareProduct
Lucas, P. (1996). Knowledge acquisition for Lines:SecondInternationalConference.Springer. forniaPress.
decision-theoreticexpertsystems. AISBQuarterly, Matuszek,C.,Cabral,J.,Witbrock,M.,andDeO-
94,23–33. Manzini,G.(1995). BIDA*:Animprovedperime- liveira,J.(2006). Anintroductiontothesyntaxand
tersearchalgorithm.AIJ,72(2),347–360.
Lucas, P., vanderGaag, L., andAbu-Hanna, A. semanticsofcyc. InProc.AAAISpringSymposium
(2004). Bayesian networks in biomedicine and Marbach,P.andTsitsiklis,J.N.(1998).Simulation- onFormalizingandCompilingBackgroundKnowl-
health-care.ArtificialIntelligenceinMedicine. based optimization of Markov reward processes. edgeandItsApplicationstoKnowledgeRepresenta-
TechnicalreportLIDS-P-2411,LaboratoryforInfor- tionandQuestionAnswering.
Luce,D.R.andRaiffa,H.(1957). GamesandDe- mationandDecisionSystems,MassachusettsInsti-
cisions.Wiley. tuteofTechnology. Maxwell,J.andKaplan,R.(1993). Theinterface
betweenphrasalandfunctionalconstraints.Compu-
Ludlow,P.,Nagasawa,Y.,andStoljar,D.(2004). Marcus,G.(2009). Kluge:TheHaphazardEvolu- tationalLinguistics,19(4),571–590.
There’sSomethingAboutMary.MITPress. tionoftheHumanMind.MarinerBooks. McAllester,D.A.(1980).Anoutlookontruthmain-
Luger,G.F.(Ed.).(1995).Computationandintelli- Marcus,M.P.,Santorini, B.,andMarcinkiewicz, tenance.Aimemo551,MITAILaboratory.
gence:Collectedreadings.AAAIPress. M.A.(1993). Buildingalargeannotatedcorpusof McAllester,D.A.(1988). Conspiracynumbersfor
Lyman, P. and Varian, H. R. (2003). How english:Thepenntreebank.ComputationalLinguis- min-maxsearch.AIJ,35(3),287–310.
much information? www.sims.berkeley. tics,19(2),313–330.
edu/how-much-info-2003. Markov, A.A.(1913). Anexampleofstatistical M ing cA is l s l u e e st f e a r c , in D g . A A I . a (1 n 9 d 9 t 8 h ) e . A W A h A at I i t s od th a e y? m C o a st nd p i r d e a ss te -
Machina, M. (2005). Choice under uncertainty. investigationinthetextof“EugeneOnegin”illus- statement, electionforCounciloroftheAmerican
InEncyclopediaofCognitiveScience,pp.505–514. tratingcouplingof“tests”inchains.Proc.Academy AssociationforArtificialIntelligence.
Wiley. ofSciencesofSt.Petersburg,7.
McAllester,D.A.andRosenblitt,D.(1991). Sys-
MacKay, D. J. C. (1992). A practicalBayesian Maron,M.E.(1961). Automaticindexing:Anex- tematicnonlinearplanning. InAAAI-91,Vol.2,pp.
frameworkforback-propagationnetworks. Neural perimentalinquiry.JACM,8(3),404–417. 634–639.
Computation,4(3),448–472.
Maron, M.E.andKuhns, J.-L.(1960). Onrel- McCallum,A.(2003).Efficientlyinducingfeatures
MacKay,D.J.C.(2002). InformationTheory,In- evance, probabilistic indexingandinformation re- ofconditionalrandomfields.InUAI-03.
ferenceandLearningAlgorithms. CambridgeUni- trieval.CACM,7,219–244.
McCarthy, J. (1958). Programs with common
versityPress.
Marr,D.(1982). Vision:AComputationalInvesti- sense. In Proc. Symposium on Mechanisation of
MacKenzie,D.(2004). MechanizingProof. MIT gationintotheHumanRepresentationandProcess- ThoughtProcesses,Vol.1,pp.77–84.
Press. ingofVisualInformation.W.H.Freeman.
McCarthy,J.(1963).Situations,actions,andcausal
Mackworth,A.K.(1977).Consistencyinnetworks Marriott,K.andStuckey,P.J.(1998). Program- laws.Memo2,StanfordUniversityArtificialIntelli-
ofrelations.AIJ,8(1),99–118. mingwithConstraints:AnIntroduction.MITPress. genceProject.
1082 Bibliography
McCarthy, J. (1968). Programs with common Mendel, G. (1866). Versuche u¨ber pflanzen- Minsky,M.L.(1975). Aframeworkforrepresent-
sense. InMinsky,M.L.(Ed.),SemanticInforma- hybriden. Verhandlungen des Naturforschenden ingknowledge.InWinston,P.H.(Ed.),ThePsychol-
tionProcessing,pp.403–418.MITPress. Vereins,Abhandlungen,Bru¨nn,4,3–47. Translated ogyofComputerVision,pp.211–277.McGraw-Hill.
intoEnglishbyC.T.Druery,publishedbyBateson OriginallyanMITAILaboratorymemo;the1975
McCarthy,J.(1980). Circumscription: Aformof
(1902). versionisabridged,butisthemostwidelycited.
non-monotonicreasoning.AIJ,13(1–2),27–39.
Mercer,J.(1909). Functionsofpositiveandnega- Minsky,M.L.(1986). Thesocietyofmind. Simon
McCarthy,J.(2007).Fromheretohuman-levelAI. tivetypeandtheirconnectionwiththetheoryofin- andSchuster.
AIJ,171(18),1174–1182.
tegralequations.Philos.Trans.Roy.Soc.London,A, Minsky,M.L.(2007).TheEmotionMachine:Com-
McCarthy,J.andHayes,P.J.(1969). Somephilo- 209,415–446. monsenseThinking,ArtificialIntelligence, andthe
sophicalproblemsfromthestandpointofartificial Merleau-Ponty,M.(1945).PhenomenologyofPer- FutureoftheHumanMind.SimonandSchuster.
intelligence.InMeltzer,B.,Michie,D.,andSwann,
M.(Eds.),MachineIntelligence4,pp.463–502.Ed- ception.Routledge. Minsky,M.L.andPapert,S.(1969). Perceptrons:
inburghUniversityPress. Metropolis, N., Rosenbluth, A., Rosenbluth, M., AnIntroduction to Computational Geometry (first
Teller,A.,andTeller,E.(1953). Equationsofstate edition).MITPress.
McCarthy, J.,Minsky, M.L.,Rochester,N.,and
Shannon,C.E.(1955). ProposalfortheDartmouth calculationsbyfastcomputingmachines.J.Chemi- Minsky,M.L.andPapert,S.(1988). Perceptrons:
summer research project on artificial intelligence. calPhysics,21,1087–1091. An Introduction to Computational Geometry (Ex-
Tech.rep.,DartmouthCollege. Metzinger,T.(2009).TheEgoTunnel:TheScience pandededition).MITPress.
oftheMindandtheMythoftheSelf.BasicBooks. Minsky,M.L.,Singh,P.,andSloman,A.(2004).
McCawley,J.D.(1988).TheSyntacticPhenomena
The st. thomas common sense symposium: De-
ofEnglish,Vol.2volumes. UniversityofChicago Me´zard,M.andNadal,J.-P.(1989). Learningin
signing architectures for human-level intelligence.
Press. feedforwardlayerednetworks:Thetilingalgorithm.
AIMag,25(2),113–124.
J.Physics,22,2191–2204.
McCorduck,P.(2004).Machineswhothink:aper-
Minton,S.(1984).Constraint-basedgeneralization:
sonalinquiryintothehistoryandprospectsofartifi- Michalski,R.S.(1969). Onthequasi-minimalso- Learninggame-playingplansfromsingleexamples.
cialintelligence(Revisededition).AKPeters. lution ofthe generalcovering problem. In Proc. InAAAI-84,pp.251–254.
FirstInternationalSymposiumonInformationPro-
McCulloch,W.S.andPitts,W.(1943). Alogical cessing,pp.125–128. Minton,S.(1988). Quantitativeresultsconcerning
calculusoftheideasimmanentinnervousactivity. theutilityofexplanation-basedlearning. InAAAI-
BulletinofMathematicalBiophysics,5,115–137. Michalski,R.S.,Mozetic,I.,Hong,J.,andLavrauc, 88,pp.564–569.
N. (1986). The multi-purpose incremental learn-
McCune,W.(1992). Automateddiscoveryofnew ingsystemAQ15anditstestingapplicationtothree Minton, S., Johnston, M.D., Philips, A.B., and
axiomatizationsoftheleftgroupandrightgroupcal- medicaldomains.InAAAI-86,pp.1041–1045. Laird,P.(1992). Minimizingconflicts: Aheuris-
culi.JAR,9(1),1–24. tic repair method for constraint satisfaction and
McCune,W.(1997).SolutionoftheRobbinsprob- Michie, D. (1966). Game-playing and game- schedulingproblems.AIJ,58(1–3),161–205.
learningautomata. InFox, L.(Ed.), Advancesin
lem.JAR,19(3),263–276. ProgrammingandNon-NumericalComputation,pp. Misak,C.(2004). TheCambridgeCompanionto
McDermott,D.(1976).Artificialintelligencemeets 183–200.Pergamon. Peirce.CambridgeUniversityPress.
naturalstupidity.SIGARTNewsletter,57,4–9. Mitchell,M.(1996).AnIntroductiontoGeneticAl-
Michie,D.(1972). MachineintelligenceatEdin-
gorithms.MITPress.
McDermott,D.(1978a).Planningandacting.Cog- burgh.ManagementInformatics,2(1),7–12.
nitiveScience,2(2),71–109. Mitchell,M.,Holland,J.H.,andForrest,S.(1996).
Michie,D.(1974). MachineintelligenceatEdin-
When will a genetic algorithm outperform hill
McDermott, D.(1978b). Tarskiansemantics, or, burgh. InOnIntelligence,pp.143–155.Edinburgh
climbing? InCowan,J.,Tesauro,G.,andAlspec-
nonotationwithoutdenotation! CognitiveScience, UniversityPress.
tor,J.(Eds.),NIPS6.MITPress.
2(3).
Michie,D.andChambers,R.A.(1968). BOXES: Mitchell,T.M.(1977).Versionspaces:Acandidate
McDermott,D.(1985). Reasoningaboutplans. In Anexperimentinadaptivecontrol. InDale,E.and eliminationapproachtorulelearning. InIJCAI-77,
Hobbs,J.andMoore,R.(Eds.),Formaltheoriesof Michie,D.(Eds.),MachineIntelligence2,pp.125– pp.305–310.
thecommonsenseworld.IntellectBooks. 133.Elsevier/North-Holland.
Mitchell,T.M.(1982). Generalizationassearch.
McDermott,D.(1987). Acritiqueofpurereason. Michie, D., Spiegelhalter, D. J., and Taylor, C. AIJ,18(2),203–226.
ComputationalIntelligence,3(3),151–237. (Eds.).(1994). MachineLearning,NeuralandSta-
tisticalClassification.EllisHorwood. Mitchell,T.M.(1990).Becomingincreasinglyreac-
McDermott,D.(1996). Aheuristicestimatorfor tive(mobilerobots). InAAAI-90,Vol.2,pp.1051–
means-endsanalysisinplanning. InICAPS-96,pp. Milch, B., Marthi, B., Sontag, D., Russell, S.J., 1058.
142–149. Ong,D.,andKolobov,A.(2005). BLOG:Proba-
bilisticmodelswithunknownobjects.InIJCAI-05. Mitchell, T. M. (1997). Machine Learning.
McDermott, D. and Doyle, J. (1980). Non- McGraw-Hill.
monotoniclogic:i.AIJ,13(1–2),41–72. Milch,B.,Zettlemoyer,L.S.,Kersting,K.,Haimes,
Mitchell, T.M.,Keller,R.,andKedar-Cabelli,S.
M.,andKaelbling,L.P.(2008).Liftedprobabilistic
McDermott,J.(1982).R1:Arule-basedconfigurer (1986). Explanation-basedgeneralization:Aunify-
inferencewithcountingformulas. InAAAI-08,pp.
ofcomputersystems.AIJ,19(1),39–88. ingview.MachineLearning,1,47–80.
1062–1068.
McEliece,R.J.,MacKay,D.J.C.,andCheng,J.- Mitchell, T. M., Utgoff, P. E., and Banerji, R.
F.(1998). TurbodecodingasaninstanceofPearl’s
Milgrom,P.(1997).Puttingauctiontheorytowork:
(1983).Learningbyexperimentation:Acquiringand
“beliefpropagation”algorithm.IEEEJournalonSe- The simultaneous ascending auction. Tech. rep. refiningproblem-solvingheuristics. InMichalski,
TechnicalReport98-0002,StanfordUniversityDe-
lectedAreasinCommunications,16(2),140–152. R.S.,Carbonell,J.G.,andMitchell,T.M.(Eds.),
partmentofEconomics. Machine Learning: An Artificial Intelligence Ap-
McGregor,J.J.(1979). Relationalconsistencyal- Mill,J.S.(1843). ASystemofLogic,Ratiocinative proach,pp.163–190.MorganKaufmann.
gorithmsandtheirapplicationinfindingsubgraph
and graph isomorphisms. Information Sciences, andInductive:BeingaConnectedViewofthePrin- Mitchell,T.M.(2005). Readingtheweb:Abreak-
19(3),229–250. ciplesofEvidence,andMethodsofScientificInves- throughgoalforAI.AIMag,26(3),12–16.
tigation.J.W.Parker,London.
McIlraith,S.andZeng,H.(2001). Semanticweb Mitchell,T.M.(2007). Learning,informationex-
services.IEEEIntelligentSystems,16(2),46–53. Mill,J.S.(1863). Utilitarianism. Parker,Sonand tractionandtheweb.InECML/PKDD,p.1.
Bourn,London.
McLachlan,G.J.andKrishnan,T.(1997).TheEM Mitchell, T. M., Shinkareva, S. V., Carlson, A.,
AlgorithmandExtensions.Wiley. Miller,A.C.,Merkhofer,M.M.,Howard,R.A., Chang, K.-M., Malave, V.L., Mason, R.A., and
Matheson,J.E.,andRice,T.R.(1976). Develop- Just,M.A.(2008). Predictinghumanbrainactiv-
McMillan,K.L.(1993).SymbolicModelChecking. mentofautomatedaidsfordecisionanalysis.Tech- ityassociatedwiththemeaningsofnouns. Science,
Kluwer. nicalreport,SRIInternational. 320,1191–1195.
Meehl,P.(1955).Clinicalvs.StatisticalPrediction. Minker, J.(2001). Logic-BasedArtificialIntelli- Mohr,R.andHenderson,T.C.(1986).Arcandpath
UniversityofMinnesotaPress. gence.Kluwer. consistencyrevisited.AIJ,28(2),225–233.
Bibliography 1083
Mohri, M., Pereira, F., and Riley, M. (2002). Morjaria,M.A.,Rink,F.J.,Smith,W.D.,Klemp- Murphy, K. and Russell, S. J. (2001). Rao-
Weightedfinite-statetransducersinspeechrecogni- ner,G.,Burns,C.,andStein,J.(1995).Elicitationof blackwellised particle filtering for dynamic
tion.ComputerSpeechandLanguage,16(1),69–88. probabilitiesforbeliefnetworks: Combiningquali- Bayesian networks. In Doucet, A., de Freitas,
tativeandquantitativeinformation. InUAI-95,pp. N., and Gordon, N. J. (Eds.), Sequential Monte
Montague, P.R., Dayan, P., Person, C., andSe- 141–148. CarloMethodsinPractice.Springer-Verlag.
jnowski,T.(1995). Beeforaginginuncertainenvi-
ronmentsusingpredictiveHebbianlearning.Nature, Morrison, P. and Morrison, E. (Eds.). (1961). Murphy, K. and Weiss, Y. (2001). The fac-
377,725–728. CharlesBabbageandHisCalculatingEngines:Se- toredfrontieralgorithmforapproximateinferencein
lected Writings by Charles Babbage and Others. DBNs.InUAI-01,pp.378–385.
Montague,R.(1970).Englishasaformallanguage. Dover.
InLinguagginellaSocieta`enellaTecnica,pp.189– Murphy, R.(2000). IntroductiontoAIRobotics.
224.EdizionidiComunita`. Moskewicz, M. W., Madigan, C. F., Zhao, Y., MITPress.
Zhang,L.,andMalik,S.(2001). Chaff: Engineer-
Murray-Rust,P.,Rzepa,H.S.,Williamson,J.,and
Montague,R.(1973).Thepropertreatmentofquan- inganefficientSATsolver. InProc.38thDesign
Willighagen,E.L.(2003).Chemicalmarkup,XML
tificationinordinaryEnglish. InHintikka,K.J.J., AutomationConference(DAC2001),pp.530–535.
andtheworld–wideweb.4.CMLschema.J.Chem.
Moravcsik, J. M. E., and Suppes, P. (Eds.), Ap- Mosteller,F.andWallace,D.L.(1964). Inference Inf.Comput.Sci.,43,752–772.
proachestoNaturalLanguage.D.Reidel. andDisputedAuthorship:TheFederalist.Addison-
Montanari, U. (1974). Networks of constraints: Wesley. M pro u o r f th o y f , H C ig .a m n a d n R ’s u l s e s m el m l, a J . . I R n . L (1 IC 99 S 0 -9 ). 0 A ,p c p o . n 2 s 5 t 7 ru – c 2 t 6 iv 9 e .
Fundamentalpropertiesandapplicationstopicture Mostow,J.andPrieditis,A.E.(1989).Discovering
processing.InformationSciences,7(2),95–132. admissibleheuristicsbyabstractingandoptimizing: Muscettola,N.(2002).Computingtheenvelopefor
Atransformationalapproach. InIJCAI-89,Vol.1, stepwise-constantresourceallocations. In CP-02,
Montemerlo,M.andThrun,S.(2004).Large-scale pp.701–707. pp.139–154.
robotic3-Dmappingofurbanstructures. InProc.
InternationalSymposiumonExperimentalRobotics. Motzkin,T.S.andSchoenberg,I.J.(1954). The Muscettola,N.,Nayak,P.,Pell,B.,andWilliams,
SpringerTractsinAdvancedRobotics(STAR). relaxationmethodforlinearinequalities. Canadian B.(1998).Remoteagent:ToboldlygowherenoAI
JournalofMathematics,6(3),393–404. systemhasgonebefore.AIJ,103,5–48.
Montemerlo,M.,Thrun,S.,Koller,D.,andWeg-
breit,B.(2002). FastSLAM:Afactoredsolutionto Moutarlier,P.andChatila,R.(1989). Stochastic Muslea,I.(1999). Extractionpatternsforinforma-
thesimultaneouslocalizationandmappingproblem. multisensorydatafusionformobilerobotlocation tionextractiontasks: Asurvey. InProc.AAAI-99
InAAAI-02. andenvironmentmodeling.InISRR-89. WorkshoponMachineLearningforInformationEx-
traction.
Mueller, E.T.(2006). CommonsenseReasoning.
Mooney,R.(1999).Learningforsemanticinterpre-
MorganKaufmann. Myerson,R.(1981).Optimalauctiondesign.Math-
tation:Scalingupwithoutdumbingdown. InProc. ematicsofOperationsResearch,6,58–73.
1stWorkshoponLearningLanguageinLogic,pp. Muggleton,S.H.(1991). Inductivelogicprogram-
7–15. ming.NewGenerationComputing,8,295–318. Myerson,R.(1986). Multistagegameswithcom-
munication.Econometrica,54,323–358.
Moore,A.andWong,W.-K.(2003). Optimalrein- Muggleton,S.H.(1992).InductiveLogicProgram-
sertion: Anewsearchoperatorforacceleratedand ming.AcademicPress. Myerson, R.(1991). GameTheory: Analysisof
moreaccurateBayesiannetworkstructurelearning. Muggleton, S.H.(1995). Inverseentailmentand Conflict.HarvardUniversityPress.
InICML-03. Progol. NewGenerationComputing,13(3-4),245– Nagel,T.(1974). Whatisitliketobeabat? Philo-
Moore,A.W.andAtkeson, C.G.(1993). Prior- 286. sophicalReview,83,435–450.
itizedsweeping—Reinforcementlearningwithless Muggleton,S.H.(2000). Learningstochasticlogic Nalwa,V.S.(1993). AGuidedTourofComputer
dataandlesstime.MachineLearning,13,103–130. programs.Proc.AAAI2000WorkshoponLearning Vision.Addison-Wesley.
StatisticalModelsfromRelationalData.
Moore,A.W.andLee,M.S.(1997). Cachedsuf- Nash, J.(1950). Equilibriumpoints in N-person
ficientstatisticsforefficientmachinelearningwith Muggleton,S.H.andBuntine,W.(1988).Machine games.PNAS,36,48–49.
largedatasets.JAIR,8,67–91. inventionoffirst-orderpredicatesbyinvertingreso- Nau,D.S.(1980).Pathologyongametrees:Asum-
lution.InICML-88,pp.339–352.
Moore,E.F.(1959). Theshortestpaththrougha maryofresults.InAAAI-80,pp.102–104.
U m Th n a e i z v o e e r . y rs I i o n t f y P S P r w r o e i c t s c . s h . a i n ng I , nt P e a r r n t a I ti I o , n p a p l . S 2 y 8 m 5– p 2 o 9 si 2 u . m H o ar n va th rd e M t L iv o u e g g ic l g o l P g e i r t c o o g n p r , r a o S m g . m r H a i m . n a g m n , i d 1 n 9 g D / : 2 e 0 T R , h 6 a e 2 e o 9 d r – t y , 6 L 7 a . n 9 d . (19 m 9 e 4 t ) h . od In s. duc J - . N i 2 t 2 e a 1 d u – , , a 2 D n 4 d 4 . . a S n .( a 1 lt 9 e 8 rn 3 a ). tiv P e a t t o ho m lo in g i y m o a n xi g n a g m .A e I t J r , e 2 es 1( r 1 e – v 2 is ) - ,
Muggleton,S.H.andFeng,C.(1990).Efficientin-
Moore,R.C.(1980). Reasoningaboutknowledge ductionoflogicprograms. InProc.Workshopon Nau, D.S., Kumar, V., andKanal, L.N.(1984).
andaction. Artificialintelligencecentertechnical AlgorithmicLearningTheory,pp.368–381. Generalbranchandbound,anditsrelationtoA*and
note191,SRIInternational. AO*.AIJ,23,29–58.
Mu¨ller,M.(2002). ComputerGo. AIJ,134(1–2),
Moore,R.C.(1985). Aformaltheoryofknowl- 145–179. Nayak, P.andWilliams, B.(1997). Fastcontext
edgeandaction. InHobbs,J.R.andMoore,R.C. switchinginreal-timepropositionalreasoning. In
(Eds.),FormalTheoriesoftheCommonsenseWorld, Mu¨ller, M. (2003). Conditional combinatorial AAAI-97,pp.50–56.
pp.319–358.Ablex. g ra a c m es es i , n a g n o d . th I e n i f r or a m pp a l t i i c o a n tio S n cie to nc a e n s a , ly 1 z 5 i 4 n ( g 3– c 4 a ) p , tu 1 r 8 in 9 g – Neal,R.(1996).BayesianLearningforNeuralNet-
Moore,R.C.(2005). Association-basedbilingual 202. works.Springer-Verlag.
word alignment. In Proc. ACL-05 Workshop on Mumford,D.andShah,J.(1989).Optimalapprox- Nebel,B.(2000). Onthecompilabilityandexpres-
BuildingandUsingParallelTexts,pp.1–8. imationsbypiece-wisesmoothfunctionsandasso- sive power of propositional planning formalisms.
Moravec,H.P.(1983). Thestanfordcartandthe ciatedvariationalproblems. Commun.PureAppl. JAIR,12,271–315.
cmurover.Proc.IEEE,71(7),872–884. Math.,42,577–685. Nefian,A.,Liang,L.,Pi,X.,Liu,X.,andMurphy,
Moravec,H.P.andElfes,A.(1985). Highresolu- Murphy, K.,Weiss,Y.,andJordan,M.I.(1999). K.(2002). Dynamicbayesiannetworksforaudio-
tionmapsfromwideanglesonar. InICRA-85,pp. Loopybeliefpropagationforapproximateinference: visualspeechrecognition.EURASIP,JournalofAp-
116–121. Anempiricalstudy.InUAI-99,pp.467–475. pliedSignalProcessing,11,1–15.
Murphy, K. (2001). The Bayes net toolbox for Nesterov,Y.andNemirovski,A.(1994). Interior-
Moravec,H.P.(1988).MindChildren:TheFuture MATLAB.ComputingScienceandStatistics,33. PointPolynomialMethodsinConvexProgramming.
ofRobotandHumanIntelligence. HarvardUniver- SIAM(SocietyforIndustrialandAppliedMathe-
sityPress. Murphy,K.(2002). DynamicBayesianNetworks: matics).
Representation,InferenceandLearning. Ph.D.the-
Moravec,H.P.(2000). Robot: MereMachineto sis,UCBerkeley. Netto,E.(1901). LehrbuchderCombinatorik. B.
TranscendentMind.OxfordUniversityPress. G.Teubner.
Murphy, K. and Mian, I. S. (1999). Modelling
Morgenstern,L.(1998). Inheritancecomesofage: gene expression data using Bayesian networks. Nevill-Manning, C. G. and Witten, I. H. (1997).
Applyingnonmonotonictechniquestoproblemsin people.cs.ubc.ca/˜murphyk/Papers/ Identifyinghierarchicalstructuresinsequences: A
industry.AIJ,103,237–271. ismb99.pdf. linear-timealgorithm.JAIR,7,67–82.
1084 Bibliography
Newell,A.(1982).Theknowledgelevel.AIJ,18(1), Nilsson,N.J.(1984). Shakeytherobot. Technical O’Reilly,U.-M.andOppacher,F.(1994). Program
82–127. note323,SRIInternational. searchwithahierarchicalvariablelengthrepresen-
tation: Geneticprogramming, simulatedannealing
Newell,A.(1990). UnifiedTheoriesofCognition. Nilsson,N.J.(1986).Probabilisticlogic.AIJ,28(1),
andhillclimbing.InProc.ThirdConferenceonPar-
HarvardUniversityPress. 71–87.
allelProblemSolvingfromNature,pp.397–406.
Newell,A.andErnst,G.(1965).Thesearchforgen- Nilsson, N.J.(1991). Logicandartificialintelli-
Ormoneit,D.andSen,S.(2002). Kernel-basedre-
erality.InProc.IFIPCongress,Vol.1,pp.17–24. gence.AIJ,47(1–3),31–56.
inforcementlearning. MachineLearning,49(2–3),
Newell,A.,Shaw,J.C.,andSimon,H.A.(1957). Nilsson,N.J.(1995). Eyeontheprize. AIMag, 161–178.
Empirical explorations with the logic theory ma- 16(2),9–17. Osborne,M.J.(2004). AnIntroductiontoGame
chine. Proc.WesternJointComputerConference, Nilsson,N.J.(1998).ArtificialIntelligence:ANew Theory.OxfordUniversityPres.
15,218–239. ReprintedinFeigenbaumandFeld- Synthesis.MorganKaufmann.
man(1963). Osborne,M.J.andRubinstein,A.(1994).ACourse
Nilsson,N.J.(2005). Human-levelartificialintelli- inGameTheory.MITPress.
Newell,A.,Shaw,J.C.,andSimon,H.A.(1958). gence?beserious!AIMag,26(4),68–75.
Chessplayingprogramsandtheproblemofcom- Osherson, D. N., Stob, M., and Weinstein, S.
plexity.IBMJournalofResearchandDevelopment, Nilsson,N.J.(2009).TheQuestforArtificialIntel- (1986). SystemsThat Learn: AnIntroduction to
4(2),320–335. ligence:AHistoryofIdeasandAchievements.Cam- LearningTheoryforCognitiveandComputerSci-
bridgeUniversityPress. entists.MITPress.
Newell,A.andSimon,H.A.(1961). GPS,apro-
gramthatsimulateshumanthought. InBilling,H. Nisan,N.,Roughgarden,T.,Tardos,E.,andVazi- Padgham,L.andWinikoff,M.(2004). Developing
(Ed.),LernendeAutomaten,pp.109–124.R.Olden- rani,V.(Eds.).(2007). AlgorithmicGameTheory. IntelligentAgentSystems:APracticalGuide.Wiley.
bourg. CambridgeUniversityPress. Page,C.D.andSrinivasan,A.(2002).ILP:Ashort
Newell,A.andSimon,H.A.(1972). HumanProb- Noe,A.(2009). OutofOurHeads: WhyYouAre lookbackandalongerlookforward. Submittedto
lemSolving.Prentice-Hall. NotYourBrain,andOtherLessonsfromtheBiology JournalofMachineLearningResearch.
ofConsciousness.HillandWang. Palacios,H.andGeffner,H.(2007). Fromconfor-
Newell, A.andSimon, H. A.(1976). Computer
Norvig,P.(1988).Multiplesimultaneousinterpreta- mantintoclassicalplanning: Efficienttranslations
scienceasempiricalinquiry: Symbolsandsearch.
tionsofambiguoussentences.InCOGSCI-88. thatmaybecompletetoo.InICAPS-07.
CACM,19,113–126.
Norvig,P.(1992). ParadigmsofArtificialIntelli- Palay,A.J.(1985). SearchingwithProbabilities.
Newton,I.(1664–1671).Methodusfluxionumetse-
genceProgramming:CaseStudiesinCommonLisp. Pitman.
rieruminfinitarum.Unpublishednotes.
Ng,A.Y.(2004). Featureselection,l1vs.l2regu- M
No
o
r
r
v
g
i
a
g
n
,
K
P.
a
(
u
2
f
0
m
0
a
9
n
).
n.
Naturallanguagecorpusdata.In
P
se
a
n
l
t
m
en
e
c
r
e
,
b
D
o
.
u
A
nd
.
a
a
r
n
y
d
d
H
is
e
a
a
m
rs
b
t
i
,
g
M
ua
.
ti
A
o
.
n.
(1
I
9
n
9
P
4
r
)
o
.
c
A
.C
d
o
ap
n
t
f
i
e
v
r
e
-
larization,androtationalinvariance.InICML-04. Segaran,T.andHammerbacher,J.(Eds.),Beautiful enceonAppliedNaturalLanguageProcessing,pp.
Ng,A.Y.,Harada,D.,andRussell,S.J.(1999).Pol- Data.O’Reilly. 78–83.
icyinvarianceunderrewardtransformations:Theory Nowick, S. M., Dean, M. E., Dill, D. L., and Palmer,S.(1999).VisionScience:PhotonstoPhe-
andapplicationtorewardshaping.InICML-99. Horowitz, M. (1993). The design of a high- nomenology.MITPress.
Ng,A.Y.andJordan,M.I.(2000). PEGASUS:A performancecachecontroller:Acasestudyinasyn- Papadimitriou,C.H.(1994).ComputationalCom-
policysearchmethodforlargeMDPsandPOMDPs. chronoussynthesis. Integration:TheVLSIJournal, plexity.AddisonWesley.
InUAI-00,pp.406–415. 15(3),241–262.
Papadimitriou, C.H., Tamaki, H., Raghavan, P.,
Ng,A.Y.,Kim,H.J.,Jordan,M.I.,andSastry,S. Nunberg,G.(1979).Thenon-uniquenessofseman- andVempala,S.(1998). Latentsemanticindexing:
(2004). Autonomoushelicopterflightviareinforce- ticsolutions:Polysemy. LanguageandPhilosophy, Aprobabilisticanalysis.InPODS-98,pp.159–168.
mentlearning.InNIPS16. 3(2),143–184.
Papadimitriou,C.H.andTsitsiklis,J.N.(1987).
Nguyen,X.andKambhampati,S.(2001).Reviving Nussbaum,M.C.(1978).Aristotle’sDeMotuAni- The complexity of Markov decision processes.
partialorderplanning.InIJCAI-01,pp.459–466. malium.PrincetonUniversityPress. MathematicsofOperationsResearch,12(3), 441–
Nguyen,X.,Kambhampati,S.,andNigenda,R.S. Oaksford,M.andChater,N.(Eds.).(1998). Ra- 450.
(2001). Planning graph as the basis for deriving tionalmodelsofcognition.OxfordUniversityPress. Papadimitriou,C.H.andYannakakis,M.(1991).
heuristicsforplansynthesisbystatespaceandCSP Och,F.J.andNey,H.(2003).Asystematiccompar- Shortestpathswithoutamap.TheoreticalComputer
search.Tech.rep.,ComputerScienceandEngineer- isonofvariousstatisticalalignmentmodel.Compu- Science,84(1),127–150.
ingDepartment,ArizonaStateUniversity. tationalLinguistics,29(1),19–51. Papavassiliou,V.andRussell,S.J.(1999).Conver-
Nicholson,A.andBrady,J.M.(1992).Thedataas- Och, F. J. and Ney, H. (2004). The alignment genceofreinforcementlearningwithgeneralfunc-
sociationproblemwhenmonitoringrobotvehicles templateapproachtostatisticalmachinetranslation. tionapproximators.InIJCAI-99,pp.748–757.
usingdynamic belief networks. In ECAI-92, pp. ComputationalLinguistics,30,417–449. Parekh,R.andHonavar,V.(2001). DFAlearning
689–693. Ogawa,S.,Lee,T.-M.,Kay,A.R.,andTank,D.W. fromsimpleexamples. MachineLearning,44,9–
Niemela¨, I., Simons, P., andSyrja¨nen, T.(2000). (1990).Brainmagneticresonanceimagingwithcon- 35.
Smodels: A system for answer set program- trastdependentonbloodoxygenation. PNAS,87, Parisi,G.(1988). Statisticalfieldtheory. Addison-
ming. InProc.8thInternationalWorkshoponNon- 9868–9872. Wesley.
MonotonicReasoning.
Oh,S.,Russell,S.J.,andSastry,S.(2009).Markov Parisi,M.M.G.andZecchina,R.(2002). Ana-
Nigam,K.,McCallum,A.,Thrun,S.,andMitchell, chainMonteCarlodataassociationformulti-target lyticandalgorithmicsolutionofrandomsatisfiabil-
T.M.(2000). Textclassificationfromlabeledand tracking. IEEETransactionsonAutomaticControl, ityproblems.Science,297,812–815.
unlabeleddocumentsusingEM.MachineLearning, 54(3),481–497.
39(2–3),103–134. Parker,A., Nau,D.S.,andSubrahmanian, V.S.
Olesen,K.G.(1993).Causalprobabilisticnetworks (2005).Game-treesearchwithcombinatoriallylarge
Niles,I.andPease,A.(2001). Towardsastandard withbothdiscreteandcontinuousvariables. PAMI, beliefstates.InIJCAI-05,pp.254–259.
upperontology. InFOIS’01: Proc.international 15(3),275–279.
conferenceonFormalOntologyinInformationSys- Parker, D. B. (1985). Learning logic. Tech-
tems,pp.2–9. Oliver,N.,Garg,A.,andHorvitz,E.J.(2004).Lay- nicalreportTR-47, CenterforComputationalRe-
eredrepresentationsforlearningandinferringoffice search in Economics and Management Science,
Nilsson, D.andLauritzen, S.(2000). Evaluating activityfrommultiplesensorychannels. Computer MassachusettsInstituteofTechnology.
influencediagramsusingLIMIDs. InUAI-00,pp. VisionandImageUnderstanding,96,163–180.
Parker,L.E.(1996). Onthedesignofbehavior-
436–445.
Oliver,R.M.andSmith,J.Q.(Eds.).(1990).Influ- based multi-robot teams. J. Advanced Robotics,
Nilsson,N.J.(1965). LearningMachines: Foun- enceDiagrams,BeliefNetsandDecisionAnalysis. 10(6).
dations of Trainable Pattern-Classifying Systems. Wiley.
Parr,R.andRussell,S.J.(1998). Reinforcement
McGraw-Hill.Republishedin1990.
Omohundro, S.(2008). ThebasicAIdrives. In learningwithhierarchiesofmachines. InJordan,
Nilsson,N.J.(1971). Problem-SolvingMethodsin AGI-08WorkshopontheSociocultural,Ethicaland M.I.,Kearns,M.,andSolla,S.A.(Eds.),NIPS10.
ArtificialIntelligence.McGraw-Hill. FuturologicalImplicationsofArtificialIntelligence. MITPress.
Bibliography 1085
Parzen,E.(1962). Onestimationofaprobability Peirce,C.S.(1870).Descriptionofanotationforthe Pfeffer,A.(2007). Thedesignandimplementation
densityfunctionandmode.AnnalsofMathematical logicofrelatives,resultingfromanamplificationof ofIBAL:Ageneral-purposeprobabilisticlanguage.
Statistics,33,1065–1076. theconceptionsofBoole’scalculusoflogic. Mem- InGetoor,L.andTaskar,B.(Eds.),Introductionto
oirsoftheAmericanAcademyofArtsandSciences, StatisticalRelationalLearning.MITPress.
Pasca,M.andHarabagiu,S.M.(2001).Highperfor-
9,317–378.
mancequestion/answering. InSIGIR-01,pp.366– Pfeifer,R., Bongard, J.,Brooks, R.A., andIwa-
374. Peirce,C.S.(1883).Atheoryofprobableinference. sawa,S.(2006). HowtheBodyShapestheWayWe
NoteB.Thelogicofrelatives.InStudiesinLogicby Think:ANewViewofIntelligence.Bradford.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., and MembersoftheJohnsHopkinsUniversity,pp.187–
Jain,A.(2006).Organizingandsearchingtheworld Pineau,J.,Gordon,G.,andThrun,S.(2003).Point-
203,Boston.
wideweboffacts—Stepone:Theone-millionfact based value iteration: An anytime algorithm for
extractionchallenge.InAAAI-06. Peirce,C.S.(1902). Logicassemiotic: Thethe- POMDPs.InIJCAI-03.
oryofsigns. Unpublishedmanuscript;reprintedin
Paskin,M.(2001).Grammaticalbigrams.InNIPS. Pinedo,M.(2008).Scheduling:Theory,Algorithms,
(Buchler1955).
andSystems.SpringerVerlag.
Pasula,H.,Marthi,B.,Milch,B.,Russell,S.J.,and Peirce,C.S.(1909). Existentialgraphs. Unpub-
Shpitser,I.(2003). Identityuncertaintyandcitation lishedmanuscript;reprintedin(Buchler1955). Pinkas,G.andDechter,R.(1995). Improvingcon-
matching.InNIPS15.MITPress. nectionistenergyminimization.JAIR,3,223–248.
Pelikan, M., Goldberg, D.E., andCantu-Paz, E.
Pasula,H.andRussell,S.J.(2001). Approximate (1999). BOA: The Bayesian optimization algo- Pinker,S.(1995). Languageacquisition. InGleit-
inferenceforfirst-orderprobabilisticlanguages. In rithm.InGECCO-99:Proc.GeneticandEvolution- man, L. R., Liberman, M., and Osherson, D. N.
IJCAI-01. aryComputationConference,pp.525–532. (Eds.),AnInvitationtoCognitiveScience(second
edition).,Vol.1.MITPress.
Pasula,H.,Russell,S.J.,Ostland,M.,andRitov,Y. Pemberton,J.C.andKorf,R.E.(1992).Incremen-
(1999). Trackingmanyobjectswithmanysensors. talplanningongraphswithcycles. InAIPS-92,pp. Pinker,S.(2003). TheBlankSlate: TheModern
InIJCAI-99. 525–532. DenialofHumanNature.Penguin.
Patashnik, O. (1980). Qubic: 4x4x4tic-tac-toe. Penberthy,J.S.andWeld,D.S.(1992). UCPOP: Pinto,D.,McCallum,A.,Wei,X.,andCroft,W.B.
MathematicsMagazine,53(4),202–216. Asound,complete,partialorderplannerforADL. (2003). Tableextractionusingconditionalrandom
fields.InSIGIR-03.
Patrick, B. G., Almulla, M., and Newborn, M. InKR-92,pp.103–114.
(1992). Anupperboundonthetimecomplexityof Peng,J.andWilliams,R.J.(1993). Efficientlearn- Pipatsrisawat,K.andDarwiche,A.(2007). RSat
iterative-deepening-A*.AIJ,5(2–4),265–278. ingandplanningwithintheDynaframework.Adap- 2.0:SATsolverdescription. Tech.rep.D–153,Au-
tomatedReasoningGroup, ComputerScienceDe-
Paul,R.P.(1981).RobotManipulators:Mathemat- tiveBehavior,2,437–454. partment,UniversityofCalifornia,LosAngeles.
ics,Programming,andControl.MITPress. Penrose,R.(1989).TheEmperor’sNewMind.Ox-
Plaat,A.,Schaeffer,J.,Pijls,W.,anddeBruin,A.
Pauls,A.andKlein,D.(2009). K-bestA*parsing. fordUniversityPress. (1996). Best-firstfixed-depthminimaxalgorithms.
InACL-09. Penrose,R.(1994). ShadowsoftheMind. Oxford AIJ,87(1–2),255–293.
Peano, G. (1889). Arithmetices principia, nova UniversityPress. Place,U.T.(1956). Isconsciousnessabrainpro-
methodoexposita.FratresBocca,Turin. Peot,M.andSmith,D.E.(1992).Conditionalnon- cess?BritishJournalofPsychology,47,44–50.
Pearce,J.,Tambe,M.,andMaheswaran,R.(2008). linearplanning.InICAPS-92,pp.189–197. Platt,J.(1999). Fasttrainingofsupportvectorma-
Solvingmultiagentnetworksusingdistributedcon- Pereira, F. and Shieber, S. (1987). Prolog and chinesusingsequentialminimaloptimization.InAd-
straintoptimization.AIMag,29(3),47–62. Natural-LanguageAnalysis.CenterfortheStudyof vancesinKernelMethods:SupportVectorLearning,
Pearl,J.(1982a). ReverendBayesoninferenceen- LanguageandInformation(CSLI). pp.185–208.MITPress.
gines:Adistributedhierarchicalapproach.InAAAI- Pereira,F.andWarren,D.H.D.(1980). Definite Plotkin,G.(1971).AutomaticMethodsofInductive
82,pp.133–136. clausegrammars forlanguageanalysis: Asurvey Inference.Ph.D.thesis,EdinburghUniversity.
Pearl,J.(1982b). Thesolutionforthebranching oftheformalismandacomparisonwithaugmented Plotkin,G.(1972).Building-inequationaltheories.
factorofthealpha–betapruningalgorithmandits transitionnetworks.AIJ,13,231–278. InMeltzer,B.andMichie,D.(Eds.),MachineIntel-
optimality.CACM,25(8),559–564. Pereira,F.andWright,R.N.(1991).Finite-stateap- ligence7,pp.73–90.EdinburghUniversityPress.
Pearl, J. (1984). Heuristics: Intelligent Search proximationofphrasestructuregrammars. InACL- Pohl,I.(1971).Bi-directionalsearch.InMeltzer,B.
StrategiesforComputerProblemSolving.Addison- 91,pp.246–255. andMichie,D.(Eds.),MachineIntelligence6,pp.
Wesley. Perlis,A.(1982). Epigramsinprogramming. SIG- 127–140.EdinburghUniversityPress.
Pearl,J.(1986). Fusion,propagation,andstructur- PLANNotices,17(9),7–13. Pohl,I.(1973).Theavoidanceof(relative)catastro-
inginbeliefnetworks.AIJ,29,241–288. Perrin,B.E.,Ralaivola,L.,andMazurie,A.(2003). phe,heuristiccompetence,genuinedynamicweight-
Genenetworks inferenceusingdynamic Bayesian ingandcomputationalissuesinheuristicproblem
Pearl,J.(1987).Evidentialreasoningusingstochas-
networks.Bioinformatics,19,II138–II148. solving.InIJCAI-73,pp.20–23.
ticsimulationofcausalmodels.AIJ,32,247–257.
Peterson,C.andAnderson,J.R.(1987). Amean Pohl,I.(1977). Practicalandtheoreticalconsidera-
Pearl,J.(1988). ProbabilisticReasoninginIntelli-
fieldtheorylearningalgorithmforneuralnetworks. tionsinheuristicsearchalgorithms.InElcock,E.W.
gentSystems:NetworksofPlausibleInference.Mor-
ComplexSystems,1(5),995–1019. andMichie,D.(Eds.),MachineIntelligence8,pp.
ganKaufmann.
55–72.EllisHorwood.
Petrik,M.andZilberstein,S.(2009). Bilinearpro-
Pearl,J.(2000).Causality:Models,Reasoning,and
grammingapproachformultiagentplanning. JAIR, Poli,R.,Langdon,W.,andMcPhee,N.(2008). A
Inference.CambridgeUniversityPress.
35,235–274. FieldGuidetoGeneticProgramming.Lulu.com.
Pearl,J.andVerma,T.(1991).Atheoryofinferred
Petrov, S.andKlein, D.(2007a). Discriminative Pomerleau,D.A.(1993). NeuralNetworkPercep-
causation.InKR-91,pp.441–452.
log-lineargrammarswithlatentvariables.InNIPS. tionforMobileRobotGuidance.Kluwer.
Pearson, J.andJeavons, P.(1997). Asurveyof
Petrov,S.andKlein,D.(2007b). Improvedinfer- Ponte,J.andCroft,W.B.(1998).Alanguagemod-
tractableconstraintsatisfactionproblems. Techni-
enceforunlexicalizedparsing.InACL-07. elingapproachtoinformationretrieval.InSIGIR-98,
calreportCSD-TR-97-15,RoyalHollowayCollege,
pp.275–281.
U.ofLondon. Petrov,S.andKlein,D.(2007c). Learningandin-
ferenceforhierarchicallysplitpcfgs.InAAAI-07. Poole,D.(1993). ProbabilisticHornabductionand
Pease,A.andNiles,I.(2002).IEEEstandardupper
Bayesiannetworks.AIJ,64,81–129.
ontology:Aprogressreport. KnowledgeEngineer- Pfeffer,A.,Koller,D.,Milch,B.,andTakusagawa,
ingReview,17(1),65–70. K.T.(1999). SPOOK:Asystemforprobabilistic Poole,D.(2003).First-orderprobabilisticinference.
object-orientedknowledgerepresentation. InUAI- InIJCAI-03,pp.985–991.
Pednault,E.P.D.(1986). Formulatingmultiagent,
99.
dynamic-world problems in the classicalplanning Poole,D.,Mackworth,A.K.,andGoebel,R.(1998).
framework. InReasoningaboutActionsandPlans: Pfeffer, A. (2000). Probabilistic Reasoning for Computational intelligence: A logical approach.
Proc.1986Workshop,pp.47–82. ComplexSystems.Ph.D.thesis,StanfordUniversity. OxfordUniversityPress.
1086 Bibliography
Popper,K.R.(1959). TheLogicofScientificDis- Pylyshyn,Z.W.(1984). ComputationandCogni- Ratner, D. andWarmuth, M.(1986). Finding a
covery.BasicBooks. tion: TowardaFoundationforCognitiveScience. shortestsolutionforthen × nextensionofthe
MITPress. 15-puzzleisintractable.InAAAI-86,Vol.1,pp.168–
Popper,K.R.(1962).ConjecturesandRefutations:
172.
TheGrowthofScientificKnowledge.BasicBooks. Quillian,M.R.(1961).Adesignforanunderstand-
Portner,P.andPartee,B.H.(2002).FormalSeman- ingmachine. Paperpresentedatacolloquium: Se- Rauch,H.E.,Tung,F.,andStriebel,C.T.(1965).
tics:TheEssentialReadings.Wiley-Blackwell. manticProblemsinNaturalLanguage,King’sCol- Maximum likelihood estimates of linear dynamic
lege,Cambridge,England. systems.AIAAJournal,3(8),1445–1450.
Post,E.L.(1921). Introductiontoageneralthe-
oryofelementarypropositions. AmericanJournal Quine,W.V.(1953). Twodogmasofempiricism. Rayward-Smith, V., Osman, I., Reeves, C., and
ofMathematics,43,163–185. InFromaLogicalPointofView,pp.20–46.Harper Smith,G.(Eds.).(1996). ModernHeuristicSearch
andRow. Methods.Wiley.
Poundstone,W.(1993). Prisoner’sDilemma. An-
chor. Quine,W.V.(1960).WordandObject.MITPress. Rechenberg,I.(1965). Cyberneticsolutionpathof
anexperimentalproblem. Librarytranslation1122,
Pourret, O., Na¨ım, P., and Marcot, B. (2008). Quine,W.V.(1982). MethodsofLogic(fourthedi- RoyalAircraftEstablishment.
BayesianNetworks: Apracticalguidetoapplica- tion).HarvardUniversityPress.
tions.Wiley. Reeson, C.G., Huang, K.-C., Bayer, K.M., and
Quinlan,J.R.(1979).Discoveringrulesfromlarge Choueiry,B.Y.(2007). Aninteractiveconstraint-
Prades,J.L.P.,Loomes,G.,andBrey,R.(2008). collectionsofexamples: Acasestudy. InMichie, basedapproachtosudoku. InAAAI-07,pp.1976–
TryingtoestmateamonetaryvaluefortheQALY. D.(Ed.),ExpertSystemsintheMicroelectronicAge. 1977.
Tech.rep.WPEcon08.09,Univ.PabloOlavide. EdinburghUniversityPress. Regin, J. (1994). A filtering algorithm for con-
Pradhan, M., Provan, G.M., Middleton, B., and Quinlan,J.R.(1986). Inductionofdecisiontrees. straintsofdifferenceinCSPs.InAAAI-94,pp.362–
Henrion, M. (1994). Knowledge engineering for MachineLearning,1,81–106. 367.
largebeliefnetworks.InUAI-94,pp.484–490.
Reichenbach,H.(1949). TheTheoryofProbabil-
Prawitz,D.(1960). Animprovedproofprocedure. Quinlan,J.R.(1990). Learninglogicaldefinitions ity: AnInquiryintotheLogicalandMathematical
Theoria,26,102–139. fromrelations.MachineLearning,5(3),239–266. FoundationsoftheCalculusofProbability(second
Press,W.H.,Teukolsky,S.A.,Vetterling,W.T.,and Quinlan,J.R.(1993).C4.5:Programsformachine edition).UniversityofCaliforniaPress.
Flannery,B.P.(2007). NumericalRecipes:TheArt learning.MorganKaufmann. Reid,D.B.(1979). Analgorithmfortrackingmul-
ofScientificComputing(thirdedition). Cambridge Quinlan, J.R.andCameron-Jones,R.M.(1993). tipletargets.IEEETrans.AutomaticControl,24(6),
UniversityPress. FOIL:Amidtermreport.InECML-93,pp.3–20. 843–854.
C I P n r h t e e in s ll t e i o g s n e e , n R c J o e . o . a m n O d : x N f B o e i r w s d h U o E p s n , s i a v M e y r s . s o ( it 2 n y 0 S P 0 e 2 r a e ) s r . s le . V a ie n w d s A i r n t t i o fic t i h a e l Q J L . a u ( n 1 i g r 9 k u 8 a , 5 g R ). e . . A ,G L C o r o e n m e g n m p b r a a e n u h . m en , s S iv ., e L G e r e a c m h, m G a . r , o a f n t d he Sv E a n r g tv li i s k h , R l I e E m e E if E a , . n J d . g (1 e 9 n 7 e 9 ra ) l . iza C ti o o m ns p . le I x n it F y O o C f S t - h 7 e 9, m p o p v . e 4 r 2 ’s 1– p 4 ro 2 b 7 - .
Prieditis,A.E.(1993).Machinediscoveryofeffec- Reiter,R.(1980).Alogicfordefaultreasoning.AIJ,
tiveadmissibleheuristics.MachineLearning,12(1– Rabani,Y.,Rabinovich,Y.,andSinclair,A.(1998). 13(1–2),81–132.
3),117–141. Acomputationalviewofpopulationgenetics. Ran-
domStructuresandAlgorithms,12(4),313–334. Reiter,R.(1991). Theframeprobleminthesitu-
Prinz,D.G.(1952).Robotchess.Research,5,261– ationcalculus: Asimplesolution(sometimes)and
266. Rabiner,L.R.andJuang,B.-H.(1993).Fundamen- acompletenessresultforgoalregression. InLif-
Prosser,P.(1993).Hybridalgorithmsforconstraint talsofSpeechRecognition.Prentice-Hall. schitz, V.(Ed.), ArtificialIntelligenceandMathe-
satisfactionproblems. ComputationalIntelligence, Ralphs, T.K., Ladanyi, L., and Saltzman, M. J. maticalTheoryofComputation:PapersinHonorof
9,268–299. (2004). Alibraryhierarchyforimplementingscal- JohnMcCarthy,pp.359–380.AcademicPress.
Pullum,G.K.(1991). TheGreatEskimoVocabu- ableparallelsearchalgorithms. J.Supercomputing, Reiter,R.(2001). KnowledgeinAction: Logical
laryHoax(andOtherIrreverentEssaysontheStudy 28(2),215–234. FoundationsforSpecifyingandImplementingDy-
ofLanguage).UniversityofChicagoPress. Ramanan, D., Forsyth, D., and Zisserman, A. namicalSystems.MITPress.
Pullum,G.K.(1996). Learnability,hyperlearning, (2007). Trackingpeoplebylearningtheirappear- Renner, G. andEkart, A. (2003). Geneticalgo-
andthepovertyofthestimulus. In22ndAnnual ance. IEEEPatternAnalysisandMachineIntelli- rithmsincomputeraideddesign. ComputerAided
MeetingoftheBerkeleyLinguisticsSociety. gence. Design,35(8),709–726.
Puterman, M.L.(1994). Markov DecisionPro- Ramsey, F.P.(1931). Truthandprobability. In Re´nyi, A. (1970). Probability Theory.
cesses:DiscreteStochasticDynamicProgramming. Braithwaite,R.B.(Ed.),TheFoundationsofMath- Elsevier/North-Holland.
Wiley. ematicsandOtherLogicalEssays.HarcourtBrace Reynolds,C.W.(1987).Flocks,herds,andschools:
Puterman,M.L.andShin,M.C.(1978).Modified Jovanovich. Adistributedbehavioralmodel. ComputerGraph-
policyiteration algorithms for discountedMarkov Ranzato,M.,Poultney,C.,Chopra,S.,andLeCun, ics,21,25–34. SIGGRAPH’87ConferencePro-
decisionproblems. Management Science, 24(11), Y.(2007). Efficientlearningofsparserepresenta- ceedings.
1127–1137. tionswithanenergy-basedmodel. InNIPS19,pp. Riazanov,A.andVoronkov,A.(2002). Thedesign
Putnam,H.(1960).Mindsandmachines.InHook, 1137–1144. andimplementationofVAMPIRE. AICommunica-
S.(Ed.),DimensionsofMind,pp.138–164.Macmil- Raphson,J.(1690). Analysisaequationumuniver- tions,15(2–3),91–110.
lan. salis.ApudAbelemSwalle,London. Rich,E.andKnight,K.(1991). ArtificialIntelli-
Putnam,H.(1963). ‘Degreeofconfirmation’and gence(secondedition).McGraw-Hill.
Rashevsky, N.(1936). Physico-mathematicalas-
inductivelogic.InSchilpp,P.A.(Ed.),ThePhiloso-
pectsofexcitationandconductioninnerves.InCold Richards,M.andAmir,E.(2007). Opponentmod-
phyofRudolfCarnap,pp.270–292.OpenCourt.
SpringsHarborSymposiaonQuantitativeBiology. elinginScrabble.InIJCAI-07.
Putnam, H.(1967). Thenatureofmentalstates. IV:ExcitationPhenomena,pp.90–97.
Richardson,M.,Bilmes,J.,andDiorio,C.(2000).
InCapitan, W.H.andMerrill, D.D. (Eds.), Art,
Mind,andReligion,pp.37–48.UniversityofPitts- Rashevsky,N.(1938). MathematicalBiophysics: Hidden-articulator Markov models: Performance
burghPress. Physico-MathematicalFoundationsofBiology.Uni- improvementsandrobustnesstonoise. InICASSP-
versityofChicagoPress. 00.
Putnam,H.(1975).Themeaningof“meaning”.In
Gunderson,K.(Ed.),Language,MindandKnowl- Rasmussen, C. E. and Williams, C. K. I. Richter,S.andWestphal,M.(2008). TheLAMA
edge: MinnesotaStudiesinthePhilosophyofSci- (2006). GaussianProcessesforMachineLearning. planner. InProc.InternationalPlanningCompeti-
ence.UniversityofMinnesotaPress. MITPress. tionatICAPS.
Ridley,M.(2004).Evolution.OxfordReader.
Pylyshyn,Z.W.(1974). Minds,machinesandphe- Rassenti,S.,Smith,V.,andBulfin,R.(1982). A
nomenology: SomereflectionsonDreyfus’“What combinatorial auction mechanismfor airport time Rieger,C.(1976).Anorganizationofknowledgefor
ComputersCan’tDo”.Int.J.CognitivePsychology, slotallocation.BellJournalofEconomics,13,402– problemsolvingandlanguagecomprehension. AIJ,
3(1),57–77. 417. 7,89–127.
Bibliography 1087
Riley,J.andSamuelson,W.(1981). Optimalauc- Rossi,F.,vanBeek,P.,andWalsh,T.(2006).Hand- Russell,S.J.andSubramanian,D.(1995).Provably
tions.AmericanEconomicReview,71,381–392. bookofConstraintProcessing.Elsevier. bounded-optimalagents.JAIR,3,575–609.
Riloff,E.(1993).Automaticallyconstructingadic- Roussel,P.(1975). Prolog:Manualdereferenceet Russell,S.J.,Subramanian,D.,andParr,R.(1993).
tionaryforinformationextractiontasks.InAAAI-93, d’utilization.Tech.rep.,Grouped’IntelligenceArti- Provablyboundedoptimalagents. InIJCAI-93,pp.
pp.811–816. ficielle,Universite´d’Aix-Marseille. 338–345.
Rintanen,J.(1999). Improvementstotheevalua- Rouveirol,C.andPuget,J.-F.(1989).Asimpleand Russell,S.J.andWefald,E.H.(1989). Onoptimal
tionofquantifiedBooleanformulae. InIJCAI-99, generalsolutionforinvertingresolution. In Proc. game-treesearchusingrationalmeta-reasoning. In
pp.1192–1197. EuropeanWorkingSessiononLearning, pp.201– IJCAI-89,pp.334–340.
Rintanen,J.(2007).Asymptoticallyoptimalencod- 210. Russell,S.J.andWefald,E.H.(1991).DotheRight
ingsofconformantplanninginQBF. InAAAI-07, Rowat,P.F.(1979). RepresentingtheSpatialEx- Thing:StudiesinLimitedRationality.MITPress.
pp.1045–1050. perienceandSolvingSpatialproblemsinaSimu- Russell, S. J. and Wolfe, J. (2005). Efficient
Ripley,B.D.(1996).PatternRecognitionandNeu- latedRobotEnvironment. Ph.D.thesis,University belief-state AND-OR search, with applications to
ralNetworks.CambridgeUniversityPress. ofBritishColumbia. Kriegspiel.InIJCAI-05,pp.278–285.
Roweis,S.T.andGhahramani,Z.(1999). Aunify- Russell, S. J. and Zimdars, A. (2003). Q-
Rissanen,J.(1984).Universalcoding,information,
ingreviewofLinearGaussianModels.NeuralCom- decompositionofreinforcementlearningagents. In
prediction, andestimation. IEEETransactionson
putation,11(2),305–345. ICML-03.
InformationTheory,IT-30(4),629–636.
Rowley,H.,Baluja,S.,andKanade,T.(1996).Neu- Rustagi,J.S.(1976).VariationalMethodsinStatis-
Rissanen,J.(2007).InformationandComplexityin
ralnetwork-basedfacedetection.InCVPR,pp.203– tics.AcademicPress.
StatisticalModeling.Springer.
208.
Sabin,D.andFreuder,E.C.(1994). Contradicting
Ritchie,G.D.andHanna,F.K.(1984).AM:Acase
Roy,N.,Gordon,G.,andThrun,S.(2005).Finding conventionalwisdominconstraintsatisfaction. In
studyinAImethodology.AIJ,23(3),249–268. approximatePOMDPsolutionsthroughbeliefcom- ECAI-94,pp.125–129.
Rivest,R.(1987).Learningdecisionlists.Machine pression.JAIR,23,1–40. Sacerdoti,E.D.(1974). Planninginahierarchyof
Learning,2(3),229–246. Rubin,D.(1988). UsingtheSIRalgorithmtosim- abstractionspaces.AIJ,5(2),115–135.
Roberts,L.G.(1963).Machineperceptionofthree- ulate posterior distributions. In Bernardo, J. M., Sacerdoti,E.D.(1975). Thenonlinearnatureof
dimensionalsolids. Technicalreport315,MITLin- deGroot,M.H.,Lindley,D.V.,andSmith,A.F.M. plans.InIJCAI-75,pp.206–214.
colnLaboratory. (Eds.),BayesianStatistics3,pp.395–402.Oxford
UniversityPress. Sacerdoti,E.D.(1977). AStructureforPlansand
Robertson,N.andSeymour,P.D.(1986). Graph Behavior.Elsevier/North-Holland.
minors.II.Algorithmicaspectsoftree-width. J.Al- Rumelhart,D.E.,Hinton,G.E.,andWilliams,R.J.
Sadri,F.andKowalski,R.(1995). Variantsofthe
gorithms,7(3),309–322. (1986a). Learninginternalrepresentationsbyerror
eventcalculus.InICLP-95,pp.67–81.
propagation. InRumelhart,D.E.andMcClelland,
Robertson,S.E.(1977). Theprobabilityranking
J.L.(Eds.),ParallelDistributedProcessing,Vol.1, Sahami, M., Dumais, S. T., Heckerman, D., and
principleinIR.J.Documentation,33,294–304.
chap.8,pp.318–362.MITPress. Horvitz,E.J.(1998). ABayesianapproachtofil-
Robertson,S.E.andSparckJones,K.(1976).Rel- teringjunkE-mail.InLearningforTextCategoriza-
Rumelhart, D. E., Hinton, G. E., and Williams,
evanceweightingofsearchterms.J.AmericanSoci- tion:Papersfromthe1998Workshop.
R.J.(1986b). Learningrepresentationsbyback-
etyforInformationScience,27,129–146. propagatingerrors.Nature,323,533–536. Sahami,M.,Hearst,M.A.,andSaund,E.(1996).
Robinson,A.andVoronkov,A.(2001). Handbook Applyingthemultiplecausemixturemodeltotext
ofAutomatedReasoning.Elsevier. Rumelhart, D. E. and McClelland, J. L. (Eds.). categorization.InICML-96,pp.435–443.
(1986).ParallelDistributedProcessing.MITPress.
Robinson,J.A.(1965). Amachine-orientedlogic Sahin,N.T.,Pinker,S.,Cash,S.S.,Schomer,D.,
basedontheresolutionprinciple.JACM,12,23–41. R lin u e m Q m - e le r a y r , ni G n . g A us . in a g nd co N n i n ra e n ct j i a o n n , is M ts . y ( s 1 te 9 m 94 s ) . . Te O c n h - . a le n x d ic H al a , l g g r r a e m n, m E a . ti ( c 2 a 0 l, 0 a 9 n ). dp S h e o q n u o e l n o t g ia ic l a p l r i o n c f e o s r s m in a g tio o n f
Roche,E.andSchabes,Y.(1997).Finite-StateLan- rep.CUED/F-INFENG/TR166,CambridgeUniver- withinBroca’sarea.Science,326(5291),445–449.
guageProcessing(Language,SpeechandCommu- sityEngineeringDepartment.
nication).BradfordBooks. Sakuta, M. and Iida, H. (2002). AND/OR-tree
Ruspini, E.H.,Lowrance,J.D.,andStrat,T.M. searchforsolvingproblemswithuncertainty:Acase
Rock,I.(1984).Perception.W.H.Freeman. (1992). Understandingevidentialreasoning. IJAR, studyusingscreen-shogiproblems. IPSJJournal,
Rosenblatt,F.(1957). Theperceptron: Aperceiv- 6(3),401–424. 43(01).
ingandrecognizingautomaton. Report85-460-1, Russell,J.G.B.(1990). Isscreeningforabdom- Salomaa, A. (1969). Probabilistic and weighted
ProjectPARA,CornellAeronauticalLaboratory. inalaorticaneurysmworthwhile? ClinicalRadiol- grammars.InformationandControl,15,529–544.
ogy,41,182–184.
Rosenblatt,F.(1960). Ontheconvergenceofrein- Salton,G.,Wong,A.,andYang,C.S.(1975). A
forcementproceduresinsimpleperceptrons.Report Russell,S.J.(1985). ThecompleatguidetoMRS. vectorspacemodelforautomaticindexing. CACM,
VG-1196-G-4,CornellAeronauticalLaboratory. ReportSTAN-CS-85-1080,ComputerScienceDe- 18(11),613–620.
partment,StanfordUniversity.
Rosenblatt,F.(1962). PrinciplesofNeurodynam- Samuel, A.L.(1959). Some studiesinmachine
ics: PerceptronsandtheTheoryofBrainMecha- Russell,S.J.(1986).Aquantitativeanalysisofanal- learningusingthegameofcheckers. IBMJournal
nisms.Spartan. ogybysimilarity.InAAAI-86,pp.284–288. ofResearchandDevelopment,3(3),210–229.
Rosenblatt,M.(1956).Remarksonsomenonpara- Russell,S.J.(1988).Tree-structuredbias.InAAAI- Samuel, A.L.(1967). Some studiesinmachine
metricestimatesofadensityfunction. Annalsof 88,Vol.2,pp.641–645. learning using the game of checkers II—Recent
MathematicalStatistics,27,832–837. progress. IBMJournalofResearchandDevelop-
Russell, S. J.(1992). Efficientmemory-bounded ment,11(6),601–617.
Rosenblueth, A., Wiener, N., and Bigelow, J. searchmethods.InECAI-92,pp.1–5.
(1943). Behavior,purpose,andteleology. Philos- Samuelsson,C.andRayner,M.(1991). Quantita-
ophyofScience,10,18–24. Russell,S.J.(1998). Learningagentsforuncertain tiveevaluationofexplanation-basedlearningasan
environments(extendedabstract). InCOLT-98,pp. optimizationtoolforalarge-scalenaturallanguage
Rosenschein,J.S.andZlotkin,G.(1994). Rulesof 101–103. system.InIJCAI-91,pp.609–615.
Encounter.MITPress.
Russell,S.J.,Binder,J.,Koller,D.,andKanazawa, Sarawagi,S.(2007).Informationextraction.Foun-
Rosenschein, S. J. (1985). Formal theories of K.(1995). Locallearninginprobabilisticnetworks dationsandTrendsinDatabases,1(3),261–377.
knowledge in AI and robotics. New Generation withhiddenvariables.InIJCAI-95,pp.1146–52.
Computing,3(4),345–357. Satia, J. K. andLave, R. E. (1973). Markovian
Russell,S.J.andGrosof,B.(1987). Adeclarative decisionprocesseswithprobabilisticobservationof
Ross,P.E.(2004). Psychingoutcomputerchess approachtobiasinconceptlearning.InAAAI-87. states.ManagementScience,20(1),1–13.
players.IEEESpectrum,41(2),14–15.
Russell,S.J.andNorvig,P.(2003).ArtificialIntelli- Sato, T. and Kameya, Y. (1997). PRISM: A
Ross,S.M.(1988). AFirstCourseinProbability gence:AModernApproach(2ndedition).Prentice- symbolic-statisticalmodelinglanguage. InIJCAI-
(thirdedition).Macmillan. Hall. 97,pp.1330–1335.
1088 Bibliography
Saul,L.K.,Jaakkola,T.,andJordan,M.I.(1996). Scott,D.andKrauss,P.(1966).Assigningprobabil- Shannon,C.E.(1948). Amathematicaltheoryof
Meanfieldtheoryforsigmoidbeliefnetworks.JAIR, itiestologicalformulas.InHintikka,J.andSuppes, communication.BellSystemsTechnicalJournal,27,
4,61–76. P.(Eds.),AspectsofInductiveLogic.North-Holland. 379–423,623–656.
Savage,L.J.(1954). TheFoundationsofStatistics. Searle,J.R.(1980). Minds,brains,andprograms. Shannon,C.E.(1950). Programmingacomputer
Wiley. BBS,3,417–457. forplayingchess. PhilosophicalMagazine,41(4),
256–275.
Sayre,K.(1993). Threemoreflawsinthecompu- Searle, J.R.(1984). Minds, BrainsandScience.
tationalmodel.PaperpresentedattheAPA(Central HarvardUniversityPress. Shaparau,D.,Pistore,M.,andTraverso,P.(2008).
Division)AnnualConference,Chicago,Illinois. Fusingproceduralanddeclarativeplanninggoalsfor
Searle,J.R.(1990).Isthebrain’smindacomputer
nondeterministicdomains.InAAAI-08.
Schaeffer,J.(2008). OneJumpAhead: Computer program?ScientificAmerican,262,26–31.
PerfectionatCheckers.Springer-Verlag. Shapiro,E.(1981).Analgorithmthatinferstheories
Searle,J.R.(1992). TheRediscoveryoftheMind. fromfacts.InIJCAI-81,p.1064.
Schaeffer,J.,Burch,N.,Bjornsson,Y.,Kishimoto, MITPress.
A.,Mu¨ller, M.,Lake, R.,Lu, P.,andSutphen, S. Shapiro,S.C.(Ed.).(1992).EncyclopediaofArtifi-
(2007). Checkersissolved. Science,317,1518– Sebastiani, F.(2002). Machinelearninginauto- cialIntelligence(secondedition).Wiley.
1522. matedtextcategorization.ACMComputingSurveys,
34(1),1–47. Shapley,S.(1953). Stochasticgames. InPNAS,
Schank,R.C.andAbelson,R.P.(1977). Scripts, Vol.39,pp.1095–1100.
Plans,Goals, andUnderstanding. LawrenceErl- Segaran, T.(2007). ProgrammingCollectiveIn-
baumAssociates. telligence: Building Smart Web 2.0 Applications. Shatkay,H.andKaelbling,L.P.(1997). Learning
O’Reilly. topologicalmapswithweaklocalodometricinfor-
Schank,R.C.andRiesbeck,C.(1981).InsideCom- mation.InIJCAI-97.
puterUnderstanding: FiveProgramsPlusMinia- Selman,B.,Kautz,H.,andCohen,B.(1996). Lo-
tures.LawrenceErlbaumAssociates. calsearchstrategiesforsatisfiabilitytesting. InDI- Shelley,M.(1818). Frankenstein: Or,theModern
MACS Series in Discrete Mathematics and Theo- Prometheus.PickeringandChatto.
S b ch o c i o h n s a e t p i L n ir g e e a - , b r R a n s i . n e E d g . , s a 3 y n 9 s d ( t 2 e S m / i 3 n ) g f , o e 1 r r 3 , t 5 Y e – x . 1 ( t 2 6 c 0 8 a 0 . te 0 g ). o B riz o a o t s i t o e n x . te M r: a A - r A e m tic e a ri l c C an om M p a u t t h e e r m S a c t i i e c n a c l e S , o V c o ie lu ty m . e26,pp.521–532. S sc h r e a p b p bl a e r . d A , I B J, . 1 (2 3 0 4 0 (1 2 – ). 2) W ,2 o 4 r 1 ld – - 2 c 7 h 5 a . mpionship-caliber
S ab c i h li a t p y. ir M e, a R ch . i E n . e ( L 19 ea 9 r 0 n ). in T g h , e 5( s 2 tr ) e , n 1 g 9 t 7 h – o 2 f 27 w . eaklearn- p 6 S l 2 e e ( l x m 2 i ) t a , y n 3 , 0 o 3 f B – p . 3 a a 3 t n h 9 d - . b L a e se v d esq d u ef e e , a H si . b J le .( i 1 n 9 h 9 e 3 ri ) t . an T c h e e . c A om IJ - , S im h a i, ge J. s a e n g d m M en a ta li t k io , n J . . P (2 A 0 M 0 I 0 , ) 2 . 2 N (8 o ), rm 88 a 8 li – z 9 e 0 d 5 c . utsand
S m c a h c a h p in ir e e l , e R ar . n E in . g ( : 2 A 00 n 3 o ). ve T rv h i e ew b . oo In sti D n e g n a is p o p n r , oa D c . h D t . o , Selman, B., Levesque, H. J., and Mitchell, D. S T h es ie t. b C er A , C S M .( , 1 3 9 7 9 , 4 7 ). 0 L –7 es 8 s . onsfromarestrictedTuring
(1992).Anewmethodforsolvinghardsatisfiability
Hansen,M.H., Holmes,C., Mallick,B.,andYu, problems.InAAAI-92,pp.440–446. Shieber,S.(Ed.).(2004). TheTuringTest. MIT
B.(Eds.),NonlinearEstimationandClassification. Press.
Springer. Sha,F.andPereira,F.(2003).Shallowparsingwith
conditionalrandomfields. TechnicalreportCISTR Shoham,Y.(1993). Agent-orientedprogramming.
Schmid,C.andMohr,R.(1996). Combininggrey- MS-CIS-02-35,Univ.ofPenn. AIJ,60(1),51–92.
value invariants with local constraints for object
recognition.InCVPR. Shachter,R.D.(1986). Evaluatinginfluencedia- Shoham, Y. (1994). Artificial Intelligence Tech-
grams.OperationsResearch,34,871–882. niquesinProlog.MorganKaufmann.
Schmolze,J.G.andLipkis,T.A.(1983). Classi-
ficationintheKL-ONErepresentationsystem. In Shachter,R.D.(1998). Bayes-ball: Therational Shoham, Y.andLeyton-Brown, K.(2009). Mul-
IJCAI-83,pp.330–332. pastime (for determining irrelevance andrequisite tiagentSystems:Algorithmic,Game-Theoretic,and
information in belief networks and influence dia- LogicalFoundations.CambridgeUniv.Press.
Scho¨lkopf,B.andSmola,A.J.(2002). Learning
withKernels.MITPress. grams).InUAI-98,pp.480–487. Shoham,Y.,Powers,R.,andGrenager,T.(2004).If
S S c A h T o¨ a n n in d g c , o T n . s ( t 1 ra 9 i 9 n 9 t ) s . a A tis p f r a o c b ti a o b n il p is r t o ic bl a e l m go s r . it I h n m FO fo C rk S - - S B be h . l a i A e c f . h n t ( e 1 e r t 9 w , 9 R o 0 r ) . k . D s. S ., I y n D m A ’ b A A o m A li I c b - r 9 o p 0 s r , o io p b , p a . b B i 1 . l , 2 is a 6 t n i – c d 13 i D n 1 f e . e l re F n a c v e er i o n , m t M io u u n l l ? t t i i - - a A I g n g e e n P n t t r l o L e c a e . r a n A r i n A n i g A ng I is . F t a h l e l a S n y s m w p e o r, si w u h m at o i n st A h r e ti q fi u c e ia s- l
99,pp.410–414.
S tiv ch e o ro p b p o e t r s s i , n M u . np J. re ( d 1 i 9 c 8 ta 7 b ). le U en n v iv ir e o rs n a m l e p n la ts n . s In fo I r J r C e A ac I - - S in h fl a u c e h n t c e e r, d R ia . g D ra . m an s d . K M en a le n y a , g C em .R en . t (1 S 9 c 8 i 9 en ). c G e, au 3 s 5 s ( i 5 a ) n , S C h o o n r s t u l l i t f a fe ti , o E ns . : H M . Y (1 C 9 I 7 N 6 . ). El C se o v m ie p r u /N te o r- r B th a - s H e o d ll M an e d d . ical
87,pp.1039–1046. 527–550. Sietsma,J.andDow,R.J.F.(1988). Neuralnet
S pl c a h n o s p a p s e c r a s c , h M es . .A J. IM (1 a 9 g 8 , 9 1 ). 0(4 I ) n ,5 d 1 e – f 6 e 0 n . seofreaction S pr h o a a c c h h t e e s r t , o R g . e D n . e a ra n l d p P r e o o b t a , b M ili . s ( t 1 ic 98 in 9 f ) e . r S e i n m ce ul o a n tio b n el a ie p f - p fe r r u e n n i c n e g— on W N h e y ur a a n l d N h e o tw w. o I r n ks I , E p E p E .3 I 2 n 5 te – r 3 n 3 a 3 ti . onalCon-
networks.InUAI-98. Siklossy, L.andDreussi, J.(1973). Anefficient
S L c o h gi r k o¨ k d a e lk r u , ¨ls E . . B ( . 1 G 87 . 7 T ) e . ubn D er e , r Le O ip p z e i r g a . tionskreis des S in h g ac b h a t c e k r w , a R r . d D f . o a r n k d n H ow ec le k d e g rm e a a n c , q D ui . s ( it 1 i 9 o 8 n 7 . ). A T I h M in a k g - , r I o n b I o J t C p A l I a - n 7 n 3 e , r pp w . h 4 i 2 ch 3– g 4 e 3 n 0 e . ratesitsownprocedures.
Schultz,W.,Dayan,P.,andMontague,P.R.(1997). 3(Fall). Silverstein, C., Henzinger, M., Marais, H., and
A 27 n 5 e , u 1 r 5 a 9 l 3 s . ubstrateofpredictionandreward.Science, S de h n a c f e e . r, P G rin . c (1 et 9 o 7 n 6) U . n A ive M rs a it t y he P m re a s ti s c . alTheoryofEvi- M qu o e r r i y cz lo , g M . . T ( e 1 c 9 h 9 . 8 r ) e . p A .1 n 9 a 9 ly 8 s - i 0 s 1 o 4 f , a D v ig er it y al la S rg y e st a e l m ta s v R is e ta -
Schulz, D., Burgard, W., Fox, D., and Cremers, searchCenter.
A us . in B g . s ( a 2 m 00 p 3 le ). -ba P s e e o d p j l o e in tr t a p c r k o in b g ab w ili i s th tic m d o a b ta ile as r s o o b c o ia ts - S pl h a a c h em oo e k n a t r t , e K ch . n a i n q d ue M s. azu C m om de p r u , t P i . n ( g 19 S 9 u 1 r ) v . ey V s L , S 2 I 3( c 2 e ) l , l S ti i c m ro m b o o n t s n , av R i . ga a t n io d n K in oe p n a i r g t , ia S ll . y ( o 1 b 9 s 9 e 5 r ) v . abl P e ro e b n a v b ir i o li n s - -
tionfilters.Int.J.RoboticsResearch,22(2),99–116. 143–220. ments.InIJCAI-95,pp.1080–1087.IJCAI,Inc.
Schulz,S.(2004). SystemDescription:E0.81. In Shanahan,M.(1997). SolvingtheFrameProblem.
Simon, D. (2006). Optimal State Estimation:
Proc.InternationalJointConferenceonAutomated MITPress.
Kalman,HInfinity,andNonlinearApproaches.Wi-
Reasoning,Vol.3097ofLNAI,pp.223–228. Shanahan,M.(1999).Theeventcalculusexplained. ley.
Schu¨tze,H.(1995). AmbiguityinLanguageLearn- In Wooldridge, M. J. and Veloso, M. (Eds.), Ar-
Simon, H. A. (1947). Administrative behavior.
ing: ComputationalandCognitiveModels. Ph.D. tificial Intelligence Today, pp.409–430. Springer-
Macmillan.
thesis,StanfordUniversity.AlsopublishedbyCSLI Verlag.
Press,1997. Simon,H.A.(1957). ModelsofMan: Socialand
Shankar, N.(1986). Proof-CheckingMetamathe-
Rational.JohnWiley.
Schwartz,J.T.,Scharir,M.,andHopcroft,J.(1987). matics.Ph.D.thesis,ComputerScienceDepartment,
Planning,GeometryandComplexityofRobotMo- UniversityofTexasatAustin. Simon,H.A.(1963). Experimentswithaheuristic
tion.AblexPublishingCorporation. compiler.JACM,10,493–506.
Shannon,C.E.andWeaver,W.(1949). TheMath-
Schwartz,S.P.(Ed.).(1977). Naming,Necessity, ematicalTheoryofCommunication. Universityof Simon,H.A.(1981). TheSciencesoftheArtificial
andNaturalKinds.CornellUniversityPress. IllinoisPress. (secondedition).MITPress.
Bibliography 1089
Simon,H.A.(1982).ModelsofBoundedRational- Smith,D.E.andWeld,D.S.(1998). Conformant Srivas,M.andBickford,M.(1990). Formalveri-
ity,Volume1.TheMITPress. Graphplan.InAAAI-98,pp.889–896. ficationofapipelinedmicroprocessor. IEEESoft-
ware,7(5),52–64.
Simon, H. A. and Newell, A. (1958). Heuristic Smith,J.Q.(1988). DecisionAnalysis. Chapman
problemsolving:Thenextadvanceinoperationsre- andHall. Staab, S. (2004). Handbook on Ontologies.
search.OperationsResearch,6,1–10. Springer.
Smith,J.E.andWinkler,R.L.(2006). Theopti-
Simon, H.A.andNewell, A.(1961). Computer mizer’scurse:Skepticismandpostdecisionsurprise Stallman,R.M.andSussman,G.J.(1977).Forward
simulationofhumanthinkingandproblemsolving. indecisionanalysis. ManagementScience,52(3), reasoninganddependency-directedbacktrackingin
Datamation,June/July,35–37. 311–322. asystemforcomputer-aidedcircuitanalysis. AIJ,
9(2),135–196.
Simon, J.C.andDubois, O.(1989). Numberof Smith,J.M.(1982). EvolutionandtheTheoryof
solutionstosatisfiabilityinstances—Applicationsto Games.CambridgeUniversityPress. Stanfill,C.andWaltz,D.(1986). Towardmemory-
knowledgebases.AIJ,3,53–65. basedreasoning.CACM,29(12),1213–1228.
Smith, J.M.andSzathma´ry,E.(1999). TheOri-
Simonis,H.(2005). Sudokuasaconstraintprob- ginsofLife:FromtheBirthofLifetotheOriginof Stefik,M.(1995). IntroductiontoKnowledgeSys-
lem.InCPWorkshoponModelingandReformulat- Language.OxfordUniversityPress. tems.MorganKaufmann.
ingConstraintSatisfactionProblems,pp.13–27. Smith, M. K., Welty, C., and McGuinness, D. Stein, L.A.(2002). InteractiveProgrammingin
Singer,P.W.(2009).WiredforWar.PenguinPress. (2004). OWLwebontologylanguageguide. Tech. Java(pre-publicationdraft).MorganKaufmann.
Singh,P.,Lin,T.,Mueller,E.T.,Lim,G.,Perkins, rep.,W3C. Stephenson,T.,Bourlard,H.,Bengio,S.,andMor-
T., andZhu, W.L.(2002). Openmindcommon Smith,R.C.andCheeseman,P.(1986).Ontherep- ris,A.(2000). Automaticspeechrecognitionusing
sense:Knowledgeacquisitionfromthegeneralpub- resentationandestimationofspatialuncertainty.Int. dynamicbayesiannetworkswithbothacousticand
lic. InProc.FirstInternationalConferenceonOn- J.RoboticsResearch,5(4),56–68. articulatoryfeatures.InICSLP-00,pp.951–954.
tologies,Databases,andApplicationsofSemantics Smith,S.J.J.,Nau,D.S.,andThroop,T.A.(1998). Stergiou,K.andWalsh,T.(1999). Thedifference
forLargeScaleInformationSystems. Successinspades:UsingAIplanningtechniquesto all-differencemakes.InIJCAI-99,pp.414–419.
Singhal,A.,Buckley,C.,andMitra,M.(1996).Piv- wintheworldchampionshipofcomputerbridge.In Stickel,M.E.(1992).Aprologtechnologytheorem
oteddocumentlengthnormalization. InSIGIR-96, AAAI-98,pp.1079–1086. prover:anewexpositionandimplementationinpro-
pp.21–29. Smolensky,P.(1988). Onthepropertreatmentof log.TheoreticalComputerScience,104,109–128.
Sittler,R.W.(1964). Anoptimaldataassociation connectionism.BBS,2,1–74. Stiller,L.(1992).KQNKRR.J.InternationalCom-
probleminsurveillancetheory. IEEETransactions
Smullyan,R.M.(1995).First-OrderLogic.Dover.
puterChessAssociation,15(1),16–18.
onMilitaryElectronics,8(2),125–139.
Stiller, L.(1996). Multilinear algebra andchess
Smyth, P., Heckerman, D., and Jordan, M. I.
Skinner,B.F.(1953). ScienceandHumanBehav- endgames. InNowakowski,R.J.(Ed.),Gamesof
(1997).Probabilisticindependencenetworksforhid-
ior.Macmillan. NoChance,MSRI,29,1996.MathematicalSciences
denMarkovprobabilitymodels. NeuralComputa-
ResearchInstitute.
Skolem,T.(1920).Logisch-kombinatorischeUnter- tion,9(2),227–269.
suchungenu¨berdieErfu¨llbarkeitoderBeweisbarkeit Stockman,G.(1979). Aminimaxalgorithmbetter
Snell,M.B.(2008). Doyouhavefreewill? John
mathematischerSa¨tzenebsteinemTheoremeu¨ber thanalpha–beta?AIJ,12(2),179–196.
Searlereflectsonvariousphilosophicalquestionsin
diedichteMengen.Videnskapsselskapetsskrifter,I. lightofnewresearchonthebrain.CaliforniaAlumni Stoffel,K.,Taylor,M.,andHendler,J.(1997).Effi-
Matematisk-naturvidenskabeligklasse,4. Magazine,March/April. cientmanagementofverylargeontologies.InProc.
Skolem,T.(1928). U¨berdiemathematischeLogik. AAAI-97,pp.442–447.
Soderland,S.andWeld,D.S.(1991). Evaluating
Norskmatematisktidsskrift,10,125–142. nonlinearplanning. TechnicalreportTR-91-02-03, Stolcke,A.andOmohundro, S.(1994). Inducing
Slagle,J.R.(1963).Aheuristicprogramthatsolves UniversityofWashingtonDepartmentofComputer probabilisticgrammarsbyBayesianmodelmerging.
symbolicintegrationproblemsinfreshmancalculus. ScienceandEngineering. InProc.SecondInternationalColloquiumonGram-
JACM,10(4). maticalInferenceandApplications(ICGI-94), pp.
Solomonoff, R.J.(1964). Aformaltheoryofin- 106–118.
Slate,D.J.andAtkin,L.R.(1977). CHESS4.5— ductiveinference.InformationandControl,7,1–22,
NorthwesternUniversitychessprogram. InFrey, 224–254. Stone,M.(1974). Cross-validatorychoiceandas-
P.W.(Ed.),ChessSkillinManandMachine, pp. sessmentofstatosticalpredictions.J.RoyalStatisti-
82–118.Springer-Verlag. Solomonoff,R.J.(2009). Algorithmicprobability– calSociety,36(111–133).
theoryandapplications. InEmmert-Streib,F.and
Slater,E.(1950). Statisticsforthechesscomputer Dehmer,M.(Eds.),InformationTheoryandStatiti- Stone,P.(2000). LayeredLearninginMulti-Agent
andthefactorofmobility. InSymposiumonInfor- calLearning.Springer. Systems: AWinningApproachtoRoboticSoccer.
mationTheory,pp.150–152.MinistryofSupply. MITPress.
Sondik,E.J.(1971). TheOptimalControlofPar-
Sleator,D.andTemperley,D.(1993). ParsingEn- tiallyObservableMarkovDecisionProcesses.Ph.D. Stone,P.(2003). Multiagentcompetitionsandre-
glishwithalinkgrammar. InThirdAnnualWork- thesis,StanfordUniversity. search:LessonsfromRoboCupandTAC. InLima,
shoponParsingtechnologies. P.U.andRojas,P.(Eds.),RoboCup-2002: Robot
Sosic,R.andGu,J.(1994). Efficientlocalsearch SoccerWorldCupVI,pp.224–237.SpringerVerlag.
Slocum,J.andSonneveld,D.(2006).The15Puzzle. withconflictminimization: Acasestudyofthen-
SlocumPuzzleFoundation. queensproblem. IEEETransactionsonKnowledge Stone, P., Kaminka, G., and Rosenschein, J. S.
andDataEngineering,6(5),661–668. (2009). Leadingabest-responseteammateinanad
Sloman, A.(1978). TheComputerRevolutionin hocteam. InAAMASWorkshopinAgentMediated
Philosophy.HarvesterPress. Sowa,J.(1999). KnowledgeRepresentation:Logi- ElectronicCommerce.
cal,Philosophical,andComputationalFoundations.
Smallwood, R.D.andSondik,E.J.(1973). The Blackwell. Stork,D.G.(2004). Opticsandrealisminrennais-
optimalcontrolofpartiallyobservableMarkovpro- sanceart.ScientificAmerican,pp.77–83.
cessesoverafinitehorizon. OperationsResearch, Spaan, M.T.J.andVlassis,N.(2005). Perseus:
21,1071–1088. Randomized point-based value iteration for Strachey,C.(1952). Logicalornon-mathematical
POMDPs.JAIR,24,195–220. programmes. InProc.1952ACMnationalmeeting
Smart,J.J.C.(1959). Sensationsandbrainpro- (Toronto),pp.46–49.
cesses.PhilosophicalReview,68,141–156. Spiegelhalter,D.J.,Dawid,A.P.,Lauritzen,S.,and
Stratonovich,R.L.(1959).Optimumnonlinearsys-
Cowell,R.(1993). Bayesiananalysisinexpertsys-
Smith,B.(2004). Ontology. InFloridi,L.(Ed.), temswhichbringaboutaseparationofasignalwith
tems.StatisticalScience,8,219–282.
TheBlackwellGuidetothePhilosophyofComput- constantparametersfromnoise. Radiofizika,2(6),
ingandInformation,pp.155–166.Wiley-Blackwell. Spielberg,S.(2001).AI.Movie. 892–901.
Smith, D. E., Genesereth, M. R., and Ginsberg, Spirtes,P.,Glymour,C.,andScheines,R.(1993). Stratonovich,R.L.(1965). Onvalueofinforma-
M.L.(1986). Controllingrecursiveinference. AIJ, Causation,prediction,andsearch.Springer-Verlag. tion. IzvestiyaofUSSRAcademyofSciences,Tech-
30(3),343–389. Srinivasan,A.,Muggleton,S.H.,King,R.D.,and nicalCybernetics,5,3–12.
Smith, D.A.andEisner,J.(2008). Dependency Sternberg,M.J.E.(1994).Mutagenesis:ILPexper- Subramanian,D.andFeldman,R.(1990).Theutil-
parsingbybeliefpropagation.InEMNLP,pp.145– imentsinanon-determinatebiologicaldomain. In ityofEBLinrecursivedomaintheories.InAAAI-90,
156. ILP-94,Vol.237,pp.217–232. Vol.2,pp.942–949.
1090 Bibliography
Subramanian,D.andWang,E.(1994).Constraint- Tarski,A.(1941). IntroductiontoLogicandtothe Thielscher,M.(1999). Fromsituationcalculusto
basedkinematicsynthesis. InProc.International MethodologyofDeductiveSciences.Dover. fluentcalculus:Stateupdateaxiomsasasolutionto
ConferenceonQualitativeReasoning,pp.228–239. theinferentialframeproblem. AIJ,111(1–2),277–
Tarski,A.(1956). Logic,Semantics,Metamathe- 299.
Sussman,G.J.(1975). AComputerModelofSkill matics:Papersfrom1923to1938. OxfordUniver-
Acquisition.Elsevier/North-Holland. sityPress. Thompson,K.(1986). Retrogradeanalysisofcer-
tainendgames.J.InternationalComputerChessAs-
Sutcliffe,G.andSuttner,C.(1998).TheTPTPProb- Tash,J.K.andRussell,S.J.(1994).Controlstrate- sociation,May,131–139.
lemLibrary:CNFReleasev1.2.1.JAR,21(2),177– giesforastochasticplanner.InAAAI-94,pp.1079–
203. 1085. Thompson,K.(1996). 6-pieceendgames.J.Inter-
nationalComputerChessAssociation,19(4),215–
Sutcliffe,G.,Schulz,S.,Claessen,K.,andGelder, Taskar,B.,Abbeel,P.,andKoller,D.(2002). Dis- 226.
A.V.(2006). UsingtheTPTPlanguageforwriting
criminativeprobabilisticmodelsforrelationaldata.
derivationsandfiniteinterpretations.InProc.Inter- InUAI-02. Thrun,S.,Burgard,W.,andFox,D.(2005).Proba-
nationalJointConferenceonAutomatedReasoning, bilisticRobotics.MITPress.
pp.67–81. Tate,A.(1975a).Interactinggoalsandtheiruse.In Thrun,S.,Fox,D.,andBurgard,W.(1998).Aprob-
Sutherland,I.(1963). Sketchpad:Aman-machine IJCAI-75,pp.215–218. abilisticapproachtoconcurrentmappingandlocal-
graphicalcommunicationsystem. InProc.Spring Tate,A.(1975b). UsingGoalStructuretoDirect izationformobilerobots. MachineLearning,31,
JointComputerConference,pp.329–346. SearchinaProblemSolver.Ph.D.thesis,University 29–53.
Sutton,C.andMcCallum,A.(2007). Anintroduc- ofEdinburgh. Thrun,S.(2006). Stanley,therobotthatwonthe
tiontoconditionalrandomfieldsforrelationallearn- Tate,A.(1977). Generatingprojectnetworks. In DARPAGrandChallenge. J.FieldRobotics,23(9),
ing.InGetoor,L.andTaskar,B.(Eds.),Introduction IJCAI-77,pp.888–893. 661–692.
toStatisticalRelationalLearning.MITPress. Tikhonov, A.N. (1963). Solution ofincorrectly
Tate,A.andWhiter,A.M.(1984). Planningwith
Sutton, R.S.(1988). Learningtopredictbythe formulatedproblemsandtheregularizationmethod.
multipleresourceconstraintsandanapplicationtoa
methodsoftemporaldifferences. MachineLearn- SovietMath.Dokl.,5,1035–1038.
navalplanningproblem. InProc.FirstConference
ing,3,9–44. onAIApplications,pp.410–416. Titterington,D.M.,Smith,A.F.M.,andMakov,
Sutton,R.S.,McAllester,D.A.,Singh,S.P.,and U.E.(1985). Statisticalanalysisoffinitemixture
Mansour,Y.(2000).Policygradientmethodsforre- Tatman,J.A.andShachter,R.D.(1990).Dynamic distributions.Wiley.
programmingandinfluencediagrams. IEEETrans-
inforcementlearningwithfunctionapproximation. actions on Systems, Man and Cybernetics, 20(2), Toffler,A.(1970).FutureShock.Bantam.
InSolla,S.A.,Leen,T.K.,andMu¨ller,K.-R.(Eds.),
NIPS12,pp.1057–1063.MITPress. 365–379. Tomasi,C.andKanade,T.(1992). Shapeandmo-
S le u a t r t n o i n ng , , R p . la S n . n ( i 1 n 9 g 9 , 0 a ) n . d I r n e t a e c g t r i a n t g ed ba a s rc e h d it o e n ctu a r p e p s ro f x o - r C Ta o t l t le e c r t s i a o l n l, o C f . C (1 h 9 e 1 ss 1) P . os A iti T o h n o s u T sa h n a d tC E a n n d- b G e a W me o s n : o A r t to io r n iza fr t o io m nm im e a th g o e d s . tr I e J a C m V s , u 9 n , d 1 e 3 r 7 o – r 1 t 5 h 4 o . graphy:Afac-
imatingdynamic programming. In ICML-90, pp. DrawnbytheBestPlay.BritishChessMagazine. Torralba, A., Fergus, R., and Weiss, Y. (2008).
216–224. Taylor,G.,Stensrud,B.,Eitelman,S.,andDunham, Smallcodesandlargeimagedatabasesforrecogni-
tion.InCVPR,pp.1–8.
Sutton,R.S.andBarto,A.G.(1998). Reinforce- C.(2007). Towardsautomatingairspacemanage-
mentLearning:AnIntroduction.MITPress. ment.InProc.ComputationalIntelligenceforSecu- Trucco,E.andVerri,A.(1998). IntroductoryTech-
rityandDefenseApplications(CISDA)Conference, niquesfor3-DComputerVision.PrenticeHall.
S le v a o rn re in , g K a . pp a r n o d ach Bu fo rg r e i s m , p C ro . ve (2 d 0 b 0 m 9) 2 . 5re A trie m v a a c l. hin In e pp.1–5. Tsitsiklis,J.N.andVanRoy,B.(1997). Ananaly-
Proc.ConferenceonInformationKnowledgeMan- Tenenbaum,J.,Griffiths,T.,andNiyogi,S.(2007). sisoftemporal-differencelearningwithfunctionap-
agement. Intuitivetheoriesasgrammarsforcausalinference. proximation.IEEETransactionsonAutomaticCon-
InGopnik,A.andSchulz,L.(Eds.),Causallearn- trol,42(5),674–690.
Swade,D.(2000).DifferenceEngine:CharlesBab-
bageAndTheQuestToBuildTheFirstComputer. ing:Psychology,Philosophy,andComputation.Ox- Tumer,K.andWolpert,D.(2000).Collectiveintel-
DianePublishingCo. fordUniversityPress. ligenceandbraess’paradox. InAAAI-00,pp.104–
Tesauro,G.(1992).Practicalissuesintemporaldif- 109.
Swerling,P.(1959).Firstordererrorpropagationin
astagewisesmoothingprocedureforsatelliteobser- ferencelearning. MachineLearning,8(3–4),257– Turcotte,M.,Muggleton,S.H.,andSternberg,M.
vations.J.AstronauticalSciences,6,46–52. 277. J.E.(2001). Automateddiscoveryofstructuralsig-
naturesofproteinfoldandfunction. J.Molecular
Swift,T.andWarren,D.S.(1994).AnalysisofSLG- Tesauro,G.(1995). Temporaldifferencelearning Biology,306,591–605.
WAMevaluationofdefiniteprograms.InLogicPro- andTD-Gammon.CACM,38(3),58–68.
g L r o a g m ic m p i r n o g g . r P am ro m c. in 1 g 9 , 9 p 4 p I . n 2 t 1 e 9 rn – a 2 t 3 io 5 n . alSymposiumon T ne e t s w au or r k o, th G at . l a e n a d rns Se to jn p o l w ay sk b i a , c T k . ga (1 m 9 m 89 o ) n . .A A IJ p , a 3 r 9 a ( l 3 le ) l , T a L n o u n r a d i p n o p g n l , ic M A at a . io t ( h 1 n e 9 m t 3 o a 6 t t ) h i . c e a O l E n S n o t c s c o c i h e m t e y p i , d u 2 u ta n n b d g l s e s p e n r r o u i b e m l s e , b m e 4 r 2 . s , , P 2 w r 3 o i 0 t c h – .
Syrja¨nen, T. (2000). Lparse 1.0 user’s manual. 357–390. 265.
saturn.tcs.hut.fi/Software/smodels.
Teyssier,M.andKoller,D.(2005).Ordering-based Turing,A.(1948).Intelligentmachinery.Tech.rep.,
Tadepalli,P.(1993).Learningfromqueriesandex- search:Asimpleandeffectivealgorithmforlearning NationalPhysical Laboratory. reprinted in (Ince,
ampleswithtree-structuredbias. InICML-93,pp. Bayesiannetworks.InUAI-05,pp.584–590. 1992).
322–329.
Thaler,R.(1992). TheWinner’sCurse:Paradoxes Turing,A.(1950).Computingmachineryandintel-
Tadepalli,P.,Givan,R.,andDriessens,K.(2004). andAnomaliesofEconomicLife.PrincetonUniver- ligence.Mind,59,433–460.
Relationalreinforcementlearning:Anoverview. In sityPress.
Turing,A.,Strachey,C.,Bates,M.A.,andBowden,
ICML-04.
Thaler,R.andSunstein,C.(2009).Nudge:Improv- B.V.(1953).Digitalcomputersappliedtogames.In
Tait,P.G.(1880). Noteonthetheoryofthe“15 ingDecisionsAboutHealth,Wealth,andHappiness. Bowden,B.V.(Ed.),FasterthanThought,pp.286–
puzzle”.Proc.RoyalSocietyofEdinburgh,10,664– Penguin. 310.Pitman.
665.
Theocharous,G.,Murphy,K.,andKaelbling,L.P. Tversky, A. and Kahneman, D. (1982). Causal
Tamaki,H.andSato,T.(1986). OLDresolution (2004). Representing hierarchical POMDPs as schematainjudgementsunderuncertainty. InKah-
withtabulation.InICLP-86,pp.84–98. DBNsformulti-scalerobotlocalization. InICRA- neman,D.,Slovic,P.,andTversky,A.(Eds.),Judge-
Tarjan,R.E.(1983). DataStructuresandNetwork 04. ment Under Uncertainty: Heuristics and Biases.
CambridgeUniversityPress.
Algorithms. CBMS-NSFRegionalConferenceSe-
Thiele, T. (1880). Om anvendelse af mindste
riesinAppliedMathematics.SIAM(SocietyforIn- Ullman, J.D.(1985). Implementation oflogical
kvadratersmethodeinogletilfælde,hvorenkom-
dustrialandAppliedMathematics). querylanguagesfordatabases. ACMTransactions
plikationafvisseslagsuensartedetilfældigefejlk-
onDatabaseSystems,10(3),289–321.
Tarski,A.(1935). DieWahrheitsbegriffindenfor- ildergiverfejleneen‘systematisk’karakter.Vidensk.
malisiertenSprachen. StudiaPhilosophica,1,261– Selsk.Skr.5.Rk.,naturvid.ogmat.Afd.,12,381– Ullman,S.(1979).TheInterpretationofVisualMo-
405. 408. tion.MITPress.
Bibliography 1091
Urmson,C.andWhittaker,W.(2008). Self-driving Veloso,M.andCarbonell,J.G.(1993).Derivational Warren,D.H.D.(1983). AnabstractPrologin-
carsandtheUrbanChallenge.IEEEIntelligentSys- analogyinPRODIGY: Automatingcaseacquisition, structionset.Technicalnote309,SRIInternational.
tems,23(2),66–68. storage,andutilization.MachineLearning,10,249–
Warren,D.H.D.,Pereira,L.M.,andPereira,F.
278.
Valiant, L. (1984). A theory of the learnable. (1977). PROLOG: The language and its imple-
CACM,27,1134–1142. Vere,S.A.(1983). Planningintime:Windowsand mentationcomparedwithLISP. SIGPLANNotices,
durationsforactivitiesandgoals.PAMI,5,246–267. 12(8),109–115.
van Beek, P. (2006). Backtracking search algo-
rithms. InRossi,F., vanBeek,P.,andWalsh, T. Verma,V.,Gordon,G.,Simmons,R.,andThrun, Wasserman,L.(2004).AllofStatistics.Springer.
(Eds.),HandbookofConstraintProgramming.Else- S.(2004). Particlefiltersforroverfaultdiagnosis. Watkins, C.J.(1989). ModelsofDelayedRein-
vier. IEEERoboticsandAutomationMagazine,June. forcementLearning. Ph.D.thesis,PsychologyDe-
vanBeek,P.andChen,X.(1999). CPlan: Acon- Vinge, V.(1993). Thecomingtechnologicalsin- partment,CambridgeUniversity.
straintprogrammingapproachtoplanning.InAAAI- gularity: How to survive in the post-human era. Watson,J.D.andCrick,F.H.C.(1953).Astructure
99,pp.585–590. InVISION-21Symposium.NASALewisResearch fordeoxyribosenucleicacid.Nature,171,737.
CenterandtheOhioAerospaceInstitute.
vanBeek,P.andManchak,D.(1996). Thedesign Waugh, K., Schnizlein, D., Bowling, M., and
andexperimentalanalysisofalgorithmsfortemporal Viola,P.andJones,M.(2002a).Fastandrobustclas- Szafron,D.(2009). Abstractionpathologiesinex-
reasoning.JAIR,4,1–18. sificationusingasymmetricadaboostandadetector tensivegames.InAAMAS-09.
cascade.InNIPS14.
vanBentham,J.andterMeulen,A.(1997). Hand- Weaver,W.(1949). Translation. InLocke,W.N.
bookofLogicandLanguage.MITPress. Viola,P.andJones,M.(2002b). Robustreal-time andBooth,D.(Eds.),Machinetranslationoflan-
objectdetection.ICCV. guages:fourteenessays,pp.15–23.Wiley.
VanEmden,M.H.andKowalski,R.(1976). The
semanticsofpredicatelogicasaprogramminglan- Visser,U.andBurkhard,H.-D.(2007). RoboCup Webber, B.L.andNilsson, N.J.(Eds.).(1981).
guage.JACM,23(4),733–742. 2006:achievementsandgoalsforthefuture.AIMag, ReadingsinArtificialIntelligence. MorganKauf-
28(2),115–130. mann.
van Harmelen, F. and Bundy, A. (1988).
Explanation-based generalisation = partial evalu- Visser,U.,Ribeiro,F.,Ohashi,T.,andDellaert,F. Weibull,J.(1995).EvolutionaryGameTheory.MIT
ation.AIJ,36(3),401–412. (Eds.).(2008).RoboCup2007:RobotSoccerWorld Press.
CupXI.Springer.
van Harmelen, F., Lifschitz, V., and Porter, B. Weidenbach,C.(2001).SPASS:Combiningsuper-
(2007). TheHandbookofKnowledgeRepresenta- Viterbi,A.J.(1967).Errorboundsforconvolutional position,sortsandsplitting. InRobinson,A.and
tion.Elsevier. codesandanasymptoticallyoptimumdecodingal- Voronkov,A.(Eds.),HandbookofAutomatedRea-
gorithm.IEEETransactionsonInformationTheory, soning.MITPress.
v to an Go¨ H d e e i l j : en A oo S r o t u , rc J e . B (E o d o . k ). in (1 M 96 a 7 t ) h . ema F ti r c o a m lL F o r g e i g c e , 13(2),260–269. Weiss,G.(2000a).Multiagentsystems.MITPress.
1879–1931.HarvardUniversityPress. Vlassis,N.(2008).AConciseIntroductiontoMulti- Weiss,Y.(2000b). Correctnessoflocalprobability
agentSystemsandDistributedArtificialIntelligence. propagationingraphicalmodelswithloops.Neural
VanHentenryck,P.,Saraswat,V.,andDeville,Y. MorganandClaypool. Computation,12(1),1–41.
(1998). Design,implementation,andevaluationof
theconstraintlanguagecc(FD). J.LogicProgram- vonMises,R.(1928). Wahrscheinlichkeit,Statistik Weiss, Y. and Freeman, W. (2001). Correctness
ming,37(1–3),139–164. undWahrheit.J.Springer. ofbeliefpropagationinGaussiangraphicalmodels
vanHoeve,W.-J.(2001).Thealldifferentconstraint: von Neumann, J. (1928). Zur Theorie der ofarbitrarytopology. NeuralComputation,13(10),
asurvey. In6thAnnualWorkshopoftheERCIM Gesellschaftsspiele. Mathematische Annalen, 2173–2200.
WorkingGrouponConstraints. 100(295–320). Weizenbaum,J.(1976). ComputerPowerandHu-
van Hoeve, W.-J.and Katriel, I. (2006). Global vonNeumann,J.andMorgenstern,O.(1944).The- manReason.W.H.Freeman.
constraints. InRossi,F.,vanBeek,P.,andWalsh, oryofGamesandEconomicBehavior(firstedition). Weld,D.S.(1994). Anintroductiontoleastcom-
T.(Eds.),HandbookofConstraintProcessing,pp. PrincetonUniversityPress. mitmentplanning.AIMag,15(4),27–61.
169–208.Elsevier. vonWinterfeldt,D.andEdwards,W.(1986).Deci- Weld,D.S.(1999).RecentadvancesinAIplanning.
van Lambalgen, M.andHamm, F.(2005). The sionAnalysisandBehavioralResearch.Cambridge AIMag,20(2),93–122.
ProperTreatmentofEvents.Wiley-Blackwell. UniversityPress. Weld, D. S., Anderson, C. R., andSmith, D. E.
vanNunen, J.A.E.E.(1976). Asetofsucces- Vossen, T., Ball, M., Lotem, A., andNau, D. S. (1998). Extendinggraphplantohandleuncertainty
siveapproximationmethodsfordiscountedMarko- (2001). ApplyingintegerprogrammingtoAIplan- andsensingactions.InAAAI-98,pp.897–904.
viandecisionproblems. ZeitschriftfurOperations ning.KnowledgeEngineeringReview,16,85–100. Weld,D.S.anddeKleer,J.(1990). Readingsin
Research,SerieA,20(5),203–208. Wainwright,M.J.andJordan,M.I.(2008).Graph- QualitativeReasoningaboutPhysicalSystems.Mor-
VanRoy,B.(1998). Learningandvaluefunction icalmodels,exponentialfamilies,andvariationalin- ganKaufmann.
approximationincomplexdecisionprocesses.Ph.D. ference.MachineLearning,1(1–2),1–305. Weld,D.S.andEtzioni,O.(1994). Thefirstlawof
thesis,LaboratoryforInformationandDecisionSys- Waldinger,R.(1975). Achievingseveralgoalssi- robotics:Acalltoarms.InAAAI-94.
tems,MIT. multaneously. In Elcock, E. W. and Michie, D. Wellman,M.P.(1985).Reasoningaboutpreference
Van Roy, P. L. (1990). Canlogic programming (Eds.), Machine Intelligence 8, pp. 94–138. Ellis models. TechnicalreportMIT/LCS/TR-340,Labo-
execute as fast as imperative programming? Re- Horwood. ratoryforComputerScience,MIT.
portUCB/CSD90/600,ComputerScienceDivision, Wallace,A.R.(1858).Onthetendencyofvarieties Wellman,M.P.(1988). FormulationofTradeoffs
UniversityofCalifornia,Berkeley,California. todepartindefinitelyfromtheoriginaltype. Proc. inPlanningunderUncertainty. Ph.D.thesis,Mas-
Vapnik,V.N.(1998). StatisticalLearningTheory. LinneanSocietyofLondon,3,53–62. sachusettsInstituteofTechnology.
Wiley. Waltz,D.(1975). Understandinglinedrawingsof Wellman,M.P.(1990a). Fundamentalconceptsof
Vapnik,V.N.andChervonenkis,A.Y.(1971). On sceneswithshadows. InWinston,P.H.(Ed.),The qualitativeprobabilisticnetworks. AIJ,44(3),257–
theuniformconvergenceofrelativefrequenciesof PsychologyofComputerVision.McGraw-Hill. 303.
eventstotheirprobabilities. TheoryofProbability Wang, Y.andGelly, S.(2007). Modificationsof Wellman,M.P.(1990b). TheSTRIPSassumption
andItsApplications,16,264–280. UCTandsequence-likesimulationsforMonte-Carlo forplanningunderuncertainty.InAAAI-90,pp.198–
Varian,H.R.(1995).Economicmechanismdesign Go. InIEEESymposiumonComputationalIntelli- 203.
genceandGames,pp.175–182.
forcomputerizedagents. InUSENIXWorkshopon Wellman, M.P.(1995). Theeconomicapproach
ElectronicCommerce,pp.13–21. Wanner,E.(1974).Onremembering,forgettingand toartificialintelligence. ACMComputingSurveys,
understandingsentences.Mouton. 27(3),360–362.
Vauquois,B.(1968).Asurveyofformalgrammars
andalgorithmsforrecognitionandtransformationin Warren,D.H.D.(1974). WARPLAN:ASystem Wellman, M. P., Breese, J.S., and Goldman, R.
mechanicaltranslation.InProc.IFIPCongress,pp. forGeneratingPlans.DepartmentofComputational (1992). Fromknowledgebasestodecisionmodels.
1114–1122. LogicMemo76,UniversityofEdinburgh. KnowledgeEngineeringReview,7(1),35–53.
1092 Bibliography
Wellman,M.P.andDoyle,J.(1992).Modularutil- Williams, R.J.andBaird,L.C.I.(1993). Tight Wos,L.,Carson,D.,andRobinson,G.(1965).Effi-
ityrepresentationfordecision-theoreticplanning.In performanceboundsongreedypoliciesbasedonim- ciencyandcompletenessoftheset-of-supportstrat-
ICAPS-92,pp.236–242. perfectvaluefunctions. Tech.rep.NU-CCS-93-14, egyintheoremproving.JACM,12,536–541.
CollegeofComputerScience,NortheasternUniver-
Wellman, M. P., Wurman, P., O’Malley, K., sity. Wos, L., Overbeek, R., Lusk, E., and Boyle, J.
Bangera, R., Lin, S., Reeves, D., and Walsh, W. (1992).AutomatedReasoning:IntroductionandAp-
(2001). Atradingagentcompetition. IEEEInter- Wilson, R. A. and Keil, F. C. (Eds.). (1999). plications(secondedition).McGraw-Hill.
netComputing. The MIT Encyclopediaofthe Cognitive Sciences. Wos,L.andRobinson,G.(1968). Paramodulation
MITPress.
Wells,H.G.(1898).TheWaroftheWorlds.William andsetofsupport.InProc.IRIASymposiumonAu-
Heinemann. Wilson,R.(2004). FourColorsSuffice. Princeton tomaticDemonstration,pp.276–310.
UniversityPress.
Werbos,P.(1974). BeyondRegression:NewTools Wos,L.,Robinson,G.,Carson,D.,andShalla,L.
forPredictionandAnalysisintheBehavioralSci- Winograd, S.andCowan,J.D.(1963). Reliable (1967). Theconceptofdemodulationintheorem
ences.Ph.D.thesis,HarvardUniversity. ComputationinthePresenceofNoise.MITPress. proving.JACM,14,698–704.
Winograd, T.(1972). Understandingnaturallan- Wos, L.and Winker, S. (1983). Openquestions
Werbos,P.(1977). Advancedforecastingmethods guage.CognitivePsychology,3(1),1–191. solvedwiththeassistanceofAURA. InAutomated
forglobalcrisiswarningandmodelsofintelligence.
GeneralSystemsYearbook,22,25–38. Winston,P.H.(1970). Learningstructuraldescrip- TheoremProving:After25Years:Proc.SpecialSes-
tionsfromexamples.TechnicalreportMAC-TR-76, sionofthe89thAnnualMeetingofthe American
Wesley, M.A.andLozano-Perez, T.(1979). An DepartmentofElectricalEngineeringandComputer MathematicalSociety,pp.71–88.AmericanMath-
algorithm for planningcollision-free paths among Science,MassachusettsInstituteofTechnology. ematicalSociety.
polyhedralobjects.CACM,22(10),560–570. Winston,P.H.(1992).ArtificialIntelligence(Third Wos, L. andPieper, G.(2003). AutomatedRea-
Wexler,Y.andMeek,C.(2009). MAS:Amulti- edition).Addison-Wesley. soningandthe DiscoveryofMissingandElegant
plicativeapproximationschemeforprobabilisticin- Proofs.RintonPress.
Wintermute, S., Xu, J., and Laird, J. (2007).
ference.InNIPS21. SORTS:Ahuman-levelapproachtoreal-timestrat- Wray,R.E.andJones,R.M.(2005). Anintro-
Whitehead,A.N.(1911).AnIntroductiontoMath- egyAI.InProc.ThirdArtificialIntelligenceandIn- ductiontoSoarasanagentarchitecture. InSun,R.
ematics.WilliamsandNorthgate. teractiveDigitalEntertainmentConference(AIIDE- (Ed.),CognitionandMulti-agentInteraction:From
07). CognitiveModelingtoSocialSimulation,pp.53–78.
Whitehead,A.N.andRussell,B.(1910).Principia CambridgeUniversityPress.
Mathematica.CambridgeUniversityPress. Witten, I. H. and Bell, T.C. (1991). The zero-
frequencyproblem: Estimatingtheprobabilitiesof Wright,S.(1921). Correlationandcausation. J.
Whorf,B.(1956).Language,Thought,andReality. novel eventsinadaptive textcompression. IEEE AgriculturalResearch,20,557–585.
MITPress. TransactionsonInformationTheory,37(4),1085– Wright,S.(1931). EvolutioninMendelianpopula-
Widrow,B.(1962).Generalizationandinformation 1094. tions.Genetics,16,97–159.
storageinnetworksofadaline“neurons”. InSelf- Witten,I.H.andFrank,E.(2005). DataMining: Wright,S.(1934).Themethodofpathcoefficients.
OrganizingSystems1962,pp.435–461. PracticalMachineLearningToolsandTechniques AnnalsofMathematicalStatistics,5,161–215.
(2ndedition).MorganKaufmann.
Widrow, B. and Hoff, M. E. (1960). Adaptive Wu,D.(1993). Estimatingprobabilitydistributions
switchingcircuits. In1960IREWESCONConven- Witten, I.H.,Moffat, A.,andBell, T.C.(1999). overhypotheseswithvariableunification.InIJCAI-
tionRecord,pp.96–104. Managing Gigabytes: Compressing and Indexing 93,pp.790–795.
DocumentsandImages(secondedition). Morgan
Wiedijk, F. (2003). Comparing mathematical Kaufmann. Wu,F.andWeld,D.S.(2008).Automaticallyrefin-
provers. InMathematicalKnowledgeManagement, ingthewikipediainfoboxontology. In17thWorld
Wittgenstein, L. (1922). Tractatus Logico-
pp.188–202. WideWebConference(WWW2008).
Philosophicus (second edition). Routledge and
Wiegley, J., Goldberg, K., Peshkin, M., and KeganPaul. Reprinted1971,editedbyD.F.Pears Yang,F.,Culberson,J.,Holte,R.,Zahavi,U.,and
Brokowski,M.(1996). Acompletealgorithmfor andB.F.McGuinness.ThiseditionoftheEnglish Felner,A.(2008).Ageneraltheoryofadditivestate
designingpassivefencestoorientparts. InICRA- translationalsocontainsWittgenstein’soriginalGer- spaceabstractions.JAIR,32,631–662.
96. mantextonfacingpages,aswellasBertrandRus-
Yang,Q.(1990). Formalizingplanningknowledge
sell’sintroductiontothe1922edition.
Wiener,N.(1942).Theextrapolation,interpolation, for hierarchical planning. Computational Intelli-
andsmoothingofstationarytimeseries. Osrd370, Wittgenstein, L.(1953). PhilosophicalInvestiga- gence,6,12–24.
ReporttotheServices19, ResearchProjectDIC- tions.Macmillan. Yarowsky,D.(1995).Unsupervisedwordsensedis-
6037,MIT. Wojciechowski,W.S.andWojcik,A.S.(1983).Au- ambiguationrivalingsupervisedmethods. InACL-
Wiener,N.(1948).Cybernetics.Wiley. tomateddesignofmultiple-valuedlogiccircuitsby 95,pp.189–196.
automatedtheoremprovingtechniques.IEEETrans-
Yedidia,J.,Freeman,W.,andWeiss,Y.(2005).Con-
Wilensky, R. (1978). Understanding goal-based actionsonComputers,C-32(9),785–798.
structing free-energy approximations and general-
stories.Ph.D.thesis,YaleUniversity.
Wolfe,J.andRussell,S.J.(2007).Exploitingbelief izedbeliefpropagationalgorithms. IEEETransac-
Wilensky,R.(1983).PlanningandUnderstanding. statestructureingraphsearch. InICAPSWorkshop tionsonInformationTheory,51(7),2282–2312.
Addison-Wesley. onPlanninginGames.
Yip,K.M.-K.(1991). KAM:ASystemforIntelli-
Wilkins,D.E.(1980). Usingpatternsandplansin Woods,W.A.(1973). Progressinnaturallanguage gentlyGuidingNumericalExperimentationbyCom-
chess.AIJ,14(2),165–203. understanding:Anapplicationtolunargeology. In puter.MITPress.
AFIPSConferenceProceedings,Vol.42,pp.441–
Wilkins,D.E.(1988).PracticalPlanning:Extend- 450. Yngve,V.(1955). Amodelandanhypothesisfor
ingtheAIPlanningParadigm.MorganKaufmann. languagestructure.InLocke,W.N.andBooth,A.D.
Woods,W.A.(1975). What’sinalink? Founda- (Eds.),MachineTranslationofLanguages,pp.208–
Wilkins,D.E.(1990). CanAIplannerssolveprac- tionsforsemanticnetworks. InBobrow,D.G.and 226.MITPress.
ticalproblems? ComputationalIntelligence, 6(4), Collins, A.M.(Eds.), RepresentationandUnder-
232–246. standing: StudiesinCognitiveScience,pp.35–82. Yob,G.(1975). Huntthewumpus! CreativeCom-
AcademicPress. puting,Sep/Oct.
Williams,B.,Ingham,M.,Chung,S.,andElliott,
P.(2003). Model-basedprogrammingofintelligent Wooldridge,M.(2002).AnIntroductiontoMultiA- Yoshikawa, T.(1990). Foundations ofRobotics:
embeddedsystemsandroboticspaceexplorers. In gentSystems.Wiley. AnalysisandControl.MITPress.
Proc.IEEE:SpecialIssueonModelingandDesign Wooldridge,M.andRao,A.(Eds.).(1999). Foun- Young,H.P.(2004).StrategicLearningandItsLim-
ofEmbeddedSoftware,pp.212–237. dationsofrationalagency.Kluwer. its.OxfordUniversityPress.
Williams,R.J.(1992). Simplestatisticalgradient- Wos,L.,Carson,D.,andRobinson,G.(1964). The Younger,D.H.(1967). Recognitionandparsingof
following algorithms for connectionist reinforce- unitpreferencestrategyintheoremproving.InProc. context-freelanguagesintimen3.Informationand
mentlearning.MachineLearning,8,229–256. FallJointComputerConference,pp.615–621. Control,10(2),189–208.
Bibliography 1093
Yudkowsky, E. (2008). Artificial intelligence as Zettlemoyer,L.S.andCollins,M.(2005).Learning Zhou, R. and Hansen, E. (2006). Breadth-first
a positive and negative factor in global risk. In tomapsentencestologicalform:Structuredclassi- heuristicsearch.AIJ,170(4–5),385–408.
Bostrom,N.andCirkovic,M.(Eds.),GlobalCatas- ficationwithprobabilisticcategorialgrammars. In
trophicRisk.OxfordUniversityPress. UAI-05. Zhu,D.J.andLatombe,J.-C.(1991). Newheuris-
tic algorithms for efficient hierarchical path plan-
Zadeh,L.A.(1965). Fuzzysets. Informationand Zhang,H.andStickel,M.E.(1996). Anefficient ning. IEEETransactionsonRoboticsandAutoma-
Control,8,338–353. algorithmforunit-propagation. InProc.FourthIn- tion,7(1),9–20.
Zadeh,L.A.(1978).Fuzzysetsasabasisforathe- ternationalSymposiumonArtificialIntelligenceand
oryofpossibility.FuzzySetsandSystems,1,3–28. Mathematics. Zimmermann,H.-J.(Ed.).(1999).Practicalappli-
cationsoffuzzytechnologies.Kluwer.
Zaritskii, V.S., Svetnik, V.B., andShimelevich, Zhang,L.,Pavlovic,V.,Cantor,C.R.,andKasif,S.
L.I.(1975). Monte-Carlotechniqueinproblemsof (2003). Human-mousegeneidentificationbycom- Zimmermann, H.-J.(2001). FuzzySetTheory—
optimalinformationprocessing.AutomationandRe- parativeevidenceintegrationandevolutionaryanal- AndItsApplications(Fourthedition).Kluwer.
moteControl,36,2015–22. ysis.GenomeResearch,pp.1–13.
Zinkevich,M.,Johanson,M.,Bowling,M.,andPic-
Zelle,J.andMooney,R.(1996). Learningtoparse Zhang,N.L.andPoole,D.(1994). Asimpleap- cione,C.(2008).Regretminimizationingameswith
databasequeriesusinginductivelogicprogramming. proachtoBayesiannetworkcomputations. InProc. incompleteinformation.InNIPS20,pp.1729–1736.
InAAAI-96,pp.1050–1055. 10thCanadianConferenceonArtificialIntelligence,
Zermelo, E. (1913). Uber Eine Anwendung der pp.171–178. Zollmann,A.,Venugopal,A.,Och,F.J.,andPonte,
MengenlehreaufdieTheoriedesSchachspiels. In Zhang,N.L.,Qi,R.,andPoole,D.(1994).Acom- J.(2008).Asystematiccomparisonofphrase-based,
Proc. Fifth International Congress of Mathemati- putationaltheoryofdecisionnetworks. IJAR,11, hierarchicalandsyntax-augmentedstatisticalMT.In
cians,Vol.2,pp.501–504. 83–158. COLING-08.
Zermelo,E.(1976). Anapplicationofsettheoryto Zhou,R.andHansen,E.(2002).Memory-bounded Zweig,G.andRussell,S.J.(1998).Speechrecogni-
thetheoryofchess-playing.FirbushNews,6,37–42. A*graphsearch. InProc.15thInternationalFlairs tionwithdynamicBayesiannetworks. InAAAI-98,
Englishtranslationof(Zermelo1913). Conference. pp.173–180.
This page intentionally left blank
Index
Pagenumbers in boldrefertodefinitions oftermsand algorithms; page numbers in italics refertoitemsin
thebibliography.
Symbols Ackley,D.H.,155,1064 active,839
acousticmodel,913 architectureof,26,1047
∧(and),244 indisambiguation,906 autonomous,236
χ2(chisquared),706 ACT,336 components,1044–1047
| (conslistcell),305 ACT*,799 decision-theoretic,483,610,664–666
(cid:5)(derives),242 actingrationally,4 goal-based,52–53,59,60
(cid:6)(determination),784 action,34,67,108,367 greedy,839
|=(entailment),240 high-level,406 hybrid,268
(cid:8)-ball,714 joint,427 intelligent,30,1036,1044
∃(thereexists),297 monitoring,423,424 knowledge-based,13,234–236,285,
∀(forall),295 primitive,406 1044
| (given),485 rational,7,30 learning,54–57,61
⇔(ifandonlyif),244 action-utilityfunction,627,831 logical,265–274,314
⇒(implies),244 actionexclusionaxiom,273,428 model-based,50,50–52
∼(indifferent),612 actionmonitoring,423,424 onlineplanning,431
λ(lambda)-expression,294 actionschema,367 passive,832
¬(not),244 activationfunction,728 passiveADP,858
∨(or),244 activelearning,831 passivelearning,858
(cid:6)(preferred),612 activesensing,928 problem-solving,64,64–69
(cid:12)→(uncertainrule),548 activevision,1025 rational,4,4–5,34,36–38,59,60,
actor,426 636,1044
A actuator,34,41 reflex,48,48–50,59,647,831
hydraulic,977 situated,1025
A(s)(actionsinastate),645 pneumatic,977 softwareagent,41
A*search,93–99 AD-tree,826 taxi-driving,56,1047
AAAI(AmericanAssociationforAI), ADABOOST,751 utility-based,53–54,59,664
31 adalines,20 vacuum,37,62–63
Aarup,M.,432,1064 Adams,J.,450 wumpus,238,305
Abbeel,P.,556,857,1068,1090 Adaprogramminglanguage,14 agentfunction,35,647
Abbott,L.F.,763,854,1070 adaptivecontroltheory,833,854 agentprogram,35,46,59
ABCcomputer,14 adaptivedynamicprogramming,834, Agerbeck,C.,228,1064
Abdennadher,S.,230,1073 834–835,853,858 Aggarwal,G.,682,1064
Abelson,R.P.,23,921,1088 adaptiveperception,985 aggregation,403
Abney,S.,921,1064 add-onesmoothing,863 Agichtein,E.,885,1064
ABO(AsymptoticBounded addlist,368 Agmon,S.,761,1064
Optimality),1050 Adelson-Velsky,G.M.,192,1064 Agre,P.E.,434,1064
Abramson,B.,110,1064 Adida,B.,469,1064 agreement(inasentence),900
absoluteerror,98 ADL(ActionDescriptionLanguage), Aguirre,A.,278,1068
abstraction,69,677 394 Aho,A.V.,1059,1064
abstractionhierarchy,432 admissibleheuristic,94,376 AI,seeartificialintelligence
ABSTRIPS,432 Adorf,H.-M.,432,1077 aircraftcarrierscheduling,434
Abu-Hanna,A.,505,1081 ADP(AdaptiveDynamic airport,drivingto,480
AC-3,209 Programming),834 airportsiting,622,626
AcademyAward,435 adversarialsearch,161 AISB(SocietyforArtificialIntelligence
accessibilityrelations,451 adversarialtask,866 andSimulationofBehaviour),
accusativecase,899 adversaryargument,149 31
Acero,A.,922,1076 AdviceTaker,19,23 AIWinter,24,28
Acharya,A.,112,1068 AFSM,1003 Aizerman,M.,760,1064
Achlioptas,D.,277,278,1064 agent,4,34,59 Al-Chang,M.,28,1064
1095
1096 Index
al-Khowarazmi,8 AND-SEARCH,136 Arlazarov,V.L.,192,1064
Alberti,L.B.,966 Andersen,S.K.,552,553,1064 Armando,A.,279,1064
Albus,J.S.,855,1064 Anderson,C.R.,395,433,1091 Arnauld,A.,7,636,1064
Aldiss,B.,1040 Anderson,C.W.,855,1065 Arora,S.,110,1064
Aldous,D.,154,1064 Anderson,J.A.,761,1075 ARPAbet,914
Alekhnovich,M.,277,1064 Anderson,J.R.,13,336,555,799, artificialflight,3
Alexandria,15 1064,1085 ArtificialGeneralIntelligence,27
algorithm,8 ANDnode,135 artificialintelligence,1,1–1052
algorithmiccomplexity,759 Andoni,A.,760,1064 applicationsof,28–29
Alhazen,966 Andre,D.,156,855,856,1064,1070, conferences,31
alignmentmethod,956 1079 foundations,5–16,845
Allais,M.,620,638,1064 ANGELIC-SEARCH,414 futureof,1051–1052
Allaisparadox,620 angelicsemantics,431 goalsof,1049–1051
Alldiffconstraint,206 answerliteral,350 historyof,16–28
Allen,B.,432,1072 answersetprogramming,359 journals,31
Allen,C.,638,1069 antecedent,244 philosophyof,1020–1043
Allen,J.F.,396,431,448,470,1064 Anthony,M.,762,1064 possibilityof,1020–1025
alliance(inmultiplayergames),166 anytimealgorithm,1048 programminglanguage,19
Allis,L.,194,1064 Aoki,M.,686,1064 real-time,1047
AlmanacGame,640 aorticcoarctation,634 societies,31
Almuallim,H.,799,1064 apparentmotion,940 strong,1020,1026–1033,1040
Almulla,M.,111,1085 appearance,942 subfields,1
ALPAC.,922,1064 appearancemodel,959 asuniversalfield,1
AlperinResnick,L.,457,471,1066 Appel,K.,227,1064 weak,1020,1040
α(normalizationconstant),497 Appelt,D.,884,921,1064,1075,1076 artificiallife,155
alpha–betapruning,167,199 APPEND,341 artificialurea,1027
alpha–betasearch,167–171,189,191 applicable,67,368,375 Arunachalam,R.,688,1064
ALPHA-BETA-SEARCH,170 apprenticeshiplearning,857,1037 Asada,M.,195,1014,1078
Alterman,R.,432,1064 approximatenear-neighbors,741 asbestosremoval,615
Altman,A.,195,1064 Apt,K.R.,228,230,1064 Ashby,W.R.,15,1064
altruism,483 Apte´,C.,884,1064 Asimov,I.,1011,1038,1064
Alveyreport,24 Arbuthnot,J.,504,1064 ASKMSR,872,873,885
AM,800 arcconsistency,208 assertion(logical),301
Amarel,S.,109,115,156,468,1064 Archibald,C.,195,1064 assignment(inaCSP),203
ambientillumination,934 architecture,46 associativememory,762
ambiguity,287,465,861,904–912,919 agent,26,1047 assumption,462
lexical,905 cognitive,336 Astrom,K.J.,156,686,1064
semantic,905 forspeechrecognition,25 astronomer,562
syntactic,905,920 hybrid,1003,1047 asymptoticanalysis,1054,1053–1054
ambiguityaversion,620 parallel,112 asymptoticboundedoptimality,1050
Amir,E.,195,278,556,1064,1070, pipeline,1005 Atanasoff,J.,14
1086 reflective,1048 Atkeson,C.G.,854,1083
Amit,D.,761,1064 rule-based,336 Atkin,L.R.,110,1089
analogicalreasoning,799 three-layer,1004 atom,295
ANALOGY,19,31 arcreversal,559 atomicrepresentation,57,64
analysisofalgorithms,1053 Arentoft,M.M.,432,1064 atomicsentence,244,295,294–295,
AnalyticalEngine,14 argmax,1059 299
analyticalgeneralization,799 argmax,166 attribute,58
Anantharaman,T.S.,192,1076 argument attribute-basedextraction,874
Anbulagan,277,1080 fromdisability,1021–1022 auction,679
anchoringeffect,621 frominformality,1024–1025 ascending-bid,679
anchortext,463 Ariely,D.,619,638,1064 Dutch,692
AND–ORgraph,257 Aristotle,4–7,10,59,60,275,313,468, English,679
And-Elimination,250 469,471,758,966,1041 first-price,681
AND-OR-GRAPH-SEARCH,136 arity,292,332 sealed-bid,681
AND–ORtree,135 Arkin,R.,1013,1064 second-price,681
Index 1097
truth-revealing,680 backgammon,177–178,186,194,846, Basye,K.,1012,1070
Vickrey,681 850 Bates,E.,921,1071
Audi,R.,1042,1064 backgroundknowledge,235,349,777, Bates,M.A.,14,192,1090
Auer,S.,439,469,1066 1024,1025 Batman,435
augmentation,919 backgroundsubtraction,961 bats,435
augmentedfinitestatemachine backingup(inasearchtree),99,165 Baum,E.,128,191,761,762,1065
(AFSM),1003 backjumping,219,229 Baum,L.E.,604,826,1065
augmentedgrammar,897 backmarking,229 Baumert,L.,228,1074
AURA,356,360 backoffmodel,863 Baxter,J.,855,1065
Austin,G.A.,798,1067 BACKTRACK,215 Bayardo,R.J.,229,230,277,1065
Australia,203,204,216 backtracking Bayer,K.M.,228,1086
authority,872 chronological,218 Bayerl,S.,359,1080
AUTOCLASS,826 dependency-directed,229 Bayes’rule,9,495,495–497,503,508
automata,1035,1041 dynamic,229 Bayes,T.,495,504,1065
automateddebugging,800 intelligent,218–220,262 Bayes–Nashequilibrium,678
automatedtaxi,40,56,236,480,694, BACKTRACKING-SEARCH,215 Bayesian,491
695,1047 backtrackingsearch,87,215,218–220, Bayesianclassifier,499
automobileinsurance,621 222,227 Bayesianlearning,752,803,803–804,
Auton,L.D.,277,1069 Backus,J.W.,919,1065 825
autonomiccomputing,60 Backus–Naurform(BNF),1060 Bayesiannetwork,26,510,510–517,
autonomousunderwatervehicle(AUV), backwardchaining,257,259,275, 551,565,827
972 337–345,358 dynamic,590,590–599
autonomy,39 backwardsearchforplanning,374–376 hybrid,520,552
averagereward,650 Bacon,F.,6 inferencein,522–530
Axelrod,R.,687,1064 bagging,760 learninghiddenvariablesin,824
axiom,235,302 Bagnell,J.A.,852,1013,1065 learningin,813–814
actionexclusion,273,428 bagofwords,866,883 multi-entity,556
ofChineseroom,1032 Baird,L.C.I.,685,1092 BayesNettoolkit,558
decomposability,614 Baker,J.,920,922,1065 Beal,D.F.,191,1065
domain-specific,439 Balashek,S.,922,1070 Beal,J.,27,1065
effectaxiom,266 Baldi,P.,604,1065 Beame,P.,277,1064
frameaxiom,267 Baldwin,J.M.,130,1065 beamsearch,125,174
Kolmogorov’s,489 Ball,M.,396,1091 Bear,J.,884,1075
ofnumbertheory,303 Ballard,B.W.,191,200,1065 Beber,G.,30,1071
ofprobability,489 Baluja,S.,155,968,1065,1087 Beckert,B.,359,1065
Peano,303,313,333 Bancilhon,F.,358,1065 beerfactoryscheduling,434
precondition,273 banditproblem,840,855 Beeri,C.,229,1065
ofprobability,488–490,1057 Banerji,R.,776,799,1082 beetle,dung,39,61,424,1004
ofsettheory,304 bang-bangcontrol,851 behaviorism,12,15,60
successor-state,267,279,389 Bangera,R.,688,1092 Bekey,G.,1014,1065
ofutilitytheory,613 Banko,M.,28,439,469,756,759,872, belief,450,453
wumpusworld,305 881,885,1065,1072 degreeof,482,489
axon,11 Bar-Hillel,Y.,920,922,1065 desiresand,610–611
Bar-Shalom,Y.,604,606,1065 belieffunction,549
B Barifaijo,E.,422,1077 beliefnetwork,seeBayesiannetwork
Barry,M.,553,1076 beliefpropagation,555
b∗(branchingfactor),103 Bartak,R.,230,1065 beliefrevision,460
B*search,191 Bartlett,F.,13 beliefstate,138,269,415,480
Baader,F.,359,471,1064 Bartlett,P.,762,855,1064,1065 ingametheory,675
Babbage,C.,14,190 Barto,A.G.,157,685,854,855,857, probabalistic,566,570
Bacchus,F.,228,230,505,555,638, 1065,1067,1090 wiggly,271
1064,1065 Barwise,J.,280,314,1065 beliefupdate,460
bachelor,441 baseline,950 Bell,C.,408,431,1065
Bachmann,P.G.H.,1059,1065 basicgroups,875 Bell,D.A.,826,1068
BACK-PROP-LEARNING,734 Basin,D.A.,191,1072 Bell,J.L.,314,1065
back-propagation,22,24,733–736,761 basisfunction,845 Bell,T.C.,883,884,1092
1098 Index
BELLE,192 Bidlack,C.,1013,1069 Boneh,D.,128,1065
BellLabs,922 Biere,A.,278,1066 Bonet,B.,156,394,395,433,686,
Bellman,R.E.,2,10,109,110,194, Bigelow,J.,15,1087 1066,1075
652,685,760,1065 Bigham,J.,885,1085 Bongard,J.,1041,1085
Bellmanequation,652 bilingualcorpus,910 Boole,G.,7,8,276,1066
Bellmanupdate,652 billiards,195 Booleankeywordmodel,867
Belongie,S.,755,762,1065 Billings,D.,678,687,1066 boosting,749,760
Ben-Tal,A.,155,1065 Bilmes,J.,604,1080,1086 Booth,J.W.,872
benchmarking,1053 binarydecisiondiagram,395 Booth,T.L.,919,1066
Bendix,P.B.,359,1078 binaryresolution,347 bootstrap,27,760
Bengio,S.,604,1089 Binder,J.,604,605,826,1066,1087 Borel,E.,687,1066
Bengio,Y.,760,1047,1065 bindinglist,301 Borenstein,J.,1012,1013,1066
BENINQ,472 Binford,T.O.,967,1066 Borgida,A.,457,471,1066
Bennett,B.,473,1069 Binmore,K.,687,1066 Boroditsky,L.,287,1066
Bennett,F.H.,156,1079 binocularstereopsis,949,949–964 Boser,B.,760,762,1066,1080
Bennett,J.,360,1074 binomialnomenclature,469 BOSS,28,1007,1008,1014
Bentham,J.,637,1065 bioinformatics,884 Bosse,M.,1012,1066
Berger,H.,11 biologicalnaturalism,1031 Botea,A.,395,1075
Berger,J.O.,827,1065 Birbeck,M.,469,1064 Bottou,L.,762,967,1080
Berkson,J.,554,1065 Bishop,C.M.,155,554,759,762,763, boundaryset,774
Berlekamp,E.R.,113,186,1065 827,1066 boundedoptimality(BO),1050
Berleur,J.,1034,1065 Bishop,M.,1042,1086 boundedrationality,1049
Berliner,H.J.,191,194,198,1065 Bishop,R.H.,60,1071 boundsconsistent,212
Bernardo,J.M.,811,1065 Bisson,T.,1042,1066 boundspropagation,212
Berners-Lee,T.,469,1065 Bistarelli,S.,228,1066 Bourlard,H.,604,1089
Bernoulli,D.,617,637,1065 Bitman,A.R.,192,1064 Bourzutschky,M.,176,1066
Bernoulli,J.,9,504 Bitner,J.R.,228,1066 Boutilier,C.,434,553,686,1066
Bernoulli,N.,641 Bizer,C.,439,469,1066 Bouzy,B.,194,1066
Bernstein,A.,192,1065 Bjornsson,Y.,194,1088 Bowden,B.V.,14,192,1090
Bernstein,P.L.,506,691,1065 BKG(backgammonprogram),194 Bower,G.H.,854,1075
Berrou,C.,555,1065 BLACKBOX,395 Bowerman,M.,314,1066
Berry,C.,14 Blake,A.,605,1077 Bowling,M.,687,1066,1091,1093
Berry,D.A.,855,1065 Blakeslee,S.,1047,1075 Box,G.E.P.,155,604,1066
Bertele,U.,553,1066 Blazewicz,J.,432,1066 BOXES,851
Bertoli,P.,433,1066 Blei,D.M.,883,1066 Boyan,J.A.,154,854,1066
Bertot,Y.,359,1066 Blinder,A.S.,691,1066 Boyd,S.,155,1066
Bertsekas,D.,60,506,685,857,1059, blindsearch,seesearch,uninformed Boyden,E.,11,1074
1066 Bliss,C.I.,554,1066 Boyen,X.,605,1066
BESM,192 Block,H.D.,20,1066 Boyen–Kolleralgorithm,605
Bessie`re,C.,228,1066 blocksworld,20,23,370,370–371,472 Boyer,R.S.,356,359,360,1066
best-firstsearch,92,108 BLOG,556 Boyer–Mooretheoremprover,359,360
bestpossibleprize,615 bluff,184 Boyle,J.,360,1092
betadistribution,592,811 Blum,A.L.,395,752,761,885,1066 Brachman,R.J.,457,471,473,1066,
Betlem,H.,422,1077 Blumer,A.,759,1066 1067,1080
Betlem,J.,422,1077 BM25scoringfunction,868,884 Bradshaw,G.L.,800,1079
bettinggame,490 BNF(Backus–Naurform),1060 Bradtke,S.J.,157,685,854,855,1065,
Bezzel,M.,109 BO,1050 1067
BGBLITZ,194 Bobick,A.,604,1077 Brady,J.M.,604,1084
Bhar,R.,604,1066 Bobrow,D.G.,19,884,1066 Brafman,O.,638,1067
Bialik,H.N.,908 Boddy,M.,156,433,1048,1070,1074 Brafman,R.,638,1067
bias,declarative,787 Boden,M.A.,275,1042,1066 Brafman,R.I.,433,434,855,1066,
Bibel,W.,359,360,1066,1080 body(ofHornclause),256 1067,1076
Bickford,M.,356,1089 boid,429,435 Brahmagupta,227
biconditional,244 Bolognesi,A.,192,1066 brain,16
Biddulph,R.,922,1070 Boltzmannmachine,763 computationalpower,12
bidirectionalsearch,90–112 Bonaparte,N.,190 computervs.,12
Index 1099
damage,optimal,737 Bryant,B.D.,435,1067 Cannyedgedetection,755,967
replacement,1029–1031,1043 Bryce,D.,157,395,433,1067 canonicaldistribution,518
super,9 Bryson,A.E.,22,761,1067 canonicalform,80
inavat,1028 Buchanan,B.G.,22,23,61,468,557, Cantor,C.R.,553,1093
brainscauseminds,11 776,799,1067,1072,1080 Cantu-Paz,E.,155,1085
Braitenberg,V.,1013,1067 Buckley,C.,870,1089 Capek,K.,1011,1037
branchingfactor,80,783 Buehler,M.,1014,1067 Capen,E.,637,1068
effective,103,111,169 BUGS,554,555 Caprara,A.,395,1068
Bransford,J.,927,1067 BUILD,472 Carbone,R.,279,1064
Brants,T.,29,883,921,1067,1072 Bulfin,R.,688,1086 Carbonell,J.G.,27,432,799,1068,
Bratko,I.,112,359,793,1067 bunch,442 1075,1091
Bratman,M.E.,60,1041,1067 Bundy,A.,799,1091 Carbonell,J.R.,799,1068
Braverman,E.,760,1064 Bunt,H.C.,470,1067 Cardano,G.,9,194,503,1068
BREADTH-FIRST-SEARCH,82 Buntine,W.,800,1083 cardgames,183
breadth-firstsearch,81,81–83,108,408 Burch,N.,194,678,687,1066,1088 Carin,L.,686,1077
Breese,J.S.,61,553,555,639,1048, Burgard,W.,606,1012–1014,1067, Carlin,J.B.,827,1073
1067,1076,1091 1068,1072,1088,1090 Carlson,A.,288,1082
Breiman,L.,758,760,1067 Burges,C.,884,1090 CARMEL,1013
Brelaz,D.,228,1067 burglaralarm,511–513 Carnap,R.,6,490,491,504,505,555,
Brent,R.P.,154,1067 Burkhard,H.-D.,1014,1091 1068
Bresina,J.,28,1064 Burns,C.,553,1083 CarnegieMellonUniversity,17,18
Bresnan,J.,920,1067 Buro,M.,175,186,1067 Carpenter,M.,432,1070
Brewka,G.,472,1067 Burstein,J.,1022,1067 Carreras,X.,920,1079
Brey,R.,637,1086 Burton,R.,638,1067 Carroll,S.,155,1068
Brickley,D.,469,1067 Buss,D.M.,638,1067 Carson,D.,359,1092
bridge(cardgame),32,186,195 Butler,S.,1042,1067 cart–poleproblem,851
BridgeBaron,189 Bylander,T.,393,395,1067 Casati,R.,470,1068
Bridle,J.S.,761,1067 Byrd,R.H.,760,1067 cascadedfinite-statetransducers,875
Briggs,R.,468,1067 case-basedreasoning,799
brightness,932 C caseagreement,900
Brill,E.,28,756,759,872,885,1065 casefolding,870
Brin,D.,881,885,1036,1067 c(stepcost),68 casestatement(inconditionplans),136
Brin,S.,870,880,884,1067 Cabeza,R.,11,1067 Cash,S.S.,288,1087
Bringsjord,S.,30,1067 Cabral,J.,469,1081 Cassandra,A.R.,686,1068,1077
Brioschi,F.,553,1066 caching,269 Cassandras,C.G.,60,1068
Britain,22,24 Cafarella,M.J.,885,1065,1067,1072 Casteran,P.,359,1066
Broadbent,D.E.,13,1067 Cajal,S.,10 Castro,R.,553,1068
Broadhead,M.,885,1065 cake,eatingandhaving,380 categorization,865
Broca,P.,10 calculus,131 category,440,440–445,453
Brock,B.,360,1076 calculusofvariations,155 causalnetwork,seeBayesiannetwork
Brokowski,M.,156,1092 Calvanese,D.,471,1064,1067 causalprobability,496
Brooks,M.J.,968,1076 Cambefort,Y.,61,1075 causalrule,317,517
Brooks,R.A.,60,275,278,434,1003, Cambridge,13 causation,246,498
1012,1013,1041,1067,1085 camera caveman,778
Brouwer,P.S.,854,1065 digital,930,943 Cazenave,T.,194,1066
Brown,C.,230,1067 forrobots,973 CCD(charge-coupleddevice),930,969
Brown,J.S.,472,800,1070,1080 pinhole,930 celldecomposition,986,989
Brown,K.C.,637,1067 stereo,949,974 exact,990
Brown,M.,604,1079 timeofflight,974 celllayout,74
Brown,P.F.,922,1067 video,929,963 center(inmechanismdesign),679
Brownston,L.,358,1067 Cameron-Jones,R.M.,793,1086 centrallimittheorem,1058
Bruce,V.,968,1067 Campbell,M.S.,192,1067,1076 cerebralcortex,11
Brunelleschi,F.,966 Campbell,W.,637,1068 certaintyeffect,620
Bruner,J.S.,798,1067 candidateelimination,773 certaintyequivalent,618
Brunnstein,K.,1034,1065 canmachinesthink?,1021 certaintyfactor,23,548,557
Brunot,A.,762,967,1080 Canny,J.,967,1013,1068 Cesa-Bianchi,N.,761,1068
1100 Index
Cesta,A.,28,1068 chimpanzee,860 clustering,553,694,817,818
CGP,433 Chineseroom,1031–1033 clustering(inBayesiannetworks),529,
CHAFF,277 CHINOOK,186,193,194 529–530
Chafin,B.,28,1064 Chklovski,T.,439,1068 clutter(indataassociation),602
chainrule(fordifferentiation),726 choicepoint,340 CMAC,855
chainrule(forprobabilities),514 Chomsky,C.,920,1074 CMU,922
Chakrabarti,P.P.,112,157,1068,1069 Chomsky,N.,13,16,883,889,919, CN2,800
Chambers,R.A.,851,854,1082 921,923,1068 CNF(ConjunctiveNormalForm),253
chancenode(decisionnetwork),626 ChomskyNormalForm,893,919 CNLP,433
chancenode(gametree),177 Chopra,S.,762,1086 co-NP,1055
chanceofwinning,172 Choset,H.,1013,1014,1068 co-NP-complete,247,276,1055
Chandra,A.K.,358,1068 Choueiry,B.Y.,228,1086 Coarfa,C.,278,1068
Chang,C.-L.,360,1068 Christmas,1026 coarticulation,913,917
Chang,K.-M.,288,1082 chronicles,470 coastalnavigation,994
Chang,K.C.,554,1073 chronologicalbacktracking,218 Coates,A.,857,1068
channelrouting,74 cHUGIN,554 Coates,M.,553,1068
Chapman,D.,394,434,1064,1068 Chung,K.L.,1059,1068 Cobham,A.,8,1068
Chapman,N.,109 Chung,S.,278,1092 Cocke,J.,922,1067
characters,861 chunking,799 coercion,416
Charest,L.,28,1064 Church,A.,8,314,325,358,1068 cognitive
charge-coupleddevice,930,969 Church,K.,883,894,920,923,1068 architecture,336
Charniak,E.,2,23,358,556,557,604, Churchland,P.M.,1042,1068 cognitivearchitecture,336
920,921,1068 Churchland,P.S.,1030,1042,1068 cognitivemodeling,3
chartparser,893,919 Ciancarini,P.,60,192,1066,1068 cognitivepsychology,13
Chase,A.,28,1064 CIGOL,800 cognitivescience,3
chatbot,1021 Cimatti,A.,396,433,1066,1068 Cohen,B.,277,1088
Chater,N.,638,1068,1084 circuitverification,312 Cohen,C.,1013,1069
Chatfield,C.,604,1068 circumscription,459,468,471 Cohen,P.R.,25,30,434,1069
Chatila,R.,1012,1083 prioritized,459 Cohen,W.W.,800,1069
Chauvin,Y.,604,1065 cityblockdistance,103 Cohn,A.G.,473,1069
checkers,18,61,186,193,850 Claessen,K.,360,1090 coinflip,548,549,641
checkmate clairvoyance,184 COLBERT,1013
accidental,182 Clamp,S.E.,505,1070 Collin,Z.,230,1069
guaranteed,181 Clapp,R.,637,1068 Collins,A.M.,799,1068
probabilistic,181 Clark,A.,1025,1041,1068 Collins,F.S.,27,1069
Cheeseman,P.,9,26,229,277,557, Clark,K.L.,472,1068 Collins,M.,760,920,921,1069,1079,
826,1012,1068,1089 Clark,P.,800,1068 1093
Chekaluk,R.,1012,1070 Clark,S.,920,1012,1068,1071 collusion,680
chemistry,22 Clarkcompletion,472 Colmerauer,A.,314,358,359,919,
Chen,R.,605,1080 Clarke,A.C.,552,1034,1068 1069
Chen,S.F.,883,1068 Clarke,E.,395,1068 Colombano,S.P.,155,1080
Chen,X.,395,1091 Clarke,M.R.B.,195,1068 color,935
Cheng,J.,554,826,1068 CLASSIC,457,458 colorconstancy,935
Cheng,J.-F.,555,1082 classification(indescriptionlogic),456 combinatorialexplosion,22
Chervonenkis,A.Y.,759,1091 classification(inlearning),696 commitment
chess,172–173,185–186 classprobability,764 epistemological,289,290,313,482
automaton,190 clause,253 ontological,289,313,482,547
history,192 Clearwater,S.H.,688,1068 commonsense,546
prediction,21 CLINT,800 commonvalue,679
Chess,D.M.,60,1078 Clocksin,W.F.,359,1068 communication,286,429,888
CHESS4.5,110 closed-worldassumption,299,344,417, commutativity(insearchproblems),214
χ2pruning,706 468,541 Compagna,L.,279,1064
Chickering,D.M.,191,826,1075,1079 closedclass,890 competitiveratio,148
Chien,S.,431,1073 closedlist,seeexploredset compilation,342,1047
CHILD-NODE,79 CLP,228,345 complementaryliterals,252
CHILL,902 CLP(R),359 complete-stateformulation,72
Index 1101
completeassignment,203 conformantplanning,415,417–421, context-freegrammar,889,918,919,
completedata,806 431,433,994 1060
completeness Congdon,C.B.,1013,1069 context-sensitivegrammar,889
ofinference,247 conjugateprior,811 contingencies,161
ofaproofprocedure,242,274 conjunct,244 contingencyplanning,133,415,
ofresolution,350–353 conjunction(logic),244 421–422,431
ofasearchalgorithm,80,108 conjunctivenormalform,253,253–254, continuation,341
completingthesquare,586 275,345–347 continuity(ofpreferences),612
completion(ofadatabase),344 conjunctordering,333 continuousdomains,206
complexity,1053–1055 Conlisk,J.,638,1069 contour(inanimage),948,953–954
sample,715 connectedcomponent,222 contour(ofastatespace),97
space,80,108 ConnectFour,194 contractionmapping,654
time,80,108 connectionism,24,727 contradiction,250
complexityanalysis,1054 connective,logical,16,244,274,295 controller,59,997
Connell,J.,1013,1069 controltheory,15,15,60,155,393,
complexphrases,876
consciousness,10,1026,1029,1030, 761,851,964,998
complexsentence,244,295
1033,1033 adaptive,833,854
complexwords,875
consequent,244 robust,836
compliantmotion,986,995
conservativeapproximation,271,419 controluncertainty,996
component(ofmixturedistribution),
consistency,105,456,769 convention,429
817
arc,208 conversiontonormalform,345–347
compositedecisionprocess,111
ofaCSPassignment,203 convexity,133
compositeobject,442
ofaheuristic,95 convexoptimization,133,153
compositionality,286
path,210,228 CONVINCE,552
compositionalsemantics,901
consistencycondition,110 convolution,938
compression,846
CONSISTENT-DET?,786 Conway,J.H.,113,1065
computability,8
consistentestimation,531 Cook,P.J.,1035,1072
computationallearningtheory,713,714,
Console,L.,60,1074 Cook,S.A.,8,276,278,1059,1069
762
Consortium,T.G.O.,469,1069 Cooper,G.,554,826,1069
computationallinguistics,16
conspiracynumber,191 cooperation,428
computer,13–14
constantsymbol,292,294 coordinateframe,956
brainvs.,12
constraint coordination,426,430
computervision,3,12,20,228,
binary,206 coordinationgame,670
929–965
global,206,211 Copeland,J.,470,1042,1069
conclusion(ofanimplication),244
nonlinear,205 Copernicus.,1035,1069
concurrentactionlist,428
preferenceconstraint,207 COQ,227,359
condensation,605
propagation,208,214,217 Cormen,T.H.,1059,1069
Condie,T.,275,1080
resourceconstraint,212 corpus,861
condition–actionrule,633
symmetry-breaking,226 correlatedsampling,850
conditionaldistributions,518 unary,206 Cortellessa,G.,28,1068
conditionaleffect,419 constraint-basedgeneralization,799 Cortes,C.,760,762,967,1069,1080
conditionalGaussian,521 constraintgraph,203,223 cotraining,881,885
conditionalindependence,498,502, constrainthypergraph,206 countnoun,445
503,517–523,551,574 constraintlanguage,205 Cournot,A.,687,1069
conditionalplan,660 constraintlearning,220,229 Cournotcompetition,678
conditionalprobability,485,503,514 constraintlogicprogramming,344–345, covariance,1059
conditionalprobabilitytable(CPT),512 359 covariancematrix,1058,1059
conditionalrandomfield(CRF),878 constraintlogicprogramming(CLP), Cover,T.,763,1069
conditioning,492 228,345 Cowan,J.D.,20,761,1069,1092
conditioningcase,512 constraintoptimizationproblem,207 Coward,N.,1022
Condon,J.H.,192,1069 constraintsatisfactionproblem(CSP), Cowell,R.,639,826,1069,1089
configurationspace,986,987 20,202,202–207 Cox,I.,606,1012,1069
confirmationtheory,6,505 constraintweighting,222 Cox,R.T.,490,504,505,1069
conflict-directedbackjumping,219,227 constructiveinduction,791 CPCS,519,552
conflictclauselearning,262 consumableresource,402 CPLAN,395
conflictset,219 context,286 CPSC,ix
1102 Index
CPT,512 CYBERLOVER,1021 DBN,566,590,590–599,603,604,
Craig,J.,1013,1069 cybernetics,15,15 646,664
Craik,K.J.,13,1069 CYC,439,469,470 DBPEDIA,439,469
Crammer,K.,761,1071 cyclicsolution,137 DCG,898,919
Craswell,N.,884,1069 Cyganiak,R.,439,469,1066 DDN(dynamicdecisionnetwork),664,
Crato,N.,229,1074 CYK-PARSE,894 685
Crauser,A.,112,1069 CYKalgorithm,893,919 Deacon,T.W.,25,1070
Craven,M.,885,1069 deadend,149
Crawford,J.M.,277,1069 D Deale,M.,432,1070
creativity,16 Dean,J.,29,921,1067
Cremers,A.B.,606,1012,1067,1088 D’Ambrosio,B.,553,1088 Dean,M.E.,279,1084
Cresswell,M.J.,470,1076 d-separation,517 Dean,T.,431,557,604,686,1012,
CRF,878 DAG,511,552 1013,1048,1070
Crick,F.H.C.,130,1091 Daganzo,C.,554,1069 Dearden,R.,686,855,1066,1070
Cristianini,N.,760,1069 Dagum,P.,554,1069 Debevec,P.,968,1070
critic(inlearning),55 Dahy,S.A.,723,724,1078 Debreu,G.,625,1070
criticalpath,403 Dalal,N.,946,968,1069 debugging,308
Crocker,S.D.,192,1074 DALTON,800 Dechter,R.,110,111,228–230,553,
Crockett,L.,279,1069 Damerau,F.,884,1064 1069,1070,1076,1085
Croft,B.,884,1069 Daniels,C.J.,112,1081 decision
Croft,W.B.,884,885,1085 Danish,907 rational,481,610,633
Cross,S.E.,29,1069 Dantzig,G.B.,155,1069 sequential,629,645
CROSS-VALIDATION,710 DARKTHOUGHT,192 DECISION-LIST-LEARNING,717
cross-validation,708,737,759,767 DARPA,29,922 DECISION-TREE-LEARNING,702
CROSS-VALIDATION-WRAPPER,710 DARPAGrandChallenge,1007,1014 decisionanalysis,633
crossover,128,153 Dartmouthworkshop,17,18 decisionboundary,723
crosswordpuzzle,44,231 Darwiche,A.,277,517,554,557,558, decisionlist,715
Cruse,D.A.,870,1069 1069,1085 decisionmaker,633
cryptarithmetic,206 Darwin,C.,130,1035,1069 decisionnetwork,510,610,626,
Csorba,M.,1012,1071 Dasgupta,P.,157,1069 626–628,636,639,664
Cuellar,J.,279,1064 data-driven,258 dynamic,664,685
Culberson,J.,107,112,1069,1092 dataassociation,599,982 evaluationof,628
culling,128 database,299 decisionnode,626
Cullingford,R.E.,23,1069 databasesemantics,300,343,367,540 decisionstump,750
cultofcomputationalism,1020 datacomplexity,334 decisiontheory,9,26,483,636
Cummins,D.,638,1069 datacompression,866 decisiontree,638,697,698
cumulativedistribution,564,623,1058 Datalog,331,357,358 expressiveness,698
cumulativelearning,791,797 datamatrix,721 pruning,705
cumulativeprobabilitydensityfunction, datamining,26 declarative,286
1058 datasparsity,888 declarativebias,787
curiosity,842 dativecase,899 declarativism,236,275
Curran,J.R.,920,1068 Daun,B.,432,1070 decomposability(oflotteries),613
current-best-hypothesis,770,798 Davidson,A.,678,687,1066 DECOMPOSE,414
CURRENT-BEST-LEARNING,771 Davidson,D.,470,1069 decomposition,378
Currie,K.W.,432,1073 Davies,T.R.,784,799,1069 DeCoste,D.,760,762,1070
curse Davis,E.,469–473,1069,1070 Dedekind,R.,313,1070
ofdimensionality,739,760,989,997 Davis,G.,432,1070 deduction,seelogicalinference
optimizer’s,619,637 Davis,K.H.,922,1070 deductiontheorem,249
winner’s,637 Davis,M.,260,276,350,358,1070 deductivedatabase,336,357,358
Cushing,W.,432,1069 Davis,R.,800,1070 deductivelearning,694
cutofftest,171 Davis–Putnamalgorithm,260 deepbeliefnetworks,1047
cutset Dawid,A.P.,553,639,826,1069, DEEPBLUE,ix,29,185,192
conditioning,225,227,554 1080,1089 DEEPFRITZ,193
cutset,cycle,225 Dayan,P.,763,854,855,1070,1083, DeepSpaceOne,60,392,432
cutsetconditioning,225,227,554 1088 DEEPTHOUGHT,192
Cybenko,G.,762,1069 daVinci,L.,5,966 Deerwester,S.C.,883,1070
Index 1103
defaultlogic,459,468,471 detectionfailure(indataassociation), directedarcconsistency,223
defaultreasoning,458–460,547 602 directutilityestimation,853
defaultvalue,456 determination,784,799,801 Dirichletdistribution,811
deFinetti’stheorem,490 minimal,787 Dirichletprocess,827
definiteclause,256,330–331 deterministicenvironment,43 disabilities,1043
definition(logical),302 deterministicnode,518 disambiguation,904–912,919
deformabletemplate,957 Detwarasiti,A.,639,1071 discontinuities,936
degreeheuristic,216,228,261 Deville,Y.,228,1091 discountfactor,649,685,833
degreeofbelief,482,489 DEVISER,431 discoverysystem,800
interval-valued,547 Devroye,L.,827,1071 discreteevent,447
degreeoffreedom,975 DeweyDecimalsystem,440 discretization,131,519
degreeoftruth,289 deBruin,A.,191,1085 discriminativemodel,878
DeGroot,M.H.,506,827,1070 deDombal,F.T.,505,1070 disjointsets,441
DeJong,G.,799,884,1070 deFinetti,B.,489,504,1070 disjunct,244
deletelist,368 deFreitas,J.F.G.,605,1070 disjunction,244
Delgrande,J.,471,1070 deFreitas,N.,605,1071 disjunctiveconstraint,205
deliberativelayer,1005 deKleer,J.,229,358,472,1070,1072, disjunctivenormalform,283
Dellaert,F.,195,1012,1072,1091 1091 disparity,949
DellaPietra,S.A.,922,1067 deMarcken,C.,921,1070 Dissanayake,G.,1012,1071
DellaPietra,V.J.,922,1067 DeMorgan,A.,1070 distantpointlightsource,934
deltarule,846 DeRaedt,L.,800,921,1070,1083 distortion,910
DelFavero,B.A.,553,1088 deSalvoBraz,R.,556,1070 distribute∨over∧,254,347
DelMoral,P.,605,1070 deSarkar,S.C.,112,157,1068,1069 distributedconstraintsatisfaction,230
demodulation,354,359,364 Diaconis,P.,620 distribution
Demopoulos,D.,278,1068 diagnosis,481,496,497,909 beta,592,811
DeMorgan’srules,298 dental,481 conditional,nonparametric,520
DeMorgan,A.,227,313 medical,23,505,517,548,629,1036 cumulative,564,623,1058
Dempster,A.P.,557,604,826,1070 diagnosticrule,317,517 mixture,817
Dempster–Shafertheory,547,549, dialysis,616 divide-and-conquer,606
549–550,557 diameter(ofagraph),88 Dix,J.,472,1067
DENDRAL,22,23,468 Dias,W.,28,1064 Dizdarevic,S.,158,1080
dendrite,11 Dickmanns,E.D.,1014,1071 DLV,472
Deng,X.,157,1070 dictionary,21 DNF(disjunctivenormalform),283
Denis,F.,921,1070 Dietterich,T.,799,856,1064,1071 Do,M.B.,390,431,1071
Denis,M.,28,1068 DifferenceEngine,14 Doctorow,C.,470,1071
Denker,J.,762,967,1080 differentialdrive,976 DOF,975
Dennett,D.C.,1024,1032,1033,1042, differentialequation,997 dolphin,860
1070 stochastic,567 domain,486
Denney,E.,360,1071 differentialGPS,975 continuous,206
densityestimation,806 differentiation,780 elementof,290
nonparametric,814 diffusealbedo,934 finite,205,344
DeOliveira,J.,469,1081 diffusereflection,933 infinite,205
depth-firstsearch,85,85–87,108,408 DigitalEquipmentCorporation(DEC), infirst-orderlogic,290
DEPTH-LIMITED-SEARCH,88 24,336 inknowledgerepresentation,300
depthlimit,173 digitrecognition,753–755 domainclosure,299,540
depthoffield,932 Dijkstra,E.W.,110,1021,1071 dominance
derivationalanalogy,799 Dill,D.L.,279,1084 stochastic,622,636
derivedsentences,242 Dillenburg,J.F.,111,1071 strict,622
Descartes,R.,6,966,1027,1041,1071 Dimopoulos,Y.,395,1078 dominantstrategy,668,680
descendant(inBayesiannetworks),517 Dinh,H.,111,1071 dominantstrategyequilibrium,668
Descotte,Y.,432,1071 Diophantineequations,227 dominatedplan(inPOMDP),662
descriptionlogic,454,456,456–458, Diophantus,227 domination(ofheuristics),104
468,471 Diorio,C.,604,1086 Domingos,P.,505,556,826,1071
descriptivetheory,619 DiPasquo,D.,885,1069 Domshlak,C.,395,434,1067,1076
detachment,547 Diplomacy,166 Donati,A.,28,1068
detailedbalance,537 directedacyclicgraph(DAG),511,552 Donninger,C.,193,1071
1104 Index
Doorenbos,R.,358,1071 664 Elisseeff,A.,759,1074
Doran,J.,110,111,1071 dynamicdecisionnetwork,664,685 ELIZA,1021,1035
Dorf,R.C.,60,1071 dynamicenvironment,44 Elkan,C.,551,826,1071
Doucet,A.,605,1070,1071 dynamicprogramming,60,106,110, Ellington,C.,1045,1072
Dow,R.J.F.,762,1088 111,342,575,685 Elliot,G.L.,228,1075
Dowling,W.F.,277,1071 adaptive,834,834–835,853,858 Elliott,P.,278,1092
Downey,D.,885,1072 nonserial,553 Ellsberg,D.,638,1071
downwardrefinementproperty,410 dynamicstate,975 Ellsbergparadox,620
Dowty,D.,920,1071 dynamicweighting,111 Elman,J.,921,1071
Doyle,J.,60,229,471,472,638,1071, Dyson,G.,1042,1071 EMalgorithm,571,816–824
1082,1092 dystopia,1052 structural,824
DPLL,261,277,494 Duzeroski,S.,796,800,1071,1078, embodiedcognition,1026
DPLL-SATISFIABLE?,261 1080 emergentbehavior,430,1002
Drabble,B.,432,1071 EMNLP,923
DRAGON,922 E empiricalgradient,132,849
Draper,D.,433,1072 empiricalloss,712
E,359
Drebbel,C.,15 empiricism,6,923
Dredze,M.,761,1071
E 0(Englishfragment),890
Empson,W.,921,1071
Earley,J.,920,1071
Dreussi,J.,432,1088 EMV(expectedmonetaryvalue),616
earlystopping,706
Dreyfus,H.L.,279,1024,1049,1071 Enderton,H.B.,314,358,1071
earthquake,511
Dreyfus,S.E.,109,110,685,1024, English,21,32
Eastlake,D.E.,192,1074
1065,1071 fragment,890
EBL,432,778,780–784,798,799
Driessens,K.,857,1090 ENIAC,14
Ecker,K.,432,1066
drillingrights,629 ensemblelearning,748,748–752
Eckert,J.,14
drone,1009 entailment,240,274
economics,9–10,59,616
droppingconditions,772 inverse,795
Edelkamp,S.,111,112,395,1071,
Drucker,H.,762,967,1080 entailmentconstraint,777,789,798
1079
Druzdzel,M.J.,554,1068 entropy,703
edge(inanimage),936
DT-AGENT,484
edgedetection,936–939
ENUMERATE-ALL,525
dualgraph,206
Edinburgh,800,1012
ENUMERATION-ASK,525
dualism,6,1027,1041 environment,34,40–46
Edmonds,D.,16
Dubois,D.,557,1071 artificial,41
Edmonds,J.,8,1071
Dubois,O.,277,1089 class,45
Edwards,D.J.,191,1075
duck,mechanical,1011 competitive,43
Edwards,P.,1042,1071
Duda,R.O.,505,557,763,825,827, continuous,44
Edwards,W.,637,1091
1071 cooperative,43
EEG,11
Dudek,G.,1014,1071 Een,N.,277,1071 deterministic,43
Duffy,D.,360,1071 effect,367 discrete,44
Duffy,K.,760,1069 missing,423 dynamic,44
Dumais,S.T.,29,872,883,885,1065, negative,398 game-playing,197,858
1070,1087 effector,971 generator,46
dungbeetle,39,61,424,1004 efficientauction,680 history,646
Dunham,B.,21,1072 Efros,A.A.,28,955,968,1075,1076 known,44
Dunham,C.,358,1090 Ehrenfeucht,A.,759,1066 multiagent,42,425
Dunn,H.L.,556,1071 8-puzzle,70,102,105,109,113 nondeterministic,43
DuPont,24 8-queensproblem,71,109 observable,42
duration,402 Einstein,A.,1 one-shot,43
Du¨rer,A.,966 Eisner,J.,920,1089 partiallyobservable,42
Durfee,E.H.,434,1071 Eitelman,S.,358,1090 properties,42
Durme,B.V.,885,1071 Eiter,T.,472,1071 semidynamic,44
Durrant-Whyte,H.,1012,1071,1080 Ekart,A.,155,1086 sequential,43
Dyer,M.,23,1071 electricmotor,977 single-agent,42
dynamicalsystems,603 electroniccircuitsdomain,309–312 static,44
dynamicbacktracking,229 Elfes,A.,1012,1083 stochastic,43
dynamicBayesiannetwork(DBN),566, ELIMINATION-ASK,528 taxi,40
590,590–599,603,604,646, Elio,R.,638,1071 uncertain,43
Index 1105
unknown,44 exclusiveor,246,766 factoredrepresentation,58,64,202,
unobservable,42 execution,66 367,486,664,694
EPAM(ElementaryPerceiverAnd executionmonitoring,422,422–434 factoring,253,347
Memorizer),758 executivelayer,1004 Fagin,R.,229,470,477,1065,1072
Ephrati,E.,434,1079 exhaustivedecomposition,441 Fahlman,S.E.,20,472,1072
epiphenomenalism,1030 existenceuncertainty,541 failuremodel,593
episodicenvironment,43 existentialgraph,454 falsealarm(indataassociation),602
epistemologicalcommitment,289,290, ExistentialInstantiation,323 falsenegative,770
313,482 ExistentialIntroduction,360 falsepositive,770
Epstein,R.,30,1071 expansion(ofstates),75 familytree,788
EQP,360 expectation,1058 Farrell,R.,358,1067
equality,353 expectedmonetaryvalue,616 FASTDIAGONALLYDOWNWARD,387
equality(inlogic),299 expectedutility,53,61,483,610,611, FASTDOWNWARD,395
equalitysymbol,299 616 FASTFORWARD,379
equilibrium,183,668 expectedvalue(inagametree),172, FASTUS,874,875,884
equivalence(logical),249 178 Faugeras,O.,968,1072
Erdmann,M.A.,156,1071 expectiminimax,178,191 Fearing,R.S.,1013,1072
ergodic,537 complexityof,179 Featherstone,R.,1013,1072
Ernst,G.,110,1084 expertsystem,468,633,636,800,1036 feature(inspeech),915
Ernst,H.A.,1012,1071 commercial,336 feature(ofastate),107,172
Ernst,M.,395,1071 decision-theoretic,633–636 featureextraction,929
Erol,K.,432,1071,1072 first,23 featureselection,713,866
error(ofahypothesis),708,714 firstcommercial,24 feed-forwardnetwork,729
errorfunction,1058 HPP(HeuristicProgramming feedbackloop,548
errorrate,708 Project),23 Feigenbaum,E.A.,22,23,468,758,
Essig,A.,505,1074 logical,546 1067,1072,1080
Etchemendy,J.,280,314,1065 medical,557 Feiten,W.,1012,1066
ethics,1034–1040 Prolog-based,339 Feldman,J.,639,1072
Etzioni,A.,1036,1072 withuncertainty,26 Feldman,R.,799,1089
Etzioni,O.,61,433,439,469,881,885, explainingaway,548 Fellbaum,C.,921,1072
1036,1050,1065,1072,1079, explanation,462,781 Fellegi,I.,556,1072
1091 explanation-basedgeneralization,187 Felner,A.,107,112,395,1072,1079,
Euclid,8,966 explanation-basedlearning(EBL),432, 1092
EURISKO,800 778,780–784,798,799 Felzenszwalb,P.,156,959,1072
Europe,24 explanatorygap,1033 Feng,C.,800,1083
EuropeanSpaceAgency,432 exploitation,839 Feng,L.,1012,1066
evaluationfunction,92,108,162, exploration,39,147–154,831,839,855 Fergus,R.,741,1090
171–173,845 safe,149 Ferguson,T.,192,827,1072
linear,107 explorationfunction,842,844 Fermat,P.,9,504
Evans,T.G.,19,31,1072 exploredset,77 Ferraris,P.,433,1072
event,446–447,450 expressiveness(ofarepresentation Ferriss,T.,1035,1072
atomic,506 scheme),58 FF,379,387,392,395
discrete,447 EXTEND-EXAMPLE,793 15-puzzle,109
exogenous,423 extendedKalmanfilter(EKF),589,982 FifthGenerationproject,24
inprobability,484,522 extension(ofaconcept),769 figureofspeech,905,906
liquid,447 extension(ofdefaulttheory),460 Fikes,R.E.,60,156,314,367,393,
eventcalculus,446,447,470,903 extensiveform,674 432,434,471,799,1012,1067,
Everett,B.,1012,1066 externalities,683 1072
evidence,485,802 extrinsicproperty,445 filtering,145,460,571–573,603,659,
reversal,605 eyes,928,932,966 823,856,978,1045
evidencevariable,522 assumed-density,605
evolution,130 F Fine,S.,604,1072
machine,21 finite-domain,205,344
evolutionarypsychology,621 fact,256 finite-stateautomata,874,889
evolutionstrategies,155 factor(invariableelimination),524 Finkelstein,L.,230,1067
exceptions,438,456 factoredfrontier,605 Finney,D.J.,554,1072
1106 Index
Firby,R.J.,431,1070 Fox,D.,606,1012,1014,1067,1072, Furst,M.,395,1066
first-orderlogic,285,285–321 1088,1090 futilitypruning,185
first-orderprobabilisticlogic,539–546 Fox,M.S.,395,432,1072 fuzzycontrol,550
Firth,J.,923,1072 frame fuzzylogic,240,289,547,550,557
Fischer,B.,360,1071 inrepresentation,24,471 fuzzyset,550,557
Fischetti,M.,395,1068 inspeech,915
Fisher,R.A.,504,1072 problem G
fitness(ingeneticalgorithms),127 inferential,267,279
fitnesslandscape,155 frameproblem,266,279 g(pathcost),78
Fix,E.,760,1072 inferential,447 G-set,774
fixation,950 representational,267 Go¨delnumber,352
FIXED-LAG-SMOOTHING,580 framingeffect,621 Gabor,Z.Z.,640
fixed-lagsmoothing,576 Franco,J.,277,1072 Gaddum,J.H.,554,1073
fixedpoint,258,331 Frank,E.,763,1092 Gaifman,H.,555,1073
Flannery,B.P.,155,1086 Frank,I.,191,1072 gainparameter,998
flaw,390 Frank,M.,231,1073 gainratio,707,765
Floreano,D.,1045,1072 Frank,R.H.,1035,1072 gait,1001
fluent,266,275,388,449–450 Frankenstein,1037 Gale,W.A.,883,1068
flyeyes,948,963 Franz,A.,883,921,1072 Galileo,G.,1,56,796
FMP,seeplanning,fine-motion Fratini,S.,28,1068 Gallaire,H.,358,1073
fMRI,11,288 FREDDY,74,156,1012 Gallier,J.H.,277,314,1071,1073
FredkinPrize,192 Gamba,A.,761,1073
focalplane,932
Freeman,W.,555,1091,1092 Gambaperceptrons,761
FOCUS,799
freespace,988 Gamberini,L.,761,1073
focusofexpansion,948
freewill,6 gambling,9,613
Fogel,D.B.,156,1072
Frege,G.,8,276,313,357,1072 game,9,161
Fogel,L.J.,156,1072
Freitag,D.,877,885,1069,1072 ofchance,177–180
FOIL,793
frequentism,491 dice,183
FOL-BC-ASK,338
Freuder,E.C.,228–230,1072,1087 Go,186,194
FOL-FC-ASK,332
Freund,Y.,760,1072 ofimperfectinformation,162
folkpsychology,473
Friedberg,R.M.,21,156,1072 inspectiongame,666
Foo,N.,279,1072
Friedgut,E.,278,1073 multiplayer,165–167
FOPC,seelogic,first-order
Friedman,G.J.,155,1073 Othello,186
Forbes,J.,855,1072
Friedman,J.,758,761,763,827,1067, partiallyobservable,180–184
FORBIN,431,432
1073,1075 ofperfectinformation,161
Forbus,K.D.,358,472,1072
Friedman,N.,553,558,605,826,827, poker,507
forcesensor,975
855,1066,1070,1073,1078 pursuit–evasion,196
Ford,K.M.,30,1072
FriendlyAI,27,1039 repeated,669,673
foreshortening,952 Fristedt,B.,855,1065 robot(withhumans),1019
Forestier,J.-P.,856,1072 frontier,75 Scrabble,187,195
Forgy,C.,358,1072 Frost,D.,230,1070 zero-sum,161,162,199,670
formulate,search,execute,66 Fruhwirth,T.,230,1073 gameplaying,161–162,190
Forrest,S.,155,1082 FRUMP,884 gameprograms,185–187
Forsyth,D.,960,968,1072,1086 Fuchs,J.J.,432,1073 GAMER,387
Fortmann,T.E.,604,606,1065 Fudenberg,D.,688,1073 gameshow,616
forward–backward,575,822 Fukunaga,A.S.,431,1073 gametheory,9,161,645,666,666–678,
FORWARD-BACKWARD,576 fullyobservable,658 685
forwardchaining,257,257–259,275, function,288 combinatorial,186
277,330–337,358 total,291 gametree,162
forwardchecking,217,217–218 functionaldependency,784,799 Gammafunction,828
forwardpruning,174 functionalism,60,1029,1030,1041, Garding,J.,968,1073
forwardsearchforplanning,373–374 1042 Gardner,M.,276,1073
four-colormapproblem,227,1023 functionapproximation,845,847 Garey,M.R.,1059,1073
Fourier,J.,227,1072 functionsymbol,292,294 Garg,A.,604,1084
Fowlkes,C.,941,967,1081 Fung,R.,554,1073 GARI,432
Fox,C.,638,1072 Furnas,G.W.,883,1070 Garofalakis,M.,275,1080
Index 1107
Garrett,C.,128,1065 Gerevini,A.,394,395,1073 Goldberg,A.V.,111,1074
Gaschnig’sheuristic,119 Gershwin,G.,917,1073 Goldberg,D.E.,155,1085
Gaschnig,J.,111,119,228,229,557, Gestaltschool,966 Goldberg,K.,156,1092
1071,1073 Getoor,L.,556,1073 Goldin-Meadow,S.,314,1073
Gasquet,A.,432,1073 Ghahramani,Z.,554,605,606,827, Goldman,R.,156,433,555,556,921,
Gasser,R.,112,194,1073 1073,1077,1087 1068,1074,1091
Gat,E.,1013,1073 Ghallab,M.,372,386,394–396,431, goldstandard,634
gate(logic),309 1073 Goldszmidt,M.,553,557,686,826,
Gauss,C.F.,227,603,759,1073 Ghose,S.,112,1068 1066,1073,1074
Gauss,K.F.,109 GIB,187,195 GOLEM,800
Gaussiandistribution,1058 Gibbs,R.W.,921,1073 Golgi,C.,10
multivariate,584,1058 GIBBS-ASK,537 Golomb,S.,228,1074
Gaussianerrormodel,592 Gibbssampling,536,538,554 Golub,G.,759,1074
Gaussianfilter,938 Gibson,J.J.,967,968,1073 Gomard,C.K.,799,1077
Gaussianprocess,827 Gil,Y.,439,1068 Gomes,C.,154,229,277,1074
Gawande,A.,1036,1073 Gilks,W.R.,554,555,826,1073 Gonthier,G.,227,1074
Gawron,J.M.,922,1078 Gilmore,P.C.,358,1073 Good,I.J.,491,552,1037,1042,1074
Gay,D.E.,275,1080 Ginsberg,M.L.,187,195,229,231, Good–Turingsmoothing,883
Gearhart,C.,686,1074 359,363,557,1069,1073,1089 goodandevil,637
Gee,A.H.,605,1070 Gionis,A.,760,1073 Gooday,J.M.,473,1069
Geffner,H.,156,394,395,431,433, Gittins,J.C.,841,855,1074 Goodman,D.,29,1074
1066,1075,1084 Gittinsindex,841,855 Goodman,J.,29,883,1068,1074
Geiger,D.,553,826,1073,1075 Giunchiglia,E.,433,1072 Goodman,N.,470,798,1074,1080
Geisel,T.,864,1073 Givan,R.,857,1090 Goodnow,J.J.,798,1067
Gelatt,C.D.,155,229,1078 Glanc,A.,1011,1074 goodold-fashionedAI(GOFAI),1024,
Gelb,A.,604,1073 Glass,J.,604,1080 1041
Gelder,A.V.,360,1090 GLAUBER,800 Google,870,883,889,922
Gelernter,H.,18,359,1073 Glavieux,A.,555,1065 GoogleTranslate,907
Gelfond,M.,359,472,1073 GLIE,840 Gopnik,A.,314,1074
Gelly,S.,194,1073,1091 globalconstraint,206,211 Gordon,D.M.,429,1074
Gelman,A.,827,1073 GlobalPositioningSystem(GPS),974 Gordon,G.,605,686,1013,1085,1087,
Geman,D.,554,967,1073 Glover,F.,154,1074 1091
Geman,S.,554,967,1073 Glymour,C.,314,826,1074,1089 Gordon,M.J.,314,1074
generality,783 Go(game),186,194 Gordon,N.,187,195,605,1071,1074
generalization,770,772 goal,52,64,65,108,369 Gorry,G.A.,505,1074
generalizationhierarchy,776 basedagent,52–53,59,60 Gottlob,G.,230,1074
generalizationloss,711 formulationof,65 Gotts,N.,473,1069
generalizedarcconsistent,210 goal-basedagent,52–53,59 GP-CSP,390
generalizedcylinder,967 goal-directedreasoning,259 GPS(GeneralProblemSolver),3,7,18,
generalontology,453 inferential,301 393
GeneralProblemSolver,3,7,18,393 serializable,392 GPS(GlobalPositioningSystem),974
generation(ofstates),75 goalclauses,256 gracefuldegradation,666
generativecapacity,889 goalmonitoring,423 gradient,131
generator,337 goalpredicate,698 empirical,132,849
Genesereth,M.R.,59,60,156,195, goaltest,67,108 gradientdescent,125,719
314,345,350,359,363,1019, God,existenceof,504 batch,720
1073,1080,1089 Go¨del,K.,8,276,358,1022,1074 stochastic,720
GENETIC-ALGORITHM,129 Goebel,J.,826,1074 Graham,S.L.,920,1074
geneticalgorithm,21,126–129,153, Goebel,R.,2,59,1085 Grama,A.,112,1074
155–156,841 Goel,A.,682,1064 grammar,860,890,1060
geneticprogramming,155 Goertzel,B.,27,1074 attribute,919
Gent,I.,230,1073 GOFAI,1024,1041 augmented,897
Gentner,D.,314,799,1073 gold,237 categorial,920
GeometryTheoremProver,18 Gold,B.,922,1074 context-free,889,918,919,1060
Georgeson,M.,968,1067 Gold,E.M.,759,921,1074 lexicalized,897
Gerbault,F.,826,1074 Goldbach’sconjecture,800 probabilistic,890,888–897,919
1108 Index
context-sensitive,889 Gupta,A.,639,1079 Harada,D.,856,1084
definiteclause(DCG),898,919 GUS,884 Haralick,R.M.,228,1075
dependency,920 Gutfreund,H.,761,1064 Hardin,G.,688,1075
English,890–892 Guthrie,F.,227 Hardy,G.H.,1035,1075
inductionof,921 GuuguYimithirr,287 Harel,D.,358,1068
lexical-functional(LFG),920 Guy,R.K.,113,1065 Harman,G.H.,1041,1075
phrasestructure,918 Guyon,I.,759,760,762,967,1066, HARPY,154,922
probabilistic,897 1074,1080 Harris,Z.,883,1075
recursivelyenumerable,889 Harrison,J.R.,637,1075
regular,889 H Harrison,M.A.,920,1074
grammaticalformalism,889 Harsanyi,J.,687,1075
H(entropy),704
GrandPrix,185 Harshman,R.A.,883,1070
h(heuristicfunction),92
graph,67 Hart,P.E.,110,156,191,432,434,
coloring,227
hMAP(MAPhypothesis),804
505,557,763,799,825,827,
Eulerian,157
hML(MLhypothesis),805
1071,1072,1075
GRAPH-SEARCH,77
HACKER,394
Hart,T.P.,191,1075
Hacking,I.,506,1074
graphicalmodel,510,558 Hartley,H.,826,1075
Haghighi,A.,896,920,1074
GRAPHPLAN,379,383,392,394–396, Hartley,R.,968,1075
Hahn,M.,760,1069
402,433 Harvard,621
Ha¨hnel,D.,1012,1067
grasping,1013 Haslum,P.,394,395,431,1075
Haimes,M.,556,1082
Grassmann,H.,313,1074 Hastie,T.,760,761,763,827,1073,
Haken,W.,227,1064
Gravano,L.,885,1064 1075
HAL9000computer,552
Grayson,C.J.,617,1074 Haugeland,J.,2,30,1024,1042,1075
Hald,A.,506,1074
Greece,275,468,470 Hauk,T.,191,1075
Halevy,A.,28,358,470,759,885,
greedysearch,92 Haussler,D.,604,759,762,800,1065,
1067,1074
Green,B.,920,1074 1066,1075,1079
Halgren,E.,288,1087
Green,C.,19,314,356,358,1074 Havelund,K.,356,1075
Halpern,J.Y.,314,470,477,505,555,
Green,P.,968,1067 Havenstein,H.,28,1075
1065,1072,1074
Greenbaum,S.,920,1086 Hawkins,J.,1047,1075
Halpin,M.P.,231,1073
Greenblatt,R.D.,192,1074 Hayes,P.J.,30,279,469–472,1072,
haltingproblem,325
Greenspan,M.,195,1079 1075,1082
ham,865
Grefenstette,G.,27,1078 Haykin,S.,763,1075
Hamm,F.,470,1091
Greiner,R.,799,826,1068,1074 Hays,J.,28,1075
Hamming,R.W.,506,1074
Grenager,T.,857,1088 head,897
Hammingdistance,738
grid,rectangular,77 head(ofHornclause),256
Hammond,K.,432,1074
Griffiths,T.,314,1090 Hearst,M.A.,879,881,883,884,922,
Hamori,S.,604,1066
Grinstead,C.,506,1074 hamsandwich,906 1075,1084,1087
GRL,1013 Hamscher,W.,60,1074 Heath,M.,759,1074
Grosof,B.,799,1087 Han,X.,11,1074 HeathRobinson,14
Grosz,B.J.,682,688,1076 Hanan,S.,395,1072 heavy-taileddistribution,154
grounding,243 Hand,D.,763,1074 Heawood,P.,1023
groundresolutiontheorem,255,350 hand–eyemachine,1012 Hebb,D.O.,16,20,854,1075
groundterm,295,323 Handschin,J.E.,605,1075 Hebbianlearning,16
Grove,A.,505,638,1064,1065 handwrittendigitrecognition,753–755 Hebert,M.,955,968,1076
Grove,W.,1022,1074 Hanks,S.,433,1072 Heckerman,D.,26,29,548,552,553,
Gruber,T.,439,470,1074 Hanna,F.K.,800,1087 557,605,634,640,826,1067,
grue,798 Hansard,911 1074–1076,1087–1089
Grumberg,O.,395,1068 Hansen,E.,112,156,422,433,686, hedoniccalculus,637
GSAT,277 1075,1093 Heidegger,M.,1041,1075
Gu,J.,229,277,1074,1089 Hansen,M.O.,228,1064 Heinz,E.A.,192,1075
Guard,J.,360,1074 Hansen,P.,277,1075 Held,M.,112,1075
Guestrin,C.,639,686,856,857,1074, Hanski,I.,61,1075 Hellerstein,J.M.,275,1080
1079,1081 Hansson,O.,112,119,1075 Helmert,M.,111,395,396,1075
Guha,R.V.,439,469,1067,1080 happygraph,703 Helmholtz,H.,12
Guibas,L.J.,1013,1074 haptics,1013 Hempel,C.,6
Gumperz,J.,314,1074 Harabagiu,S.M.,885,1085 Henderson,T.C.,210,228,1082
Index 1109
Hendler,J.,27,396,432,469,1064, hierarchicaltasknetwork(HTN),406, horizon(inMDPs),648
1065,1071,1072,1075,1089 431 horizoneffect,174
Henrion,M.,61,519,552,554,639, Hierholzer,C.,157,1075 Horn,A.,276,1076
1075,1076,1086 higher-orderlogic,289 Horn,B.K.P.,968,1076
Henzinger,M.,884,1088 highlevelaction,406 Horn,K.V.,505,1076
Henzinger,T.A.,60,1075 Hilgard,E.R.,854,1075 Hornclause,256,791
Hephaistos,1011 HILL-CLIMBING,122 Hornform,275,276
Herbrand’stheorem,351,358 hillclimbing,122,153,158 Horning,J.J.,1076
Herbrand,J.,276,324,351,357,358, first-choice,124 Horowitz,E.,110,1076
1075 random-restart,124 Horowitz,M.,279,1084
Herbrandbase,351 stochastic,124 Horrocks,J.C.,505,1070
Herbranduniverse,351,358 Hingorani,S.L.,606,1069 horse,1028
Hernadvolgyi,I.,112,1076 Hinrichs,T.,195,1080 Horswill,I.,1013,1076
Herskovits,E.,826,1069 Hintikka,J.,470,1075 Horvitz,E.J.,26,29,61,553,604,639,
Hessian,132 Hinton,G.E.,155,761,763,1047, 1048,1076,1084,1087
Heule,M.,278,1066 1075,1087 Hovel,D.,553,1076
Hirsch,E.A.,277,1064 Howard,R.A.,626,637–639,685,
heuristic,108
Hirsh,H.,799,1075 1076,1082
admissible,94,376
Hitachi,408 Howe,A.,394,1073
composite,106
hitlist,869 Howe,D.,360,1076
degree,216,228,261
HITS,871,872 HSCP,433
forplanning,376–379
HMM,578,590,876,922 HSP,387,395
function,92,102–107
Ho,Y.-C.,22,761,1067 HSPR,395
least-constraining-value,217
Hoane,A.J.,192,1067 Hsu,F.-H.,192,1067,1076
levelsum,382
Hobbes,T.,5,6 Hsu,J.,28,1064
Manhattan,103
Hobbs,J.R.,473,884,921,1075,1076 HTML,463,875
max-level,382
Hodges,J.L.,760,1072 HTN,406,431
min-conflicts,220
Hoff,M.E.,20,833,854,1092 HTNplanning,856
minimum-remaining-values,216,
Hoffmann,J.,378,379,395,433,1076, Hu,J.,687,857,1076
228,333,405
1078 Huang,K.-C.,228,1086
minimumremainingvalues,216,228,
Hogan,N.,1013,1076 Huang,T.,556,604,1076
333,405
HOGfeature,947 Huang,X.D.,922,1076
nullmove,185
Hoiem,D.,955,968,1076 hub,872
search,81,110
holdoutcross-validation,708 HubbleSpaceTelescope,206,221,432
set-level,382
holisticcontext,1024 Hubel,D.H.,968,1076
straight-line,92
Holland,J.H.,155,1076,1082 Huber,M.,1013,1069
heuristicpathalgorithm,118 Hollerbach,J.M.,1013,1072 HubsandAuthorities,872
HeuristicProgrammingProject(HPP), holonomic,976 Huddleston,R.D.,920,1076
23 Holte,R.,107,112,678,687,1066, Huet,G.,359,1066
Hewitt,C.,358,1075 1072,1076,1092 Huffman,D.A.,20,1076
hexapodrobot,1001 Holzmann,G.J.,356,1076 Huffman,S.,1013,1069
hiddenMarkovmodel homeostatic,15 Hughes,B.D.,151,1076
factorial,605 homophones,913 Hughes,G.E.,470,1076
hiddenMarkovmodel(HMM),25,566, Homosapiens,1,860 HUGIN,553,604
578,578–583,590,603,604, Hon,H.,922,1076 Huhns,M.N.,61,1076
822–823 Honavar,V.,921,1084 human-levelAI,27,1034
hiddenMarkovmodel(HMM)(HMM), Hong,J.,799,1082 humanjudgment,546,557,619
578,590,876,922 Hood,A.,10,1076 humanoidrobot,972
hiddenunit,729 Hooker,J.,230,1076 humanperformance,1
hiddenvariable,522,816 Hoos,H.,229,1076 humanpreference,649
HIERARCHICAL-SEARCH,409 Hopcroft,J.,1012,1059,1064,1088 Hume,D.,6,1076
hierarchicaldecomposition,406 Hope,J.,886,1076 Humphrys,M.,1021,1076
hierarchicallookahead,415 Hopfield,J.J.,762,1076 Hungarianalgorithm,601
hierarchicalreinforcementlearning, Hopfieldnetwork,762 Hunkapiller,T.,604,1065
856,1046 HopkinsBeast,1011 Hunsberger,L.,682,688,1076
hierarchicalstructure,1046 horizon(inanimage),931 Hunt,W.,360,1076
1110 Index
Hunter,L.,826,1076 imperfectinformation,190,666 informationvalue,629,639
Hurst,M.,885,1076 implementation(ofahigh-levelaction), informedsearch,64,81,92,92–102,
Hurwicz,L.,688,1076 407 108
Husmeier,D.,605,1076 implementationlevel,236 infuencediagram,510,610,626,
Hussein,A.I.,723,724,1078 implication,244 626–628,636,639,664
Hutchinson,S.,1013,1014,1068 implicativenormalform,282,345 Ingerman,P.Z.,919,1076
Huth,M.,314,1076 importancesampling,532,554 Ingham,M.,278,1092
Huttenlocher,D.,959,967,1072,1076 incentive,426 inheritance,440,454,478
Huygens,C.,504,687,1076 incentivecompatible,680 multiple,455
Huyn,N.,111,1076 inclusion–exclusionprinciple,489 initialstate,66,108,162,369
Hwa,R.,920,1076 incompleteness,342 Inoue,K.,795,1076
Hwang,C.H.,469,1076 theorem,8,352,1022 inputresolution,356
HYBRID-WUMPUS-AGENT,270 inconsistentsupport,381 inside–outsidealgorithm,896
hybridA*,991 incrementalformulation,72 instance(ofaschema),128
hybridarchitecture,1003,1047 incrementallearning,773,777 instance-basedlearning,737,737–739,
HYDRA,185,193 independence,494,494–495,498,503 855
hyperparameter,811 absolute,494 insufficientreason,principleof,504
hypertreewidth,230 conditional,498,502,503,517–523, insurancepremium,618
hypothesis,695 551,574 intelligence,1,34
approximatelycorrect,714 context-specific,542,563 intelligentbacktracking,218–220,262
consistent,696 marginal,494 intentionality,1026,1042
null,705
independentsubproblems,222 intentionalstate,1028
prior,803,810
index,869 intercausalreasoning,548
hypothesisprior,803,810
indexical,904 interior-pointmethod,155
hypothesisspace,696,769
indexing,328,327–329 interleaving,147
Hyun,S.,1012,1070
India,16,227,468 interleaving(actions),394
indicatorvariable,819 interleaving(searchandaction),136
I
indifference,principleof,491,504 interlingua,908
individual(ingeneticalgorithms),127 internalstate,50
i.i.d.(independentandidentically
individuation,445 Internetsearch,464
distributed),708,803
inducedwidth,229 Internetshopping,462–467
Iagnemma,K.,1014,1067
induction,6 interpolationsmoothing,883
IBAL,556
constructive,791 interpretation,292,313
IBM,18,19,29,185,193,922
mathematical,8 extended,313
IBM704computer,193
icecream,483
inductivelearning,694,695–697 intended,292
ID3,800 inductivelogicprogramming(ILP), pragmatic,904
IDA*search,99,111 779,800 interreflections,934,953
identificationinthelimit,759 Indyk,P.,760,1064,1073 interval,448
identitymatrix(I),1056 inference,208,235 Intille,S.,604,1077
identityuncertainty,541,876 probabilistic,490,490–494,510 intractability,21
idiotBayes,499 inferenceprocedure,308 intrinsicproperty,445
IEEE,469 inferencerule,250,275 introspection,3,12
ignorance,547,549 inferentialequivalence,323 intuitionpump,1032
practical,481 inferentialframeproblem,267,279 inverse(ofamatrix),1056
theoretical,481 infinitehorizonproblems,685 inverseentailment,795
ignoredeletelists,377 influencediagram,552,610,626 inversegametheory,679
ignorepreconditionsheuristic,376 INFORMATION-GATHERING-AGENT, inversekinematics,987
Iida,H.,192,1087 632 inversereinforcementlearning,857
IJCAI(InternationalJointConference informationextraction,873,873–876, inverseresolution,794,794–797,800
onAI),31 883 invertedpendulum,851
ILOG,359 informationgain,704,705 invertedspectrum,1033
ILP,779,800 informationgathering,39,994 Inza,I.,158,1080
image,929 informationretrieval(IR),464,867, IPL,17
formation,929–935,965 867–872,883,884 IPP,387,395
processing,965 informationsets,675 IQtest,19,31
segmentation,941–942 informationtheory,703–704,758 IR,464,867,867–872,883,884
Index 1111
irrationality,2,613 Johanson,M.,687,1066,1093 Kameyama,M.,884,1075
irreversible,149 Johnson,C.R.,61,1067 Kaminka,G.,688,1089
IS-Alinks,471 Johnson,D.S.,1059,1073 Kan,A.,110,405,432,1080
Isard,M.,605,1077 Johnson,M.,920,921,927,1041,1067, Kanade,T.,951,968,1087,1090
ISBN,374,541 1068,1071,1079 Kanal,L.N.,111,112,1077,1079,
ISIS,432 Johnson,W.W.,109,1077 1083
Israel,D.,884,1075 Johnston,M.D.,154,229,432,1077, Kanazawa,K.,604,605,686,826,
ITEP,192 1082 1012,1066,1070,1077,1087
ITEPchessprogram,192 jointaction,427 Kanefsky,B.,9,28,229,277,1064,
ITERATIVE-DEEPENING-SEARCH,89 jointprobabilitydistribution,487 1068
iterativedeepeningsearch,88,88–90, full,488,503,510,513–517 Kanodia,N.,686,1074
108,110,173,408 jointree,529 Kanoui,H.,314,358,1069
iterativeexpansion,111 Jones,M.,968,1025,1091 Kant,E.,358,1067
iterativelengtheningsearch,117 Jones,N.D.,799,1077 Kantor,G.,1013,1014,1068
ITOU,800 Jones,R.,358,885,1077 Kantorovich,L.V.,155,1077
Itsykson,D.,277,1064 Jones,R.M.,358,1092 Kaplan,D.,471,1077
Iwama,K.,277,1077 Jones,T.,59,1077 Kaplan,H.,111,1074
Iwasawa,S.,1041,1085 Jonsson,A.,28,60,431,1064,1077 Kaplan,R.,884,920,1066,1081
IXTET,395 Jordan,M.I.,555,605,606,686,761, Karmarkar,N.,155,1077
827,850,852,855,857,883, Karmiloff-Smith,A.,921,1071
J 1013,1066,1073,1077,1083, Karp,R.M.,8,110,112,1059,1075,
1084,1088,1089,1091
1077
Jouannaud,J.-P.,359,1077
Jaakkola,T.,555,606,855,1077,1088 Kartam,N.A.,434,1077
Joule,J.,796
JACK,195 Kasami,T.,920,1077
Juang,B.-H.,604,922,1086
Jackel,L.,762,967,1080 Kasif,S.,553,1093
Judd,J.S.,762,1077
Jackson,F.,1042,1077 Kasparov,G.,29,192,193,1077
Juels,A.,155,1077
Jacobi,C.G.,606 Kassirer,J.P.,505,1074
Junker,U.,359,1077
Jacquard,J.,14 Katriel,I.,212,228,1091
Jurafsky,D.,885,886,920,922,1077
Jacquardloom,14 Katz,S.,230,1069
Just,M.A.,288,1082
Jaffar,J.,359,1077 Kaufmann,M.,360,1077
justification(inaJTMS),461
Jaguar,431 Kautz,D.,432,1070
Jain,A.,885,1085 Kautz,H.,154,229,277,279,395,
K
James,W.,13 1074,1077,1078,1088
janitorialscience,37 k-consistency,211 Kavraki,L.,1013,1014,1068,1078
Japan,24 k-DL(decisionlist),716 Kay,A.R.,11,1084
Jasra,A.,605,1070 k-DT(decisiontree),716 Kay,M.,884,907,922,1066,1078
Jaumard,B.,277,1075 k-dtree,739 KB,235,274,315
Jaynes,E.T.,490,504,505,1077 k-foldcross-validation,708 KB-AGENT,236
Jeavons,P.,230,1085 Kadane,J.B.,639,687,1077 Keane,M.A.,156,1079
Jefferson,G.,1026,1077 Kaelbling,L.P.,278,556,605,686, Kearns,M.,686,759,763,764,855,
Jeffrey,R.C.,504,637,1077 857,1012,1068,1070,1077, 1078
Jeffreys,H.,883,1077 1082,1088,1090 Kebeasy,R.M.,723,724,1078
Jelinek,F.,883,922,923,1067,1077 Kager,R.,921,1077 Kedar-Cabelli,S.,799,1082
Jenkin,M.,1014,1071 Kahn,H.,855,1077 Keene,R.,29,1074
Jenkins,G.,604,1066 Kahneman,D.,2,517,620,638,1077, Keeney,R.L.,621,625,626,638,1078
Jennings,H.S.,12,1077 1090 Keil,F.C.,3,1042,1092
Jenniskens,P.,422,1077 Kaindl,H.,112,1077 Keim,G.A.,231,1080
Jensen,F.,552,553,1064 Kalman,R.,584,604,1077 Keller,R.,799,1082
Jensen,F.V.,552,553,558,1064,1077 Kalmanfilter,566,584,584–591,603, Kelly,J.,826,1068
Jevons,W.S.,276,799,1077 604,981 Kemp,M.,966,1078
Ji,S.,686,1077 switching,589,608 Kempe,A.B.,1023
Jimenez,P.,156,433,1077 Kalmangainmatrix,588 Kenley,C.R.,553,1088
Jitnah,N.,687,1079 Kambhampati,S.,157,390,394,395, Kephart,J.O.,60,1078
Joachims,T.,760,884,1077 431–433,1067,1069,1071, Kepler,J.,966
job,402 1077,1084 kernel,743
job-shopschedulingproblem,402 Kameya,Y.,556,1087 kernelfunction,747,816
1112 Index
polynomial,747 model-based,497 Kripke,S.A.,470,1079
kernelization,748 prior,39,768,778,787 Krishnan,T.,826,1082
kernelmachine,744–748 knowledge-basedagents,234 Krogh,A.,604,1079
kerneltrick,744,748,760 knowledge-basedsystem,22–24,845 KRYPTON,471
Kernighan,B.W.,110,1080 knowledgeacquisition,23,307,860 KtesibiosofAlexandria,15
Kersting,K.,556,1078,1082 knowledgecompilation,799 Ku¨bler,S.,920,1079
Kessler,B.,862,883,1078 knowledgemap,seeBayesiannetwork Kuhn,H.W.,601,606,687,1079
Keynes,J.M.,504,1078 knowledgerepresentation,2,16,19,24, Kuhns,J.-L.,884,1081
Khare,R.,469,1078 234,285–290,437–479 Kuijpers,C.,158,1080
Khatib,O.,1013,1078 analogical,315 Kuipers,B.J.,472,473,1012,1079
Khmelev,D.V.,886,1078 everything,437 Kumar,P.R.,60,1079
Khorsand,A.,112,1077 language,235,274,285 Kumar,V.,111,112,230,1074,1077,
Kietz,J.-U.,800,1078 uncertain,510–513 1079,1083
Kilgarriff,A.,27,1078 Knuth,D.E.,73,191,359,919,1013, Kuniyoshi,Y.,195,1014,1078
killermove,170 1059,1074,1078 Kuppuswamy,N.,1022,1078
Kim,H.J.,852,857,1013,1084 Kobilarov,G.,439,469,1066 Kurien,J.,157,1079
Kim,J.-H.,1022,1078 Kocsis,L.,194,1078 Kurzweil,R.,2,12,28,1038,1079
Kim,J.H.,552,1078 Koditschek,D.,1013,1078 Kwok,C.,885,1079
Kim,M.,194 Koehler,J.,395,1078 Kyburg,H.E.,505,1079
kinematics,987 Koehn,P.,922,1078
L
kinematicstate,975 Koenderink,J.J.,968,1078
King,R.D.,797,1078,1089 Koenig,S.,157,395,434,685,1012,
L-BFGS,760
Kinsey,E.,109 1075,1078,1088
label(inplans),137,158
kinshipdomain,301–303 Koller,D.,191,505,553,556,558,604,
Laborie,P.,432,1079
Kirchner,C.,359,1077 605,639,677,686,687,826,
Ladanyi,L.,112,1086
Kirk,D.E.,60,1078 827,884,1012,1065,1066,
Ladkin,P.,470,1079
Kirkpatrick,S.,155,229,1078 1073,1074,1076–1078,1083,
Lafferty,J.,884,885,1079
Kirman,J.,686,1070 1085,1087,1090
Lagoudakis,M.G.,854,857,1074,
Kishimoto,A.,194,1088 Kolmogorov’saxioms,489
1079
Kister,J.,192,1078 Kolmogorov,A.N.,504,604,759,1078
Laguna,M.,154,1074
Kisynski,J.,556,1078 Kolmogorovcomplexity,759
Laird,J.,26,336,358,432,799,1047,
Kitano,H.,195,1014,1078 Kolobov,A.,556,1082
1077,1079,1092
Kjaerulff,U.,604,1078 Kolodner,J.,24,799,1078
Laird,N.,604,826,1070
KL-ONE,471 Kondrak,G.,229,230,1078
Laird,P.,154,229,1082
Kleer,J.D.,60,1074 Konolige,K.,229,434,472,1012,
Lake,R.,194,1088
Klein,D.,883,896,900,920,921, 1013,1067,1078,1079 Lakemeyer,G.,1012,1067
1074,1078,1085 Koo,T.,920,1079 Lakoff,G.,469,921,1041,1079
Kleinberg,J.M.,884,1078 Koopmans,T.C.,685,1079 Lam,J.,195,1079
Klemperer,P.,688,1078 Korb,K.B.,558,687,1079 LAMA,387,395
Klempner,G.,553,1083 Koren.,Y.,1013,1066 Lamarck,J.B.,130,1079
Kneser,R.,883,1078 Korf,R.E.,110–112,157,191,394, Lambert’scosinelaw,934
Knight,B.,20,1066 395,1072,1079,1085 Lambertiansurface,969
Knight,K.,2,922,927,1078,1086 Kortenkamp,D.,1013,1069 Landauer,T.K.,883,1070
Knoblock,C.A.,394,432,1068,1073 Koss,F.,1013,1069 Landhuis,E.,620,1079
KNOWITALL,885 Kotok,A.,191,192,1079 landmark,980
knowledge Koutsoupias,E.,154,277,1079 landscape(instatespace),121
acquisition,860 Kowalski,R.,282,314,339,345,359, Langdon,W.,156,1079,1085
andaction,7,453 470,472,1079,1087,1091 Langley,P.,800,1079
background,235,349,777,1024, Kowalskiform,282,345 Langlotz,C.P.,26,1076
1025 Koza,J.R.,156,1079 Langton,C.,155,1079
base(KB),235,274,315 Kramer,S.,556,1078 language,860,888,890
commonsense,19 Kraus,S.,434,1079 abhorssynonyms,870
diagnostic,497 Kraus,W.F.,155,1080 formal,860
engineering,307,307–312,514 Krause,A.,639,1079 model,860,909,913
fordecision-theoreticsystems,634 Krauss,P.,555,1088 indisambiguation,906
level,236,275 Kriegspiel,180 natural,4,286,861
Index 1113
processing,16,860 computationaltheory,713 Lederberg,J.,23,468,1072,1080
translation,21,784,907–912 decisionlists,715–717 Lee,C.-H.,1022,1078
understanding,20,23 decisiontrees,697–703 Lee,K.-H.,1022,1078
languagegeneration,899 determinations,785 Lee,M.S.,826,1083
languageidentification,862 element,55 Lee,R.C.-T.,360,1068
Laplace,P.,9,491,504,546,883,1079 ensemble,748,748–752 Lee,T.-M.,11,1084
Laplacesmoothing,863 explanation-based,780–784 Leech,G.,920,921,1080,1086
Laptev,I.,961,1080 gameplaying,850–851 legalreasoning,32
large-scalelearning,712 grammar,921 Legendre,A.M.,759,1080
Lari,K.,896,920,1080 heuristics,107 Lehmann,D.,434,1079
Larkey,P.D.,687,1077 hiddenMarkovmodel,822–823 Lehmann,J.,439,469,1066
Larran˜aga,P.,158,1080 hiddenvariables,820 Lehrer,J.,638,1080
Larsen,B.,553,1080 hiddenvariables,822 Leibniz,G.W.,6,131,276,504,687
Larson,G.,778 incremental,773,777 Leimer,H.,553,1080
Larson,S.C.,759,1080 inductive,694,695–697 Leipzig,12
Laruelle,H.,395,431,1073 knowledge-based,779,788,798 Leiserson,C.E.,1059,1069
Laskey,K.B.,556,1080 instance-based,737,737–739,855 Lempel-Ziv-Welchcompression(LZW),
Lassez,J.-L.,359,1077 knowledgein,777–780 867
Lassila,O.,469,1065 linearlyseparablefunctions,731 Lenat,D.B.,27,439,469,474,800,
latentDirichletallocation,883 logical,768–776 1070,1075,1080
latentsemanticindexing,883 MAP,804–805 lenssystem,931
latentvariable,816 maximumlikelihood,806–810 Lenstra,J.K.,110,405,432,1080
Latham,D.,856,1081 metalevel,102 Lenzerini,M.,471,1067
Latombe,J.-C.,432,1012,1013,1071, mixturesofGaussians,817–820 Leonard,H.S.,470,1080
1078,1080,1093 naiveBayes,808–809 Leonard,J.,1012,1066,1080
latticetheory,360 neuralnetwork,16,736–737 Leone,N.,230,472,1071,1074
Laugherty,K.,920,1074 newpredicates,790,796 Lesh,N.,433,1072
Lauritzen,S.,553,558,639,826,1069, noise,705–706 Les´niewski,S.,470,1080
1080,1084,1089 nonparametric,737 Lesser,V.R.,434,1071
LaValle,S.,396,1013,1014,1080 online,752,846 Lettvin,J.Y.,963,1080
Lave,R.E.,686,1087 PAC,714,759,784 Letz,R.,359,1080
Lavrauc,N.,796,799,800,1080,1082 parameter,806,810–813 level(inplanninggraphs),379
LAWALY,432 passive,831 levelcost,382
Lawler,E.L.,110,111,405,432,1080 Q,831,843,844,848,973 leveledoff(planninggraph),381
lawsofthought,4 rateof,719,836 Levesque,H.J.,154,277,434,471,
layers,729 reinforcement,685,695,830–859, 473,1067,1069,1080,1088
Lazanas,A.,1013,1080 1025 Levin,D.A.,604,1080
laziness,481 inverse,857 Levinson,S.,314,1066,1074
LaMettrie,J.O.,1035,1041,1079 relational,857 Levitt,G.M.,190,1080
LaMura,P.,638,1079 relevance-based,784–787 Levitt,R.E.,434,1077
LCF,314 restaurantproblem,698 Levitt,T.S.,1012,1079
Leacock,C.,1022,1067 statistical,802–805 Levy,D.,195,1022,1080
leafnode,75 temporaldifference,836–838,853, Lewis,D.D.,884,1080
leaknode,519 854 Lewis,D.K.,60,1042,1080
Leaper,D.J.,505,1070 top-down,791–794 LEX,776,799
leapingtoconclusions,778 tosearch,102 lexicalcategory,888
learning,39,44,59,236,243,693, unsupervised,694,817–820,1025 lexicalizedgrammar,897
1021,1025 utilityfunctions,831 lexicalizedPCFG,897,919,920
active,831 weak,749 lexicon,890,920
apprenticeship,857,1037 learningcurve,702 Leyton-Brown,K.,230,435,688,1080,
assessingperformance,708–709 least-constraining-valueheuristic,217 1088
Bayesian,752,803,803–804,825 leastcommitment,391 LFG,920
Bayesiannetwork,813–814 leave-one-outcross-validation Li,C.M.,277,1080
blocks-world,20 (LOOCV),708 Li,H.,686,1077
cart–poleproblem,851 LeCun,Y.,760,762,967,1047,1065, Li,M.,759,1080
checkers,18 1080,1086 liability,1036
1114 Index
Liang,G.,553,1068 Liu,J.S.,605,1080 logicalreasoning,249–264,284
Liang,L.,604,1083 Liu,W.,826,1068 logicism,4
Liao,X.,686,1077 Liu,X.,604,1083 logicprogramming,257,314,337,
Liberatore,P.,279,1080 Livescu,K.,604,1080 339–345
Lifchits,A.,885,1085 Livnat,A.,434,1080 constraint,344–345,359
lifeinsurance,621 lizardtoasting,778 inductive(ILP),779,788–794,798
Lifschitz,V.,472,473,1073,1080, localbeamsearch,125,126 tabled,343
1091 localconsistency,208 LogicTheorist,17,276
lifting,326,325–329,367 locality,267,547 LOGISTELLO,175,186
inprobabilisticinference,544 locality-sensitivehash(LSH),740 logisticfunction,522,760
liftinglemma,350,353 localization,145,581,979 logisticregression,726
light,932 Markov,1012 logitdistribution,522
Lighthill,J.,22,1080 locallystructuredsystem,515 loglikelihood,806
Lighthillreport,22,24 locallyweightedregression,742 Lohn,J.D.,155,1080
likelihood,803 localoptimum,669 London,14
LIKELIHOOD-WEIGHTING,534 localsearch,120–129,154,229, Long,D.,394,395,1072,1073
likelihoodweighting,532,552,596 262–263,275,277 long-distancedependencies,904
Lim,G.,439,1089 locationsensors,974 long-termmemory,336
limitedrationality,5 Locke,J.,6,1042,1080 Longley,N.,692,1080
Lin,D.,885,1085 Lodge,D.,1051,1080 Longuet-Higgins,H.C.,1080
Lin,J.,872,885,1065 LoebnerPrize,1021 Loo,B.T.,275,1080
Lin,S.,110,688,1080,1092 Loftus,E.,287,1080 LOOCV,708
Lin,T.,439,1089 Logemann,G.,260,276,1070 Lookma,nohands,18
Lincoln,A.,872 logic,4,7,240–243 lookuptable,736
Lindley,D.V.,639,1080 atoms,294–295 Loomes,G.,637,1086
Lindsay,R.K.,468,1080 default,459,468,471 looselycoupledsystem,427
linear-chainconditionalrandomfield, equalityin,299 Lorenz,U.,193,1071
878 first-order,285,285–321 lossfunction,710
linearalgebra,1055–1057 inference,322–325 Lotem,A.,396,1091
linearconstraint,205 semantics,290 lottery,612,642
linearfunction,717 syntax,290 standard,615
linearGaussian,520,553,584,809 fuzzy,240,289,547,550,557 love,1021
linearization,981 higher-order,289 Love,N.,195,1080
linearprogramming,133,153,155,206, inductive,491,505 Lovejoy,W.S.,686,1080
673 interpretations,292–294 Lovelace,A.,14
linearregression,718,810 modelpreference,459 Loveland,D.,260,276,359,1070,1080
linearresolution,356,795 models,290–292 low-dimensionalembedding,985
linearseparability,723 nonmonotonic,251,458,458–460, Lowe,D.,947,967,968,1081
linearseparator,746 471 Lo¨wenheim,L.,314,1081
linesearch,132 notation,4 Lowerre,B.T.,154,922,1081
linguistics,15–16 propositional,235,243–247,274,286 Lowrance,J.D.,557,1087
link,870 inference,247–263 Lowry,M.,356,360,1075,1081
link(inaneuralnetwork),728 semantics,245–246 Loyd,S.,109,1081
linkageconstraints,986 syntax,244–245 Lozano-Perez,T.,1012,1013,1067,
Linnaeus,469 quantifier,295–298 1081,1092
LINUS,796 resolution,252–256 LPG,387,395
Lipkis,T.A.,471,1088 sampling,554 LRTA*,151,157,415
liquidevent,447 temporal,289 LRTA*-AGENT,152
liquids,472 terms,294 LRTA*-COST,152
Lisp,19,294 variablein,340 LSH(locality-sensitivehash),740
lists,305 logicalconnective,16,244,274,295 LT,17
literal(sentence),244 logicalinference,242,322–365 Lu,F.,1012,1081
literal,watched,277 logicalminimization,442 Lu,P.,194,760,1067,1088
Littman,M.L.,155,231,433,686,687, logicalomniscience,453 Luby,M.,124,554,1069,1081
857,1064,1068,1077,1080, logicalpiano,276 Lucas,J.R.,1023,1081
1081 logicalpositivism,6 Lucas,P.,505,634,1081
Index 1115
Luce,D.R.,9,687,1081 Manning,C.,883–885,920,921,1078, Mason,M.,156,433,1013,1014,1071,
Lucene,868 1081 1081
Ludlow,P.,1042,1081 Mannion,M.,314,1081 Mason,R.A.,288,1082
Luger,G.F.,31,1081 Manolios,P.,360,1077 mass(inDempster–Shafertheory),549
Lugosi,G.,761,1068 Mansour,Y.,686,764,855,856,1078, massnoun,445
Lull,R.,5 1090 massspectrometer,22
Luong,Q.-T.,968,1072 mantisshrimp,935 Mataric,M.J.,1013,1081
Lusk,E.,360,1092 Manzini,G.,111,1081 Mateescu,R.,230,1070
Lygeros,J.,60,1068 map,65 Mateis,C.,472,1071
Lyman,P.,759,1081 MAP(maximumaposteriori),804 materialism,6
Lynch,K.,1013,1014,1068 MAPGEN,28 materialvalue,172
LZW,867 Marais,H.,884,1088 Mates,B.,276,1081
Marbach,P.,855,1081 mathematicalinductionschema,352
M March,J.G.,637,1075 mathematics,7–9,18,30
Marcinkiewicz,M.A.,895,921,1081 Matheson,J.E.,626,638,1076,1082
MA*search,101,101–102,112 Marcot,B.,553,1086 matrix,1056
MACHACK-6,192 Marcus,G.,638,1081 Matsubara,H.,191,195,1072,1078
Machina,M.,638,1081 Marcus,M.P.,895,921,1081 Maturana,H.R.,963,1080
machineevolution,21 margin,745 Matuszek,C.,469,1081
machinelearning,2,4 marginalization,492 Mauchly,J.,14
machinereading,881 Markov Mausam.,432,1069
machinetranslation,32,907–912,919
assumption MAVEN,195
statistical,909–912
sensor,568 MAX-VALUE,166,170
Machover,M.,314,1065
process maximin,670
MacKay,D.J.C.,555,761,763,1081,
first-order,568 maximinequilibrium,672
1082
Markov,A.A.,603,883,1081 maximum
MacKenzie,D.,360,1081
Markovassumption,568,603 global,121
Mackworth,A.K.,2,59,209,210,228,
Markovblanket,517,560 local,122
230,1072,1081,1085
Markovchain,537,568,861 maximumaposteriori,804,825
macrop(macrooperator),432,799
MarkovchainMonteCarlo(MCMC), maximumexpectedutility,483,611
madalines,761
535,535–538,552,554,596 maximumlikelihood,805,806–810,
Madigan,C.F.,277,1083
decayed,605 825
magicsets,336,358
Markovdecisionprocess(MDP),10, maximummarginseparator,744,745
Mahalanobisdistance,739
647,684,686,830 maxnorm,654
Mahanti,A.,112,1081
Mahaviracarya,503
factored,686 MAXPLAN,387
Maheswaran,R.,230,1085
partiallyobservable(POMDP),658, Maxwell,J.,546,920,1081
Maier,D.,229,358,1065 658–666,686 Mayer,A.,112,119,1075
Mailath,G.,688,1081 Markovgames,857 Mayne,D.Q.,605,1075
Majercik,S.M.,433,1081 Markovnetwork,553 Mazumder,P.,110,1088
majorityfunction,731 Markovprocess,568 Mazurie,A.,605,1085
makespan,402 Markovproperty,577,603,646 MBP,433
Makov,U.E.,826,1090 Maron,M.E.,505,884,1081 McAllester,D.A.,25,156,191,198,
Malave,V.L.,288,1082 Marr,D.,968,1081 394,395,472,855,856,1072,
Maldague,P.,28,1064 Marriott,K.,228,1081 1077,1081,1090
Mali,A.D.,432,1077 Marshall,A.W.,855,1077 MCC,24
Malik,J.,604,755,762,941,942,953, Marsland,A.T.,195,1081 McCallum,A.,877,884,885,1069,
967,968,1065,1070,1076, Marsland,S.,763,1081 1072,1077,1079,1081,1084,
1081,1088 Martelli,A.,110,111,156,1081 1085,1090
Malik,S.,277,1083 Marthi,B.,432,556,605,856,1081, McCarthy,J.,17–19,27,59,275,279,
Manchak,D.,470,1091 1082,1085 314,395,440,471,1020,1031,
Maneva,E.,278,1081 Martin,D.,941,967,1081 1081,1082
Maniatis,P.,275,1080 Martin,J.H.,885,886,920–922,1077, McCawley,J.D.,920,1082
manipulator,971 1081 McClelland,J.L.,24,1087
Manna,Z.,314,1081 Martin,N.,358,1067 McClure,M.,604,1065
Mannila,H.,763,1074 Martin,P.,921,1076 McCorduck,P.,1042,1082
1116 Index
McCulloch,W.S.,15,16,20,278,727, metadata,870 MINIMAL-CONSISTENT-DET,786
731,761,963,1080,1082 metalevel,1048 minimalmodel,459
McCune,W.,355,360,1082 metalevelstatespace,102 MINIMAX-DECISION,166
McDermott,D.,2,156,358,394,433, metaphor,906,921 minimaxalgorithm,165,670
434,454,470,471,1068,1073, metaphysics,6 minimaxdecision,165
1082 metareasoning,189 minimaxsearch,165–168,188,189
McDermott,J.,24,336,358,1082 decision-theoretic,1048 minimaxvalue,164,178
McDonald,R.,288,920,1079 metarule,345 minimum
McEliece,R.J.,555,1082 meteorite,422,480 global,121
McGregor,J.J.,228,1082 metonymy,905,921 local,122
McGuinness,D.,457,469,471,1064, Metropolis,N.,155,554,1082 minimum-remaining-values,216,333
1066,1089 Metropolis–Hastings,564 minimumdescriptionlength(MDL),
McIlraith,S.,314,1082 Metropolisalgorithm,155,554 713,759,805
McLachlan,G.J.,826,1082 Metzinger,T.,1042,1082 minimumslack,405
McMahan,B.,639,1079 Metzler,D.,884,1069 minimumspanningtree(MST),112,
MCMC,535,535–538,552,554,596 MEXAR2,28 119
McMillan,K.L.,395,1082 Meyer,U.,112,1069 MINISAT,277
McNealy,S.,1036 Me´zard,M.,762,1082 Minker,J.,358,473,1073,1082
McPhee,N.,156,1085 MGONZ,1021 Minkowskidistance,738
MDL,713,759,805 MGSS*,191 Minsky,M.L.,16,18,19,22,24,27,
MDP,10,647,684,686,830 MGU(mostgeneralunifier),327,329, 434,471,552,761,1020,1039,
mean-fieldapproximation,554 353,361 1042,1082
measure,444 MHT(multiplehypothesistracker),606 Minton,S.,154,229,432,799,1068,
measurement,444 Mian,I.S.,604,605,1079,1083 1082
mechanism,679 Michalski,R.S.,799,1082 Miranker,D.P.,229,1065
strategy-proof,680 Michaylov,S.,359,1077 Misak,C.,313,1082
mechanismdesign,679,679–685 Michie,D.,74,110,111,156,191,763, missingattributevalues,706
medicaldiagnosis,23,505,517,548, 851,854,1012,1071,1082 missionariesandcannibals,109,115,
629,1036 micro-electromechanicalsystems 468
Meehan,J.,358,1068 (MEMS),1045 MIT,17–19,1012
Meehl,P.,1022,1074,1082 micromort,616,637,642 Mitchell,D.,154,277,278,1069,1088
Meek,C.,553,1092 Microsoft,553,874 Mitchell,M.,155,156,1082
Meet(intervalrelation),448 microworld,19,20,21 Mitchell,T.M.,61,288,763,776,798,
Megarianschool,275 Middleton,B.,519,552,1086 799,884,885,1047,1066,1067,
megavariable,578 Miikkulainen,R.,435,1067 1069,1082,1084
Meggido,N.,677,687,1078 Milch,B.,556,639,1078,1082,1085 Mitra,M.,870,1089
Mehlhorn,K.,112,1069 Milgrom,P.,688,1082 mixedstrategy,667
melfrequencycepstralcoefficient Milios,E.,1012,1081 mixingtime,573
(MFCC),915 militaryusesofAI,1035 mixture
Mellish,C.S.,359,1068 Mill,J.S.,7,770,798,1082 distribution,817
memoization,343,357,780 Miller,A.C.,638,1082 mixturedistribution,817
memoryrequirements,83,88 Miller,D.,431,1070 mixtureofGaussians,608,817,820
MEMS,1045 millionqueensproblem,221,229 Mizoguchi,R.,27,1075
Mendel,G.,130,1082 Millstein,T.,395,1071 ML,seemaximumlikelihood
meningitis,496–509 Milner,A.J.,314,1074 modallogic,451
mentalmodel,indisambiguation,906 MIN-CONFLICTS,221 model,50,240,274,289,313,451
mentalobjects,450–453 min-conflictsheuristic,220,229 causal,517
mentalstates,1028 MIN-VALUE,166,170 (inrepresentation),13
Mercer’stheorem,747 mind,2,1041 sensor,579,586,603
Mercer,J.,747,1082 dualisticview,1041 theory,314
Mercer,R.L.,883,922,1067,1077 andmysticism,12 transition,67,108,134,162,266,
mereology,470 philosophyof,1041 566,597,603,646,684,832,
Merkhofer,M.M.,638,1082 asphysicalsystem,6 979
Merleau-Ponty,M.,1041,1082 theoryof,3 MODEL-BASED-REFLEX-AGENT,51
Meshulam,R.,112,1072 mind–bodyproblem,1027 model-basedreflexagents,59
Meta-DENDRAL,776,798 minesweeper,284 modelchecking,242,274
Index 1117
modelselection,709,825 mostgeneralunifier(MGU),327,329, mutation,21,128,153
ModusPonens,250,276,356,357,361 353,361 mutex,380
Generalized,325,326 mostlikelyexplanation,553,603 mutualexclusion,380
Moffat,A.,884,1092 mostlikelystate,993 mutualpreferentialindependence
MOGO,186,194 Mostow,J.,112,119,1083 (MPI),625
Mohr,R.,210,228,968,1082,1088 motion,948–951 mutualutilityindependence(MUI),626
Mohri,M.,889,1083 compliant,986,995 MYCIN,23,548,557
Molloy,M.,277,1064 guarded,995 Myerson,R.,688,1083
monism,1028 motionblur,931 myopicpolicy,632
monitoring,145 motionmodel,979 mysticism,12
monkeyandbananas,113,396 motionparallax,949,966
monotonecondition,110 motionplanning,986 N
monotonicity Motwani,R.,682,760,1064,1073
ofaheuristic,95 Motzkin,T.S.,761,1083 n-armedbandit,841
ofalogicalsystem,251,458 Moutarlier,P.,1012,1083 n-grammodel,861
ofpreferences,613 movies Nadal,J.-P.,762,1082
Montague,P.R.,854,1083,1088 movies Nagasawa,Y.,1042,1081
Montague,R.,470,471,920,1077, 2001:ASpaceOdyssey,552 Nagel,T.,1042,1083
1083 movies Na¨ım,P.,553,1086
Montanari,U.,111,156,228,1066, A.I.,1040 naiveBayes,499,503,505,808–809,
1081,1083 movies 820,821,825
MONTE-CARLO-LOCALIZATION,982 TheMatrix,1037 naked,214
movies Nalwa,V.S.,12,1083
MonteCarlo(ingames),183
TheTerminator,1037 Naor,A.,278,1064
MonteCarlo,sequential,605
Mozetic,I.,799,1082 Nardi,D.,471,1064,1067
MonteCarloalgorithm,530
MPI(mutualpreferential narrowcontent,1028
MonteCarlolocalization,981
independence),625 NASA,28,392,432,472,553,972
MonteCarlosimulation,180
MRS(metalevelreasoningsystem),345 Nash,J.,1083
Montemerlo,M.,1012,1083
MST,112,119 Nashequilibrium,669,685
Mooney,R.,799,902,921,1070,1083,
Mueller,E.T.,439,470,1083,1089 NASL,434
1093
Muggleton,S.H.,789,795,797,800, NATACHATA,1021
Moore’sLaw,1038
921,1071,1083,1089,1090 naturalkind,443
Moore,A.,826,1083
Mu¨ller,M.,186,194,1083,1088 naturalnumbers,303
Moore,A.W.,154,826,854,857,1066,
Muller,U.,762,967,1080 naturalstupidity,454
1077,1083
multiagentenvironments,161 Nau,D.S.,111,187,191,192,195,
Moore,E.F.,110,1083
multiagentplanning,425–430 372,386,395,396,432,
Moore,J.S.,356,359,360,1066,1077
multiagentsystems,60,667 1071–1073,1079,1083,1084,
Moore,R.C.,470,473,922,1076,1083
multiattributeutilitytheory,622,638 1089,1091
Moravec,H.P.,1012,1029,1038,1083
multibodyplanning,425,426–428 navigationfunction,994
More,T.,17
multiplexer,543 Nayak,P.,60,157,432,472,1079,1083
Morgan,J.,434,1069
multiplyconnectednetwork,528 Neal,R.,762,1083
Morgan,M.,27,1069 multivariatelinearregression,720 Nealy,R.,193
Morgan,N.,922,1074 Mumford,D.,967,1083 nearest-neighborfilter,601
Morgenstern,L.,470,472,473,1070, MUNIN,552 nearest-neighbors,738,814
1083 Murakami,T.,186 nearest-neighborsregression,742
Morgenstern,O.,9,190,613,637,1091 Murga,R.,158,1080 neatvs.scruffy,25
Moricz,M.,884,1088 Murphy,K.,555,558,604,605,1012, Nebel,B.,394,395,1076,1078,1083
Morjaria,M.A.,553,1083 1066,1071,1073,1083,1090 needleinahaystack,242
Morris,A.,604,1089 Murphy,R.,1014,1083 Nefian,A.,604,1083
Morris,P.,28,60,431,1064,1077 Murray-Rust,P.,469,1083 negation,244
Morrison,E.,190,1083 Murthy,C.,360,1083 negativeexample,698
Morrison,P.,190,1083 Muscettola,N.,28,60,431,432,1077, negativeliteral,244
Moses,Y.,470,477,1072 1083 negligence,1036
Moskewicz,M.W.,277,1083 music,14 Nelson,P.C.,111,1071
Mossel,E.,278,1081 Muslea,I.,885,1083 Nemirovski,A.,155,1065,1083
Mosteller,F.,886,1083 mutagenicity,797 NERO,430,435
1118 Index
Nesterov,Y.,155,1083 Nivre,J.,920,1079 noughtsandcrosses,162,190,197
Netto,E.,110,1083 Nixon,R.,459,638,906 Nourbakhsh,I.,156,1073
networktomography,553 Nixondiamond,459 Nowak,R.,553,1068
neuralnetwork,16,20,24,186,727, Niyogi,S.,314,1090 Nowatzyk,A.,192,1076
727–737 NLP(naturallanguageprocessing),2, Nowick,S.M.,279,1084
expressiveness,16 860 Nowlan,S.J.,155,1075
feed-forward,729 no-good,220,385 NP(hardproblems),1054–1055
hardware,16 no-regretlearning,753 NP-complete,8,71,109,250,276,471,
learning,16,736–737 NOAH,394,433 529,762,787,1055
multilayer,22,731–736 NobelPrize,10,22 NQTHM,360
perceptron,729–731 Nocedal,J.,760,1067 NSSchessprogram,191
radialbasisfunction,762 Noda,I.,195,1014,1078 nuclearpower,561
singlelayer,seeperceptron node numbertheory,800
neurobiology,968 child,75 Nunberg,G.,862,883,921,1078,1084
NEUROGAMMON,851 current,inlocalsearch,121 NUPRL,360
neuron,10,16,727,1030 parent,75 Nussbaum,M.C.,1041,1084
neuroscience,10,10–12,728 nodeconsistency,208 Nyberg,L.,11,1067
computational,728 Noe,A.,1041,1084
Nevill-Manning,C.G.,921,1083 noise,701,705–706,712,776,787,802 O
NEW-CLAUSE,793 noisy-AND,561
Newborn,M.,111,1085 noisy-OR,518 O()notation,1054
Newell,A.,3,17,18,26,60,109,110, noisychannelmodel,913 O’Malley,K.,688,1092
191,275,276,336,358,393, nominativecase,899 O’Reilly,U.-M.,155,1084
432,799,1047,1079,1084, nondeterminism O-PLAN,408,431,432
1089 angelic,411 Oaksford,M.,638,1068,1084
Newman,P.,1012,1066,1071 demonic,410 object,288,294
Newton,I.,1,47,131,154,570,760, nondeterministicenvironment,43 composite,442
1084 nonholonomic,976 object-levelstatespace,102
Newton–Raphsonmethod,132 NONLIN,394 object-orientedprogramming,14,455
Ney,H.,604,883,922,1078,1084 NONLIN+,431,432 objectivecase,899
Ng,A.Y.,686,759,850,852,855–857, nonlinear,589 objectivefunction,15,121
883,1013,1066,1068,1078, nonlinearconstraints,205 objectivism,491
1084 nonmonotonicity,458 objectmodel,928
Nguyen,H.,883,1078 nonmonotoniclogic,251,458, observable,42
Nguyen,X.,394,395,1084 458–460,471 observationmodel,568
Niblett,T.,800,1068 Nono,330 observationprediction,142
Nicholson,A.,558,604,686,687, nonstationary,857 observationsentences,6
1070,1079,1084 nonterminalsymbol,889,890,1060 occupancygrid,1012
Nielsen,P.E.,358,1077 Normal–Wishart,811 occupiedspace,988
Niemela¨,I.,472,1084 normaldistribution,1058 occurcheck,327,340
Nigam,K.,884,885,1069,1077,1084 standard,1058 Och,F.J.,29,604,921,922,1067,
Nigenda,R.S.,395,1084 normalform,667 1084,1093
Niles,I.,469,1084,1085 normalization(ofaprobability Ockham’srazor,696,757–759,777,
Nilsson,D.,639,1084 distribution),493 793,805
Nilsson,N.J.,2,27,31,59,60, normalization(ofattributeranges),739 Ockham,W.,696,758
109–111,119,156,191,275, Norman,D.A.,884,1066 Oddi,A.,28,1068
314,350,359,367,393,432, normativetheory,619 odometry,975
434,555,761,799,1012,1019, North,O.,330 Odyssey,1040
1034,1072,1073,1075,1084, North,T.,21,1072 OfficeAssistant,553
1091 Norvig,P.,28,358,444,470,604,759, offlinesearch,147
Nine-Men’sMorris,194 883,921,922,1074,1078,1084, Ogasawara,G.,604,1076
Niranjan,M.,605,855,1070,1087 1087 Ogawa,S.,11,1084
Nisan,N.,688,1084 notation Oglesby,F.,360,1074
NIST,753 infix,303 Oh,S.,606,1084
nitroaromaticcompounds,797 logical,4 Ohashi,T.,195,1091
Niv,Y.,854,1070 prefix,304 Olalainty,B.,432,1073
Index 1119
Olesen,K.G.,552–554,1064,1084 ORnode,135 parameterindependence,812
Oliver,N.,604,1084 Osawa,E.,195,1014,1078 parametricmodel,737
Oliver,R.M.,639,1084 Osborne,M.J.,688,1084 paramodulation,354,359
Oliver,S.G.,797,1078 Oscar,435 Parekh,R.,921,1084
Olshen,R.A.,758,1067 Osherson,D.N.,759,1084 Paretodominated,668
omniscience,38 Osindero,S.,1047,1075 Paretooptimal,668
Omohundro,S.,27,920,1039,1084, Osman,I.,112,1086 Parisi,D.,921,1071
1089 Ostland,M.,556,606,1085 Parisi,G.,555,1084
Ong,D.,556,1082 Othello,186 Parisi,M.M.G.,278,1084
ONLINE-DFS-AGENT,150 OTTER,360,364 Park,S.,356,1075
onlinelearning,752,846 OUPM,545,552 Parker,A.,192,1084
onlineplanning,415 outcome,482,667 Parker,D.B.,761,1084
onlinereplanning,993 outofvocabulary,864 Parker,L.E.,1013,1084
onlinesearch,147,147–154,157 Overbeek,R.,360,1092 Parr,R.,686,854,856,857,1050,1074,
ontologicalcommitment,289,313,482, overfitting,705,705–706,736,802,805 1077–1079,1084,1087
547 overgeneration,892 Parrod,Y.,432,1064
ontologicalengineering,437,437–440 overhypotheses,798 parsetree,890
ontology,308,310 Overmars,M.,1013,1078 parsing,892,892–897
upper,467 overriding,456 Partee,B.H.,920,1086
open-coding,341 Owens,A.J.,156,1072 partialassignment,203
open-loop,66 OWL,469 partialevaluation,799
open-universeprobabilitymodel partialobservability,180,658
P
(OUPM),545,552 partialprogram,856
open-worldassumption,417 PARTICLE-FILTERING,598
P(probabilityvector),487
openclass,890 P(s(cid:3)|s,a)(transitionmodel),646,832 particlefiltering,597,598,603,605
OPENCYC,469 Rao-Blackwellized,605,1012
PAClearning,714,716,759
openlist,seefrontier partition,441
Padgham,L.,59,1084
OPENMIND,439
Page,C.D.,800,1069,1084
partof,441
operationality,783 partofspeech,888
Page,L.,870,884,1067
operationsresearch,10,60,110,111 Parzen,E.,827,1085
PageRank,870
Oppacher,F.,155,1084 Parzenwindow,827
Palacios,H.,433,1084
OPS-5,336,358
Palay,A.J.,191,1084
Pasca,M.,885,1071,1085
opticalflow,939,964,967 Pascal’swager,504,637
Palmer,D.A.,922,1084
optimalbraindamage,737 Pascal,B.,5,9,504
Palmer,J.,287,1080
optimalcontrollers,997 Pasero,R.,314,358,1069
Palmer,S.,968,1084
optimalcontroltheory,155 Paskin,M.,920,1085
Palmieri,G.,761,1073
optimality,121 Panini,16,919 PASSIVE-ADP-AGENT,834
optimality(ofasearchalgorithm),80, Papadimitriou,C.H.,154,157,277, PASSIVE-TD-AGENT,837
108 685,686,883,1059,1070,1079, passivelearning,831
optimalitytheory(Linguistics),921 1084 Pasula,H.,556,605,606,1081,1085
optimallyefficientalgorithm,98 Papadopoulo,T.,968,1072 Patashnik,O.,194,1085
optimalsolution,68 Papavassiliou,V.,855,1084 Patel-Schneider,P.,471,1064
optimismunderuncertainty,151 Papert,S.,22,761,1082 path,67,108,403
optimisticdescription(ofanaction),412 PARADISE,189 loopy,75
optimisticprior,842 paradox,471,641 redundant,76
optimization,709 Allais,620 pathconsistency,210,228
convex,133,153 Ellsberg,620 pathcost,68,108
optimizer’scurse,619,637 St.Petersburg,637 PATHFINDER,552
OPTIMUM-AIV,432 paralleldistributedprocessing,see pathplanning,986
OR-SEARCH,136 neuralnetwork Patil,R.,471,894,920,1068,1071
orderability,612 parallelism Patrick,B.G.,111,1085
ordinalutility,614 AND-,342 Patrinos,A.,27,1069
Organon,275,469 OR-,342 patterndatabase,106,112,379
orientation,938 parallellines,931 disjoint,107
originfunction,545 parallelsearch,112 patternmatching,333
Ormoneit,D.,855,1084 parameter,520,806 Paul,R.P.,1013,1085
1120 Index
Paulin-Mohring,C.,359,1066 perceptschema,416 Pi,X.,604,1083
Paull,M.,277,1072 perceptsequence,34,37 Piccione,C.,687,1093
Pauls,A.,920,1085 Pereira,F.,28,339,341,470,759,761, Pickwick,Mr.,1026
Pavlovic,V.,553,1093 884,885,889,919,1025,1071, pictorialstructuremodel,958
Pax-6gene,966 1074,1079,1083,1085,1088, PIDcontroller,999
payofffunction,162,667 1091 Pieper,G.,360,1092
Pazzani,M.,505,826,1071 Pereira,L.M.,341,1091 pigeons,13
PCFG Peres,Y.,278,604,605,1064,1080, Pijls,W.,191,1085
lexicalized,897,919,920 1081 pinealgland,1027
Pcontroller,998 Perez,P.,961,1080 Pineau,J.,686,1013,1085
PDcontroller,999 perfectinformation,666 Pinedo,M.,432,1085
PDDL(PlaningDomainDefinition perfectrecall,675 ping-pong,32,830
Language),367 performanceelement,55,56 pinholecamera,930
PDP(paralleldistributedprocessing), performancemeasure,37,40,59,481, Pinkas,G.,229,1085
761 611 Pinker,S.,287,288,314,921,1085,
Peano,G.,313,1085 Perkins,T.,439,1089 1087
Peanoaxioms,303,313,333 Perlis,A.,1043,1085 Pinto,D.,885,1085
Pearce,J.,230,1085 Perona,P.,967,1081 Pipatsrisawat,K.,277,1085
Pearl,J.,26,61,92,110–112,154,191, perpetualpunishment,674 Pippenger,N.,434,1080
229,509,511,517,549, perplexity,863 Pisa,towerof,56
552–555,557,558,644,826, Perrin,B.E.,605,1085 Pistore,M.,275,1088
827,1070,1073,1074,1076, persistenceaction,380 pit,bottomless,237
1078,1085 persistencearc,594 Pitts,W.,15,16,20,278,727,731,761,
Pearson,J.,230,1085 persistent(variable),1061 963,1080,1082
PEASdescription,40,42 persistentfailuremodel,593 pixel,930
Pease,A.,469,1084,1085 Person,C.,854,1083 PL-FC-ENTAILS?,258
Pecheur,C.,356,1075 perspective,966 PL-RESOLUTION,255
Pednault,E.P.D.,394,434,1085 perspectiveprojection,930 Plaat,A.,191,1085
peeking,708,737 Pesch,E.,432,1066 Place,U.T.,1041,1085
PEGASUS,850,852,859 Peshkin,M.,156,1092 PLAN-ERS1,432
Peirce,C.S.,228,313,454,471,920, pessimisticdescription(ofanaction), PLAN-ROUTE,270
1085 412 planetaryrover,971
Pelikan,M.,155,1085 Peters,S.,920,1071 PLANEX,434
Pell,B.,60,432,1083 Peterson,C.,555,1085 Plankalku¨l,14
Pemberton,J.C.,157,1085 Petrie,K.,230,1073 planmonitoring,423
penalty,56 Petrie,T.,604,826,1065 PLANNER,24,358
Penberthy,J.S.,394,1085 Petrik,M.,434,1085 planning,52,366–436
Peng,J.,855,1085 Petrov,S.,896,900,920,1085 andacting,415–417
PENGI,434 Pfeffer,A.,191,541,556,687,1078, asconstraintsatisfaction,390
penguin,435 1085 asdeduction,388
Penix,J.,356,1075 Pfeifer,G.,472,1071 asrefinement,390
Pennachin,C.,27,1074 Pfeifer,R.,1041,1085 assatisfiability,387
Pennsylvania,Univ.of,14 phasetransition,277 blocksworld,20
PennTreebank,881,895 phenomenology,1026 case-based,432
Penrose,R.,1023,1085 Philips,A.B.,154,229,1082 conformant,415,417–421,431,433,
PentagonPapers,638 PhiloofMegara,275 994
Peot,M.,433,554,1085,1088 philosophy,5–7,59,1020–1043 contingency,133,415,421–422,431
percept,34 phone(speechsound),914 decentralized,426
perception,34,305,928,928–965 phoneme,915 fine-motion,994
perceptionlayer,1005 phonemodel,915 graph,379,379–386,393
perceptron,20,729,729–731,761 phoneticalphabet,914 serial,382
convergencetheorem,20 photometry,932 hierarchical,406–415,431
learningrule,724 photosensitivespot,963 hierarchicaltasknetwork,406
network,729 phrasestructure,888,919 historyof,393
representationalpower,22 physicalism,1028,1041 linear,394
sigmoid,729 physicalsymbolsystem,18 multibody,425,426–428
Index 1121
multieffector,425 Porter,B.,473,1091 priorityqueue,80,858
non-interleaved,398 Portner,P.,920,1086 priorknowledge,39,768,778,787
online,415 Portuguese,778 priorprobability,485,503
reactive,434 pose,956,958,975 prismaticjoint,976
regression,374,394 Posegga,J.,359,1065 prisoner’sdilemma,668
route,19 positiveexample,698 privatevalue,679
searchspace,373–379 positiveliteral,244 probabilisticnetwork,seeBayesian
sensorless,415,417–421 positivism,logical,6 network
planningandcontrollayer,1006 possibilityaxiom,388 probabilisticroadmap,993
planrecognition,429 possibilitytheory,557 probability,9,26,480–565,1057–1058
PlanSAT,372 possibleworld,240,274,313,451,540 alternativesto,546
bounded,372 Post,E.L.,276,1086 axiomsof,488–490
plateau(inlocalsearch),123 post-decisiondisappointment,637 conditional,485,503,514
Plato,275,470,1041 posteriorprobability,seeprobability, conjunctive,514
Platt,J.,760,1085 conditional densityfunction,487,1057
player(inagame),667 potentialfield,991 distribution,487,522
Plotkin,G.,359,800,1085 potentialfieldcontrol,999 history,506
Plunkett,K.,921,1071 Poultney,C.,762,1086 judgments,516
ply,164 Poundstone,W.,687,1086 marginal,492
poetry,1 Pourret,O.,553,1086 model,484,1057
Pohl,I.,110,111,118,1085 Powers,R.,857,1088 open-universe,545
point-to-pointmotion,986 Prade,H.,557,1071 prior,485,503
pointwiseproduct,526 Prades,J.L.P.,637,1086 theory,289,482,636
poker,507 Pradhan,M.,519,552,1086 probablyapproximatelycorrect(PAC),
Poland,470 pragmaticinterpretation,904 714,716,759
Poli,R.,156,1079,1085 pragmatics,904 PROBCUT,175
Policella,N.,28,1068 Prawitz,D.,358,1086 probitdistribution,522,551,554
policy,176,434,647,684,994 precedenceconstraints,204 problem,66,108
evaluation,656,832 precision,869 airport-siting,643
gradient,849 precondition,367 assemblysequencing,74
improvement,656 missing,423 bandit,840,855
iteration,656,656–658,685,832 preconditionaxiom,273 conformant,138
asynchronous,658 predecessor,91 constraintoptimization,207
modified,657 predicate,902 8-queens,71,109
loss,655 predicatecalculus,seelogic,first-order 8-puzzle,102,105
optimal,647 predicateindexing,328 formulation,65,68–69
proper,650,858 predicatesymbol,292 frame,266,279
search,848,848–852,1002 prediction,139,142,573,603 generator,56
stochastic,848 preference,482,612 halting,325
value,849 monotonic,616 inherentlyhard,1054–1055
POLICY-ITERATION,657 preferenceelicitation,615 millionqueens,221,229
politeconvention(Turing’s),1026,1027 preferenceindependence,624 missionariesandcannibals,115
Pollack,M.E.,434,1069 premise,244 monkeyandbananas,113,396
polytree,528,552,575 president,449 nqueens,263
POMDP-VALUE-ITERATION,663 Presley,E.,448 optimization,121
Pomerleau,D.A.,1014,1085 Press,W.H.,155,1086 constrained,132
Ponce,J.,968,1072 Preston,J.,1042,1086 pianomovers,1012
Ponte,J.,884,922,1085,1093 Price,B.,686,1066 real-world,69
Poole,D.,2,59,553,556,639,1078, PriceWaterhouse,431 relaxed,105,376
1085,1093 Prieditis,A.E.,105,112,119,1083, robotnavigation,74
Popat,A.C.,29,921,1067 1086 sensorless,138
Popescu,A.-M.,885,1072 Princeton,17 solving,22
Popper,K.R.,504,759,1086 PrincipiaMathematica,18 touring,74
population(ingeneticalgorithms),127 Prinz,D.G.,192,1086 toy,69
Porphyry,471 PRIOR-SAMPLE,531 travelingsalesperson,74
Port-RoyalLogic,636 prioritizedsweeping,838,854 underconstrained,263
1122 Index
VLSIlayout,74,125 Puterman,M.L.,60,685,1086 Rabiner,L.R.,604,922,1086
proceduralapproach,236,286 Putnam,H.,60,260,276,350,358,505, Rabinovich,Y.,155,1086
proceduralattachment,456,466 1041,1042,1070,1086 racingcars,1050
process,447,447 Puzicha,J.,755,762,1065 radar,10
PRODIGY,432 Pylyshyn,Z.W.,1041,1086 radialbasisfunction,762
production,48 RadioRex,922
productionsystem,322,336,357,358 Q Raedt,L.D.,556,1078
productrule,486,495 Raghavan,P.,883,884,1081,1084
PROGOL,789,795,797,800 Q(s,a)(valueofactioninstate),843 Raiffa,H.,9,621,625,638,687,1078,
programminglanguage,285 Q-function,627,831 1081
progression,393 Q-learning,831,843,844,848,973 Rajan,K.,28,60,431,1064,1077
Prolog,24,339,358,394,793,899 Q-LEARNING-AGENT,844 Ralaivola,L.,605,1085
parallel,342 QA3,314 Ralphs,T.K.,112,1086
PrologTechnologyTheoremProver QALY,616,637 Ramakrishnan,R.,275,1080
(PTTP),359 Qi,R.,639,1093 Ramanan,D.,960,1086
pronunciationmodel,917 QUACKLE,187 Ramsey,F.P.,9,504,637,1086
proof,250 quadraticdynamicalsystems,155 RANDCorporation,638
properpolicy,650,858 quadraticprogramming,746 randomization,35,50
property(unaryrelation),288 qualia,1033 randomizedweightedmajority
proposaldistribution,565 qualificationproblem,268,481,1024, algorithm,752
proposition 1025 randomrestart,158,262
probabilistic,483 qualitativephysics,444,472 randomset,551
symbol,244 qualitativeprobabilisticnetwork,557, randomsurfermodel,871
propositionalattitude,450 624 randomvariable,486,515
propositionalization,324,357,368,544 quantification,903 continuous,487,519,553
propositionallogic,235,243–247,274, quantifier,295,313 indexed,555
286 existential,297 randomwalk,150,585
proprioceptivesensor,975 inlogic,295–298 rangefinder,973
PROSPECTOR,557 nested,297–298 laser,974
Prosser,P.,229,1086 universal,295–296,322 rangesensorarray,981
proteindesign,75 quantizationfactor,914 Ranzato,M.,762,1086
prototypes,896 quasi-logicalform,904 Rao,A.,61,1092
Proust,M.,910 Qubic,194 Rao,B.,604,1076
Provan,G.M.,519,552,1086 query(logical),301 Rao,G.,678
pruning,98,162,167,705 querylanguage,867 Raphael,B.,110,191,358,1074,1075
forward,174 queryvariable,522 Raphson,J.,154,760,1086
futility,185 questionanswering,872,883 rapidprototyping,339
incontingencyproblems,179 queue,79 Raschke,U.,1013,1069
inEBL,783 FIFO,80,81 Rashevsky,N.,10,761,1086
pseudocode,1061 LIFO,80,85 Rasmussen,C.E.,827,1086
pseudoexperience,837 priority,80,858 Rassenti,S.,688,1086
pseudoreward,856 Quevedo,T.,190 RatioClub,15
PSPACE,372,1055 quiescence,174 rationalagent,4,4–5,34,36–38,59,60,
PSPACE-complete,385,393 Quillian,M.R.,471,1086 636,1044
psychologicalreasoning,473 Quine,W.V.,314,443,469,470,1086 rationalism,6,923
psychology,12–13 Quinlan,J.R.,758,764,791,793,800, rationality,1,36–38
experimental,3,12 1086 calculative,1049
psychophysics,968 Quirk,R.,920,1086 limited,5
publickeyencryption,356 QXTRACT,885 perfect,5,1049
Puget,J.-F.,230,800,1073,1087 rationalthought,4
Pullum,G.K.,889,920,921,1076, R Ratner,D.,109,1086
1086 rats,13
PUMA,1011 R1,24,336,358 Rauch,H.E.,604,1086
Purdom,P.,230,1067 Rabani,Y.,155,1086 Rayner,M.,784,1087
purestrategy,667 Rabenau,E.,28,1068 Rayson,P.,921,1080
puresymbol,260 Rabideau,G.,431,1073 Rayward-Smith,V.,112,1086
Index 1123
RBFS,99–101,109 REINFORCE,849,859 resourceconstraints,401
RBL,779,784–787,798 reinforcement,830 resources,401–405,430
RDF,469 reinforcementlearning,685,695, response,13
reachableset,411 830–859,1025 restauranthygieneinspector,183
reactivecontrol,1001 active,839–845 result,368
reactivelayer,1004 Bayesian,835 resultset,867
reactiveplanning,434 distributed,856 rete,335,358
real-worldproblem,69 generalizationin,845–848 retrograde,176
realizability,697 hierarchical,856,1046 reusableresource,402
reasoning,4,19,234 multiagent,856 revelationprinciple,680
default,458–460,547 off-policy,844 revenueequivalencetheorem,682
intercausal,548 on-policy,844 Reversi,186
logical,249–264,284 Reingold,E.M.,228,1066 revolutejoint,976
uncertain,26 Reinsel,G.,604,1066 reward,56,646,684,830
recall,869 Reiter,R.,279,395,471,686,1066, additive,649
Rechenberg,I.,155,1086 1086 discounted,649
recognition,929 REJECTION-SAMPLING,533 shaping,856
recommendation,539 rejectionsampling,532 reward-to-go,833
reconstruction,929 relation,288 rewardfunction,832,1046
recurrentnetwork,729,762 relationalextraction,874 rewriterule,364,1060
RECURSIVE-BEST-FIRST-SEARCH,99 relationalprobabilitymodel(RPM), Reynolds,C.W.,435,1086
RECURSIVE-DLS,88 541,552 Riazanov,A.,359,360,1086
recursivedefinition,792 relationalreinforcementlearning,857 Ribeiro,F.,195,1091
recursiveestimation,571 relativeerror,98 Rice,T.R.,638,1082
Reddy,R.,922,1081 relaxedproblem,105,376 Rich,E.,2,1086
reduction,1059 relevance,246,375,779,799 Richards,M.,195,1086
Reeson,C.G.,228,1086 relevance(ininformationretrieval),867 Richardson,M.,556,604,1071,1086
Reeves,C.,112,1086 relevance-basedlearning(RBL),779, Richardson,S.,554,1073
Reeves,D.,688,1092 784–787,798 Richter,S.,395,1075,1086
referenceclass,491,505 relevant-states,374 ridge(inlocalsearch),123
referencecontroller,997 RemoteAgent,28,60,356,392,432 Ridley,M.,155,1086
referencepath,997 REMOTEAGENT,28 Rieger,C.,24,1086
referentialtransparency,451 renaming,331 Riesbeck,C.,23,358,921,1068,1088
refinement(inhierarchicalplanning), renderingmodel,928 rightthing,doingthe,1,5,1049
407 Renner,G.,155,1086 Riley,J.,688,1087
reflectance,933,952 Re´nyi,A.,504,1086 Riley,M.,889,1083
REFLEX-VACUUM-AGENT,48 repeatedgame,669,673 Riloff,E.,885,1077,1087
reflexagent,48,48–50,59,647,831 replanning,415,422–434 Rink,F.J.,553,1083
refutation,250 REPOP,394 Rintanen,J.,433,1087
refutationcompleteness,350 representation,seeknowledge Ripley,B.D.,763,1087
regex,874 representation riskaversion,617
Regin,J.,228,1086 atomic,57 riskneutrality,618
regions,941 factored,58 riskseeking,617
regression,393,696,760 structured,58 Rissanen,J.,759,1087
linear,718,810 representationtheorem,624 Ritchie,G.D.,800,1087
nonlinear,732 REPRODUCE,129 Ritov,Y.,556,606,1085
tree,707 reservebid,679 Rivest,R.,759,1059,1069,1087
regressiontothemean,638 resolution,19,21,253,252–256,275, RMS(rootmeansquare),1059
regret,620,752 314,345–357,801 Robbinsalgebra,360
regularexpression,874 closure,255,351 Roberts,G.,30,1071
regularization,713,721 completenessprooffor,350 Roberts,L.G.,967,1087
Reichenbach,H.,505,1086 input,356 Roberts,M.,192,1065
Reid,D.B.,606,1086 inverse,794,794–797,800 Robertson,N.,229,1087
Reid,M.,111,1079 linear,356 Robertson,S.,868
Reif,J.,1012,1013,1068,1086 strategies,355–356 Robertson,S.E.,505,884,1069,1087
reification,440 resolvent,252,347,794 Robinson,A.,314,358,360,1087
1124 Index
Robinson,G.,359,1092 Rubin,D.,604,605,826,827,1070, SAM,360
Robinson,J.A.,19,276,314,350,358, 1073,1087 samplecomplexity,715
1087 Rubinstein,A.,688,1084 samplespace,484
Robocup,1014 rule,244 sampling,530–535
robot,971,1011 causal,317,517 samplingrate,914
game(withhumans),1019 condition–action,48 Samuel,A.L.,17,18,61,193,850,
hexapod,1001 default,459 854,855,1087
mobile,971 diagnostic,317,517 Samuelson,L.,688,1081
navigation,74 if–then,48,244 Samuelson,W.,688,1087
soccer,161,434,1009 implication,244 Samuelsson,C.,784,1087
robotics,3,592,971–1019 situation–action,48 Sanders,P.,112,1069
robustcontrol,994 uncertain,548 Sankaran,S.,692,1080
Roche,E.,884,1087 rule-basedsystem,547,1024 Sanna,R.,761,1073
Rochester,N.,17,18,1020,1082 withuncertainty,547–549 Sanskrit,468,919
Rock,I.,968,1087 Rumelhart,D.E.,24,761,1087 Santorini,B.,895,921,1081
RockefellerFoundation,922 Rummery,G.A.,855,1087 SAPA,431
Ro¨ger,G.,111,1075 Ruspini,E.H.,557,1087 Sapir–Whorfhypothesis,287
rollout,180 Russell,A.,111,1071 Saraswat,V.,228,1091
Romania,65,203 Russell,B.,6,16,18,357,1092 Sarawagi,S.,885,1087
Roomba,1009 Russell,J.G.B.,637,1087 SARSA,844
Roossin,P.,922,1067 Russell,J.R.,360,1083 Sastry,S.,60,606,852,857,1013,
rootmeansquare,1059 Russell,S.J.,111,112,157,191,192, 1075,1084
Roscoe,T.,275,1080 198,278,345,432,444,556, SAT,250
Rosenblatt,F.,20,761,1066,1087 604–606,686,687,799,800, Satia,J.K.,686,1087
Rosenblatt,M.,827,1087 826,855–857,1012,1048,1050, satisfaction(inlogic),240
Rosenblitt,D.,394,1081 1064,1066,1069–1071,1073, satisfiability,250,277
Rosenbloom,P.S.,26,27,336,358, 1076,1077,1081–1085,1087, satisfiabilitythresholdconjecture,264,
432,799,1047,1075,1079 1090,1092,1093 278
Rosenblueth,A.,15,1087 Russia,21,192,489 satisficing,10,1049
Rosenbluth,A.,155,554,1082 Rustagi,J.S.,554,1087 SATMC,279
Rosenbluth,M.,155,554,1082 Ruzzo,W.L.,920,1074 Sato,T.,359,556,1087,1090
Rosenholtz,R.,953,968,1081 Ryan,M.,314,1076 SATPLAN,387,392,396,402,420,433
Rosenschein,J.S.,688,1087,1089 RYBKA,186,193 SATPLAN,272
Rosenschein,S.J.,60,278,279,1077, Rzepa,H.S.,469,1083 saturation,351
1087 SATZ,277
Ross,P.E.,193,1087 S Saul,L.K.,555,606,1077,1088
Ross,S.M.,1059,1087 Saund,E.,883,1087
Rossi,F.,228,230,1066,1087 S-set,774 Savage,L.J.,489,504,637,1088
rotation,956 Sabharwal,A.,277,395,1074,1076 Sayre,K.,1020,1088
Roth,D.,556,1070 Sabin,D.,228,1087 scaledorthographicprojection,932
Roughgarden,T.,688,1084 Sacerdoti,E.D.,394,432,1087 scanninglidars,974
Roussel,P.,314,358,359,1069,1087 Sackinger,E.,762,967,1080 Scarcello,F.,230,472,1071,1074
routefinding,73 Sadeh,N.M.,688,1064 scene,929
Rouveirol,C.,800,1087 Sadri,F.,470,1087 Schabes,Y.,884,1087
Roveri,M.,396,433,1066,1068 Sagiv,Y.,358,1065 Schaeffer,J.,112,186,191,194,195,
Rowat,P.F.,1013,1087 Sahami,M.,29,883,884,1078,1087 678,687,1066,1069,1081,
Roweis,S.T.,554,605,1087 Sahin,N.T.,288,1087 1085,1088
Rowland,J.,797,1078 Sahni,S.,110,1076 Schank,R.C.,23,921,1088
Rowley,H.,968,1087 SAINT,19,156 Schapire,R.E.,760,761,884,1072,
Roy,N.,1013,1087 St.Petersburgparadox,637,641 1088
Rozonoer,L.,760,1064 Sakuta,M.,192,1087 Scharir,M.,1012,1088
RPM,541,552 Salisbury,J.,1013,1081 Schaub,T.,471,1070
RSA(Rivest,Shamir,andAdelman), Salmond,D.J.,605,1074 Schauenberg,T.,678,687,1066
356 Salomaa,A.,919,1087 scheduling,403,401–405
RSAT,277 Salton,G.,884,1087 Scheines,R.,826,1089
Rubik’sCube,105 Saltzman,M.J.,112,1086 schema(inageneticalgorithm),128
Index 1125
schemaacquisition,799 informed,64,81,92,92–102,108 sensor,34,41,928
Schervish,M.J.,506,1070 Internet,464 active,973
Schickard,W.,5 iterativedeepening,88,88–90,108, failure,592,593
Schmid,C.,968,1088 110,173,408 model,579,586,603
Schmidt,G.,432,1066 iterativedeepeningA*,99,111 passive,973
Schmolze,J.G.,471,1088 learningto,102 sensorinterfacelayer,1005
Schneider,J.,852,1013,1065 local,120–129,154,229,262–263, sensorlessplanning,415,417–421
Schnitzius,D.,432,1070 275,277 sensormodel,566,579,586,603,658,
Schnizlein,D.,687,1091 greedy,122 928,979
Schoenberg,I.J.,761,1083 local,forCSPs,220–222 sentence
Scho¨lkopf,B.,760,762,1069,1070, localbeam,125,126 atomic,244,294–295,299
1088 memory-bounded,99–102,111 complex,244,295
Schomer,D.,288,1087 memory-boundedA*,101,101–102, inaKB,235,274
Scho¨ning,T.,277,1088 112 asphysicalconfiguration,243
Schoppers,M.J.,434,1088 minimax,165–168,188,189 separator(inBayesnet),499
Schrag,R.C.,230,277,1065 nondeterministic,133–138 sequenceform,677
Schro¨der,E.,276,1088 online,147,147–154,157 sequential
Schubert,L.K.,469,1076 parallel,112 environment,43
Schulster,J.,28,1068 partiallyobservable,138–146 sequentialdecisionproblem,645–651,
Schultz,W.,854,1088 policy,848,848–852,1002 685
Schultze,P.,112,1079 quiescence,174 sequentialenvironment,43
Schulz,D.,606,1012,1067,1088 real-time,157,171–175 sequentialimportance-sampling
Schulz,S.,360,1088,1090 recursivebest-first(RBFS),99–101, resampling,605
Schumann,J.,359,360,1071,1080 111 serendipity,424
Schu¨tze,H.,883–885,920,921,1081, simulatedannealing,125 Sergot,M.,470,1079
1088 stochasticbeam,126 serializablesubgoals,392
Schu¨tze,H.,862,883,1078 strategy,75 Serina,I.,395,1073
Schwartz,J.T.,1012,1088 tabu,154,222 Sestoft,P.,799,1077
Schwartz,S.P.,469,1088 tree,163 set(infirst-orderlogic),304
Schwartz,W.B.,505,1074 uniform-cost,83,83–85,108 set-coverproblem,376
scientificdiscovery,759 uninformed,64,81,81–91,108,110 SETHEO,359
Scott,D.,555,1088 searchcost,80 setofsupport,355
Scrabble,187,195 searchtree,75,163 setsemantics,367
scruffyvs.neat,25 Searle,J.R.,11,1027,1029–1033, Settle,L.,360,1074
search,22,52,66,108 1042,1088 Seymour,P.D.,229,1087
A*,93–99 Sebastiani,F.,884,1088 SGP,395,433
alpha–beta,167–171,189,191 Segaran,T.,688,763,1088 SGPLAN,387
B*,191 segmentation(ofanimage),941 Sha,F.,1025,1088
backtracking,87,215,218–220,222, segmentation(ofwords),886,913 Shachter,R.D.,517,553,554,559,615,
227 Sejnowski,T.,763,850,854,1075, 634,639,687,1071,1088,1090
beam,125,174 1083,1090 shading,933,948,952–953
best-first,92,108 Self,M.,826,1068 shadow,934
bidirectional,90–112 Selfridge,O.G.,17 Shafer,G.,557,1088
breadth-first,81,81–83,108,408 Selman,B.,154,229,277,279,395, shaftdecoder,975
conformant,138–142 471,1074,1077,1078,1088 Shah,J.,967,1083
continuousspace,129–133,155 semanticinterpretation,900–904,920 Shahookar,K.,110,1088
current-best-hypothesis,770 semanticnetworks,453–456,468,471 Shaked,T.,885,1072
cuttingoff,173–175 semantics,240,860 Shakey,19,60,156,393,397,434,1011
depth-first,85,85–87,108,408 database,300,343,367,540 Shalla,L.,359,1092
depth-limited,87,87–88 logical,274 Shanahan,M.,470,1088
general,108 SemanticWeb,469 Shankar,N.,360,1088
greedybest-first,92,92 semi-supervisedlearning,695 Shannon,C.E.,17,18,171,192,703,
heuristic,81,110 semidecidable,325,357 758,763,883,913,1020,1082,
hill-climbing,122–125,150 semidynamicenvironment,44 1088
inaCSP,214–222 Sen,S.,855,1084 Shaparau,D.,275,1088
incrementalbelief-state,141 sensitivityanalysis,635 shape,957
1126 Index
fromshading,968 simulatedannealing,120,125,153,155, Smith,D.E.,156,157,345,359,363,
Shapiro,E.,800,1088 158,536 395,433,1067,1073,1079,
Shapiro,S.C.,31,1088 simulationofworld,1028 1085,1089,1091
Shapley,S.,687,1088 simultaneouslocalizationandmapping Smith,G.,112,1086
Sharir,M.,1013,1074 (SLAM),982 Smith,J.E.,619,637,1089
Sharp,D.H.,761,1069 Sinclair,A.,124,155,1081,1086 Smith,J.M.,155,688,1089
Shatkay,H.,1012,1088 Singer,P.W.,1035,1089 Smith,J.Q.,638,639,1084,1089
Shaw,J.C.,109,191,276,1084 Singer,Y.,604,884,1072,1088 Smith,M.K.,469,1089
Shawe-Taylor,J.,760,1069 Singh,M.P.,61,1076 Smith,R.C.,1012,1089
Shazeer,N.M.,231,1080 Singh,P.,27,439,1082,1089 Smith,R.G.,61,1067
Shelley,M.,1037,1088 Singh,S.,1014,1067 Smith,S.J.J.,187,195,1089
Sheppard,B.,195,1088 Singh,S.P.,157,685,855,856,1065, Smith,V.,688,1086
Shewchuk,J.,1012,1070 1077,1078,1090 Smith,W.D.,191,553,1065,1083
Shi,J.,942,967,1088 Singhal,A.,870,1089 SMODELS,472
Shieber,S.,30,919,1085,1088 singlyconnectednetwork,528 Smola,A.J.,760,1088
Shimelevich,L.I.,605,1093 singular,1056 Smolensky,P.,24,1089
singularextension,174
Shin,M.C.,685,1086 smoothing,574–576,603,822,862,
Shinkareva,S.V.,288,1082 singularity,12 863,938
Shmoys,D.B.,110,405,432,1080 technological,1038 linearinterpolation,863
sins,sevendeadly,122
Shoham,Y.,60,195,230,359,435, online,580
638,688,857,1064,1079,1080,
SIPE,431,432,434
Smullyan,R.M.,314,1089
SIR,605
1088 Smyth,P.,605,763,1074,1089
Sittler,R.W.,556,606,1089
short-termmemory,336 SNARC,16
situatedagent,1025
shortestpath,114 Snell,J.,506,1074
situation,388
Shortliffe,E.H.,23,557,1067,1088 Snell,M.B.,1032,1089
situationcalculus,279,388,447
shoulder(instatespace),123 SNLP,394
Sjolander,K.,604,1079
Shpitser,I.,556,1085 Snyder,W.,359,1064
skeletonization,986,991
SHRDLU,20,23,370 SOAR,26,336,358,432,799,1047
Skinner,B.F.,15,60,1089
Shreve,S.E.,60,1066 soccer,195
Skolem,T.,314,358,1089
sibylattack,541 sociallaws,429
Skolemconstant,323,357
sidewaysmove(instatespace),123 societyofmind,434
Skolemfunction,346,358
Sietsma,J.,762,1088 Socrates,4
skolemization,323,346
SIGART,31 Soderland,S.,394,469,885,1065,
slack,403
sigmoidfunction,726 1072,1089
Slagle,J.R.,19,1089
sigmoidperceptron,729 softbot,41,61
SLAM,982
signalprocessing,915 slant,957 softmargin,748
significancetest,705 Slate,D.J.,110,1089 softmaxfunction,848
signs,888 Slater,E.,192,1089 softthreshold,521
Siklossy,L.,432,1088 Slattery,S.,885,1069 softwareagent,41
Silver,D.,194,1073 Sleator,D.,920,1089 softwarearchitecture,1003
Silverstein,C.,884,1088 sliding-blockpuzzle,71,376 Soika,M.,1012,1066
Simard,P.,762,967,1080 slidingwindow,943 Solomonoff,R.J.,17,27,759,1089
Simmons,R.,605,1012,1088,1091 Slocum,J.,109,1089 solution,66,68,108,134,203,668
Simon’spredictions,20 Sloman,A.,27,1041,1082,1089 optimal,68
Simon,D.,60,1088 Slovic,P.,2,638,1077 solvinggames,163–167
Simon,H.A.,3,10,17,18,30,60,109, small-scalelearning,712 soma,11
110,191,276,356,393,639, Smallwood,R.D.,686,1089 Sompolinsky,H.,761,1064
800,1049,1077,1079,1084, Smarr,J.,883,1078 sonarsensors,973
1088,1089 Smart,J.J.C.,1041,1089 Sondik,E.J.,686,1089
Simon,J.C.,277,1089 SMA∗,109 sonnet,1026
Simonis,H.,228,1089 Smith,A.,9 Sonneveld,D.,109,1089
Simons,P.,472,1084 Smith,A.F.M.,605,811,826,1065, Sontag,D.,556,1082
SIMPLE-REFLEX-AGENT,49 1074,1090 So¨rensson,N.,277,1071
simplexalgorithm,155 Smith,B.,28,60,431,470,1077,1089 Sosic,R.,229,1089
SIMULATED-ANNEALING,126 Smith,D.A.,920,1089 soul,1041
Index 1127
soundness(ofinference),242,247,258, STAN,395 Stoica,I.,275,1080
274,331 standardizingapart,327,363,375 Stoicschool,275
sourgrapes,37 Stanfill,C.,760,1089 Stokes,I.,432,1064
Sowa,J.,473,1089 StanfordUniversity,18,19,22,23,314 Stolcke,A.,920,1089
Spaan,M.T.J.,686,1089 StanhopeDemonstrator,276 Stoljar,D.,1042,1081
spacecomplexity,80,108 Staniland,J.R.,505,1070 Stone,C.J.,758,1067
spacecraftassembly,432 STANLEY,28,1007,1008,1014,1025 Stone,M.,759,1089
spamdetection,865 startsymbol,1060 Stone,P.,434,688,1089
spamemail,886 state,367 Stork,D.G.,763,827,966,1071,1089
SparckJones,K.,505,868,884,1087 repeated,75 Story,W.E.,109,1077
sparsemodel,721 world,69 Strachey,C.,14,192,193,1089,1090
sparsesystem,515 State-Action-Reward-State-Action straight-linedistance,92
SPASS,359 (SARSA),844 Strat,T.M.,557,1087
spatialreasoning,473 stateabstraction,377 strategicform,667
spatialsubstance,447 stateestimation,145,181,269,275, strategy,133,163,181,667
specialization,771,772 570,978 strategyprofile,667
species,25,130,439–441,469,817, recursive,145,571 Stratonovich,R.L.,604,639,1089
860,888,948,1035,1042 States,D.J.,826,1076 strawberries,enjoy,1021
spectrophotometry,935 statespace,67,108 Striebel,C.T.,604,1086
specularities,933 metalevel,102 string(inlogic),471
specularreflection,933 statevariable STRIPS,367,393,394,397,432,434,
speechact,904 missing,423 799
speechrecognition,25,912,912–919, staticenvironment,44 Stroham,T.,884,1069
922 stationarity(forpreferences),649 Strohm,G.,432,1072
sphexwasp,39,425 stationarityassumption,708 strongAI,1020,1026–1033,1040
SPI(SymbolicProbabilisticInference), stationarydistribution,537,573 strongdomination,668
553 stationaryprocess,568,568–570,603 structuredrepresentation,58,64
Spiegelhalter,D.J.,553–555,639,763, statisticalmechanics,761 Stuckey,P.J.,228,359,1077,1081
826,1069,1073,1080,1082, Stefik,M.,473,557,1089 STUDENT,19
1089 Stein,J.,553,1083 stuff,445
Spielberg,S.,1040,1089 Stein,L.A.,1051,1089 stupidpettricks,39
SPIKE,432 Stein,P.,192,1078 Stutz,J.,826,1068
SPIN,356 Steiner,W.,1012,1067 stylometry,886
spinglass,761 stemming,870 Su,Y.,111,1071
Spirtes,P.,826,1089 Stensrud,B.,358,1090 subcategory,440
splitpoint,707 stepcost,68 subgoalindependence,378
Sproull,R.F.,639,1072 Stephenson,T.,604,1089 subjectivecase,899
Sputnik,21 stepsize,132 subjectivism,491
squareroots,47 stereopsis,binocular,948 submodularity,644
SRI,19,314,393,638 stereovision,974 subproblem,106
Srinivasan,A.,797,800,1084,1089 Stergiou,K.,228,1089 Subrahmanian,V.S.,192,1084
Srinivasan,M.V.,1045,1072 Stern,H.S.,827,1073 Subramanian,D.,278,472,799,1050,
Srivas,M.,356,1089 Sternberg,M.J.E.,797,1089,1090 1068,1087,1089,1090
Srivastava,B.,432,1077 Stickel,M.E.,277,359,884,921,1075, substance,445
SSD(sumofsquareddifferences),940 1076,1089,1093 spatial,447
SSS*algorithm,191 stiffneck,496 temporal,447
Staab,S.,469,1089 Stiller,L.,176,1089 substitutability(oflotteries),612
stability stimulus,13 substitution,301,323
ofacontroller,998 Stob,M.,759,1084 subsumption
staticvs.dynamic,977 stochasticbeamsearch,126 indescriptionlogic,456
strict,998 stochasticdominance,622,636 inresolution,356
stack,80 stochasticenvironment,43 subsumptionarchitecture,1003
Stader,J.,432,1064 stochasticgames,177 subsumptionlattice,329
STAGE,154 stochasticgradientdescent,720 successor-stateaxiom,267,279,389
STAHL,800 Stockman,G.,191,1089 successorfunction,67
Stallman,R.M.,229,1089 Stoffel,K.,469,1089 Sudoku,212
1128 Index
Sulawesi,223 systemsreply,1031 temporaldifferencelearning,836–838,
SUMMATION,1053 Szafron,D.,678,687,1066,1091 853,854
summer’sday,1026 Szathma´ry,E.,155,1089 temporalinference,570–578
summingout,492,527 Szepesvari,C.,194,1078 temporallogic,289
sumofsquareddifferences,940 temporalprojection,278
SunMicrosystems,1036 T temporalreasoning,566–609
Sunstein,C.,638,1090 temporalsubstance,447
Sunter,A.,556,1072 T (fluentholds),446 Tenenbaum,J.,314,1090
Superman,286 T-SCHED,432 Teng,C.-M.,505,1079
superpixels,942 T4,431 Tennenholtz,M.,855,1067
supervisedlearning,695,846,1025 TABLE-DRIVEN-AGENT,47 tennis,426
supportvectormachine,744,744–748, tablelookup,737 tense,902
754 tabletennis,32 term(inlogic),294,294
surething,617 tabusearch,154,222 terMeulen,A.,314,1091
surveillance,1036 tactilesensors,974 terminalstates,162
surveypropagation,278 Tadepalli,P.,799,857,1090 terminalsymbol,890,1060
survivalofthefittest,605 Tait,P.G.,109,1090 terminaltest,162
Sussman,G.J.,229,394,1089,1090 Takusagawa,K.T.,556,1085 terminationcondition,995
Sussmananomaly,394,398 Talos,1011 termrewriting,359
Sutcliffe,G.,360,1090 TALPLANNER,387 Tesauro,G.,180,186,194,846,850,
Sutherland,G.L.,22,1067 Tamaki,H.,359,883,1084,1090 855,1090
Sutherland,I.,228,1090 Tamaki,S.,277,1077 testset,695
Sutphen,S.,194,1088 Tambe,M.,230,1085 TETRAD,826
Suttner,C.,360,1090 Tank,D.W.,11,1084 Teukolsky,S.A.,155,1086
Sutton,C.,885,1090 Tardos,E.,688,1084 texel,951
Sutton,R.S.,685,854–857,1065,1090 Tarjan,R.E.,1059,1090 textclassification,865,882
Svartvik,J.,920,1086 Tarski,A.,8,314,920,1090 TEXTRUNNER,439,881,882,885
Svestka,P.,1013,1078 Tash,J.K.,686,1090 texture,939,948,951
Svetnik,V.B.,605,1093 Taskar,B.,556,1073,1090 texturegradient,967
Svore,K.,884,1090 taskenvironment,40,59 Teyssier,M.,826,1090
Swade,D.,14,1090 tasknetwork,394 Thaler,R.,637,638,1090
Swartz,R.,1022,1067 Tasmania,222 theeandthou,890
Swedish,32 Tate,A.,394,396,408,431,432,1064, THEO,1047
Swerling,P.,604,1090 1065,1090 Theocharous,G.,605,1090
Swift,T.,359,1090 Tatman,J.A.,687,1090 theorem,302
switchingKalmanfilter,589,608 Tattersall,C.,176,1090 incompleteness,8,352,1022
syllogism,4,275 taxi,40,694 theoremprover,2,356
symbolicdifferentiation,364 inAthens,509 theoremproving,249,393
symbolicintegration,776 automated,56,236,480,695,1047 mathematical,21,32
symmetrybreaking(inCSPs),226 taxonomichierarchy,24,440 Theseus,758
synapse,11 taxonomy,440,465,469 Thiele,T.,604,1090
synchrodrive,976 Taylor,C.,763,968,1070,1082 Thielscher,M.,279,470,1090
synchronization,427 Taylor,G.,358,1090 thingification,440
synonymy,465,870 Taylor,M.,469,1089 thinkinghumanly,3
syntacticambiguity,905,920 Taylor,R.,1013,1081 thinkingrationally,4
syntacticcategories,888 Taylor,W.,9,229,277,1068 Thitimajshima,P.,555,1065
syntacticsugar,304 Taylorexpansion,982 Thomas,A.,554,555,826,1073
syntactictheory(ofknowledge),470 TD-GAMMON,186,194,850,851 Thomas,J.,763,1069
syntax,23,240,244 Teh,Y.W.,1047,1075 Thompson,H.,884,1066
oflogic,274 telescope,562 Thompson,K.,176,192,1069,1090
ofnaturallanguage,888 television,860 thought,4,19,234
ofprobability,488 Teller,A.,155,554,1082 lawsof,4
synthesis,356 Teller,E.,155,554,1082 thrashing,102
deductive,356 Teller,S.,1012,1066 3-SAT,277,334,362
synthesisofalgorithms,356 Temperley,D.,920,1089 thresholdfunction,724
Syrja¨nen,T.,472,1084,1090 template,874 Throop,T.A.,187,195,1089
Index 1129
Thrun,S.,28,605,686,884, transitivity(ofpreferences),612 Tweedie,F.J.,886,1078
1012–1014,1067,1068,1072, translationmodel,909 twinearths,1041
1083–1085,1087,1090,1091 transpose,1056 two-fingerMorra,666
Tibshirani,R.,760,761,763,827,1073, transposition(inagame),170 2001:ASpaceOdyssey,552
1075 transpositiontable,170 typesignature,542
tic-tac-toe,162,190,197 travelingsalespersonproblem,74 typicalinstance,443
Tikhonov,A.N.,759,1090 travelingsalespersonproblem(TSP), Tyson,M.,884,1075
tiling,737 74,110,112,119
tilt,957 Traverso,P.,275,372,386,395,396, U
time(ingrammar),902 433,1066,1068,1073,1088
timecomplexity,80,108 tree,223 U (utility),611
timeexpressions,925 TREE-CSP-SOLVER,224
u(cid:4)(bestprize),615
timeinterval,470 TREE-SEARCH,77
u⊥(worstcatastrophe),615
UCPOP,394
timeofflightcamera,974 treebank,895,919
UCT(upperconfidenceboundson
timeslice(inDBNs),567 Penn,881,895
trees),194
Tinsley,M.,193 treedecomposition,225,227
UI(UniversalInstantiation),323
Tirole,J.,688,1073 treewidth,225,227,229,434,529
Ulam,S.,192,1078
Tishby,N.,604,1072 trial,832
Ullman,J.D.,358,1059,1064,1065,
titfortat,674 triangleinequality,95
1090
Titterington,D.M.,826,1090 trichromacy,935
Ullman,S.,967,968,1076,1090
TLPLAN,387 Triggs,B.,946,968,1069
ultraintelligentmachine,1037
TMS,229,461,460–462,472,1041 Troyanskii,P.,922
Ulysses,1040
Tobarra,L.,279,1064 Trucco,E.,968,1090
unbiased(estimator),618
Toffler,A.,1034,1090 truth,240,295
uncertainenvironment,43
tokenization,875 functionality,547,552
uncertainty,23,26,438,480–509,549,
Tomasi,C.,951,968,1090 preservinginference,242
1025
toothache,481 table,245,276
existence,541
topologicalsort,223 truthmaintenancesystem(TMS),229,
identity,541,876
torquesensor,975 461,460–462,472,1041
relational,543
Torralba,A.,741,1090 assumption-based,462
rule-basedapproachto,547
Torrance,M.C.,231,1073 justification-based,461
summarizing,482
Torras,C.,156,433,1077 truthvalue,245
andtime,566–570
totalcost,80,102 Tsang,E.,229,1076
unconditionalprobability,see
Toth,P.,395,1068 Tsitsiklis,J.N.,506,685,686,847,855,
probability,prior
touringproblem,74 857,1059,1066,1081,1084,
undecidability,8
toyproblem,69 1090
undergeneration,892
TPTP,360 TSP,74,110,112,119 unicorn,280
trace,904 TT-CHECK-ALL,248 unification,326,326–327,329,357
tractabilityofinference,8,457 TT-ENTAILS?,248 andequality,353
trading,477 Tumer,K.,688,1090 equational,355
tragedyofthecommons,683 Tung,F.,604,1086 unifier,326
trail,340 tuple,291 mostgeneral(MGU),327,329,353,
training turbodecoding,555 361
curve,724 Turcotte,M.,797,1090 UNIFORM-COST-SEARCH,84
set,695 Turing,A.,2,8,14,16,17,19,30,31, uniform-costsearch,83,83–85,108
replicated,749 54,192,325,358,552,761,854, uniformconvergencetheory,759
weighted,749 1021,1022,1024,1026,1030, uniformprior,805
transfermodel(inMT),908 1043,1052,1090 uniformprobabilitydistribution,487
transhumanism,1038 Turingaward,1059 uniformresourcelocator(URL),463
transientfailure,592 Turingmachine,8,759 UNIFY,328
transientfailuremodel,593 TuringTest,2,2–4,30,31,860,1021 UNIFY-VAR,328
transitionmatrix,564 total,3 Unimate,1011
transitionmodel,67,108,134,162,266, Turk,190 uninformedsearch,64,81,81–91,108,
566,597,603,646,684,832, Tversky,A.,2,517,620,638,1072, 110
979 1077,1090 uniqueactionaxioms,389
transitionprobability,536 TWEAK,394 uniquenamesassumption,299,540
1130 Index
unit(inaneuralnetwork),728 validation variationalparameter,554
unitclause,253,260,355 cross,737,759,767 Varzi,A.,470,1068
UnitedStates,13,629,640,753,755, validation,cross,708 Vaucanson,J.,1011
922,1034,1036 validationset,709 Vauquois,B.,909,1091
unitpreference,355 validity,249,274 Vazirani,U.,154,763,1064,1078
unitpreferencestrategy,355 value,58 Vazirani,V.,688,1084
unitpropagation,261 VALUE-ITERATION,653 VCdimension,759
unitresolution,252,355 valuedetermination,691 VCG,683
unitsfunction,444 valuefunction,614 Vecchi,M.P.,155,229,1078
universalgrammar,921 additive,625 vector,1055
UniversalInstantiation,323 valueiteration,652,652–656,684 vectorfieldhistograms,1013
universalplan,434 point-based,686 vectorspacemodel,884
unmannedairvehicle(UAV),971 valuenode,seeutilitynode vehicleinterfacelayer,1006
unmannedgroundvehicle(UGV),971 valueofcomputation,1048 Veloso,M.,799,1091
UNPOP,394 valueofinformation,628–633,636, Vempala,S.,883,1084
unrolling,544,595 644,659,839,1025,1048 Venkataraman,S.,686,1074
unsatisfiability,274 valueofperfectinformation,630 Venugopal,A.,922,1093
unsupervisedlearning,694,817–820, valuesymmetry,226 Vere,S.A.,431,1091
1025 VAMPIRE,359,360 verification,356
UOSAT-II,432 vanBeek,P.,228–230,395,470,1065, hardware,312
update,142 1078,1087,1091 Verma,T.,553,826,1073,1085
upperontology,467 vanBentham,J.,314,1091 Verma,V.,605,1091
URL,463 Vandenberghe,L.,155,1066 Verri,A.,968,1090
Urmson,C.,1014,1091 vanHarmelen,F.,473,799,1091 VERSION-SPACE-LEARNING,773
urn-and-ball,803 vanHeijenoort,J.,360,1091 VERSION-SPACE-UPDATE,773
URP,638 vanHoeve,W.-J.,212,228,1091 versionspace,773,774,798
Uskov,A.V.,192,1064 vanishingpoint,931 versionspacecollapse,776
Utgoff,P.E.,776,799,1082 vanLambalgen,M.,470,1091 Vetterling,W.T.,155,1086
utilitarianism,7 vanMaaren,H.,278,1066 Vickrey,W.,681
utility,9,53,162,482 vanNunen,J.A.E.E.,685,1091 Vickrey-Clarke-Groves,683
axiomsof,613 vanRun,P.,230,1065 Vienna,1028
estimation,833 vanderGaag,L.,505,1081 views,multiple,948
expected,53,61,483,610,611,616 VanEmden,M.H.,472,1091 Vinge,V.,12,1038,1091
function,53,54,162,611,615–621, VanHentenryck,P.,228,1091 Viola,P.,968,1025,1091
846 VanRoy,B.,847,855,1090,1091 virtualcounts,812
independence,626 VanRoy,P.L.,339,342,359,1091 visibilitygraph,1013
maximumexpected,483,611 Vapnik,V.N.,759,760,762,763,967, vision,3,12,20,228,929–965
ofmoney,616–618 1066,1069,1080,1091 Visser,U.,195,1014,1091
multiattribute,622–626,636,648 Varaiya,P.,60,856,1072,1079 Visser,W.,356,1075
multiplicative,626 Vardi,M.Y.,470,477,1072 Vitaliset,489
node,627 variabilization(inEBL),781 Vitanyi,P.M.B.,759,1080
normalized,615 variable,58 Viterbi,A.J.,604,1091
ordinal,614 atemporal,266 Viterbialgorithm,578
theory,482,611–615,636 elimination,524,524–528,552,553, Vlassis,N.,435,686,1089,1091
utility-basedagent,1044 596 VLSIlayout,74,110,125
utopia,1052 incontinuousstatespace,131 vocabulary,864
UWL,433 indicator,819 Volk,K.,826,1074
logic,340 vonMises,R.,504,1091
V inlogic,295 vonNeumann,J.,9,15,17,190,613,
ordering,216,527 637,687,1091
vacuumtube,16 random,486,515 vonStengel,B.,677,687,1078
vacuumworld,35,37,62,159 Boolean,486 vonWinterfeldt,D.,637,1091
erratic,134 continuous,487,519,553 vonKempelen,W.,190
slippery,137 relevance,528 vonLinne,C.,469
vagueness,547 Varian,H.R.,688,759,1081,1091 Voronkov,A.,314,359,360,1086,
Valiant,L.,759,1091 variationalapproximation,554 1087
Index 1131
Voronoigraph,991 Wegbreit,B.,1012,1083 Williams,C.K.I.,827,1086
Vossen,T.,396,1091 Weglarz,J.,432,1066 Williams,R.,640
votedperceptron,760 Wei,X.,885,1085 Williams,R.J.,685,761,849,855,
VPI(valueofperfectinformation),630 Weibull,J.,688,1091 1085,1087,1092
Weidenbach,C.,359,1091 Williamson,J.,469,1083
W weight,718 Williamson,M.,433,1072
weight(inaneuralnetwork),728 Willighagen,E.L.,469,1083
Wadsworth,C.P.,314,1074 WEIGHTED-SAMPLE,534 Wilmer,E.L.,604,1080
Wahba,G.,759,1074 weightedlinearfunction,172 Wilson,A.,921,1080
Wainwright,M.J.,278,555,1081,1091
weightspace,719 Wilson,R.,227,1092
Walden,W.,192,1078
Weinstein,S.,759,1084 Wilson,R.A.,3,1042,1092
Waldinger,R.,314,394,1081,1091
Weiss,G.,61,435,1091 Windows,553
Walker,E.,29,1069
Weiss,S.,884,1064 Winikoff,M.,59,1084
Walker,H.,826,1074
Weiss,Y.,555,605,741,1083, Winker,S.,360,1092
WALKSAT,263,395
1090–1092 Winkler,R.L.,619,637,1089
Wall,R.,920,1071
Weissman,V.,314,1074 winner’scurse,637
Wallace,A.R.,130,1091
Weizenbaum,J.,1035,1041,1091 Winograd,S.,20,1092
Wallace,D.L.,886,1083
Weld,D.S.,61,156,394–396,432, Winograd,T.,20,23,884,1066,1092
Walras,L.,9
433,469,472,885,1036,1069, Winston,P.H.,2,20,27,773,798,
Walsh,M.J.,156,1072
1071,1072,1079,1085,1089, 1065,1092
Walsh,T.,228,230,278,1066,1087,
1091,1092 Wintermute,S.,358,1092
1089
Wellman,M.P.,10,555,557,604,638, Witbrock,M.,469,1081
Walsh,W.,688,1092
685–688,857,1013,1070,1076, Witten,I.H.,763,883,884,921,1083,
Walter,G.,1011
1091,1092 1092
Waltz,D.,20,228,760,1089,1091
Wells,H.G.,1037,1092 Wittgenstein,L.,6,243,276,279,443,
WAM,341,359
Wells,M.,192,1078 469,1092
Wang,D.Z.,885,1067
Welty,C.,469,1089 Wizard,553
Wang,E.,472,1090
Werbos,P.,685,761,854,1092 Wo¨hler,F.,1027
Wang,Y.,194,1091
Wermuth,N.,553,1080 Wojciechowski,W.S.,356,1092
Wanner,E.,287,1091
Werneck,R.F.,111,1074 Wojcik,A.S.,356,1092
Warmuth,M.,109,759,1066,1086
Wertheimer,M.,966 Wolf,A.,920,1074
WARPLAN,394
Wesley,M.A.,1013,1092 Wolfe,D.,186,1065
Warren,D.H.D.,339,341,359,394,
West,Col.,330 Wolfe,J.,157,192,432,1081,1087,
889,1085,1091
Westinghouse,432 1092
Warren,D.S.,359,1090
Westphal,M.,395,1086 Wolpert,D.,688,1090
WarrenAbstractMachine(WAM),341,
Wexler,Y.,553,1092 Wong,A.,884,1087
359
washingclothes,927
Weymouth,T.,1013,1069 Wong,W.-K.,826,1083
Washington,G.,450 White,J.L.,356,1075 Wood,D.E.,111,1080
wasp,sphex,39,425 Whitehead,A.N.,16,357,781,1092 Woods,W.A.,471,921,1092
Wasserman,L.,763,1091 Whiter,A.M.,431,1090 Wooldridge,M.,60,61,1068,1092
Watkins,C.J.,685,855,1091 Whittaker,W.,1014,1091 Woolsey,K.,851
Watson,J.,12 Whorf,B.,287,314,1092 workspacerepresentation,986
Watson,J.D.,130,1091 widecontent,1028 worldmodel,indisambiguation,906
Watt,J.,15 Widrow,B.,20,761,833,854,1092 worldstate,69
Wattenberg,M.,155,1077 Widrow–Hoffrule,846 WorldWarII,10,552,604
Waugh,K.,687,1091 Wiedijk,F.,360,1092 WorldWideWeb(WWW),27,462,
WBRIDGE5,195 Wiegley,J.,156,1092 867,869
weakAI,1020,1040 Wiener,N.,15,192,604,761,922, worstpossiblecatastrophe,615
weakdomination,668 1087,1092 Wos,L.,359,360,1092
weakmethod,22 wigglybeliefstate,271 wrapper(forInternetsite),466
Weaver,W.,703,758,763,883,907, Wilczek,F.,761,1065 wrapper(forlearning),709
908,922,1088,1091 Wilensky,R.,23,24,1031,1092 Wray,R.E.,358,1092
Webber,B.L.,31,1091 Wilfong,G.T.,1012,1069 Wright,O.andW.,3
Weber,J.,604,1076 Wilkins,D.E.,189,431,434,1092 Wright,R.N.,884,1085
Wefald,E.H.,112,191,198,1048, Williams,B.,60,278,432,472,1083, Wright,S.,155,552,1092
1087 1092 Wu,D.,921,1092
1132 Index
Wu,E.,885,1067 Yedidia,J.,555,1092 zero-sumgame,161,162,199,670
Wu,F.,469,1092 Yglesias,J.,28,1064 Zettlemoyer,L.S.,556,921,1082,1093
wumpusworld,236,236–240,246–247, Yip,K.M.-K.,472,1092 Zhai,C.,884,1079
279,305–307,439,499–503, Yngve,V.,920,1092 Zhang,H.,277,1093
509 Yob,G.,279,1092 Zhang,L.,277,553,1083,1093
Wundt,W.,12 Yoshikawa,T.,1013,1092 Zhang,N.L.,553,639,1093
Wurman,P.,688,1092 Young,H.P.,435,1092 Zhang,W.,112,1079
WWW,27,462,867,869 Young,M.,797,1078 Zhang,Y.,885,1067
Young,S.J.,896,920,1080 Zhao,Y.,277,1083
X Younger,D.H.,920,1092 Zhivotovsky,A.A.,192,1064
Yu,B.,553,1068 Zhou,R.,112,1093
XCON,336
Yudkowsky,E.,27,1039,1093 Zhu,C.,760,1067
XML,875
Yung,M.,110,119,1064,1075 Zhu,D.J.,1012,1093
xor,246,766
Yvanovich,M.,432,1070 Zhu,W.L.,439,1089
Xu,J.,358,1092
Zilberstein,S.,156,422,433,434,
Xu,P.,29,921,1067
Z 1075,1085
Zimdars,A.,857,1087
Y
Z-3,14 Zimmermann,H.-J.,557,1093
Yakimovsky,Y.,639,1072 Zadeh,L.A.,557,1093 Zinkevich,M.,687,1093
Yale,23 Zahavi,U.,107,1092 Zisserman,A.,960,968,1075,1086
Yan,D.,431,1073 Zapp,A.,1014,1071 Zlotkin,G.,688,1087
Yang,C.S.,884,1087 Zaragoza,H.,884,1069 Zog,778
Yang,F.,107,1092 Zaritskii,V.S.,605,1093 Zollmann,A.,922,1093
Yang,Q.,432,1092 zebrapuzzle,231 Zuckerman,D.,124,1081
Yannakakis,M.,157,229,1065,1084 Zecchina,R.,278,1084 Zufferey,J.C.,1045,1072
Yap,R.H.C.,359,1077 Zeldner,M.,908 Zuse,K.,14,192
Yardi,M.,278,1068 Zelle,J.,902,921,1093 Zweben,M.,432,1070
Yarowsky,D.,27,885,1092 Zeng,H.,314,1082 Zweig,G.,604,1093
Yates,A.,885,1072 Zermelo,E.,687,1093 Zytkow,J.M.,800,1079